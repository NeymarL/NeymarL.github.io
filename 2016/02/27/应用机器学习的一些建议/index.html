<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <title>
    应用机器学习的一些建议 | NIUHE | 日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE&#39;s Blog">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="应用机器学习的一些建议">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      应用机器学习的一些建议</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2016/02/27/应用机器学习的一些建议/" rel="bookmark">
        <time class="entry-date published" datetime="2016-02-27T02:32:00.000Z">
          2016-02-27
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <!-- toc -->
<ul>
<li><a href="#deciding-what-to-try-next">Deciding What to Try Next</a></li>
<li><a href="#evaluating-a-hypothesis">Evaluating a Hypothesis</a></li>
<li><a href="#the-test-set-error">The test set error</a></li>
<li><a href="#model-selection-and-trainvalidationtest-sets">Model Selection and Train/Validation/Test Sets</a></li>
<li><a href="#diagnosing-bias-vs-variance">Diagnosing Bias vs. Variance</a></li>
<li><a href="#regularization-and-biasvariance">Regularization and Bias/Variance</a></li>
<li><a href="#learning-curves">Learning Curves</a></li>
<li><a href="#deciding-what-to-do-next-revisited">Deciding What to Do Next Revisited</a>
<ul>
<li><a href="#diagnosing-neural-networks">Diagnosing Neural Networks</a></li>
</ul></li>
<li><a href="#model-selection">Model Selection:</a></li>
</ul>
<!-- tocstop -->
<a id="more"></a>
<h2><span id="deciding-what-to-try-next">Deciding What to Try Next</span></h2>
<p>Errors in your predictions can be troubleshooted by: * Getting more training examples * Trying smaller sets of features * Trying additional features * Trying polynomial features * Increasing or decreasing <span class="math inline">\(\lambda\)</span></p>
<p>Don't just pick one of these avenues at random. We'll explore diagnostic techniques for choosing one of the above solutions in the following sections.</p>
<h2><span id="evaluating-a-hypothesis">Evaluating a Hypothesis</span></h2>
<p>A hypothesis may have low error for the training examples but still be inaccurate (because of overfitting).</p>
<p>With a given dataset of training examples, we can split up the data into two sets: a training set and a test set.</p>
<p>The new procedure using these two sets is then: 1. Learn <span class="math inline">\(\Theta\)</span> and minimize <span class="math inline">\(J_{train}(\Theta)\)</span> using the training set 2. Compute the test set error <span class="math inline">\(J_{test}(\Theta)\)</span></p>
<h2><span id="the-test-set-error">The test set error</span></h2>
<p>For linear regression: <span class="math display">\[J_{test}(\Theta) = \dfrac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_\Theta(x^{(i)}_{test}) - y^{(i)}_{test})^2\]</span> For classification ~ Misclassification error (aka 0/1 misclassification error): <span class="math display">\[err(h_\Theta(x),y) = \begin{matrix} 1 &amp; \mbox{if } h_\Theta(x) \geq 0.5\ and\ y = 0\ or\ h_\Theta(x) &lt; 0.5\ and\ y = 1\newline 0 &amp; \mbox otherwise \end{matrix} \]</span> This gives us a binary 0 or 1 error result based on a misclassification.</p>
<p>The average test error for the test set is <span class="math display">\[ \large \text{Test Error} = \dfrac{1}{m_{test}} \sum^{m_{test}}_{i=1} err(h_\Theta(x^{(i)}_{test}), y^{(i)}_{test}) \]</span></p>
<p>This gives us the proportion(比例) of the test data that was misclassified.</p>
<h2><span id="model-selection-and-trainvalidationtest-sets">Model Selection and Train/Validation/Test Sets</span></h2>
<p>Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis.</p>
<p>The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than any other data set.</p>
<p>In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result.</p>
<p><strong>Without the Validation Set</strong> 1. Optimize the parameters in <span class="math inline">\(\Theta\)</span> using the training set for each polynomial degree. 2. Find the polynomial degree <span class="math inline">\(d\)</span> with the least error using the test set. 3. Estimate the generalization error also using the test set with <span class="math inline">\(J_{test}(\Theta^{(d)})\)</span>, (d = theta from polynomial with lower error);</p>
<p>In this case, we have trained one variable, <span class="math inline">\(d\)</span>, or the degree of the polynomial, using the test set. This will cause our error value to be greater for any other set of data.</p>
<p>To solve this, we can introduce a third set, the <strong>Cross Validation Set</strong>, to serve as an intermediate set that we can train <span class="math inline">\(d\)</span> with. Then our test set will give us an accurate, non-optimistic error.</p>
<p>One example way to break down our dataset into the three sets is: * Training set: 60% * Cross validation set: 20% * Test set: 20%</p>
<p>We can now calculate three separate error values for the three different sets.</p>
<p><strong>With the Validation Set</strong> 1. Optimize the parameters in <span class="math inline">\(\Theta\)</span> using the training set for each polynomial degree. 2. Find the polynomial degree <span class="math inline">\(d\)</span> with the least error using the cross validation set. 3. Estimate the generalization error using the test set with <span class="math inline">\(J_{test}(\Theta^{(d)})\)</span>, (d = theta from polynomial with lower error);</p>
<p>This way, the degree of the polynomial <span class="math inline">\(d\)</span> has not been trained using the test set.</p>
<h2><span id="diagnosing-bias-vs-variance">Diagnosing Bias vs. Variance</span></h2>
<p>In this section we examine the relationship between the degree of the polynomial <span class="math inline">\(d\)</span> and the underfitting or overfitting of our hypothesis.</p>
<ul>
<li>We need to distinguish whether <strong>bias</strong>(偏离) or <strong>variance</strong>(方差) is the problem contributing to bad predictions.</li>
<li><strong>High bias</strong> is <strong>underfitting</strong> and <strong>high variance</strong> is <strong>overfitting</strong>. We need to find a golden mean between these two.</li>
</ul>
<p>The training error will tend to <strong>decrease</strong> as we increase the degree <span class="math inline">\(d\)</span> of the polynomial.</p>
<p>At the same time, the cross validation error will tend to <strong>decrease</strong> as we increase <span class="math inline">\(d\)</span> up to a point, and then it will <strong>increase</strong> as <span class="math inline">\(d\)</span> is increased, forming a convex curve.</p>
<ul>
<li>High bias (underfitting): both <span class="math inline">\(J_{train}(\Theta)\)</span> and <span class="math inline">\(J_{CV}(\Theta)\)</span> will be high. Also, <span class="math inline">\(J_{CV}(\Theta) \approx J_{train}(\Theta)\)</span>.</li>
<li>High variance (overfitting): <span class="math inline">\(J_{train}(\Theta)\)</span> will be low and <span class="math inline">\(J_{CV}(\Theta)\)</span> will be much greater than <span class="math inline">\(J_{train}(\Theta)\)</span>.</li>
</ul>
<p>The is represented in the figure below:</p>
<p><img src="/images/1456537039223.png"></p>
<h2><span id="regularization-and-biasvariance">Regularization and Bias/Variance</span></h2>
<p>Instead of looking at the degree <span class="math inline">\(d\)</span> contributing to bias/variance, now we will look at the regularization parameter <span class="math inline">\(\lambda\)</span>.</p>
<ul>
<li>Large <span class="math inline">\(\lambda\)</span>: High bias (underfitting)</li>
<li>Intermediate <span class="math inline">\(\lambda\)</span>: just right</li>
<li>Small <span class="math inline">\(\lambda\)</span>: High variance (overfitting)</li>
</ul>
<p>A large lambda heavily penalizes all the <span class="math inline">\(\Theta\)</span> parameters, which greatly simplifies the line of our resulting function, so causes underfitting.</p>
<p>The relationship of <span class="math inline">\(\lambda\)</span> to the training set and the variance set is as follows:</p>
<ul>
<li>Low <span class="math inline">\(\lambda\)</span>: <span class="math inline">\(J_{train}(\Theta)\)</span> is low and <span class="math inline">\(J_{CV}(\Theta)\)</span> is high (<strong>high variance/overfitting</strong>).</li>
<li>Intermediate <span class="math inline">\(\lambda\)</span>: <span class="math inline">\(J_{train}(\Theta)\)</span> and <span class="math inline">\(J_{CV}(\Theta)\)</span> are somewhat low and <span class="math inline">\(J_{train}(\Theta) \approx J_{CV}(\Theta)\)</span>.</li>
<li>Large <span class="math inline">\(\lambda\)</span>: both <span class="math inline">\(J_{train}(\Theta)\)</span> and <span class="math inline">\(J_{CV}(\Theta)\)</span> will be high (<strong>underfitting/high bias</strong>)</li>
</ul>
<p>The figure below illustrates the relationship between lambda and the hypothesis:</p>
<p><img src="/images/1456537437295.png"></p>
<p>In order to choose the model and the regularization <span class="math inline">\(\lambda\)</span>, we need: 1. Create a list of lambda (i.e. <span class="math inline">\(\lambda \in \lbrace0, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24\rbrace\)</span>); 2. Select a lambda to compute; 3. Create a model set like degree of the polynomial or others; 4. Select a model to learn <span class="math inline">\(\Theta\)</span>; 5. Learn the parameter <span class="math inline">\(\Theta\)</span> for the model selected, using <span class="math inline">\(J_{train}(\Theta)\)</span> with <span class="math inline">\(\lambda\)</span> selected (this will learn <span class="math inline">\(\Theta\)</span> for the next step); 6. Compute the train error using the learned <span class="math inline">\(\Theta\)</span> (computed with <span class="math inline">\(\lambda\)</span> ) on the <span class="math inline">\(J_{train}(\Theta)\)</span> without regularization or <span class="math inline">\(\lambda\)</span> = 0; 7. Compute the cross validation error using the learned <span class="math inline">\(\Theta\)</span> (computed with <span class="math inline">\(\lambda\)</span>) on the <span class="math inline">\(J_{CV}(\Theta)\)</span> without regularization or <span class="math inline">\(\lambda\)</span> = 0; 8. Do this for the entire model set and lambdas, then select the best combo that produces the lowest error on the cross validation set; 9. Now if you need visualize to help you understand your decision, you can plot to the figure like above with: (<span class="math inline">\(\lambda\)</span> x Cost <span class="math inline">\(J_{train}(\Theta)\)</span>) and (<span class="math inline">\(\lambda\)</span> x Cost <span class="math inline">\(J_{CV}(\Theta)\)</span>); 10. Now using the best combo <span class="math inline">\(\Theta\)</span> and <span class="math inline">\(\lambda\)</span>, apply it on <span class="math inline">\(J_{test}(\Theta)\)</span> to see if it have a good generalization of the problem. 11. To help decide the best polynomial degree and <span class="math inline">\(\lambda\)</span> to use, we can diagnose(诊断) with the learning curves, that is the next subject.</p>
<h2><span id="learning-curves">Learning Curves</span></h2>
<p>Training 3 examples will easily have 0 errors because we can always find a quadratic curve that exactly touches 3 points.</p>
<ul>
<li>As the training set gets larger, the error for a quadratic function increases.</li>
<li>The error value will plateau out after a certain <span class="math inline">\(m\)</span>, or training set size.</li>
</ul>
<p><strong>With high bias</strong></p>
<ul>
<li><strong>Low training set size</strong>: causes <span class="math inline">\(J_{train}(\Theta)\)</span> to be low and <span class="math inline">\(J_{CV}(\Theta)\)</span> to be high.</li>
<li><strong>Large training set size</strong>: causes both <span class="math inline">\(J_{train}(\Theta)\)</span> and <span class="math inline">\(J_{CV}(\Theta)\)</span> to be high with <span class="math inline">\(J_{train}(\Theta) \approx J_{CV}(\Theta)\)</span>.</li>
</ul>
<p>If a learning algorithm is suffering from <strong>high bias</strong>, getting more training data will <strong>not (by itself) help much</strong>.</p>
<p>For high variance, we have the following relationships in terms of the training set size: <strong>With high variance</strong></p>
<ul>
<li><strong>Low training set size</strong>: <span class="math inline">\(J_{train}(\Theta)\)</span> will be low and <span class="math inline">\(J_{CV}(\Theta)\)</span> will be high.</li>
<li><strong>Large training set size</strong>: <span class="math inline">\(J_{train}(\Theta)\)</span> increases with training set size and <span class="math inline">\(J_{CV}(\Theta)\)</span> continues to decrease without leveling off. Also, <span class="math inline">\(J_{train}(\Theta) &lt; J_{CV}(\Theta)\)</span> but the difference between them remains significant.</li>
</ul>
<p>If a learning algorithm is suffering from <strong>high variance</strong>, getting more training data is <strong>likely to help</strong>.</p>
<p><img src="/images/1456538443823.png"> <img src="/images/1456538447332.png"></p>
<h2><span id="deciding-what-to-do-next-revisited">Deciding What to Do Next Revisited</span></h2>
<p>Our decision process can be broken down as follows:</p>
<ul>
<li>Getting more training examples
<ul>
<li>Fixes high variance</li>
</ul></li>
<li>Trying smaller sets of features
<ul>
<li>Fixes high variance</li>
</ul></li>
<li>Adding features
<ul>
<li>Fixes high bias</li>
</ul></li>
<li>Adding polynomial features
<ul>
<li>Fixes high bias</li>
</ul></li>
<li>Decreasing <span class="math inline">\(\lambda\)</span>
<ul>
<li>Fixes high bias</li>
</ul></li>
<li>Increasing <span class="math inline">\(\lambda\)</span>
<ul>
<li>Fixes high variance</li>
</ul></li>
</ul>
<h3><span id="diagnosing-neural-networks">Diagnosing Neural Networks</span></h3>
<ul>
<li>A neural network with <strong>fewer</strong> parameters is prone(倾向于) to <strong>underfitting</strong>. It is also computationally cheaper.</li>
<li>A <strong>large</strong> neural network with more parameters is prone to <strong>overfitting</strong>. It is also computationally expensive. In this case you can use regularization (increase <span class="math inline">\(\lambda\)</span>) to address the overfitting.</li>
</ul>
<p>Using a <strong>single hidden layer</strong> is a good <strong>starting default</strong>. You can train your neural network on a number of hidden layers using your cross validation set.</p>
<h2><span id="model-selection">Model Selection:</span></h2>
<ul>
<li>Choosing M the order of polynomials.</li>
<li>How can we tell which parameters Θ to leave in the model (known as &quot;model selection&quot;)?</li>
</ul>
<p>There are several ways to solve this problem:</p>
<ol type="1">
<li>Get more data (very difficult).</li>
<li>Choose the model which best fits the data without overfitting (very difficult).</li>
<li>Reduce the opportunity for overfitting through regularization.</li>
</ol>
<p><strong>Bias: approximation(近似值) error (Difference between expected value and optimal value)</strong> * High Bias = UnderFitting (BU) * <span class="math inline">\(J_{train}(\Theta)\)</span> and <span class="math inline">\(J_{CV}(\Theta)\)</span> both will be high and <span class="math inline">\(J_{train}(\Theta) \approx J_{CV}(\Theta)\)</span></p>
<p><strong>Variance: estimation(估计) error due to finite(有限的) data</strong> * High Variance = OverFitting (VO) * <span class="math inline">\(J_{train}(\Theta)\)</span> is low and <span class="math inline">\(J_{CV}(\Theta) \gg J_{train}(\Theta)\)</span></p>
<p><strong>Intuition for the bias-variance trade-off:</strong></p>
<ul>
<li>Complex model =&gt; sensitive to data =&gt; much affected by changes in X =&gt; high variance, low bias.</li>
<li>Simple model =&gt; more rigid =&gt; does not change as much with changes in X =&gt; low variance, high bias.</li>
</ul>
<p>One of the most important goals in learning: finding a model that is just right in the bias-variance trade-off.</p>
<p><strong>Regularization Effects:</strong></p>
<ul>
<li>Small values of λ allow model to become finely tuned to noise leading to large variance =&gt; overfitting.</li>
<li>Large values of λ pull weight parameters to zero leading to large bias =&gt; underfitting.</li>
</ul>
<p><strong>Model Complexity Effects:</strong></p>
<ul>
<li>Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.</li>
<li>Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.</li>
</ul>
<p>In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.</p>
<p><strong>A typical rule of thumb when running diagnostics is:</strong></p>
<ul>
<li>More training examples fixes high variance but not high bias.</li>
<li>Fewer features fixes high variance but not high bias.</li>
<li>Additional features fixes high bias but not high variance.</li>
<li>The addition of polynomial and interaction features fixes high bias but not high variance.</li>
<li>When using gradient descent, decreasing lambda can fix high bias and increasing lambda can fix high variance (lambda is the regularization parameter).</li>
<li>When using neural networks, small neural networks are more prone to under-fitting and big neural networks are prone to over-fitting. Cross-validation of network size is a way to choose alternatives.</li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/Bias/">Bias</a><a href="/tags/Variance/">Variance</a><a href="/tags/Overfitting/">Overfitting</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>


	<section id="comment" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'Niuhe';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>



      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2018
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>