<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>
    线性回归中的一些公式推导 | NIUHE | Be Myself Enjoy My Life
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE&#39;s Blog">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="线性回归中的一些公式推导">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">Be Myself Enjoy My Life</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      线性回归中的一些公式推导</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2016/08/30/线性回归中的一些公式推导/" rel="bookmark">
        <time class="entry-date published" datetime="2016-08-30T11:42:06.000Z">
          2016-08-30
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2><span id="使用极大似然估计推导损失函数">使用极大似然估计推导损失函数</span></h2>
<p><strong>回归方程</strong></p>
<p><span class="math display">\[h_\theta(x) = \sum^n_{i=0}\theta_ix_i = \theta^Tx\]</span></p>
<p>考虑任意一样本 <span class="math inline">\(y^{(i)}\)</span> ，则有： <span class="math display">\[y^{(i)} = \theta^Tx^{(i)} + \epsilon^{(i)}\]</span> 其中 <span class="math inline">\(\epsilon^{(i)}\)</span> 是误差。</p>
<a id="more"></a>
<p>由于每个样本的误差 <span class="math inline">\(\epsilon^{(i)}\)</span>： * 独立同分布（IDD） * 具有有限的数学期望和方差</p>
<p>所以由<a href="https://www.wikiwand.com/zh/%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86" target="_blank" rel="noopener">中心极限定理</a>得：<span class="math inline">\(\epsilon^{(i)}\)</span> 服从均值为0，方差为某一定值 <span class="math inline">\(\sigma^2\)</span> 的正态分布。</p>
<blockquote>
<p><strong>中心极限定理</strong>是概率论中的一组定理。中央极限定理说明，<strong>大量相互独立的随机变量，其均值的分布以正态分布为极限</strong>。这组定理是数理统计学和误差分析的理论基础，指出了大量随机变量之和近似服从正态分布的条件。 ——from WikiPedia</p>
</blockquote>
<p>则 <span class="math inline">\(\epsilon^{(i)}\)</span> 的概率密度函数为： <span class="math display">\[P(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi\sigma}}e^{\frac{-(\epsilon^{(i)})^2}{2\sigma^2}} \]</span></p>
<p>把 $ ^{(i)} = y^{(i)} - <sup>Tx</sup>{(i)}$ 代入上式得：</p>
<p><span class="math display">\[ P(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi\sigma}}\exp(\frac{-(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}) \]</span></p>
<p><span class="math inline">\(\because\)</span> 所有样本发生的概率独立同分布 <span class="math inline">\(\therefore\)</span> <a href="https://www.wikiwand.com/zh/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">似然函数</a>为所有样本发生概率的乘积，即：</p>
<p><span class="math display">\[\begin{array}{lcl}
L(\theta) = \prod^{m}_{i=1} P(y^{(i)}|x^{(i)};\theta) \\
 = \prod^{m}_{i=1} \frac{1}{\sqrt{2\pi\sigma}}\exp(\frac{-(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}) 
 \end{array}
 \]</span></p>
<blockquote>
<p>在数理统计学中，<strong>似然函数</strong>是一种关于统计模型中的参数的函数，表示模型参数中的似然性。似然函数在统计推断中有重大作用，如在最大似然估计和费雪信息之中的应用等等。“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性，但是在统计学中，“似然性”和“或然性”或“概率”又有明确的区分。概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而似然性则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。 ——from WikiPedia</p>
</blockquote>
<p>对 <span class="math inline">\(L(\theta)\)</span> 取对数得对数似然函数：</p>
<p><span class="math display">\[\begin{array}{lcl}
l(\theta) = \log L(\theta) \\
 = \log \prod^{m}_{i=1} \frac{1}{\sqrt{2\pi\sigma}}\exp(\frac{-(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2})  \\
 = m\log \frac{1}{\sqrt{2\pi\sigma}} + \sum^m_{i=1} \frac{-(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2} \\
 = m\log \frac{1}{\sqrt{2\pi\sigma}} - \frac{1}{\sigma^2} \cdot \frac{1}{2} \sum^m_{i=1} (y^{(i)} - \theta^Tx^{(i)})^2
\end{array}
\]</span></p>
<p>若要使对数似然函数取得最大值，把常数项和定值去掉，令 <span class="math display">\[J(\theta) = \frac{1}{2}\sum^{m}_{i=1}(y^{(i)} - \theta^Tx^{(i)})^2\]</span> 则 <span class="math inline">\(J(\theta)\)</span> 应取得最小值， 即为损失函数（目标函数）。</p>
<hr>
<h2><span id="求解最小二乘意义下参数最优解">求解最小二乘意义下参数最优解</span></h2>
<p><strong>目标函数</strong></p>
<p><span class="math display">\[ J(\theta) = \frac{1}{2}\sum^{m}_{i=1}(y^{(i)} - \theta^Tx^{(i)})^2 = \frac{1}{2}(X\theta - y)^T(X\theta - y)\]</span></p>
<p><strong>求梯度</strong></p>
<blockquote>
<p>在向量微积分中，标量场的梯度是一个向量场。标量场中某一点的梯度指向在這點标量场增长最快的方向。 —— from WikiPedia</p>
</blockquote>
<p><span class="math display">\[\begin{array}{lcr}
\nabla_\theta J(\theta) = \frac{\partial}{\partial \theta}(\frac{1}{2}(X\theta - y)^T(X\theta - y)) \\
= \frac{\partial}{\partial \theta}(\frac{1}{2}(\theta^TX^T - y^T)(X\theta - y))
\end{array} \\
= \frac{\partial}{\partial \theta} (\frac{1}{2}(\theta^TX^TX\theta - \theta^TX^Ty - y^TX\theta + y^Ty) ) \\
= \frac{1}{2}(2X^TX\theta - X^Ty - (y^TX)^T) \\
= X^TX\theta - X^Ty
\]</span></p>
<p><strong>标量求梯度</strong> <img src="/images/1472541353509.png"></p>
<p><strong>求驻点</strong></p>
<p>令 <span class="math inline">\(\nabla_\theta J(\theta) = 0\)</span>，即 <span class="math inline">\(X^TX\theta - X^Ty = 0\)</span>，解得： <span class="math display">\[\theta = (X^TX)^{-1}X^Ty\]</span></p>
<p>若 <span class="math inline">\(X^TX\)</span> <strong>不可逆</strong>或防止<strong>过拟合</strong>，增加 <span class="math inline">\(\lambda\)</span> 扰动： <span class="math display">\[\theta = (X^TX + \lambda I)^{-1}X^Ty\]</span></p>
<p>上两式即为最小二乘意义下的参数最优解。</p>
<h2><span id="logistic-回归的损失函数及参数估计">Logistic 回归的损失函数及参数估计</span></h2>
<p><strong>Sigmoid函数</strong></p>
<ul>
<li>$g(z) =  $</li>
</ul>
<p><strong>Sigmoid函数的导数</strong></p>
<p><span class="math display">\[\begin{array}{lcr}
g&#39;(z) = (\frac{1}{1 + e^{-z}})&#39; \\
= \frac{e^{-z}}{(1 + e^{-z})^2} \\
= \frac{1}{1 + e^{-z}} - \frac{1}{(1 + e^{-z})^2} \\
= g(z)(1 - g(z))
\end{array}
\]</span></p>
<p><strong>回归方程</strong></p>
<ul>
<li><span class="math inline">\(h_\theta(x) = g(\theta^TX) = \frac{1}{1 + e^{-\theta^TX}}\)</span></li>
</ul>
<p><strong>损失函数</strong></p>
<p>设 $y {1, 0} $ 并且服从二向分布，即：</p>
<p><span class="math display">\[P(y = 1 | x; \theta) = h_\theta(x)\]</span> <span class="math display">\[P(y = 0 | x; \theta) = 1- h_\theta(x)\]</span></p>
<p>则任意样本 <span class="math inline">\(y^{(i)}\)</span> 的概率密度为</p>
<p><span class="math display">\[P(y^{(i)} | x^{(i)}; \theta) = h_\theta(x^{(i)})^{y^{(i)}}(1 - h_\theta(x^{(i)}))^{1 - y^{(i)}} \]</span></p>
<p><span class="math inline">\(\because\)</span> 所有样本发生的概率独立同分布 <span class="math inline">\(\therefore\)</span> <a href="https://www.wikiwand.com/zh/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">似然函数</a>为所有样本发生概率的乘积，即：</p>
<p><span class="math display">\[\begin{array}{lcr}
L(\theta) =  \prod^{m}_{i=1} P(y^{(i)}|x^{(i)};\theta) \\
=  \prod^{m}_{i=1} h_\theta(x^{(i)})^{y^{(i)}}(1 - h_\theta(x^{(i)}))^{1 - y^{(i)}}
\end{array}
\]</span></p>
<p>对两边同时取对数得对数似然函数为：</p>
<p><span class="math display">\[\begin{array}{lcr}
l(\theta) = \log L(\theta) \\
= \log \prod^{m}_{i=1} h_\theta(x^{(i)})^{y^{(i)}}(1 - h_\theta(x^{(i)}))^{1 - y^{(i)}} \\
= \sum^m_{i = 1}   y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))  \\
= \sum^m_{i = 1} y^{(i)} \log(\frac{1}{1 + e^{-\theta x^{(i)}}}) + (1 - y^{(i)})\log(\frac{e^{-\theta x^{(i)}}}{1 + e^{-\theta x^{(i)}}}) \\
= \sum^m_{i = 1} y^{(i)} \log(\frac{1}{1 + e^{-\theta x^{(i)}}}) + (1 - y^{(i)})\log(\frac{1}{1 + e^{\theta x^{(i)}}}) \\
\end{array}
\]</span></p>
<p>令 $J() = -l() = ^m_{i = 1} y^{(i)} (1 + e<sup>{-x</sup>{(i)}}) + (1 - y^{(i)})(1 + e<sup>{x</sup>{(i)}}) $，即为logistic回归的损失函数。</p>
<p>若令 $y {1, -1} $，进行同样的推导可得另一种形式的损失函数:</p>
<p><span class="math display">\[J(\theta) = \sum^m_{i = 1} \log(1 + e^{-y^{(i)}\theta x^{(i)}}) \]</span></p>
<p>证明如下：</p>
<p><img src="/images/1472545621202.png"></p>
<p><strong>参数估计</strong></p>
<p>求梯度，对对数似然函数（损失函数）对 <span class="math inline">\(\theta\)</span> 求偏导得 ：</p>
<p><span class="math display">\[\begin{array}{lcr}
\frac{\partial}{\partial \theta_j}l(\theta) = \frac{\partial}{\partial \theta_j}\sum^m_{i = 1}   y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)})) \\
= \sum^m_{i = 1} y^{(i)}\frac{1}{h_\theta(x^{(i)})}\frac{\partial h_\theta(x^{(i)})}{\partial \theta_j} + (1 - y^{(i)}) \frac{1}{1 - h_\theta(x^{(i)})}\frac{-\partial h_\theta(x^{(i)})}{\partial \theta_j}  \\
=  \sum^m_{i = 1} y^{(i)}(1 - h_\theta(x^{(i)}))x_j^{(i)} - (1 - y^{(i)}) h_\theta(x^{(i)})x_j^{(i)} \\
= \sum^m_{i = 1} (y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}
\end{array}
\]</span></p>
<p>所以参数的学习规则为：</p>
<p><span class="math display">\[\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)} \]</span></p>
<p>与线性回归的更新规则相同！</p>
<p><strong>对数线性模型</strong></p>
<p><img src="/images/1472546918441.png"></p>
<h2><span id="softmax-回归">Softmax 回归</span></h2>
<p><img src="/images/1472547052270.png"></p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/Linear-Regression/">Linear Regression</a><a href="/tags/Logistic-Regression/">Logistic Regression</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>


	<section id="comment" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'Niuhe';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>



      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2018
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-71540601-1', 'auto');
    ga('send', 'pageview');

</script>

      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>