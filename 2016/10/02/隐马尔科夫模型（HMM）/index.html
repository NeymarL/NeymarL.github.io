<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    隐马尔科夫模型（HMM） | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="隐马尔科夫模型（HMM）">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script>
  <script type="text/javascript" src="/js/search.js"></script>
  <script type="text/javascript" src="/js/jquery.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">博客</a></li>
      
        <li><a href="/categories">笔记</a></li>
      
        <li><a href="/archives">归档</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/search">搜索</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      隐马尔科夫模型（HMM）</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2016/10/02/隐马尔科夫模型（HMM）/" rel="bookmark">
        <time class="entry-date published" datetime="2016-10-02T07:42:06.000Z">
          2016-10-02
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p>隐马尔科夫模型(HMM, Hidden Markov Model)可用标注问题，在语音识别、NLP、生物信息、模式识别等领域被实践证明是有效的算法。</p>
<!-- toc -->
<ul>
<li><a href="#前言">前言</a>
<ul>
<li><a href="#hmm定义">HMM定义</a></li>
<li><a href="#马尔可夫链">马尔可夫链</a></li>
</ul></li>
<li><a href="#hmm的确定">HMM的确定</a></li>
<li><a href="#hmm的3个基本问题">HMM的3个基本问题</a>
<ul>
<li><a href="#概率计算">概率计算</a>
<ul>
<li><a href="#暴力算法">暴力算法</a></li>
<li><a href="#前向-后向算法">前向－后向算法</a></li>
</ul></li>
<li><a href="#学习算法">学习算法</a>
<ul>
<li><a href="#监督学习">监督学习</a></li>
<li><a href="#baum-welch-算法">Baum-Welch 算法</a></li>
</ul></li>
<li><a href="#预测算法">预测算法</a>
<ul>
<li><a href="#近似算法">近似算法</a></li>
<li><a href="#viterbi算法">Viterbi算法</a></li>
</ul></li>
</ul></li>
<li><a href="#总结">总结</a></li>
<li><a href="#参考">参考</a></li>
</ul>
<!-- tocstop -->
<a id="more"></a>
<h2><span id="前言">前言</span></h2>
<h3><span id="hmm定义">HMM定义</span></h3>
<p>隐马尔可夫模型（HMM）是关于时序的概率模型，描述由一个隐藏的马尔可夫链生成不可预测的状态随机序列，再由各个状态生成观测随机序列的过程。</p>
<p><img src="/images/1475388928648.png"></p>
<p>​ 图1. 隐马尔可夫模型的图结构</p>
<p>图1中的 <span class="math inline">\(z_1, z_2 … z_{n+1}\)</span> 是 <span class="math inline">\(n+1\)</span> 个不可观测状态，由马尔可夫链连接，称为<strong>状态序列</strong>；<span class="math inline">\(x_1, x_2, …, x_{n+1}\)</span> 是由状态序列生成的观测随机序列，称为<strong>观测序列</strong>。其中，序列中的每个位置可以看作是一个<strong>时刻</strong>。</p>
<h3><span id="马尔可夫链">马尔可夫链</span></h3>
<p>下面的介绍摘自Wikipedia：</p>
<blockquote>
<p><strong>马尔可夫链</strong>（英语：Markov chain），又称<strong>离散时间马可夫链</strong>（discrete-time Markov chain，缩写为<strong>DTMC</strong>），因俄国数学家<a href="https://www.wikiwand.com/zh-hans/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB" target="_blank" rel="noopener">安德烈·马尔可夫</a>（俄语：Андрей Андреевич Марков）得名，为<a href="https://www.wikiwand.com/zh-hans/%E7%8B%80%E6%85%8B%E7%A9%BA%E9%96%93_(%E8%A8%88%E7%AE%97%E6%A9%9F%E7%A7%91%E5%AD%B8)" target="_blank" rel="noopener">状态空间</a>中经过从一个状态到另一个状态的转换的<a href="https://www.wikiwand.com/zh-hans/%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B" target="_blank" rel="noopener">随机过程</a>。该过程要求具备“无记忆”的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。这种特定类型的“无记忆性”称作<a href="https://www.wikiwand.com/zh-hans/%E9%A6%AC%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B3%AA" target="_blank" rel="noopener">马可夫性质</a>。马尔科夫链作为实际过程的统计模型具有许多应用。</p>
<p>在马尔可夫链的每一步，系统根据概率分布，可以从一个状态变到另一个状态，也可以保持当前状态。状态的改变叫做转移，与不同的状态改变相关的概率叫做转移概率。<a href="https://www.wikiwand.com/zh-hans/%E9%9A%8F%E6%9C%BA%E6%BC%AB%E6%AD%A5" target="_blank" rel="noopener">随机漫步</a>就是马尔可夫链的例子。随机漫步中每一步的状态是在图形中的点，每一步可以移动到任何一个相邻的点，在这里移动到每一个点的概率都是相同的（无论之前漫步路径是如何的）。</p>
</blockquote>
<p>回到我们的模型中，由于 <span class="math inline">\(z_1, z_2, …, z_{n+1}\)</span> 是一条马尔可夫链，所以链中某一状态 <span class="math inline">\(z_i\)</span> 发生的概率只与 <span class="math inline">\(z_{i-1}\)</span> 有关，表达成公式就是：</p>
<p><span class="math display">\[P(Z_i = z_i|Z_{i-1}=z_{i-1}, Z_{i-2}=z_{i-2}, …, Z_1 = z_1) = P(Z_i = z_i|Z_{i-1} = z_{i-1})\]</span></p>
<p>还有一点就是，由于 <span class="math inline">\(z_2\)</span> 和 <span class="math inline">\(x_1\)</span> 都是由 <span class="math inline">\(z_1\)</span> “生成”出来的，所以在不知道 <span class="math inline">\(z_1\)</span> 的情况下， <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(z_2\)</span> 是<strong>不</strong>独立的；由于 <span class="math inline">\(x_2\)</span> 是由 <span class="math inline">\(z_2\)</span> 生成的，若把 <span class="math inline">\(x_2\)</span> 和 <span class="math inline">\(z_2\)</span> 看成一个整体，那么这个整体与 <span class="math inline">\(x_1\)</span> 也是<strong>不</strong>独立的；所以在 <span class="math inline">\(z_1\)</span>, <span class="math inline">\(z_2\)</span> 都不可观察（未知）的前提下，<span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span> <strong>不相互独立</strong>。</p>
<p>这一点与其它机器学习算法比如 Logistic Regression, SVM, Decision Tree 等都不一样，那些算法都是假设每个样本相互独立，而HMM就恰恰相反，要求样本间有某种联系。就拿中文分词来举例，正常人说的每一句话里，前一个字和后一个字都应该是<strong>有联系的</strong>，有的联系很紧密，我们把它认为是一个词语，有的联系不那么强，我们认为是词与词之间的分隔，显然每个字之间都不是独立的。这也就解释了为什么HMM可以用做中文分词而那些算法不可以。</p>
<h2><span id="hmm的确定">HMM的确定</span></h2>
<p>HMM由初始概率分布 <span class="math inline">\(\pi\)</span>，状态转移概率分布 <span class="math inline">\(A\)</span> 以及观测概率分布 <span class="math inline">\(B\)</span> 确定，把它们三个合起来表示成 <span class="math inline">\(\lambda\)</span>，即：<span class="math display">\[\lambda = (A, B, \pi)\]</span></p>
<p>分别来解释一下这三个都是什么东西。</p>
<p>定义 <span class="math inline">\(Q\)</span> 为所有可能的状态的集合，<span class="math inline">\(N\)</span> 是所有可能的状态数，即 <span class="math inline">\(Q=\{q_1, q_2, …, q_N\}\)</span>。</p>
<ul>
<li>对于中文分词的例子来说，可能的状态只有两个：<span class="math inline">\(0, 1\)</span>，<span class="math inline">\(0\)</span> 表示这个字不是这个词的最后一个字，<span class="math inline">\(1\)</span> 表示这个字是这个词的末尾，该分割了。所以 <span class="math inline">\(Q = \{0, 1\}, N = 2\)</span>。</li>
</ul>
<p>定义 <span class="math inline">\(V\)</span> 是所有可能的观测的集合，<span class="math inline">\(M\)</span> 是可能的观测数，即 <span class="math inline">\(V = \{v_1, v_2, …, v_M\}\)</span></p>
<ul>
<li>对于中文分词的例子来说，<span class="math inline">\(V\)</span> 就是语料库里的所有字，而汉字在计算机里实际是用编码表示的，若是两字节编码，则只有 <span class="math inline">\(2^{16} = 65536\)</span> 种情况，所以 <span class="math inline">\(M = 65536\)</span>。</li>
</ul>
<p>再定义 <span class="math inline">\(I\)</span> 是长度为 <span class="math inline">\(T\)</span> 的状态序列，<span class="math inline">\(O\)</span> 是对应的观测序列，即：<span class="math inline">\(I = \{i_1, i_2, …, i_T\}\ \ O = \{o_1, o_2, …, o_T\}\)</span>。</p>
<ul>
<li>状态序列 <span class="math inline">\(I\)</span> 对应图1中的 <span class="math inline">\(z_1, z_2, …, z_n\)</span>，观测序列 <span class="math inline">\(O\)</span> 对应图1的 <span class="math inline">\(x_1, x_2, … , x_n\)</span>，只不过换了个名字。</li>
</ul>
<p><strong>状态转移概率分布<span class="math inline">\(A\)</span></strong></p>
<p>状态转移概率是个什么东西呢，其实就是从一个状态转移到另一个状态的概率，具体来说就是 <span class="math inline">\(t\)</span> 时刻的隐状态处于状态 <span class="math inline">\(q_i\)</span>，即 <span class="math inline">\(i_t = q_i\)</span> 的条件下，<span class="math inline">\(t+1\)</span> 时刻隐状态转换到状态 <span class="math inline">\(q_j\)</span>，即 <span class="math inline">\(i_{t+1} = q_j\)</span> 的概率，这个就是从 <span class="math inline">\(q_i\)</span> 转换到 <span class="math inline">\(q_j\)</span> 的转移概率，定义为 <span class="math inline">\(a_{ij}\)</span>，那么它的公式就是：<span class="math inline">\(a_{ij} = P(i_{t+1} = q_j|i_t=q_i)\)</span>。</p>
<p>我们总共有 <span class="math inline">\(N\)</span> 的可能的状态，每个状态都有一定的概率转移到其它的状态，所以状态转移概率应该有 <span class="math inline">\(N*N\)</span> 个，为方便表示，我们把它写成一个 <span class="math inline">\(N*N\)</span> 的矩阵，定义为 <span class="math inline">\(A\)</span>，也就是<strong>状态转移概率分布</strong>，也叫<strong>状态转移概率矩阵</strong>。</p>
<p><span class="math display">\[A = [a_{ij}]_{N*N}\]</span>，其中 <span class="math inline">\(a_{ij} = P(i_{t+1} = q_j|i_t = q_i)\)</span></p>
<p><strong>观测概率分布 <span class="math inline">\(B\)</span> </strong></p>
<p>观测概率又是什么鬼呢，其实也很简单，就是由隐状态 <span class="math inline">\(i_t\)</span> 生成观测 <span class="math inline">\(o_t\)</span> 的概率，具体来说就是时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_i\)</span>，即 <span class="math inline">\(i_t = q_i\)</span> 的条件下，生成观测 <span class="math inline">\(v_k\)</span> 的概率，记为 <span class="math inline">\(b_{ik}\)</span>，写成公式就是 <span class="math inline">\(b_{ik} = P(o_t = v_k|i_t=q_i)\)</span>。</p>
<p>由于每个隐状态有 <span class="math inline">\(N\)</span> 种可能，观测有 <span class="math inline">\(M\)</span> 种可能，所以观测概率应有 <span class="math inline">\(M*N\)</span> 个，为方便表示，我们把它写成一个 <span class="math inline">\(N*M\)</span> 的矩阵，定义为 <span class="math inline">\(B\)</span>，也就是 <strong>观测概率分布</strong>，也叫<strong>观测概率矩阵</strong>，又叫<strong>发射矩阵</strong>或<strong>混淆矩阵</strong>。</p>
<p><span class="math display">\[B = [b_{ij}]_{N*M}\]</span>，其中 <span class="math inline">\(b_{ik} = P(o_t = v_k|i_t=q_i)\)</span></p>
<p><strong>初始概率分布 <span class="math inline">\(\pi\)</span></strong></p>
<p>所谓初始概率就是在 <span class="math inline">\(t = 1\)</span> 时刻选择某一状态的概率。如 <span class="math inline">\(\pi_i\)</span> 是时刻 <span class="math inline">\(t=1\)</span> 处于状态 <span class="math inline">\(q_i\)</span> 的概率，即 <span class="math inline">\(\pi_i = P(i_1 = q_i)\)</span>。</p>
<p>显然应该有 <span class="math inline">\(N\)</span> 个初始概率，定义 <span class="math inline">\(\pi\)</span> 为<strong>初始概率向量</strong>，共有 <span class="math inline">\(N\)</span> 的元素。</p>
<p><strong>参数总结</strong></p>
<p>HMM由<strong>初始概率分布 <span class="math inline">\(\pi\)</span></strong>、<strong>状态转移概率分布<span class="math inline">\(A\)</span></strong> 以及<strong>观测概率分布 <span class="math inline">\(B\)</span> </strong>确定。因此，HMM可以用三元符号表示，称为HMM的三要素：<span class="math inline">\(\lambda = (A, B, \pi)\)</span></p>
<p><strong>HMM的两个基本性质</strong></p>
<p>齐次假设： <span class="math display">\[
P(i_t|i_{t-1}, o_{t-1}, i_{t-2}, o_{t-2}, …, i_1, o_1) = P(i_t|i_{t-1})
\]</span> 观测独立性假设： <span class="math display">\[
P(o_t|i_t, i_{t-1}, o_{t-1}, …, i_1, o_1) = P(o_t|i_t)
\]</span> 这两个性质或者说假设均来自<a href="#马尔可夫链">马尔可夫链</a>，在前面也略有提到，应该可以理解吧。</p>
<h2><span id="hmm的3个基本问题">HMM的3个基本问题</span></h2>
<p>不知道大家看到这里是什么感觉，有没有觉得这个模型很怪，心想：这玩意咋训练？</p>
<p>反正我刚学到这的时候是有这种感觉的，这也就引出了HMM的3个最重要的问题：<strong>概率计算</strong>、<strong>学习</strong>和<strong>预测</strong>。</p>
<p>大体来解释一下：</p>
<ul>
<li>概率计算问题：给定模型 <span class="math inline">\(\lambda = (A, B, \pi)\)</span> 和观测序列 O = {o_1, o_2, …, o_T}，计算模型 <span class="math inline">\(\lambda\)</span> 下观测序列 <span class="math inline">\(O\)</span> 出现的概率 <span class="math inline">\(P(O|\lambda)\)</span></li>
<li>学习问题：已知观测序列 <span class="math inline">\(O=\{o_1, o_2, …, o_T\}\)</span>，估计模型 <span class="math inline">\(\lambda=(A, B, \pi)\)</span> 的参数，使得在该模型下观测序列 <span class="math inline">\(P(O|\lambda)\)</span> 最大。</li>
<li>预测问题：已知模型 <span class="math inline">\(\lambda=(A,B,\pi)\)</span> 和观测序列 <span class="math inline">\(O=\{o_1, o_2, …, o_T\}\)</span>，求给定观测序列条件概率 <span class="math inline">\(P(I|O, \lambda)\)</span> 最大的状态序列 <span class="math inline">\(I\)</span>。</li>
</ul>
<h3><span id="概率计算">概率计算</span></h3>
<p>目标：给定模型 <span class="math inline">\(\lambda = (A, B, \pi)\)</span> 和观测序列 O = {o_1, o_2, …, o_T}，计算模型 <span class="math inline">\(\lambda\)</span> 下观测序列 <span class="math inline">\(O\)</span> 出现的概率 <span class="math inline">\(P(O|\lambda)\)</span>。</p>
<p>概率计算有三种方法：暴力求解，<strong>前向算法</strong>和<strong>后向算法</strong>，后两者实际上式<strong>动态规划</strong>算法。</p>
<h4><span id="暴力算法">暴力算法</span></h4>
<p>算法思想：按照概率公式，列举所有可能的长度为 <span class="math inline">\(T\)</span> 的状态序列 <span class="math inline">\(I = \{i_1, i_2, …, i_T\}\)</span>，求各个状态序列 <span class="math inline">\(I\)</span> 与观测序列 <span class="math inline">\(O=\{o_1, o_2, …, o_T\}\)</span> 的联合概率 <span class="math inline">\(P(O, I|\lambda)\)</span>，然后对所有可能的状态序列求和，从而得到 <span class="math inline">\(P(O|\lambda)\)</span>。</p>
<p>先求 <span class="math inline">\(P(O, I|\lambda)\)</span>，然后对所有 <span class="math inline">\(I\)</span> 加和即可。</p>
<p>我们可以把联合概率分解为条件概率的乘积：<span class="math inline">\(P(O, I|\lambda) = P(O|I, \lambda)P(I|\lambda)\)</span></p>
<p>状态序列 <span class="math inline">\(\{i_1, i_2, …, i_T\}\)</span> 的概率怎么求呢？一步一步来。 <span class="math display">\[
\begin{array}{lcl}
P(I|\lambda) = P(i_1, i_2, …, i_T|\lambda) \\
\ \ \ \ \ \ \ \ \ \ \ \ = P(i_2, …, i_T|\lambda)\cdot P(i_1|\lambda) \\
\ \ \ \ \ \ \ \ \ \ \ \ = P(i_2, …, i_T|\lambda)\cdot \pi_{i_1} \\
\ \ \ \ \ \ \ \ \ \ \ \ = \pi_{i_1} \cdot P(i_3, ..., i_T|\lambda)P(i_2|\lambda) \\
\ \ \ \ \ \ \ \ \ \ \ \ = \pi_{i_1} a_{i_1i_2} \cdot P(i_3, ..., i_T|\lambda) \\
\ \ \ \ \ \ \ \ \ \ \ \ = \ .... \\
\ \ \ \ \ \ \ \ \ \ \ \ = \pi_{i_1} a_{i_1i_2}a_{i_2i_3}...a_{i_{T-1}i_T}
\end{array}
\]</span> 同理可以求观测序列的概率： <span class="math display">\[
P(O|I, \lambda) = b_{i_1o_1}b_{i_2o_2}...b_{i_To_T}
\]</span> 所以 <span class="math inline">\(O\)</span> 和 <span class="math inline">\(I\)</span> 同时出现的联合概率是： <span class="math display">\[
P(O,I|\lambda) = P(O|I, \lambda)P(I|\lambda) = \pi_{i_1}b_{i_1o_1}a_{i_1i_2}b_{i_2o_2}...a_{i_{T-1}i_T}b_{i_ToT}
\]</span> 对所有可能的状态序列 <span class="math inline">\(I\)</span> 求和，得到观测序列 <span class="math inline">\(O\)</span> 的概率： <span class="math display">\[
P(O|\lambda) = \sum_I P(O, I|\lambda) = \sum_{i_1, i_2, ..., i_T}\pi_{i_1}b_{i_1o_1}a_{i_1i_2}b_{i_2o_2}...a_{i_{T-1}i_T}b_{i_ToT}
\]</span> 这样就求解出观测序列 <span class="math inline">\(O\)</span> 的概率，也不是很难，不是吗？</p>
<p>接下来我们来分析一下这个算法的<strong>时间复杂度</strong>。</p>
<p>由于要穷举所有 <span class="math inline">\(I\)</span> 的可能性， <span class="math inline">\(I\)</span> 的长度是 <span class="math inline">\(T\)</span>，每个 <span class="math inline">\(i\)</span> 有 <span class="math inline">\(N\)</span> 种可能性，所以总共有 <span class="math inline">\(N^T\)</span> 个 <span class="math inline">\(I\)</span>。计算每个 <span class="math inline">\(I\)</span> 需要做 <span class="math inline">\(2T\)</span> 次乘法，因此，时间复杂度为 <span class="math inline">\(O(TN^T)\)</span>，超级高。</p>
<h4><span id="前向-后向算法">前向－后向算法</span></h4>
<p>我们思考一个问题，我们在求最长递增子序列、最大连续子数组或是KMP中next数组的计算，无不用到同一个思想：在前 <span class="math inline">\(k\)</span> 个已知的情况下求第 <span class="math inline">\(k+1\)</span> 个。</p>
<p>我们借鉴这种优化思想来定义两个概念：前向概率和后向概率。</p>
<p><img src="/images/1475388965126.png"></p>
<p>​ 图2:前向概率和后向概率示意图</p>
<p>定义前向概率 <span class="math inline">\(\alpha_t(i)\)</span> 为 <span class="math inline">\(t\)</span> 时刻下隐状态为 <span class="math inline">\(q_i\)</span> 并且观测到观测序列 <span class="math inline">\(o_1, o_2, …, o_t\)</span> 的概率，即 <span class="math display">\[
\alpha_t(i) = P(o_1, o_2, ..., o_t, i_t = q_i|\lambda)
\]</span> 其中，<span class="math inline">\(o\)</span> 等于图2中的 <span class="math inline">\(y\)</span>，<span class="math inline">\(i\)</span> 等于图2中的 <span class="math inline">\(q\)</span>，和我们之前的定义相同。</p>
<p>同理定义后向概率 <span class="math inline">\(\beta_t(i)\)</span> 为已知 <span class="math inline">\(t\)</span> 时刻的状态为 <span class="math inline">\(q_i\)</span>，观测到输出序列为 <span class="math inline">\(o_{y+1}, …, o_T\)</span> 的概率，即： <span class="math display">\[
\beta_t(i) = P(o_{t+1}, o_{t+2}, ..., o_T|i_t = q_i, \lambda)
\]</span> <strong>前向算法</strong></p>
<p>思想：递推计算前向概率 <span class="math inline">\(\alpha_t(i)\)</span> 及观测序列概率 <span class="math inline">\(P(O|\lambda)\)</span>。</p>
<p>初值：<span class="math inline">\(\alpha_1(i) = P(o_1, i_1=q_i|\lambda) = \pi_ib_{io_1}\)</span></p>
<p>递推：对于 <span class="math inline">\(t = 1, 2, …, T-1\)</span>， <span class="math display">\[
\begin{array}{lcl}
\alpha_{t+1}(i) = P(o_1, ..., o_{t+1}, i_{t+1}=q_i|\lambda) \\
\ \ \ \ \ \ \ \ \ \ \ \ = (\sum_{j=1}^N P(o_1, ..., o_t, i_t=q_j|\lambda) \cdot a_{ji})\cdot b_{io_{t+1}} \\
\ \ \ \ \ \ \ \ \ \ \ \ = (\sum_{j=1}^Na_t(j)a_{ji})b_{io_{i+1}}
\end{array}
\]</span> 最终：<span class="math inline">\(P(O|\lambda) = \sum_{i=1}^N\alpha_T(i)\)</span></p>
<p>我们再分析一下该算法的复杂度，每计算一个 <span class="math inline">\(\alpha_T(i)\)</span> 需要 <span class="math inline">\(T\)</span> 次迭代，每次迭代需要计算约 <span class="math inline">\(N\)</span> 次乘法，共计算 <span class="math inline">\(N\)</span> 次，因此，前向算法的时间复杂度为 <span class="math inline">\(O(N^2T)\)</span>。</p>
<p>相比于暴力求解，通过递推的方式大大降低了时间复杂度，原因在于暴力求解重复计算了好多东西，而递推公式每一步都可以利用上一步的结果。</p>
<p><strong>后向算法</strong></p>
<p>思想：和前向算法类似，也是递推的思想。</p>
<p>初值：<span class="math inline">\(\beta_T(i) = 1\)</span></p>
<p>递推：对于 <span class="math inline">\(t = T-1, T-2, …, 1\)</span> <span class="math display">\[
\begin{array}{lcl}
\beta_t(i) = P(o_{t+1}, ..., o_T|i_t = q_i, \lambda) \\
\ \ \ \ \ \ \ \ = \sum_{j=1}^N(P(o_{t+2}, ..., o_T|i_{t+1}=q_j, \lambda) \cdot a_{ji}b_{jo_{t+1}}) \\
\ \ \ \ \ \ \ \ = \sum_{j=1}^N(a_{ji}b_{jo_{t+1}}\beta_{t+1}(j))
\end{array}
\]</span> 最终：<span class="math inline">\(P(O|\lambda) = \sum_{i=1}^N\pi_ib_{io_1}\beta_1(i)\)</span></p>
<p>同理后向算法的时间复杂度也是 <span class="math inline">\(O(N^2T)\)</span>。</p>
<p><strong>前后向关系</strong></p>
<p>我们计算一下观测到观测序列 <span class="math inline">\(O\)</span> 并且 <span class="math inline">\(t\)</span> 时刻的状态为 <span class="math inline">\(q_i\)</span> 的概率： <span class="math display">\[
\begin{array}{lcl}
\ \ \ \ P(i_t = q_i, O|\lambda) \\
= P(O|i_t = q_i, \lambda)P(i_t = q_i|\lambda)\\
= P(o_1, ..., o_t, o_{t+1}, ...., o_T|i_t=q_i,\lambda)P(i_t=q_i|\lambda)\\
= P(o_1, ..., o_t|i_t=q_i, \lambda)P(o_{i+1}, ..., o_T|i_t=q_i,\lambda)P(i_t=q_i|\lambda)\\
= P(o_1, ..., o_t, i_t=q_i|\lambda)P(o_{i+1}, ..., o_T|i_t=q_i,\lambda)\\
= \alpha_t(i)\beta_t(i)
\end{array}
\]</span> 正好等于 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\beta\)</span> 的乘积，是不是很有意思。</p>
<p><strong>单个状态的概率</strong></p>
<p>我们定义单个状态的概率为 <span class="math inline">\(\gamma_t(i)\)</span>，即给定模型 <span class="math inline">\(\lambda\)</span> 和观测 <span class="math inline">\(O\)</span>，在时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_i\)</span> 的概率，写成公式就是： <span class="math display">\[
\gamma_t(i) = P(i_t = q_i|O, \lambda)
\]</span> 用上前面的结论进一步推导一下： <span class="math display">\[
\gamma_t(i) = P(i_t=q_i|O,\lambda)=\frac{P(i_t=q_i,O|\lambda)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{\sum_{i=1}^N\alpha_t(i)\beta_t(i)}
\]</span> 所以求它为了干嘛？？？</p>
<p>当然是有用的了，我们想想 <span class="math inline">\(\gamma_t(i)\)</span> 代表了啥，是给定模型 <span class="math inline">\(\lambda\)</span> 和观测 <span class="math inline">\(O\)</span>，在时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_i\)</span> 的概率。那我们可以把时刻 <span class="math inline">\(t\)</span> 处于所有状态的概率都求出来，然后取一个最大的作为预测值，对所有时刻都这么做，从而得到一个状态序列 <span class="math inline">\(I^*=\{i_1^*, i_2^*, …, i_T^*\}\)</span>，将它作为预测结果，这不就解决了预测问题吗！</p>
<p><strong>两个状态的联合概率</strong></p>
<p>定义两个状态的联合概率 <span class="math inline">\(\xi_t(i,j)\)</span> 为给定模型 <span class="math inline">\(\lambda\)</span> 和观测 <span class="math inline">\(O\)</span>，在时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_i\)</span> 且时刻 <span class="math inline">\(t+1\)</span> 处于状态 <span class="math inline">\(q_j\)</span> 的概率，即： <span class="math display">\[
\xi_t(i,j) = P(i_t = q_i, i_{t+1}=q_j|O, \lambda)
\]</span> 进一步推导： <span class="math display">\[
\begin{array}{lcl}
\xi_t(i, j) = P(i_t = q_i, i_{t+1}=q_j|O, \lambda) \\
\ \ \ \ \ \ \ \ \ \ \ = \frac{P(i_t=q_i, i_{t+1=q_j},O|\lambda)}{P(O|\lambda)}\\
\ \ \ \ \ \ \ \ \ \ \ = \frac{P(i_t=q_i, i_{t+1=q_j},O|\lambda)}{\sum_{i=1}^N\sum_{j=1}^NP(i_t=q_i, i_{t+1=q_j},O|\lambda)}
\end{array}
\]</span> 其中，<span class="math inline">\(P(i_t=q_i, i_{t+1=q_j},O|\lambda) = \alpha_t(i)a_{ij}b_{jo_{t+1}}\beta_{t+1}(j)\)</span>。</p>
<p><strong>期望</strong></p>
<p>在观测 <span class="math inline">\(O\)</span> 下状态 <span class="math inline">\(i\)</span> 出现的期望： <span class="math display">\[
\sum_{t=1}^T\gamma_t(i)
\]</span> 在观测 <span class="math inline">\(O\)</span> 下状态 <span class="math inline">\(i\)</span> 转移到状态 <span class="math inline">\(j\)</span> 的期望： <span class="math display">\[
\sum_{t=1}^{T-1}\xi_t(i, j)
\]</span></p>
<p>这些都是在为后面做铺垫。</p>
<h3><span id="学习算法">学习算法</span></h3>
<p>HMM的学习方法分为监督学习和无监督学习两种。</p>
<ul>
<li>若训练数据包括观测序列和壮态序列，则HMM的学习非常简单，是监督学习；</li>
<li>若训练数据只有观测序列，则HMM的学习需要使用<a href="https://www.liuhe.website/index.php?/Articles/single/53" target="_blank" rel="noopener">EM算法</a>，是非监督学习。</li>
</ul>
<h4><span id="监督学习">监督学习</span></h4>
<p><strong>Bernoulli大数定理</strong></p>
<p>一言以概：<strong>频率的极限是概率</strong>。</p>
<p><strong>学习方法</strong></p>
<p>HMM的监督学习方法很简单，而且有点无趣，直接统计就行，这里直接列出公式。</p>
<p>初始概率 <span class="math display">\[
\hat{\pi_i} = \frac{|q_i|}{\sum_i|q_i|}
\]</span> 转移概率 <span class="math display">\[
\hat{a_{ij}} = \frac{|q_{ij}|}{\sum_{j=1}^N|q_{ij}|}
\]</span> 观测概率 $$</p>
<p> = </p>
<p>$$</p>
<h4><span id="baum-welch-算法">Baum-Welch 算法</span></h4>
<p>若训练数据只有观测序列，则HMM的学习需要使用<a href="https://neymarl.github.io/2016/09/22/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3EM%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">EM算法</a>，是非监督学习。</p>
<p><img src="/images/1475389008451.png"></p>
<p>​ 图3.EM算法框架</p>
<p>所有观测数据写成 <span class="math inline">\(O = (o_1, o_2, …, o_T)\)</span>，所有隐数据写成 <span class="math inline">\(I=(i_1,i_2, …, i_T)\)</span>，完全数据是 <span class="math inline">\((O, I) = (o_1, o_2, …, o_T, i_1, i_2, …, i_T)\)</span>，则完全数据的对数似然为 <span class="math inline">\(\ln P(O,I|\lambda)\)</span>。</p>
<p>设 <span class="math inline">\(\overline{\lambda}\)</span> 是HMM参数的当前估计值，<span class="math inline">\(\lambda\)</span> 为待求的参数，则EM算法中的 <span class="math inline">\(Q\)</span>，即条件概率为： <span class="math display">\[
\begin{array}{lcr}
Q(\lambda, \overline{\lambda}) = P(I|O,\overline{\lambda})\\
\ \ \ \ \ \ \ \ \ \ \ \ \ = \frac{P(O,I|\overline{\lambda})}{P(O|\overline{\lambda})}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \propto P(O,I|\overline{\lambda})
\end{array}
\]</span> EM算法中的M步为： <span class="math display">\[
\lambda = \arg\max_\lambda \sum_IP(O,I|\overline{\lambda})\cdot \ln P(O,I|\lambda)
\]</span> 根据<a href="#暴力算法">暴力算法</a>中的计算结果： <span class="math display">\[
P(O,I|\lambda) =\pi_{i_1}b_{i_1o_1}a_{i_1i_2}b_{i_2o_2}...a_{i_{T-1}i_T}b_{i_ToT}
\]</span> 化简得： <span class="math display">\[
\begin{array}{lcr}
\ \ \ \sum_IP(O,I|\overline{\lambda})\cdot \ln P(O,I|\lambda)\\
= \sum_I\ln \pi_{i_1}P(O,I|\overline{\lambda})\\
+ \sum_I(\sum_{t=1}^{T-1}\ln a_{i_ti_{t+1}})P(O,I|\overline{\lambda})\\
+ \sum_I(\sum_{t=1}^T\ln b_{i_to_t})P(O,I|\overline{\lambda})
\end{array}
\]</span> 接下来就是极大化上面的式子，求得参数 <span class="math inline">\(A, B, \pi\)</span>。</p>
<p>由于该三个参数分别位于三个项中，可分别极大化。</p>
<p>先求 <span class="math inline">\(\pi\)</span> 看看：<span class="math inline">\(\sum_I\ln \pi_{i_1}P(O,I|\overline{\lambda}) = \sum_{i=1}^N\ln \pi_{i}P(O,i_1=i|\overline{\lambda})\)</span>，注意到 <span class="math inline">\(\pi_i\)</span> 满足加和为 <span class="math inline">\(1\)</span>，利用Lagrange乘数法，构造Lagrange函数得： <span class="math display">\[
L(\pi_i, \gamma) = \sum_{i=1}^NP(O,i_1=i|\overline{\lambda}) + \gamma(\sum_{i=1}^N\pi_i - 1)
\]</span> 其中，<span class="math inline">\(\gamma\)</span> 为Lagrange乘子，对 <span class="math inline">\(L\)</span> 求偏导并令其等于零，得到： <span class="math display">\[
P(O,i_1=1|\overline{\lambda}) + \gamma\pi_i = 0
\]</span> 对所有 <span class="math inline">\(i\)</span> 求和，得到： <span class="math display">\[
\gamma = -P(O|\overline{\lambda})
\]</span> 把 <span class="math inline">\(\gamma\)</span> 代回上式，得到初始状态概率： <span class="math display">\[
\pi_i = \frac{P(O,i_1=1|\overline{\lambda})}{P(O|\overline{\lambda})} = \frac{P(O,i_1=i|\overline{\lambda})}{\sum_{i=1}^NP(O,i_1=i|\overline{\lambda})} ＝ \frac{\gamma_1(i)}{\sum_{i=1}^N\gamma_1(i)}
\]</span> 对于转移概率和观测概率，用同样的方法，可以得到： <span class="math display">\[
a_{ij} = \frac{\sum_{t=1}^{T-1}P(O,i_t=i, i_{t+1}=j|\overline{\lambda})}{\sum_{t=1}^{T-1}P(O,i_t=i|\overline{\lambda})} = \frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1}\gamma_t(i)}
\]</span></p>
<p><span class="math display">\[
b_{ik} = \frac{\sum_{t=1}^TP(O,i_t=i|\overline{\lambda})I(o_t=v_k)}{\sum_{t=1}^TP(O,i_t=i|\overline{\lambda})} = \frac{\sum_{t=1,o_t=v_k}^T\gamma_t(i)}{\sum_{t=1}^T\gamma_t(i)}
\]</span></p>
<p>至此，我们通过EM算法推出了参数更新公式，参数学习的问题就全部解决了。</p>
<h3><span id="预测算法">预测算法</span></h3>
<p>预测算法有两种，一个是近似算法，另一个是Viterbi算法。</p>
<h4><span id="近似算法">近似算法</span></h4>
<p>其实近似算法在讲单个状态的概率 <span class="math inline">\(\gamma_t(i)\)</span> 的时候就已经提到了。</p>
<p>在每个时刻 <span class="math inline">\(t\)</span> 选择在该时刻最有可能出现的状态 <span class="math inline">\(i_t^*\)</span>，从而得到一个状态序列 <span class="math inline">\(I^* = \{i_1^*, i_2^*, …, i_T^*\}\)</span>，将它作为预测结果。</p>
<p>我们已经知道单个状态的概率： <span class="math display">\[
\gamma_t(i) =\frac{P(i_t=q_i,O|\lambda)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{\sum_{i=1}^N\alpha_t(i)\beta_t(i)}
\]</span> 直接选择概率最大的 <span class="math inline">\(i\)</span> 作为最有可能的状态即可。</p>
<p>但是这么计算有个问题就是可能会出现此状态在实际中可能不会发生的情况。</p>
<h4><span id="viterbi算法">Viterbi算法</span></h4>
<p>Viterbi算法实际是用<strong>动态规划</strong>解HMM预测问题，用DP求概率最大的路径（最优路径），这时一条路径对应一个状态序列。</p>
<p>定义变量 <span class="math inline">\(\delta_t(i)\)</span>：在时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(i\)</span> 的所有路径中，概率的最大值。即： <span class="math display">\[
\delta_t(i) = \max_{i_1,i_2, ..., i_{t-1}}P(i_t=i, i_{t-1}, ..., i_1, o_t, ..., o_1|\lambda)
\]</span> 初值： <span class="math display">\[
\delta_1(i) = P(i_1=i,o_1|\lambda) = \pi_ib_{io_1}
\]</span> 递推： <span class="math display">\[
\delta_{t+1}(i) =\max_{i_1,i_2, ..., i_{t}}P(i_{t+1}=i, i_{t}, ..., i_1, o_{t+1}, ..., o_1|\lambda) = \max_{1\leqslant j \leqslant N}(\delta_t(j)a_{ji})\cdot b_{io_{t+1}}
\]</span> 终止： <span class="math display">\[
P^* = \max_{1\leqslant i \leqslant N}\delta_T(i)
\]</span> 最后求出来的 <span class="math inline">\(P^*\)</span> 是最大概率路径的概率，若要知道具体是哪条路还需在递推时标记一下。</p>
<p>我们多想一点，看到这里的时候有木有觉得Viterbi算法和<a href="#前向－后向算法">前向算法</a>很像？如果你翻回去看一眼的话，就会发现Viterbi算法和前向算法唯一的区别就是把求和号 <span class="math inline">\(\sum\)</span> 换成了取最大值 <span class="math inline">\(\max\)</span> ！是不是很有意思，其实数学里有很多这种情况，仅仅是换个符号就可以得到另一个很有用的结论！你有没有感受到数学之美呢？</p>
<h2><span id="总结">总结</span></h2>
<ul>
<li>HMM解决标注问题，在语音识别、NLP、生物信息、模式识别等领域被广泛使用。</li>
<li>如果观测状态是连续值，可将多项式分布改成高斯分布或高斯混合分布。</li>
<li>隐马尔可夫模型虽然有点难，但也非常好玩。</li>
</ul>
<h2><span id="参考">参考</span></h2>
<ul>
<li><p>李航,统计学习方法,清华大学出版社,2012</p></li>
<li><p>Christopher M. Bishop. Pattern Recognition and Machine Learning</p>
<p>Chapter 10. Springer-Verlag, 2006</p></li>
<li><p>Radiner L,Juang B. An introduction of hidden markov Models. IEEEASSP Magazine, 1986</p></li>
<li><p>Lawrence R. Rabiner. A tutorial on hidden Markov models andselected applications in speech recognition. Proceedings of theIEEE 77.2, pp. 257-286, 1989</p></li>
<li><p>Jeff A. Bilmes. A gentle tutorial of the EM algorithm and itsapplication to parameter estimation for Gaussian mixture andhidden Markov models. 1998.</p></li>
<li><p>https://en.wikipedia.org/wiki/Hidden_Markov_model</p></li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/笔记/">笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/NLP/">NLP</a><a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/HMM/">HMM</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>
<script type="text/javascript">
  var $config = {
    image: "icon.png",
  };
  socialShare('.social-share-cs', $config);
</script>



<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80MTI4MC8xNzgyOA==">
	<script type="text/javascript">
		(function (d, s) {
			var j, e = d.getElementsByTagName(s)[0];

			if (typeof LivereTower === 'function') {
				return;
			}

			j = d.createElement(s);
			j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
			j.async = true;

			e.parentNode.insertBefore(j, e);
		})(document, 'script');
	</script>
	<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->


      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2019
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>