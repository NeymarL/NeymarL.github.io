<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>
    初识SVM | NIUHE | Be Myself Enjoy My Life
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE&#39;s Blog">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="初识SVM">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>


<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">Be Myself Enjoy My Life</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      初识SVM</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2016/03/03/Support Vector Machines (SVMs)/" rel="bookmark">
        <time class="entry-date published" datetime="2016-03-03T12:32:00.000Z">
          2016-03-03
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <!-- toc -->
<ul>
<li><a href="#optimization-objective">Optimization Objective</a></li>
<li><a href="#large-margin-intuition">Large Margin Intuition</a></li>
<li><a href="#mathematics-behind-large-margin-classification">Mathematics Behind Large Margin Classification</a></li>
<li><a href="#kernels-i">Kernels I</a></li>
<li><a href="#kernels-ii">Kernels II</a></li>
<li><a href="#using-an-svm">Using An SVM</a></li>
</ul>
<!-- tocstop -->
<a id="more"></a>
<h2><span id="optimization-objective">Optimization Objective</span></h2>
<p>The Support Vector Machine (SVM) is yet another type of supervised machine learning algorithm. It is sometimes cleaner and more powerful.</p>
<p>Recall that in logistic regression, we use the following rules: * if <span class="math inline">\(y=1\)</span>, then <span class="math inline">\(h_θ(x)≈1\)</span> and <span class="math inline">\(Θ^Tx≫0\)</span> * if <span class="math inline">\(y=0\)</span>, then <span class="math inline">\(h_θ(x)≈0\)</span> and <span class="math inline">\(Θ^Tx≪0\)</span></p>
<p>Recall the cost function for (unregularized) logistic regression:</p>
<p><span class="math display">\[J(θ)=\frac{1}{m}\sum_{i=1}^m−y^{(i)}\log(h_θ(x^{(i)}))−(1−y^{(i)})\log(1−h_θ(x^{(i)}))\]</span> <span class="math display">\[=\frac{1}{m}\sum_{i=1}^m−y^{(i)}\log(\frac{1}{1+e^{−θ^Tx^{(i)}}})−(1−y^{(i)})\log(1−\frac{1}{1+e^{−θ^Tx^{(i)}}})\]</span></p>
<p>To make a support vector machine, we will modify the first term of the cost function <span class="math inline">\((−\log(h_θ(x))=−\log(\frac{1}{1+e^{−θ^Tx}}))\)</span> so that when <span class="math inline">\(θ^Tx\)</span> (from now on, we shall refer to this as <span class="math inline">\(z\)</span>) is greater than 1, it outputs 0. Furthermore, for values of <span class="math inline">\(z\)</span> less than 1, we shall use a straight decreasing line instead of the sigmoid curve.(In the literature, this is called a hinge loss function.)</p>
<p><img src="/images/1456900464932.png"></p>
<p>Similarly, we modify the second term of the cost function <span class="math inline">\((−\log(1−h_θ(x))=−log(1−\frac{1}{1+e^{−θ^Tx}}))\)</span> so that when <span class="math inline">\(z\)</span> is less than -1, it outputs 0. We also modify it so that for values of <span class="math inline">\(z\)</span> greater than -1, we use a straight increasing line instead of the sigmoid curve.</p>
<p><img src="/images/1456900546259.png"></p>
<p>We shall denote these as <span class="math inline">\(\text{cost}_1(z)\)</span> and <span class="math inline">\(\text{cost}_0(z)\)</span> (respectively, note that <span class="math inline">\(\text{cost}_1(z)\)</span> is the cost for classifying when y=1, and <span class="math inline">\(\text{cost}_0(z)\)</span> is the cost for classifying when <span class="math inline">\(y=0\)</span>), and we may define them as follows (where k is an arbitrary constant defining the magnitude(大小) of the slope of the line): <span class="math display">\[z = \theta^Tx\]</span> <span class="math display">\[\text{cost}_0(z) = \max(0, k(1+z))\]</span> <span class="math display">\[\text{cost}_1(z) = \max(0, k(1-z))\]</span></p>
<p>Recall the full cost function from (regularized) logistic regression:</p>
<p><span class="math display">\[ J(\theta) = \frac{1}{m} \sum_{i=1}^m y^{(i)}(-\log(h_\theta(x^{(i)}))) + (1 - y^{(i)})(-\log(1 - h_\theta(x^{(i)}))) + \dfrac{\lambda}{2m}\sum_{j=1}^n \Theta^2_j \]</span></p>
<p>Note that the negative sign has been distributed into the sum in the above equation.</p>
<p>We may transform this into the cost function for support vector machines by substituting <span class="math inline">\(\text{cost}_0(z)\)</span> and <span class="math inline">\(\text{cost}_1(z)\)</span>:</p>
<p><span class="math display">\[ J(\theta) = \frac{1}{m} \sum_{i=1}^m y^{(i)} \ \text{cost}_1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{\lambda}{2m}\sum_{j=1}^n \Theta^2_j \]</span></p>
<p>We can optimize this a bit by multiplying this by <span class="math inline">\(m\)</span> (thus removing the <span class="math inline">\(m\)</span> factor in the denominators). Note that this does not affect our optimization, since we're simply multiplying our cost function by a positive constant (for example, minimizing <span class="math inline">\((u-5)^2 + 1\)</span> gives us 5; multiplying it by 10 to make it <span class="math inline">\(10(u-5)^2 + 10\)</span> still gives us 5 when minimized). <span class="math display">\[ J(\theta) = \sum_{i=1}^m y^{(i)} \ \text{cost}_1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{\lambda}{2}\sum_{j=1}^n \Theta^2_j \]</span></p>
<p>Furthermore, convention dictates that we regularize using a factor <span class="math inline">\(C\)</span>, instead of <span class="math inline">\(\lambda\)</span>, like so: <span class="math display">\[ J(\theta) = C\sum_{i=1}^m y^{(i)} \ \text{cost}_1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j \]</span></p>
<p>This is equivalent to multiplying the equation by <span class="math inline">\(C = \dfrac{1}{\lambda}\)</span>, and thus results in the same values when optimized. Now, when we wish to regularize more (that is, reduce overfitting), we decrease <span class="math inline">\(C\)</span>, and when we wish to regularize less (that is, reduce underfitting), we increase <span class="math inline">\(C\)</span>.</p>
<p>Finally, note that the hypothesis of the Support Vector Machine is not interpreted as the probability of <span class="math inline">\(y\)</span> being 1 or 0 (as it is for the hypothesis of logistic regression). Instead, it outputs either 1 or 0. (In technical terms, it is a discriminant function.)</p>
<p><span class="math display">\[ h_\theta(x) = \begin{cases} 1 &amp; \text{if} \ \Theta^Tx \geq 0 \\ 0 &amp; \text{otherwise} \end{cases} \]</span></p>
<h2><span id="large-margin-intuition">Large Margin Intuition</span></h2>
<p>A useful way to think about Support Vector Machines is to think of them as <strong>Large Margin Classifiers</strong>. * If <span class="math inline">\(y = 1\)</span>, we want <span class="math inline">\(\Theta^Tx \geq 1\)</span> (not just <span class="math inline">\(\geq 0\)</span>) * If <span class="math inline">\(y = 0\)</span>, we want <span class="math inline">\(\Theta^Tx \leq -1\)</span> (not just <span class="math inline">\(&lt; 0\)</span>)</p>
<p>Now when we set our constant <span class="math inline">\(C\)</span> to a very large value (e.g. 100,000), our optimizing function will constrain(强迫) <span class="math inline">\(\Theta\)</span> such that the equation <span class="math inline">\(A\)</span> (the summation of the cost of each example) equals 0. We impose(强加) the following constraints on <span class="math inline">\(\Theta\)</span>:</p>
<p><span class="math display">\[ \Theta^Tx \geq 1\ if\ y = 1\ and\ \Theta^Tx \leq -1\ if\ y=0 \]</span></p>
<p>If <span class="math inline">\(C\)</span> is very large, we must choose <span class="math inline">\(\Theta\)</span> parameters such that: <span class="math display">\[ \sum_{i=1}^m y^{(i)}\text{cost}_1(\Theta^Tx) + (1 - y^{(i)})\text{cost}_0(\Theta^Tx) = 0 \]</span></p>
<p>This reduces our cost function to: <span class="math display">\[ \large \begin{align*} J(\theta) = C \cdot 0 + \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j \newline = \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j \end{align*} \]</span></p>
<p>Recall the decision boundary from logistic regression (the line separating the positive and negative examples). In SVMs, the decision boundary has the special property that it is <strong>as far away as possible</strong> from both the positive and the negative examples.</p>
<p>The distance of the decision boundary to the nearest example is called the margin. Since SVMs maximize this margin, it is often called a Large Margin Classifier.</p>
<p>The SVM will separate the negative and positive examples by a <strong>large margin</strong>.</p>
<p>This large margin is only achieved when <span class="math inline">\(C\)</span> is very large.</p>
<p>Data is linearly separable when a straight line can separate the positive and negative examples.</p>
<p>If we have outlier examples that we don't want to affect the decision boundary, then we can reduce <span class="math inline">\(C\)</span>.</p>
<p>Increasing and decreasing <span class="math inline">\(C\)</span> is similar to respectively decreasing and increasing <span class="math inline">\(\lambda\)</span>, and can simplify our decision boundary.</p>
<h2><span id="mathematics-behind-large-margin-classification">Mathematics Behind Large Margin Classification</span></h2>
<p><strong>Vector Inner Product</strong></p>
<p>Say we have two vectors, <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>: <span class="math display">\[ \begin{align*} u = \begin{bmatrix} u_1 \newline u_2 \end{bmatrix} &amp; v = \begin{bmatrix} v_1 \newline v_2 \end{bmatrix} \end{align*} \]</span></p>
<p>The length of vector <span class="math inline">\(v\)</span> is denoted <span class="math inline">\(||v||\)</span>, and it describes the line on a graph from origin (0,0) to <span class="math inline">\((v_1,v_2)\)</span>.</p>
<p>The length of vector <span class="math inline">\(v\)</span> can be calculated with <span class="math inline">\(\sqrt{v_1^2 + v_2^2}\)</span> by the Pythagorean theorem(勾股定理).</p>
<p>The projection(投影) of vector <span class="math inline">\(v\)</span> onto vector <span class="math inline">\(u\)</span> is found by taking a right angle from <span class="math inline">\(u\)</span> to the end of <span class="math inline">\(v\)</span>, creating a right triangle.</p>
<ul>
<li>$p = $ length of projection of <span class="math inline">\(v\)</span> onto the vector <span class="math inline">\(u\)</span>.</li>
<li><span class="math inline">\(u^Tv= p \cdot ||u||\)</span></li>
</ul>
<p>Note that $ u^Tv = ||u|| ||v|| $ where $ $ is the angle between <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>. Also, $ p = ||v|| $. If you substitute <span class="math inline">\(p\)</span> for $ ||v|| $, you get <span class="math inline">\(u^Tv= p \cdot ||u||\)</span>.</p>
<p>So the product <span class="math inline">\(u^Tv\)</span> is equal to the length of the projection times the length of vector <span class="math inline">\(u\)</span>.</p>
<p>In our example, since <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are vectors of the same length, <span class="math inline">\(u^Tv = v^Tu\)</span>. <span class="math display">\[u^Tv = v^Tu = p \cdot ||u|| = u_1v_1 + u_2v_2 \]</span></p>
<p>If the angle between the lines for <span class="math inline">\(v\)</span> and <span class="math inline">\(u\)</span> is greater than 90 degrees, then the projection <span class="math inline">\(p\)</span> will be negative. <span class="math display">\[ \begin{align*} &amp;min_\Theta \dfrac{1}{2}\sum_{j=1}^n \Theta_j^2 \newline &amp;= \dfrac{1}{2}(\Theta_1^2 + \Theta_2^2 + \dots + \Theta_n^2) \newline &amp;= \dfrac{1}{2}(\sqrt{\Theta_1^2 + \Theta_2^2 + \dots + \Theta_n^2})^2 \newline &amp;= \dfrac{1}{2}||\Theta ||^2 \newline \end{align*} \]</span></p>
<p>We can use the same rules to rewrite <span class="math inline">\(\Theta^Tx^{(i)}\)</span>:</p>
<p><span class="math display">\[ \Theta^Tx^{(i)} = p^{(i)} \cdot ||\Theta || = \Theta_1x_1^{(i)} + \Theta_2x_2^{(i)} + \dots + \Theta_nx_n^{(i)} \]</span></p>
<p>So we now have a new optimization objective by substituting <span class="math inline">\(p^{(i)} \cdot ||\Theta ||\)</span> in for <span class="math inline">\(\Theta^Tx^{(i)}\)</span>: * If <span class="math inline">\(y = 1\)</span>, we want <span class="math inline">\(p^{(i)} \cdot ||\Theta || \geq 1\)</span> * If <span class="math inline">\(y = 0\)</span>, we want <span class="math inline">\(p^{(i)} \cdot ||\Theta || \leq -1\)</span></p>
<p>The reason this causes a &quot;large margin&quot; is because: the vector for $$ is perpendicular(垂直的、正交的) to the decision boundary. In order for our optimization objective (above) to hold true, we need the absolute value of our projections <span class="math inline">\(p^{(i)}\)</span> to be as large as possible.</p>
<p>If <span class="math inline">\(\Theta_0 = 0\)</span>, then all our decision boundaries will intersect (0,0). If $ _0 0 $, the support vector machine will still find a large margin for the decision boundary.</p>
<h2><span id="kernels-i">Kernels I</span></h2>
<p>Kernels allow us to make complex, non-linear classifiers using Support Vector Machines.</p>
<p>Given <span class="math inline">\(x\)</span>, compute new feature depending on proximity(接近度) to landmarks <span class="math inline">\(l^{(1)},\ l^{(2)},\ l^{(3)}\)</span>.</p>
<p>To do this, we find the &quot;similarity&quot; of <span class="math inline">\(x\)</span> and some landmark <span class="math inline">\(l^{(i)}\)</span>: <span class="math display">\[ f_i = similarity(x, l^{(i)}) = \exp(-\dfrac{||x - l^{(i)}||^2}{2\sigma^2}) \]</span></p>
<p>This &quot;similarity&quot; function is called a Gaussian Kernel. It is a specific example of a kernel. The similarity function can also be written as follows: <span class="math display">\[ f_i = similarity(x, l^{(i)}) = \exp(-\dfrac{\sum^n_{j=1}(x_j-l_j^{(i)})^2}{2\sigma^2}) \]</span></p>
<p>There are a couple properties of the similarity function: * If <span class="math inline">\(x \approx l^{(i)}\)</span>, then <span class="math inline">\(f_i = \exp(-\dfrac{\approx 0^2}{2\sigma^2}) \approx 1\)</span> * If <span class="math inline">\(x\)</span> is far from <span class="math inline">\(l^{(i)}\)</span>, then <span class="math inline">\(f_i = \exp(-\dfrac{(large\ number)^2}{2\sigma^2}) \approx 0\)</span></p>
<p>In other words, if <span class="math inline">\(x\)</span> and the landmark are close, then the similarity will be close to 1, and if <span class="math inline">\(x\)</span> and the landmark are far away from each other, the similarity will be close to 0.</p>
<p>Each landmark gives us the features in our hypothesis: <span class="math display">\[ \begin{align*} l^{(1)} \rightarrow f_1 \newline l^{(2)} \rightarrow f_2 \newline l^{(3)} \rightarrow f_3 \newline \dots \newline h_\Theta(x) = \Theta_1f_1 + \Theta_2f_2 + \Theta_3f_3 + \dots \end{align*} \]</span></p>
<p><span class="math inline">\(\sigma^2\)</span> is a parameter of the Gaussian Kernel, and it can be modified to increase or decrease the drop-off of our feature <span class="math inline">\(f_i\)</span>.</p>
<p>Combined with looking at the values inside <span class="math inline">\(\Theta\)</span>, we can choose these landmarks to get the general shape of the decision boundary.</p>
<h2><span id="kernels-ii">Kernels II</span></h2>
<p>One way to get the landmarks is to put them in the exact same locations as all the training examples. This gives us <span class="math inline">\(m\)</span> landmarks, with one landmark per training example.</p>
<p>Given example <span class="math inline">\(x\)</span>:</p>
<ul>
<li><span class="math inline">\(f_1 = similarity(x,l^{(1)})\)</span>,</li>
<li><span class="math inline">\(f_2 = similarity(x,l^{(2)})\)</span>,</li>
<li><span class="math inline">\(f_3 = similarity(x,l^3)\)</span>,</li>
<li>and so on.</li>
</ul>
<p>This gives us a &quot;feature vector,&quot; <span class="math inline">\(f^{(i)}\)</span> of all our features for example <span class="math inline">\(x^{(i)}\)</span>. We may also set <span class="math inline">\(f_0 = 1\)</span> to correspond with <span class="math inline">\(\Theta_0\)</span>. Thus given training example <span class="math inline">\(x^{(i)}\)</span>: <span class="math display">\[ x^{(i)} \rightarrow \begin{bmatrix} f_1^{(i)} = similarity(x^{(i)}, l^{(1)}) \newline f_2^{(i)} = similarity(x^{(i)}, l^{(2)}) \newline \dots \newline f_m^{(i)} = similarity(x^{(i)}, l^{(m)}) \newline \end{bmatrix} \]</span></p>
<p>Now to get the parameters <span class="math inline">\(\Theta\)</span> we can use the SVM minimization algorithm but with <span class="math inline">\(f^{(i)}\)</span> substituted in for <span class="math inline">\(x^{(i)}\)</span>: <span class="math display">\[ \large min_\Theta C \sum_{i=1}^m y^{(i)}\text{cost}_1(\Theta^Tf^{(i)}) + (1 - y^{(i)})\text{cost}_0(\theta^Tf^{(i)}) + \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j \]</span></p>
<p>Using kernels to generate <span class="math inline">\(f^{(i)}\)</span> is not exclusive to SVMs and may also be applied to logistic regression. However, because of computational optimizations on SVMs, kernels combined with SVMs is much faster than with other algorithms, so kernels are almost always found combined only with SVMs.</p>
<p><strong>Choosing SVM Parameters</strong></p>
<p>Choosing <span class="math inline">\(C\)</span> (recall that <span class="math inline">\(C = \dfrac{1}{\lambda}\)</span>) * If <span class="math inline">\(C\)</span> is large, then we get higher variance/lower bias * If <span class="math inline">\(C\)</span> is small, then we get lower variance/higher bias</p>
<p>The other parameter we must choose is <span class="math inline">\(\sigma^2\)</span> from the Gaussian Kernel function:</p>
<ul>
<li>With a large <span class="math inline">\(\sigma^2\)</span>, the features <span class="math inline">\(f_i\)</span> vary more smoothly, causing higher bias and lower variance.</li>
<li>With a small <span class="math inline">\(\sigma^2\)</span>, the features <span class="math inline">\(f_i\)</span> vary less smoothly, causing lower bias and higher variance.</li>
</ul>
<h2><span id="using-an-svm">Using An SVM</span></h2>
<p>There are lots of good SVM libraries already written. A. Ng often uses 'liblinear' and 'libsvm'. In practical application, you should use one of these libraries rather than rewrite the functions.</p>
<p>In practical application, the choices you do need to make are:</p>
<ul>
<li>Choice of parameter C</li>
<li>Choice of kernel (similarity function)
<ul>
<li>No kernel (&quot;linear&quot; kernel) -- gives standard linear classifier</li>
<li>Choose when <span class="math inline">\(n\)</span> is large and when <span class="math inline">\(m\)</span> is small</li>
<li>Gaussian Kernel (above) -- need to choose <span class="math inline">\(\sigma^2\)</span></li>
<li>Choose when <span class="math inline">\(n\)</span> is small and <span class="math inline">\(m\)</span> is large</li>
</ul></li>
</ul>
<p>The library may ask you to provide the kernel function.</p>
<p>Note: do perform <strong>feature scaling</strong> before using the Gaussian Kernel.</p>
<p>Note: not all similarity functions are valid kernels. They must satisfy &quot;Mercer's Theorem,&quot; which guarantees that the SVM package's optimizations run correctly and do not diverge.</p>
<p>You want to train <span class="math inline">\(C\)</span> and the parameters for the kernel function using the <strong>training</strong> and <strong>cross-validation</strong> datasets.</p>
<p><strong>Multi-class Classification</strong></p>
<p>Many SVM libraries have multi-class classification built-in.</p>
<p>You can use the one-vs-all method just like we did for logistic regression, where <span class="math inline">\(y \in {1,2,3,\dots,K}\)</span> with <span class="math inline">\(\Theta^{(1)}, \Theta^{(2)}, \dots,\Theta{(K)}\)</span>. We pick class <span class="math inline">\(i\)</span> with the largest <span class="math inline">\((\Theta^{(i)})^Tx\)</span>.</p>
<p><strong>Logistic Regression vs. SVMs</strong></p>
<ul>
<li>If <span class="math inline">\(n\)</span> is large (relative to <span class="math inline">\(m\)</span>), then use logistic regression, or SVM without a kernel (the &quot;linear kernel&quot;)</li>
<li>If <span class="math inline">\(n\)</span> is small and <span class="math inline">\(m\)</span> is intermediate, then use SVM with a Gaussian Kernel</li>
<li>If <span class="math inline">\(n\)</span> is small and <span class="math inline">\(m\)</span> is large, then manually create/add more features, then use logistic regression or SVM without a kernel.</li>
</ul>
<p>In the first case, we don't have enough examples to need a complicated polynomial hypothesis. In the second example, we have enough examples that we may need a complex non-linear hypothesis. In the last case, we want to increase our features so that logistic regression becomes applicable.</p>
<p>Note: a neural network is likely to work well for any of these situations, but may be <strong>slower</strong> to train.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/SVM/">SVM</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>


	<section id="comment" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'Niuhe';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>



      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2018
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>