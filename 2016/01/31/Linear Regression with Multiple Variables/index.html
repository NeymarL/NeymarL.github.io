<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    Linear Regression with Multiple Variables | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="Linear Regression with Multiple Variables">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script>
  <script type="text/javascript" src="/js/search.js"></script>
  <script type="text/javascript" src="/js/jquery.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/search">搜索</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      Linear Regression with Multiple Variables</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2016/01/31/Linear Regression with Multiple Variables/" rel="bookmark">
        <time class="entry-date published" datetime="2016-01-31T10:14:06.000Z">
          2016-01-31
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2><span id="multiple-features">Multiple Features</span></h2>
<p>Linear regression with multiple variables is also known as &quot;<strong>multivariate linear regression</strong>&quot;.</p>
<p>We now introduce notation for equations where we can have any number of input variables.</p>
<a id="more"></a>
<ul>
<li><span class="math inline">\(x^{(i)}_j\)</span> = value of feature j in the <span class="math inline">\(i^{th}\)</span> training example</li>
<li><span class="math inline">\(x^{(i)}\)</span> = the column vector of all the feature inputs of the <span class="math inline">\(i^{th}\)</span> training example</li>
<li><span class="math inline">\(m\)</span> = the number of training examples</li>
<li><span class="math inline">\(n\)</span> = <span class="math inline">\(∣x^{(i)}∣\)</span> (the number of features)</li>
</ul>
<p>Now define the multivariable form of the hypothesis function as follows, accomodating these multiple features:</p>
<p><span class="math display">\[h_θ(x)=θ_0+θ_1x_1 + θ_2x_2 + θ_3x_3 +⋯+ θ_nx_n\]</span></p>
<p>In order to have little intuition about this function, we can think about <span class="math inline">\(θ_0\)</span> as the basic price of a house, <span class="math inline">\(θ_1\)</span> as the price per square meter, <span class="math inline">\(θ_2\)</span> as the price per floor, etc. <span class="math inline">\(x_1\)</span> will be the number of square meters in the house, <span class="math inline">\(x_2\)</span> the number of floors, etc.</p>
<p>Using the definition of matrix multiplication, our <strong>multivariable hypothesis function</strong> can be concisely represented as:</p>
<p><span class="math display">\[
h_θ(x)=
\begin{bmatrix}
θ_0      &amp; \cdots &amp; θ_n      \\
\end{bmatrix}
\begin{bmatrix}
x_0\\ 
\\
\vdots \\
\\
x_n      \\
\end{bmatrix} = θ^{T}x\]</span></p>
<p>This is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more.</p>
<p>[<strong>Note</strong>: So that we can do matrix operations with theta and x, we will set <span class="math inline">\(x^{(i)}_0 = 1\)</span>, for all values of <span class="math inline">\(i\)</span>. This makes the two vectors <span class="math inline">\(\theta\)</span> and <span class="math inline">\(x^{(i)}\)</span> match each other element-wise (that is, have the same number of elements: n+1).]</p>
<p>Now we can collect all <em>m</em> training examples each with <em>n</em> features and record them in an <strong>n+1 by m</strong> matrix. In this matrix we let the value of the subscript (feature) also represent the row number (except the initial row is the &quot;zeroth&quot; row), and the value of the superscript (the training example) also represent the column number, as shown here: <span class="math display">\[X = \begin{bmatrix}
x^{(1)}_0 &amp; x^{(2)}_0     &amp; \cdots &amp;  x^{(m)}_0     \\
x^{(1)}_1 &amp; x^{(2)}_1     &amp; \cdots &amp;  x^{(m)}_1     \\
&amp; &amp; \vdots  &amp; \\
x^{(1)}_n &amp; x^{(2)}_n     &amp; \cdots &amp;  x^{(m)}_n     
\end{bmatrix} = 
\begin{bmatrix}
1 &amp; 1     &amp; \cdots &amp;  1     \\
x^{(1)}_1 &amp; x^{(2)}_1     &amp; \cdots &amp;  x^{(m)}_1     \\
&amp; &amp; \vdots  &amp; \\
x^{(1)}_n &amp; x^{(2)}_n     &amp; \cdots &amp;  x^{(m)}_n     
\end{bmatrix}
\]</span></p>
<p>Notice above that the first column is the first training example (like the vector above), the second column is the second training example, and so forth.</p>
<p>Now we can define <span class="math inline">\(h_θ(X)\)</span> as a row vector that gives the value of <span class="math inline">\(h_θ(x)\)</span> at each of the <em>m</em> training examples: <span class="math display">\[hθ(X)=\begin{bmatrix}
θ_0x^{(1)}_0+θ_1x^{(1)}_1+...+θ_nx^{(1)}_n       &amp; \cdots &amp; θ_0x^{(m)}_0+θ_1x^{(m)}_1+...+θ_nx^{(m)}_n      \\
\end{bmatrix}
\]</span></p>
<h2><span id="cost-function">Cost function</span></h2>
<p>For the parameter vector <span class="math inline">\(θ\)</span> (of type <span class="math inline">\(R^{n+1}\)</span> or in <span class="math inline">\(R^{(n+1)×1}\)</span>), the cost function is:</p>
<p><span class="math display">\[J(\theta) = \frac{1}{2m}\sum^{m}_{i = 1}(h_\theta(x^{(i)}) - y^{(i)})^2\]</span></p>
<p>The vectorized version is:</p>
<p><span class="math display">\[J(\theta) = \frac{1}{2m}(X\theta - \overrightarrow{y})^T(X\theta - \overrightarrow{y})\]</span></p>
<p>Where <span class="math inline">\(\overrightarrow{y}\)</span> denotes the vector of all <em>y</em> values, <span class="math inline">\(X\)</span> represent a matrix of training examples <span class="math inline">\(x^{(i)}\)</span> stored <strong>row-wise</strong>.</p>
<h2><span id="gradient-descent-for-multiple-variables">Gradient Descent for Multiple Variables</span></h2>
<p>The gradient descent equation itself is generally the same form; we just have to repeat it for our '<strong>n</strong>' features:</p>
<p><strong>repeat until convergence:{</strong> <span class="math display">\[\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum^{1}_{m}(h_\theta(x^{(i)}) - y^{(i)})  x^{(i)}_0\]</span> <span class="math display">\[\theta_1 := \theta_1 - \alpha\frac{1}{m}\sum^{1}_{m}(h_\theta(x^{(i)}) - y^{(i)})  x^{(i)}_1\]</span> <span class="math display">\[\theta_2 := \theta_2 - \alpha\frac{1}{m}\sum^{1}_{m}(h_\theta(x^{(i)}) - y^{(i)})  x^{(i)}_2\]</span> <span class="math display">\[...\]</span> <strong>}</strong></p>
<p>In other words:</p>
<p><strong>repeat until convergence:{</strong> <span class="math display">\[\theta_j := \theta_j - \alpha\frac{1}{m}\sum^{1}_{m}(h_\theta(x^{(i)}) - y^{(i)})  x^{(i)}_j\]</span> <strong>for <span class="math inline">\(j\)</span> <span class="math inline">\(:=\)</span> <span class="math inline">\(0\)</span>...<span class="math inline">\(n\)</span></strong> <strong>}</strong></p>
<h2><span id="matrix-notation">Matrix Notation</span></h2>
<p>The Gradient Descent rule can be expressed as:</p>
<p><span class="math display">\[\theta := \theta - \alpha \bigtriangledown J(\theta)\]</span></p>
<p>Where <span class="math inline">\(\bigtriangledown J(\theta)\)</span> is a column vector of the form:</p>
<p><span class="math display">\[\bigtriangledown J(\theta) = \begin{bmatrix}
\frac{\partial J(\theta)}{\partial \theta_0}\\
\frac{\partial J(\theta)}{\partial \theta_1}\\
\vdots \\
\frac{\partial J(\theta)}{\partial \theta_n}\\
\end{bmatrix}\]</span></p>
<p>The j-th component of the gradient is the summation of the product of two terms: <span class="math display">\[\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}\sum^m_{i = 1}(h_\theta(x^{(i)}) - y^{(i)}) x^{(i)}_j\]</span></p>
<p>Sometimes, the summation of the product of two terms can be expressed as the product of two vectors.</p>
<p>Here, the term <span class="math inline">\(x^{(i)}_j\)</span> represents the <span class="math inline">\(m\)</span> elements of the j-th column <span class="math inline">\(\overrightarrow {x_j}\)</span> (j-th feature <span class="math inline">\(\overrightarrow {x_j}\)</span>) of the training set <span class="math inline">\(X\)</span>.</p>
<p>The other term <span class="math inline">\((h_\theta(x^{(i)}) - y^{(i)})\)</span> is the vector of the deviations between the predictions <span class="math inline">\(h_\theta(x^{(i)})\)</span> and the true values <span class="math inline">\(y^{(i)}\)</span> . Re-writing <span class="math inline">\(\frac{\partial J(\theta)}{\partial \theta_j}\)</span> , we have: <span class="math display">\[\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}\overrightarrow{x_j}^T(X\theta - \overrightarrow{y})\]</span> <span class="math display">\[\bigtriangledown J(\theta) = \frac{1}{m}X^T(X\theta - \overrightarrow{y})\]</span></p>
<p>Finally, the matrix notation (vectorized) of the Gradient Descent rule is: <span class="math display">\[\theta := \theta - \frac{\alpha}{m}X^T(X\theta - \overrightarrow{y})\]</span></p>
<h2><span id="feature-normalization">Feature Normalization</span></h2>
<p>We can speed up gradient descent by having each of our input values in roughly the same range. This is because <span class="math inline">\(\theta\)</span> will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p>
<p>The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally: <span class="math inline">\(-1 \le x_i \le 1\)</span> or <span class="math inline">\(-0.5 \le x_i \le 0.5\)</span></p>
<p>These aren't exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.</p>
<p>Two techniques to help with this are <strong>feature scaling</strong> and <strong>mean normalization</strong>. * Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. * Mean normalization involves subtracting the average value for an input variable from the values for that input variable, resulting in a new average value for the input variable of just zero.</p>
<p>To implement both of these techniques, adjust your input values as shown in this formula: <span class="math display">\[x_i := \frac{x_i - \mu_i}{s_i}\]</span></p>
<p>Where <span class="math inline">\(\mu_i\)</span> is the <strong>average</strong> of all the values and <span class="math inline">\(s_i\)</span> is the maximum of the range of values <em>minus</em> the minimum or is the <em>standard deviation</em>.</p>
<p><strong>Example</strong>: <span class="math inline">\(x_i\)</span> is housing prices in range 100-2000. Then, <span class="math inline">\(x_i := \frac{price - 1000}{1900}\)</span>, where 1000 is the average price and 1900 is the maximum (2000) minus the minimum (100).</p>
<h2><span id="gradient-descent-tips">Gradient Descent Tips</span></h2>
<p><strong>Debugging gradient descent</strong> Make a plot with <strong>number of iterations</strong> on the x-axis. Now plot the cost function, <span class="math inline">\(J(\theta)\)</span> over the number of iterations of gradient descent. If <span class="math inline">\(J(\theta)\)</span> ever increases, then you probably need to decrease <span class="math inline">\(\alpha\)</span>.</p>
<p><strong>Automatic convergence test</strong> Declare convergence if <span class="math inline">\(J(\theta)\)</span> decreases by less than <span class="math inline">\(\epsilon\)</span> in one iteration, where <span class="math inline">\(\epsilon\)</span> is some small value such as <span class="math inline">\(10^{-3}\)</span>. However in practice it's difficult to choose this threshold value.</p>
<p>It has been proven that if learning rate <span class="math inline">\(\alpha\)</span> is sufficiently small, then <span class="math inline">\(J(\theta)\)</span> will decrease on every iteration. <em>Andrew Ng</em> recommends decreasing <span class="math inline">\(\alpha\)</span> by multiples of 3.</p>
<h2><span id="features-and-polynomial-regression">Features and Polynomial Regression</span></h2>
<p>We can improve our features and the form of our hypothesis function in a couple different ways.</p>
<p>We can <strong>combine multiple features into one</strong>. For example, we can combine <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> into a new feature <span class="math inline">\(x_3\)</span> by taking <span class="math inline">\(x_1 \cdot x_2\)</span>.</p>
<h3><span id="polynomial-regression">Polynomial Regression</span></h3>
<p>Our hypothesis function need not be linear (a straight line) if that does not fit the data well.</p>
<p>We can <strong>change the behavior or curve</strong> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).</p>
<p>For example, if our hypothesis function is <span class="math inline">\(h_\theta(x) = \theta_0 + \theta_1x_1\)</span> then we can simply <strong>duplicate</strong> the instances of <span class="math inline">\(x_1\)</span> to get the quadratic function <span class="math inline">\(h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_1^2\)</span> or the cubic function <span class="math inline">\(h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_1^2 + \theta_3x_1^3\)</span></p>
<p>In the cubic version, we have created new features <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> where <span class="math inline">\(x_2 = x_1^2\)</span> and <span class="math inline">\(x_3 = x_1^3\)</span>.</p>
<p>To make it a square root function, we could do: <span class="math inline">\(h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2\sqrt{x_1}\)</span></p>
<p>One important thing to keep in mind is, if you choose your features this way then <strong>feature scaling</strong> becomes very important.</p>
<p>eg. if <span class="math inline">\(x_1\)</span> has range 1 - 1000 then <span class="math inline">\(x_1^2\)</span> range of becomes 1 - 1000000 and that of <span class="math inline">\(x_1^3\)</span> becomes 1 - 1000000000</p>
<h2><span id="normal-equation">Normal Equation</span></h2>
<p>The &quot;normal equation&quot; is a version of finding the optimum <strong>without iteration</strong>.</p>
<p>The <a href="http://www.wikiwand.com/zh-hans/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95" target="_blank" rel="noopener">proof</a> for this equation requires knowledge of linear algebra and is fairly involved, so you do not need to worry about the details.</p>
<p><span class="math display">\[\theta = (X^TX)^{-1}X^Ty\]</span></p>
<p>There is <strong>no need to do feature scaling</strong> with the normal equation.</p>
<p>The following is a comparison of gradient descent and the normal equation: | Gradient Descent | Normal Equation | | :--- | :--- | | Need to choose <span class="math inline">\(\alpha\)</span> | No need to choose <span class="math inline">\(\alpha\)</span> | | Needs many iterations | No need to iterate | | Works well when n is large | Slow if n is very large |</p>
<p>With the normal equation, computing the inversion has complexity <span class="math inline">\(O(n^3)\)</span>. So if we have a very large number of features, the normal equation will be slow. In practice, according to <em>A. Ng</em>, when <strong>n</strong> exceeds <strong>10,000</strong> it might be a good time to go from a normal solution to an iterative process.</p>
<h3><span id="normal-equation-demonstration">Normal Equation Demonstration</span></h3>
<ul>
<li><span class="math inline">\(\theta\)</span> is a <span class="math inline">\((n+1)\)</span> x <span class="math inline">\(1\)</span> matrix</li>
<li><span class="math inline">\(X\)</span> is a <span class="math inline">\(m\)</span> x <span class="math inline">\((n+1)\)</span> matrix so <span class="math inline">\(X^T\)</span> is a <span class="math inline">\((n+1)\)</span> x <span class="math inline">\(m\)</span> matrix</li>
<li><span class="math inline">\(\overrightarrow{y}\)</span> is a <span class="math inline">\(m\)</span> x <span class="math inline">\(1\)</span> vector</li>
</ul>
<p><span class="math inline">\(X\theta = \overrightarrow{y}\)</span> The point is to inverse <span class="math inline">\(X\)</span> , but as <span class="math inline">\(X\)</span> is not a square matrix we need to use <span class="math inline">\(X^T\)</span> to have a square matrix <span class="math inline">\((X^T)X\theta = (X^T)\overrightarrow{y}\)</span></p>
<p>Associative matrix multiplication <span class="math inline">\((X^TX)\theta = X^T\overrightarrow{y}\)</span> Assuming <span class="math inline">\((X^TX)\)</span> invertible <span class="math inline">\(\theta = (X^TX)^{-1}X^T\overrightarrow{y}\)</span></p>
<h3><span id="normal-equation-noninvertibility">Normal Equation Noninvertibility</span></h3>
<p>When implementing the normal equation in octave we want to use the <code>pinv</code> function rather than <code>inv</code>.</p>
<p><span class="math inline">\(X^TX\)</span> may be <strong>noninvertible</strong>. The common causes are: * Redundant features, where two features are very closely related (i.e. they are linearly dependent) * Too many features (e.g. <span class="math inline">\(m \le n\)</span>). In this case, delete some features or use &quot;<strong>regularization</strong>&quot; (to be explained in a later lesson).</p>
<p>Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/Linear-Regression/">Linear Regression</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>
<script type="text/javascript">
  var $config = {
    image: "icon.png",
  };
  socialShare('.social-share-cs', $config);
</script>



<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80MTI4MC8xNzgyOA==">
	<script type="text/javascript">
		(function (d, s) {
			var j, e = d.getElementsByTagName(s)[0];

			if (typeof LivereTower === 'function') {
				return;
			}

			j = d.createElement(s);
			j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
			j.async = true;

			e.parentNode.insertBefore(j, e);
		})(document, 'script');
	</script>
	<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->


      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2019
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>