<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    决策树 &amp; 随机森林中的公式推导 | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="决策树 &amp; 随机森林中的公式推导">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script>
  <script type="text/javascript" src="/js/search.js"></script>
  <script type="text/javascript" src="/js/jquery.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/search">搜索</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      决策树 &amp; 随机森林中的公式推导</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2016/09/02/决策树和随机森林中的公式推导/" rel="bookmark">
        <time class="entry-date published" datetime="2016-09-02T11:42:06.000Z">
          2016-09-02
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p><strong>决策树学习的生成算法</strong>主要有三种： * ID3 * Iterative Dichotomiser * C4.5 * CART * Classification And Regression Tree</p>
<p>决策树建立就是从特征中选择一个作为分类条件把样本划分为两个子集，然后对每个子集做同样的事情直到每个子集都属于相同的标签。所以建立决策树的关键在于每次划分子集的时候选择哪个特征，这也是上面三种算法的最主要区别，下面将一一探讨。</p>
<a id="more"></a>
<h2><span id="id3">ID3</span></h2>
<p>要了解ID3算法首先要了解一下<strong>信息增益</strong>等知识。该算法也可以参考我前面的文章：<a href="http://www.liuhe.website/index.php?/Articles/single/46" target="_blank" rel="noopener">决策树之ID3算法详解</a>。</p>
<p><strong>信息熵</strong></p>
<pre><code>在信息论中，熵是接收的每条消息中包含的信息的平均量，又被稱為信息熵、信源熵、平均自信息量。这里，消息代表来自分布或数据流中的事件、样本或特征。（熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大。）来自信源的另一个特征是样本的概率分布。这里的想法是，比较不可能发生的事情，当它发生了，会提供更多的信息。由于一些其他的原因（下面会有解释），把信息（熵）定义为概率分布的对数的相反数是有道理的。事件的概率分布和每个事件的信息量构成了一个随机变量，这个随机变量的均值（即期望）就是这个分布产生的信息量的平均值（即熵）。                      -- from WikiPedia</code></pre>
<p>熵的公式定义为： <span class="math display">\[H(X) = E[-\ln P(x)] = -\sum_i P(x_i)ln(P(x_i)) \]</span></p>
<p>熵的直观解释是对<strong>不确定性</strong>的度量，熵越大不确定性越大，熵越小不确定性越小。</p>
<p><span class="math inline">\(H(X)\)</span> 随 <span class="math inline">\(P(x_i)\)</span> 变化的曲线为</p>
<p><img src="/images/1472795562715.png"></p>
<p>设随机变量 <span class="math inline">\(X\)</span> 只能取两个值 <span class="math inline">\(0\)</span> 和 <span class="math inline">\(1\)</span>，显然当取两个数的概率都为50%的时候不确定性最大，即熵最大为1；若取 <span class="math inline">\(0\)</span> 或取 <span class="math inline">\(1\)</span> 的概率为 100%，则事件完全确定，此时熵为0 。</p>
<p><strong>条件熵</strong></p>
<p>下面来讨论一下条件熵，它的定义式为：<span class="math inline">\(H(X, Y) - H(X)\)</span>。</p>
<p><span class="math inline">\((X,Y)\)</span> 发生所包含的熵,减去 <span class="math inline">\(X\)</span> 单独发生包含的熵： * 在 <span class="math inline">\(X\)</span> 发生的前提下, <span class="math inline">\(Y\)</span> 发生“新”带来的熵 * 该式子定义为 <span class="math inline">\(X\)</span> 发生前提下, <span class="math inline">\(Y\)</span> 的熵: * 条件熵 <span class="math inline">\(H(Y|X)\)</span></p>
<p>我们来推导一下这个式子等于什么。</p>
<p><span class="math display">\[\begin{array}{lcl}
H(Y|X) = H(X, Y) - H(X) \\
= -\sum_{x, y}p(x, y) \ln p(x, y) + \sum_x p(x)\ln p(x) \\
= -\sum_{x, y}p(x, y) \ln p(x, y) + \sum_x\sum_y p(x, y) \ln p(x) \\
= - \sum_{x, y} p(x, y) (\ln p(x, y) - \ln p(x)) \\
= - \sum_{x, y} p(x, y) \ln\frac{p(x, y)}{p(x)} \\
= - \sum_{x, y} p(x, y) \ln p(y\ |\ x)
\end{array}\]</span></p>
<p>继续推导我们可以得到：</p>
<p><span class="math display">\[\begin{array}{lcl}
H(X, Y) - H(X) \\
=  - \sum_{x, y} p(x, y) \ln p(y\ |\ x) \\
= -\sum_{x, y} p(x) p(y\ |\ x) \ln p(y\ |\ x) \\
= \sum_x p(x) (- \sum_y p(y\ |\ x) \ln p(y\ |\ x)) \\
= \sum_x p(x) H(Y\ |\ X = x) \\
= E[H(Y\ |\ X = x)]
\end{array}\]</span></p>
<p>是熵的<strong>数学期望</strong>！</p>
<p><strong>信息增益</strong></p>
<ul>
<li>信息增益表示得知特征 <span class="math inline">\(A\)</span> 的信息而使得类 <span class="math inline">\(X\)</span> 的信息的不确定性减少的程度。</li>
<li><strong>经验熵</strong>：当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时,所对应的熵和条件熵分别称为<strong>经验熵</strong>和<strong>经验条件熵</strong>。</li>
<li><strong>定义</strong>：特征 <span class="math inline">\(A\)</span> 对训练数据集 <span class="math inline">\(D\)</span> 的信息增益 <span class="math inline">\(g(D,A)\)</span>, 定义为集合 <span class="math inline">\(D\)</span> 的经验熵 <span class="math inline">\(H(D)\)</span> 与特征 <span class="math inline">\(A\)</span> 给定条件下 <span class="math inline">\(D\)</span> 的经验条件熵 <span class="math inline">\(H(D|A)\)</span> 之差, 即:
<ul>
<li><span class="math inline">\(g(D,A)=H(D)\ – H(D\ |\ A)\)</span></li>
</ul></li>
</ul>
<p><em>基本记号</em></p>
<ul>
<li>设训练数据集为 <span class="math inline">\(D\)</span> , <span class="math inline">\(|D|\)</span> 表示样本个数。</li>
<li>设有 <span class="math inline">\(K\)</span> 个类 <span class="math inline">\(C_k\)</span> , <span class="math inline">\(k = 1, 2, ... K\)</span> , <span class="math inline">\(C_k\)</span> 为属于类 <span class="math inline">\(C_k\)</span> 的样本个数, 有: $_k | C_k | = | D | $</li>
<li>设特征 <span class="math inline">\(A\)</span> 有 <span class="math inline">\(n\)</span> 个不同的取值 <span class="math inline">\(\{a_1, a_2, ... a_n\}\)</span> ,根据特征 <span class="math inline">\(A\)</span> 的取值将 <span class="math inline">\(D\)</span> 划分为 <span class="math inline">\(n\)</span> 个子集 <span class="math inline">\(D_1, D_2, ... D_n\)</span> , 为 <span class="math inline">\(| D_i |\)</span> 为 <span class="math inline">\(D_i\)</span> 的样本个数，有：<span class="math inline">\(\sum_i |D_i| = |D|\)</span></li>
<li>记子集 <span class="math inline">\(D\)</span> 中属于类 <span class="math inline">\(C\)</span> 的样本的集合为 <span class="math inline">\(D_{ik}\)</span> ，<span class="math inline">\(|D_{ik}|\)</span> 为 <span class="math inline">\(D_{ik}\)</span> 的样本个数。</li>
</ul>
<p><strong>ID3算法</strong></p>
<ol type="1">
<li>计算数据集 <span class="math inline">\(D\)</span> 的经验熵 <span class="math inline">\(H(D) = -\sum_{k=1}^K \frac{|C_k|}{|D|} log\frac{|C_k|}{|D|}\)</span></li>
<li>遍历所有特征, 对于特征 <span class="math inline">\(A\)</span> :
<ul>
<li>计算特征 <span class="math inline">\(A\)</span> 对数据集 <span class="math inline">\(D\)</span> 的经验条件熵 <span class="math inline">\(H(D\ |\ A)\)</span></li>
<li>计算特征 <span class="math inline">\(A\)</span> 的信息增益 : <span class="math inline">\(g(D,A)=H(D) – H(D\ |\ A)\)</span></li>
<li>选择信息增益<strong>最大</strong>的特征作为当前的分裂特征</li>
</ul></li>
</ol>
<p><strong>条件经验熵 <span class="math inline">\(H(D\ |\ A)\)</span> 计算方法</strong></p>
<p><span class="math display">\[\begin{array}{lcl}
H(D\ |\ A) = H(D, A) - H(A) \\
= -\sum_{i,k} p(D_k, A_i) \log p(D_k, A_i) \\
= - \sum_{i=1}^n p(A_i) \sum_{k=1}^K p(D_k\ |\ A_i) \log p(D_k\ |\ A_i) \\
= - \sum_{i=1}^n \frac{|D_i|}{|D|} \sum_{k=1}^K \frac{|D_{ik}|}{|D_i|} \log \frac{|D_{ik}|}{|D_i|}
\end{array}\]</span></p>
<p>以上就是 ID3 算法的核心内容。</p>
<p>但是ID3算法有一个问题，算法每次选择一个信息增益最大的特征进行子集划分，这样就会倾向于选择一些取值很多而且每个值对应的样本很少的特征，比如样本序号（如果你把它加入特征的话），若选择样本序号作为划分子集的特征，则可一次划分结束，所有子集都变为叶子节点，<strong>熵为零</strong>，但是这样分类并没有意义，因为它完全<strong>过拟合</strong>了而且<strong>没有任何预测能力</strong>。为了避免这种情况的发生，我们需要改进这种选择特征标准，于是就有了C4.5和CART。</p>
<h2><span id="c45">C4.5</span></h2>
<p>C4.5 对 ID3 的主要改进在于每次分割选择特征的标准，由<strong>信息增益</strong>换成了<strong>信息增益率</strong>。</p>
<p><strong>信息增益率</strong></p>
<p>定义式： <span class="math display">\[g_r(D, A) = \frac{g(D, A)}{H(A)}\]</span></p>
<p>其中 <span class="math inline">\(H(A)\)</span> 为选择特征 <span class="math inline">\(A\)</span> 的熵，<span class="math inline">\(H(A) = -\sum_i p(A_i) \log p(A_i) = -\sum \frac{|D_i|}{|D|} \log \frac{|D_i|}{|D|}\)</span></p>
<h2><span id="cart">CART</span></h2>
<p><strong>Classification And Regression Tree</strong></p>
<p>CART 再次更换了选择特征标准，它采用<strong>Gini系数</strong>作为评判标准。</p>
<p><strong>Gini 系数</strong></p>
<p>定义式：</p>
<p><span class="math display">\[Gini(p) = \sum_{k=1}^Kp_k(1-p_k) = 1 - \sum_{k=1}^Kp_k^2 ＝1 - \sum_{k=1}^K(\frac{|C_k|}{|D|})^2 \]</span></p>
<p>Gini系数看起来比较突兀，根据著名机器学习讲师<a href="http://weibo.com/u/2306141363" target="_blank" rel="noopener">邹博</a>的观点，Gini系数实则是对<strong>信息增益</strong>的近似： * 将 <span class="math inline">\(f(x) = -\ln x\)</span> 在 <span class="math inline">\(x = 1\)</span> 处<strong>一阶展开</strong>，忽略高阶无穷小，得到 <span class="math inline">\(f(x) \approx 1 - x\)</span> * $H(x) = -<em>{k=1}^Kp_k p_k </em>{k=1}^Kp_k(1 - p_k) = Gini(p) $ * Gini系数与熵的对比图如下：</p>
<p><img src="/images/1472819902545.png"></p>
<p><strong>三种决策树的学习算法</strong></p>
<ul>
<li>ID3 : 使用信息增益/互信息 <span class="math inline">\(g(D,A)\)</span> 进行特征选择
<ul>
<li>取值多的属性,更容易使数据更纯 ,其信息增益更大。</li>
<li>训练得到的是一棵庞大且深度浅的树 : 不合理。</li>
</ul></li>
<li>C4.5 : 信息增益率 <span class="math inline">\(g_r(D,A) = g(D,A) / H(A)\)</span></li>
<li>CART : 基尼指数</li>
<li>一个属性的<strong>信息增益(率)/Gini指数越大</strong>, 表明属性对样本的熵减少的能力更强, 这个属性使得<strong>数据由不确定性变成确定性的能力越强</strong>。</li>
</ul>
<p><strong>决策树的评价</strong></p>
<ul>
<li>假定样本的总类别为 <span class="math inline">\(K\)</span> 个</li>
<li>对于决策树的某叶结点,假定该叶结点含有样本数目为 <span class="math inline">\(n\)</span> , 其中 第 <span class="math inline">\(k\)</span> 类的样本点数目为 <span class="math inline">\(n_k\)</span>, <span class="math inline">\(k=1,2,...,K\)</span>。
<ul>
<li>若某类样本 <span class="math inline">\(n_j=n\)</span> 而 <span class="math inline">\(n_1,...,n_{j-1},n_{j+1},...,n_K=0\)</span>, 称该结点为<strong>纯结点</strong>;</li>
<li>若各类样本数目 <span class="math inline">\(n_1=n_2=...=n_k=n/K\)</span>, 称该样本为<strong>均结点</strong>。</li>
</ul></li>
<li>纯结点的熵 <span class="math inline">\(H_p=0\)</span>, <strong>最小</strong></li>
<li>均结点的熵 <span class="math inline">\(H_u=\ln K\)</span>, <strong>最大</strong></li>
<li>对所有叶结点的<strong>熵求和</strong>, 该值<strong>越小</strong>说明对样本的分类<strong>越精确</strong>
<ul>
<li>各叶结点包含的样本数目不同,可使用样本数加权求熵和</li>
</ul></li>
<li><strong>评价函数</strong> :
<ul>
<li><span class="math inline">\(C(T) = \sum_{t \in leaf} N_t \cdot H(t)\)</span></li>
<li>由于该评价函数越小越好, 所以可以称之为“损失函数”。</li>
</ul></li>
</ul>
<p><strong>剪枝</strong></p>
<p>决策树对训练属于有很好的分类能力, 但对未知的测试数据未必有好的分类能力, 泛化能力弱, 即可能发生<strong>过拟合</strong>现象。</p>
<p>适当的剪枝可以防止过拟合现象，<strong>三种决策树的剪枝过程算法相同</strong>, 区别仅是对于当前树的评价标准不同。</p>
<p><strong>剪枝总体思路</strong></p>
<ul>
<li>由完全树 <span class="math inline">\(T_0\)</span> 开始,剪枝部分结点得到 <span class="math inline">\(T_1\)</span>, 再次剪枝部分结点得到<span class="math inline">\(T_2\)</span>...直到仅剩树根的树 <span class="math inline">\(T_k\)</span>;</li>
<li>在验证数据集上对这 <span class="math inline">\(k\)</span> 个树分别评价, 选择损失函数最小的树 <span class="math inline">\(T_α\)</span>。</li>
</ul>
<h2><span id="随机森林">随机森林</span></h2>
<p><strong>Bagging</strong></p>
<p>在谈随机森林之前先来看看 Bagging 的策略 * bootstrap aggregation * 从样本集中重采样(有重复的)选出 <span class="math inline">\(n\)</span> 个样本 * 在所有属性上, 对这 <span class="math inline">\(n\)</span> 个样本建立分类器 (ID3、C4.5、CART、SVM、Logistic回归等) * 重复以上两步 <span class="math inline">\(m\)</span> 次, 即获得了 <span class="math inline">\(m\)</span> 个分类器 * 将数据放在这 <span class="math inline">\(m\)</span> 个分类器上,最后根据这 <span class="math inline">\(m\)</span> 个分类器的投票结果, 决定数据属于哪一类</p>
<blockquote>
<p>Bootstraping的名称来自成语“pull up by your own bootstraps”, 意思是依靠你自己的资源, 称为自助法, 它是一种<strong>有放回的抽样方法</strong>。</p>
</blockquote>
<p><img src="/images/1472826789356.png"></p>
<p><strong>随机森林</strong></p>
<p>随机森林就是在Bagging的基础上做了些修改： * 从样本中用 Bootstrap 采养出 <span class="math inline">\(n\)</span> 个样本 * 从所有属性中随机选择 <span class="math inline">\(k\)</span> 个属性，建立 CART 树 * 重复以上步骤 <span class="math inline">\(m\)</span> 次，即建立了 <span class="math inline">\(m\)</span> 棵 CART 树 * 这 <span class="math inline">\(m\)</span> 个 CART 形成随机森林,通过投票表决结果，决定数据属于哪一类</p>
<p>当然可以使用决策树作为基本分类器，也可以使用SVM、Logistic回归等其他分类器, 习惯上, 这些分类器组成的“总分类器”, 仍然叫做<strong>随机森林</strong>。不过随机森林的思想是使用很多个<strong>弱分类器</strong>（受异常点影响较小）投票得出一个<strong>强分类器</strong>，所以把小树们换成SVM、LR等强分类器效果不一定会更好，反而可能会更容易受异常点影响（理论上）。</p>
<p><strong>投票机制</strong></p>
<p>投票机制一般有以下几种，少数服从多数用的比较多了： * 简单投票机制 * 一票否决 * 少数服从多数 * 有效多数（加权） * 阈值表决 * 贝叶斯投票机制 * Laplace平滑</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/Decision-Tree/">Decision Tree</a><a href="/tags/ID3/">ID3</a><a href="/tags/C4-5-CART/">C4.5 CART</a><a href="/tags/随机森林/">随机森林</a><a href="/tags/Bagging/">Bagging</a>
      </span>
      

    </div>

    
  </div>
</article>

      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2018
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>