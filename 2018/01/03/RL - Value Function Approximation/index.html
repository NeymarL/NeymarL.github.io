<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <title>
    RL - Value Function Approximation | NIUHE | 日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれん
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE&#39;s Blog">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="RL - Value Function Approximation">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれん</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      RL - Value Function Approximation</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/01/03/RL - Value Function Approximation/" rel="bookmark">
        <time class="entry-date published" datetime="2018-01-03T10:09:09.000Z">
          2018-01-03
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2><span id="introduction">Introduction</span></h2>
<p>This lecture will introduce how to scale up our algorithm to real practical RL problems by value function approximation.</p>
<p>Reinforcement learning can be used to solve <em>large</em> problems, e.g.</p>
<ul>
<li>Backgammon: <span class="math inline">\(10^{20}\)</span> states</li>
<li>Computer Go: <span class="math inline">\(10^{170}\)</span> states</li>
<li>Helicopter: continuous state space</li>
</ul>
<a id="more"></a>
<p>How can we scale up the model-free methods for prediction and control from the last two lectures?</p>
<p>So far we have represented value function by a <strong>lookup</strong> table:</p>
<ul>
<li>Every state <span class="math inline">\(s\)</span> has an entry <span class="math inline">\(V(s)\)</span></li>
<li>Or every state-action pair <span class="math inline">\(s, a\)</span> has an entry <span class="math inline">\(Q(s, a)\)</span></li>
</ul>
<p>Problems with large MDPs:</p>
<ul>
<li>There are too many states and/or actions to store in memory</li>
<li>It is too slow to learn the value of each state individually</li>
</ul>
<p>Solution for large MDPs: Estimate value function with <em>function approximation</em> <span class="math display">\[
\hat{v}(s, \mathbb{w})\approx v_\pi(s)\\
\mbox{or }\hat{q}(s, a, \mathbb{w})\approx q_\pi(s, a)
\]</span> where <span class="math inline">\(\hat{v}\)</span> or <span class="math inline">\(\hat{q}\)</span> are function approximations of real <span class="math inline">\(v_\pi\)</span> or <span class="math inline">\(q_\pi\)</span>, and <span class="math inline">\(\mathbb{w}\)</span> are the parameters. This apporach has a major advantage:</p>
<ul>
<li><strong>Generalise</strong> from seen state to unseen states</li>
</ul>
<p>We can fit the <span class="math inline">\(\hat{v}\)</span> or <span class="math inline">\(\hat{q}\)</span> to <span class="math inline">\(v_\pi\)</span> or <span class="math inline">\(q_\pi\)</span> by MC or TD learning.</p>
<p><strong>Types of Value Function Approximation</strong></p>
<p><img src="/images/vftypes.png"></p>
<p>We consider <span class="math inline">\(\color{red}{\mbox{differentiable}}\)</span> function approximators, e.g.</p>
<ul>
<li>Linear combinations of features</li>
<li>Neural network</li>
</ul>
<p>Futhermore, we require a training method that is suitable for <span class="math inline">\(\color{red}{\mbox{non-stationary}}\)</span>, <span class="math inline">\(\color{red}{\mbox{non-idd}}\)</span> (idd = independent and identical distributed) data.</p>
<p><strong>Table of Contents</strong></p>
<!-- toc -->
<ul>
<li><a href="#incremental-methods">Incremental Methods</a>
<ul>
<li><a href="#value-function-approximation">Value Function Approximation</a></li>
<li><a href="#action-value-function-approximation">Action-Value Function Approximation</a></li>
</ul></li>
<li><a href="#batch-methods">Batch Methods</a>
<ul>
<li><a href="#least-square-prediction">Least Square Prediction</a></li>
<li><a href="#least-squares-control">Least Squares Control</a></li>
</ul></li>
</ul>
<!-- tocstop -->
<h2><span id="incremental-methods">Incremental Methods</span></h2>
<h3><span id="value-function-approximation">Value Function Approximation</span></h3>
<p><strong>Gradient Descent</strong></p>
<p>Let <span class="math inline">\(J(\mathbb{w})\)</span> be a differentiable function of parameter vector <span class="math inline">\(\mathbb{w}\)</span>.</p>
<p>Define the gradient of <span class="math inline">\(J(\mathbb{w})\)</span> to be <span class="math display">\[
\bigtriangledown_wJ(\mathbb{w})=\begin{pmatrix}
\frac{\partial J(\mathbb{w})}{\partial \mathbb{w}_1} \\
\vdots\\
\frac{\partial J(\mathbb{w})}{\partial \mathbb{w}_n} 
\end{pmatrix}
\]</span> To find a local minimum of <span class="math inline">\(J(\mathbb{w})\)</span>, adjust <span class="math inline">\(\mathbb{w}\)</span> in direction of -ve gradient <span class="math display">\[
\triangle \mathbb{w}=-\frac{1}{2}\alpha \bigtriangledown_\mathbb{w}J(\mathbb{w})
\]</span> where <span class="math inline">\(\alpha\)</span> is a step-size parameter.</p>
<p>So let's apply the <em>stochastic gradient descent</em> to <strong>value fucntion approximation</strong>.</p>
<p>Goal: find parameter vector <span class="math inline">\(\mathbb{w}\)</span> minimising mean-squared error between approximate value function <span class="math inline">\(\hat{v}(s, \mathbb{w})\)</span> and true value function <span class="math inline">\(v_\pi(s)\)</span>. <span class="math display">\[
J(\mathbb{w})=\mathbb{E}_\pi[(v_\pi(S)-\hat{v}(S, \mathbb{w}))^2]
\]</span> Gradient descent finds a local minimum <span class="math display">\[
\begin{align}
\triangle\mathbb{w}&amp;=-\frac{1}{2}\alpha \bigtriangledown_\mathbb{w}J(\mathbb{w})\\
&amp; = \alpha\mathbb{E}_\pi[(v_\pi(S)-\hat{v}(S, \mathbb{w}))\bigtriangledown_\mathbb{w}\hat{v}(S, \mathbb{w})] \\
\end{align}
\]</span> Stochastic gradient descent <em>samples</em> the gradient <span class="math display">\[
\triangle\mathbb{w}=\alpha(v_\pi(S)-\hat{v}(S, \mathbb{w}))\bigtriangledown_\mathbb{w}\hat{v}(S, \mathbb{w})
\]</span> Expected update is equal to full gradient update.</p>
<p><strong>Feature Vectors</strong></p>
<p>Let's make this idea more concrete.</p>
<p>Represent state by a <em>feature vector</em>: <span class="math display">\[
x(S) =\begin{pmatrix}
x_1(S) \\
\vdots\\
x_n(S)
\end{pmatrix}
\]</span> For example:</p>
<ul>
<li>Distance of robot from landmarks</li>
<li>Trend in the stock market</li>
<li>Piece and pawn configurations in chess</li>
</ul>
<p><strong>Linear Value Function Approximation</strong></p>
<p>Represent value function by a linear combination of features <span class="math display">\[
\hat{v}(S, \mathbb{w})=x(S)^T\mathbb{w}=\sum^n_{j=1}x_j(S)\mathbb{w}_j
\]</span> Objective function is quadratic in parameters <span class="math inline">\(\mathbb{w}\)</span> <span class="math display">\[
J(\mathbb{w})=\mathbb{E}_\pi[(v_\pi(S)-x(S)^T\mathbb{w})^2]
\]</span> Stochastic gradient descent converges on global optimum.</p>
<p>Update rule is particularly simple <span class="math display">\[
\bigtriangledown_\mathbb{w}\hat{v}(S, \mathbb{w})=x(S)\\
\triangle \mathbb{w}=\alpha(v_\pi(S)-\hat{v}(S, \mathbb{w}))x(S)
\]</span> Update = step-size <span class="math inline">\(\times\)</span> prediction error <span class="math inline">\(\times\)</span> feature value.</p>
<p>So far we have assumed true value function <span class="math inline">\(v_\pi(s)\)</span> given by supervisor. But in RL there is <strong>no supervisor, only rewards</strong>.</p>
<p>In practice, we substitute a <em>target</em> for <span class="math inline">\(v_\pi(s)\)</span>:</p>
<ul>
<li><p>For MC, the target is return <span class="math inline">\(G_t​\)</span> <span class="math display">\[
\triangle \mathbb{w}=\alpha(\color{red}{G_t}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w})
\]</span></p></li>
<li><p>For TD(0), the target is the TD target <span class="math inline">\(R_{t+1}+\gamma\hat{v}(S_{t+1}, \mathbb{w})\)</span> <span class="math display">\[
\triangle \mathbb{w}=\alpha(\color{red}{R_{t+1}+\gamma\hat{v}(S_{t+1}, \mathbb{w})}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w})
\]</span></p></li>
<li><p>For TD(<span class="math inline">\(\lambda\)</span>), the target is the return <span class="math inline">\(G_t^\lambda\)</span> <span class="math display">\[
\triangle \mathbb{w}=\alpha(\color{red}{G_t^\lambda}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w})
\]</span></p></li>
</ul>
<p><strong>Monte-Carlo with Value Function Approximation</strong></p>
<p>Return <span class="math inline">\(G_t\)</span> is unbiased, noisy sample of true value <span class="math inline">\(v_\pi(S_t)\)</span>. We can build our &quot;training data&quot; to apply supervised learning: <span class="math display">\[
&lt;S_1, G_1&gt;, &lt;S_2, G_2&gt;, ..., &lt;S_T, G_T&gt;
\]</span> For example, using <em>linear Monte-Carlo policy evaluation</em> <span class="math display">\[
\begin{align}
\triangle \mathbb{w}&amp;=\alpha(\color{red}{G_t}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w}) \\
&amp; = \alpha(G_t-\hat{v}(S_t, \mathbb{w}))x(S_t)\\
\end{align}
\]</span> Monte-Carlo evaluation converges to a local optimum even when using non-linear value function approximation.</p>
<p><strong>TD Learning with Value Function Approximation</strong></p>
<p>The TD-target <span class="math inline">\(R_{t+1}+\gamma \hat{v}(S_{t+1}, \mathbb{w})\)</span> is a biased sample of true value <span class="math inline">\(v_\pi(S_t)\)</span>. We can still apply supervised learning to &quot;traning data&quot;: <span class="math display">\[
&lt;S_1, R_2 +\gamma\hat{v}(S_2, \mathbb{w})&gt;,&lt;S_2, R_3 +\gamma\hat{v}(S_3, \mathbb{w})&gt;,...,&lt;S_{T-1}, R_T&gt;
\]</span> For example, using <em>linear TD(0)</em> <span class="math display">\[
\begin{align}
\triangle \mathbb{w}&amp;=\alpha(\color{red}{R+\gamma\hat{v}(S&#39;, \mathbb{w})}-\hat{v}(S, \mathbb{w}))\bigtriangledown_w \hat{v}(S, \mathbb{w}) \\
&amp; = \alpha\delta x(S)\\
\end{align}
\]</span> Linear TD(0) converges (close) to global optimum.</p>
<p><strong>TD(<span class="math inline">\(\lambda\)</span>) with Value Function Approximation</strong></p>
<p>The <span class="math inline">\(\lambda\)</span>-return <span class="math inline">\(G_t^\lambda\)</span> is also a biased sample of true value <span class="math inline">\(v_\pi(s)\)</span>. We can also apply supervised learning to &quot;training data&quot;: <span class="math display">\[
&lt;S_1, G_1^\lambda&gt;, &lt;S_2, G_2^\lambda&gt;, ..., &lt;S_{T-1}, G_{T-1}^\lambda&gt;
\]</span> Can use either forward view linear TD(<span class="math inline">\(\lambda\)</span>): <span class="math display">\[
\begin{align}
\triangle \mathbb{w}&amp;=\alpha(\color{red}{G_t^\lambda}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w}) \\
&amp; = \alpha(G_t-\hat{v}(S_t, \mathbb{w}))x(S_t)\\
\end{align}
\]</span> or backward view linear TD(<span class="math inline">\(\lambda\)</span>): <span class="math display">\[
\begin{align}
\delta_t &amp;= R_{t+1}+\gamma \hat{v}(S_{t+1}, \mathbb{w})-\hat{v}(S_t, \mathbb{w}) \\
E_t&amp; = \gamma\lambda E_{t-1} +x(S_t) \\
\triangle\mathbb{w}&amp;=\alpha\delta_tE_t
\end{align}
\]</span></p>
<h3><span id="action-value-function-approximation">Action-Value Function Approximation</span></h3>
<p><img src="/images/avfa.png"></p>
<p>Approximate the action-value function: <span class="math display">\[
\hat{q}(S, A, \mathbb{w}) \approx q_\pi(S, A)
\]</span> Minimise mean-squared error between approximate action-value function <span class="math inline">\(\hat{q}(S, A, \mathbb{w})\)</span> and true action-value function <span class="math inline">\(q_\pi(S, A)\)</span>: <span class="math display">\[
J(\mathbb{w})=\mathbb{E}_\pi[(q_\pi(S, A)-\hat{q}(S, A, \mathbb{w}))^2]
\]</span> Use stochastic gradient descent to find a local minimum: <span class="math display">\[
-\frac{1}{2}\bigtriangledown_w J(\mathbb{w})=(q_\pi(S, A)-\hat{q}(S, A, \mathbb{w}))\bigtriangledown_w\hat{q}(S, A, \mathbb{w})\\
\triangle\mathbb{w}=\alpha (q_\pi(S, A)-\hat{q}(S, A, \mathbb{w}))\bigtriangledown_w\hat{q}(S, A, \mathbb{w})
\]</span> Represent state and action by a feature vector: <span class="math display">\[
\mathbb{x}(S, A)=\begin{pmatrix}
x_1(S, A) \\
\vdots\\
x_n(S, A)
\end{pmatrix}
\]</span> Represent action-value function by linear combination of features: <span class="math display">\[
\hat{q}(S, A, \mathbb{w})=\mathbb{x}(S, A)^T\mathbb{w}=\sum^n_{j=1}x_j (S, A)\mathbb{w}_j
\]</span> Stochastic gradient descent update: <span class="math display">\[
\bigtriangledown_w\hat{q}(S, A, \mathbb{w})=\mathbb{x}(S, A)\\
\triangle \mathbb{w}=\alpha(q_\pi(S, A)-\hat{q}(S,  A, \mathbb{w}))\mathbb{x}(S, A)
\]</span> Like prediction, we must subsitute a target for <span class="math inline">\(q_\pi(S, A)\)</span>:</p>
<ul>
<li><p>For MC, the target is the return <span class="math inline">\(G_t\)</span> <span class="math display">\[
\triangle \mathbb{w}=\alpha(\color{red}{G_t}-\hat{q}(S_t,  A_t, \mathbb{w}))\bigtriangledown_w\hat{q}(S_t, A_t, \mathbb{w})
\]</span></p></li>
<li><p>For TD(0), the target is the TD target <span class="math inline">\(R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})​\)</span> <span class="math display">\[
\triangle \mathbb{w}=\alpha(\color{red}{R_{t+1}+\gamma \hat{q}(S_{t+1},  A_{t+1}, \mathbb{w})}-\hat{q}(S_t,  A_t, \mathbb{w}))\bigtriangledown_w\hat{q}(S_t, A_t, \mathbb{w})
\]</span></p></li>
<li><p>For forward-view TD(<span class="math inline">\(\lambda\)</span>), target is the action-value <span class="math inline">\(\lambda\)</span>-return <span class="math display">\[
\triangle\mathbb{w}=\alpha(\color{red}{q_t^\lambda}-\hat{q}(S_t, A_t,\mathbb{w}))\bigtriangledown\hat{q}(S_t, A_t, \mathbb{w})
\]</span></p></li>
<li><p>For backward-view TD(<span class="math inline">\(\lambda\)</span>), equivalent update is <span class="math display">\[
\begin{align}
\delta_t&amp; =R_{t+1}+\gamma\hat{q}(S_{t+1}, A_{t+1}, \mathbb{w})-\hat{q}(S_t, A_t, \mathbb{w}) \\
E_t&amp; = \gamma\lambda E_{t-1}+\bigtriangledown_w\hat{q}(S_t, A_t, \mathbb{w}) \\
\triangle\mathbb{w}&amp;= \alpha\delta_t E_t
\end{align}
\]</span></p></li>
</ul>
<p><strong>Linear Sarsa with Coarse Coding in Mountain Car</strong></p>
<p><img src="/images/linsarsa.png"></p>
<p>The goal is to control our car to reach the top of the mountain. We represent state by the car's position and velocity. The height of the diagram shows the value of each state. Finally, the value function is like:</p>
<p><img src="/images/linsarfin.png"></p>
<p><strong>Study of <span class="math inline">\(\lambda\)</span>: Should We Bootstrap?</strong></p>
<p><img src="/images/lambdastudy.png"></p>
<p>The answer is <strong>yes</strong>. We can see from above picture, choose some approprite <span class="math inline">\(\lambda\)</span> can certainly reduce the training steps as well as the cost.</p>
<p>However, temporal-difference learning in many cases doesn't guarantee to converge. It may also diverge.</p>
<p><strong>Convergence of Prediction Algorithms</strong></p>
<p><img src="/images/converge.png"></p>
<p>TD dose not follow the gradient of <em>any</em> objective function. This is why TD can diverge when off-policy or using non-linear function approximation. <strong>Gradient TD</strong> follows true gradient of projected Bellman error.</p>
<p><img src="/images/gtd.png"></p>
<p><strong>Convergence of Control Algorithms</strong></p>
<p><img src="/images/convca.png"></p>
<h2><span id="batch-methods">Batch Methods</span></h2>
<p>Gradient descent is simple and appealing. But it is not <strong>sample efficient</strong>. Batch methods seek to find the best fitting value function given the agent's experience.</p>
<h3><span id="least-square-prediction">Least Square Prediction</span></h3>
<p>Give value function approximation <span class="math inline">\(\hat{v}(s, \mathbb{w})\approx v_\pi(s)\)</span> and experience <span class="math inline">\(\mathcal{D}\)</span> consisting of <em>&lt;state, value&gt;</em> pairs: <span class="math display">\[
\mathcal{D} = \{&lt;s_1, v_1^\pi&gt;, &lt;s_2, v_2^\pi&gt;, ..., &lt;s_T, v_T^\pi&gt; \}
\]</span> Which parameters <span class="math inline">\(\mathbb{w}\)</span> give the best fitting value function <span class="math inline">\(\hat{v}(s, \mathbb{w})\)</span> ?</p>
<p><span class="math inline">\(\color{red}{\mbox{Least squares}}\)</span> algorithms find parameter vector <span class="math inline">\(\mathbb{w}\)</span> minimising sum-squared error between <span class="math inline">\(\hat{v}(s_t, \mathbb{w})\)</span> and target values <span class="math inline">\(v_t^\pi\)</span>, <span class="math display">\[
\begin{align}
LS(\mathbb{w}) &amp; = \sum^T_{t=1}(v_t^\pi-\hat{v}(s_t, \mathbb{w}))^2 \\
&amp; = \mathbb{E}_\mathcal{D}[(v^\pi-\hat{v}(s, \mathbb{w}))^2] \\
\end{align}
\]</span> Given experience consisting of <em>&lt;state, value&gt;</em> pairs <span class="math display">\[
\mathcal{D}=\{&lt;s_1, v_1^\pi&gt;, &lt;s_2, v_2^\pi&gt;, ..., &lt;s_T, v_T^\pi&gt;\}
\]</span> Repeat:</p>
<ol type="1">
<li><p>Sample state, value from experience <span class="math display">\[
&lt;s, v^\pi&gt; \sim \mathcal{D}
\]</span></p></li>
<li><p>Apply stochastic gradient descent update <span class="math display">\[
\triangle \mathbb{w}=\alpha(v^\pi-\hat{v}(s, \mathbb{w}))\bigtriangledown_w\hat{v}(s, w)
\]</span></p></li>
</ol>
<p>Converges to least squares solution <span class="math display">\[
\mathbb{w}^\pi=\arg\min_w LS(w)
\]</span> <strong>Deep Q-Networks (DQN)</strong></p>
<p>DQN uses <span class="math inline">\(\color{red}{\mbox{experience replay}}\)</span> and <span class="math inline">\(\color{red}{\mbox{fixed Q-targets}}\)</span>:</p>
<ul>
<li><p>Take action <span class="math inline">\(a_t\)</span> according to <span class="math inline">\(\epsilon\)</span>-greedy policy</p></li>
<li><p>Store transition <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> in replay memory <span class="math inline">\(\mathcal{D}\)</span></p></li>
<li><p>Sample random mini-batch of transitions <span class="math inline">\((s, a, r, s&#39;)\)</span> from <span class="math inline">\(\mathcal{D}\)</span></p></li>
<li><p>Compute Q-learning targets w.r.t. old, fixed parameters <span class="math inline">\(w^-\)</span></p></li>
<li><p>Optimise MSE between Q-network and Q-learning target <span class="math display">\[
\mathcal{L}(w_i)=\mathbb{E}_{s,a,r,s&#39;\sim\mathcal{D}_i}[(r+\gamma\max_{a&#39;}Q(s&#39;,a&#39;;w^-_i)-Q(s, a;w_i))^2]
\]</span></p></li>
<li><p>Using variant of stochastic gradient descent</p></li>
</ul>
<p>Note: <span class="math inline">\(\color{red}{\mbox{fixed Q-targets}}\)</span> means we use two Q-networks. One of it using fixed old parameters to generate the Q-target to update the fresh Q-network, which can keep the update <strong>stable</strong>. Otherwise, when you update the Q-network, you also update the Q-target, which can cause diverge.</p>
<p>DQN in Atari</p>
<ul>
<li>End-to-end learning of values <span class="math inline">\(Q(s, a)\)</span> from pixels <span class="math inline">\(s\)</span></li>
<li>Input state <span class="math inline">\(s\)</span> is stack of raw pixels from last 4 frames</li>
<li>Output is <span class="math inline">\(Q(s, a)\)</span> for 18 joystick/button positions</li>
<li>Rewards is change in score for that step</li>
</ul>
<p><img src="/images/ataridqn.png"></p>
<p>Results</p>
<p><img src="/images/dqnres.png"></p>
<p>How much does DQN help?</p>
<p><img src="/images/dqnhelp.png"></p>
<p><strong>Linear Least Squares Prediction - Normal Equation</strong></p>
<p>Experience replay finds least squares solution but it may take many iterations. Using <em>linear value function approximation</em> <span class="math inline">\(\hat{v}(s, w) = x(s)^Tw\)</span>, we can solve squares soluton directly.</p>
<p>At minimum of <span class="math inline">\(LS(w)\)</span>, the expected update must be zero:</p>
<p><img src="/images/llsp.png"></p>
<p>For <span class="math inline">\(N\)</span> features, direct solution time is <span class="math inline">\(O(N^3)\)</span>. Incremental solution time is <span class="math inline">\(O(N^2)\)</span> using Shermann-Morrison.</p>
<p>We do not know true values <span class="math inline">\(v_t^\pi\)</span>. In practice, our &quot;training data&quot; must be noisy or biased samples of <span class="math inline">\(v_t^\pi\)</span>:</p>
<p><img src="/images/llspa.png"></p>
<p>In each case solve directly for fixed point of MC / TD / TD(<span class="math inline">\(\lambda\)</span>).</p>
<p><strong>Convergence of Linear Least Squares Prediction Algorithms</strong></p>
<p><img src="/images/cllspa.png"></p>
<h3><span id="least-squares-control">Least Squares Control</span></h3>
<p><strong>Least Squares Policy Iteration</strong></p>
<p><img src="/images/lspi.png"></p>
<p><strong>Least Squares Action-Value Function Approximation</strong></p>
<p>Approximate action-value function <span class="math inline">\(q_\pi(s, a)\)</span> using linear combination of features <span class="math inline">\(\mathbb{x}(s, a)\)</span>: <span class="math display">\[
\hat{q}(s, a, \mathbb{w})=\mathbb{x}(s, a)^T\mathbb{w}\approx q_\pi(s, a)
\]</span> Minimise least squares error between <span class="math inline">\(\hat{q}(s, a, \mathbb{w})\)</span> and <span class="math inline">\(q_\pi(s, a)\)</span> from experience generated using policy <span class="math inline">\(\pi\)</span> consisting of <em>&lt;(state, action), value&gt;</em> pairs: <span class="math display">\[
\mathcal{D}=\{&lt;(s_1,a_1),v_1^\pi&gt;,&lt;(s_2,a_2),v_2^\pi&gt;,...,&lt;(s_T,a_T),v_T^\pi&gt;\}
\]</span> <strong>Least Squares Control</strong></p>
<p>For policy evaluation, we want to efficiently use all experience. For control, we also want to improve the policy. This experience is generated from many policies. So to evaluate <span class="math inline">\(q_\pi(S, A)\)</span> we must learn <span class="math inline">\(\color{red}{\mbox{off-policy}}\)</span>.</p>
<p>We use the same idea as Q-learning:</p>
<ul>
<li>Use experience generated by old policy <span class="math inline">\(S_t, A_t, R_{t+1}, S_{t+1} \sim \pi_{old}\)</span></li>
<li>Consider alternative successor action <span class="math inline">\(A&#39;=\pi_{new}(S_{t+1})\)</span></li>
<li>Update <span class="math inline">\(\hat{q}(S_t, A_t,\mathbb{w})\)</span> towards value of alternative action <span class="math inline">\(R_{t+1}+\gamma \hat{q}(S_{t+1}, A&#39;, \mathbb{w})\)</span></li>
</ul>
<p>Consider the following linear Q-learning update <span class="math display">\[
\delta=R_{t+1}+\gamma \hat{q}(S_{t+1}, \color{red}{\pi(S_{t+1})}, \mathbb{w})-\hat{q}(S_t, A_t, \mathbb{w})\\
\triangle \mathbb{w}=\alpha\delta\mathbb{x}(S_t, A_t)
\]</span> LSTDQ algorithm: solve for total update = zero:</p>
<p><img src="/images/lstdq.png"></p>
<p>The following pseudocode uses LSTDQ for policy evaluation. It repeatedly re-evaluates experience <span class="math inline">\(\mathcal{D}\)</span> with different policies.</p>
<p><img src="/images/lspipseudo.png"></p>
<p><strong>Convergence of Control Algorithms</strong></p>
<p><img src="/images/ccal.png"></p>
<p>End.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Deep-Learning/">Deep Learning</a><a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/DQN/">DQN</a><a href="/tags/Neural-Network/">Neural Network</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>


	<section id="comment" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'Niuhe';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>



      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2018
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>