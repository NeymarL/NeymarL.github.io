<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    Recognizing Faces | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="Recognizing Faces">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script>
  <script type="text/javascript" src="/js/search.js"></script>
  <script type="text/javascript" src="/js/jquery.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">博客</a></li>
      
        <li><a href="/notes">笔记</a></li>
      
        <li><a href="/archives">归档</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/search">搜索</a></li>
      
        <li><a href="/about">关于</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      Recognizing Faces</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/12/06/RS - Recognizing Faces/" rel="bookmark">
        <time class="entry-date published" datetime="2018-12-06T14:03:09.000Z">
          2018-12-06
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p>HKUST CSIT5401 Recognition System lecture notes 3. 识别系统复习笔记。</p>
<!-- toc -->
<ul>
<li><a href="#histogram-equalization">Histogram Equalization</a></li>
<li><a href="#image-pyramid-and-neural-networks">Image Pyramid and Neural Networks</a></li>
<li><a href="#integral-image">Integral Image</a></li>
<li><a href="#adaboost">Adaboost</a></li>
<li><a href="#face-recognition-pca">Face Recognition (PCA)</a></li>
</ul>
<!-- tocstop -->
<a id="more"></a>
<h2><span id="histogram-equalization">Histogram Equalization</span></h2>
<p><a href="http://en.wikipedia.org/wiki/Face_detection" target="_blank" rel="noopener">Face detection</a> is the first step in automated face recognition. Its reliability has a major influence on the performance and usability of the entire face recognition system.</p>
<p>Due to lighting or shadow, intensity can vary significantly in an image. Normalization of pixel intensity helps correct variations in imaging parameters in cameras as well as changes in illumination conditions. One widely used technique is <a href="http://en.wikipedia.org/wiki/Histogram_equalization" target="_blank" rel="noopener">histogram equalization</a>, which is based on image histogram. It helps reduce extreme illumination.</p>
<p><strong>Image histogram</strong></p>
<p>It is assumed that there is a digital image with <span class="math inline">\(L\)</span> gray levels <span class="math inline">\(r_k\)</span>. The probability of occurrence of gray level <span class="math inline">\(r_k\)</span> is given by <span class="math display">\[
p_r(r_k)=\frac{n_k}{N}
\]</span> where <span class="math inline">\(n_k\)</span> is number of pixels with gray level <span class="math inline">\(r_k\)</span>; <span class="math inline">\(N\)</span> is total number of pixels in an image; <span class="math inline">\(k = 0,1,2,...,L-1\)</span>.</p>
<p><img src="/images/imagehis.png"></p>
<p>We want an image with equally many pixels at every gray level, or the output intensity approx follows <strong>uniform distribution</strong>.</p>
<p>That is, a flat histogram, where each gray level, <span class="math inline">\(r_k\)</span>, appears equal number of times, i.e., <span class="math inline">\(N/L\)</span> times.</p>
<p><img src="/images/imgequ.png"></p>
<p>Assume that variable <span class="math inline">\(r\)</span> has been normalized between <span class="math inline">\([0,1]\)</span>. The intensity transformation is <span class="math inline">\(s = T(r)\)</span>, such that</p>
<ul>
<li><span class="math inline">\(T(r)\)</span> is single-valued and non-decreasing in the interval <span class="math inline">\(0≤r≤1\)</span>.</li>
<li><span class="math inline">\(0≤T(r)≤1\)</span> for <span class="math inline">\(0≤r≤1\)</span>.</li>
</ul>
<p><strong>Histogram equalization transform</strong></p>
<p>The intensity transformation is the cumulative distribution function (CDF) of <span class="math inline">\(r\)</span>, which is represented by <span class="math display">\[
s=T(r)=\int_0^rp_r(w)dw
\]</span> The discrete implementation is given by <span class="math display">\[
s_k=T(r_k)=\sum_{j=0}^k\frac{n_j}{N}=\sum_{j=0}^kp_r(r_j)
\]</span> where <span class="math inline">\(s_k\)</span> is the <strong>output intensity</strong>; <span class="math inline">\(r_k\)</span> is the input intensity; <span class="math inline">\(n_j\)</span> is the number of pixels with gray level <span class="math inline">\(r_j\)</span>.</p>
<p>Below are some examples:</p>
<p><img src="/images/hisequeg.png"></p>
<p><img src="/images/hisequeg2.png"></p>
<p>Histogram equalization can significantly improve image appearance</p>
<ul>
<li>Automatic</li>
<li>User doesn’t have to perform windowing</li>
</ul>
<p>Nice pre-processing step before face detection</p>
<ul>
<li>Account for different lighting conditions</li>
<li>Account for different camera/device properties</li>
</ul>
<p>There are two methods for <strong>face detection</strong>:</p>
<ol type="1">
<li>Method using image pyramid and neural networks [Rowley-Baluja-Kanade-98]</li>
<li>Method using integral image and AdaBoost learning [Viola-Jones-04]</li>
</ol>
<h2><span id="image-pyramid-and-neural-networks">Image Pyramid and Neural Networks</span></h2>
<p>With the neural networks, a classifier may be trained directly using preprocessed and normalized face and nonface training subwindows.</p>
<p><a href="http://www.cs.cmu.edu/~har/" target="_blank" rel="noopener">Rowley et al</a> use the preprocessed 20x20 subwindows as the input to a neural network. The final decision is made to classify the 20x20 subwindow into face and nonface. The architecture is shown below.</p>
<p><img src="/images/facepy.png"></p>
<p>Instead of upright, frontal faces, a <strong>router network</strong> can be trained to process each input window so that orientation can be estimated. Once the orientation is estimated, the input window can be prepared for detector neural network.</p>
<p><img src="/images/router.png"></p>
<p><strong>Rowley et al.</strong> proposed two neural networks, as presented in the previous slides. The first one is the router network which is trained to estimate the orientation of an assumed face in the 20x20 sub-window. The second one is the normal frontal, upright face detector. However, it only handles <strong>in-plane rotation</strong>.</p>
<p><strong>Huang et al.</strong> proposed a multi-view face tree structure for handling both in-plane and <strong>out-of-plane rotations</strong>. Every node corresponds to a strong classifier.</p>
<p><img src="/images/hung.png"></p>
<h2><span id="integral-image">Integral Image</span></h2>
<p><strong>Method using integral image and AdaBoost learning</strong></p>
<p>The <a href="http://en.wikipedia.org/wiki/Summed_area_table" target="_blank" rel="noopener">integral image</a> <span class="math inline">\(ii(x, y)\)</span> at location <span class="math inline">\((x, y)\)</span> contains the <strong>sum of the pixel intensity values above and to the left</strong> of the location <span class="math inline">\((x, y)\)</span>, inclusive.</p>
<p>The <span class="math inline">\(ii\)</span> is defined as <span class="math display">\[
ii(x,y)=\sum_{x&#39;≤x,y&#39;≤y}i(x&#39;,y&#39;)
\]</span> where <span class="math inline">\(ii(x,y)\)</span> is the integral image and <span class="math inline">\(i(x,y)\)</span> is the original input image.</p>
<p><img src="/images/integralimg.png"></p>
<p>Using the following pair of recurrences: <span class="math display">\[
s(x, y)=s(x, y-1)+i(x,y)\\
ii(x,y)=ii(x-1, y)+s(x,y)
\]</span> where <span class="math inline">\(s(x,y)\)</span> is the cumulative row sum, <span class="math inline">\(s(x, -1) = 0\)</span>, and <span class="math inline">\(ii(-1, y)=0\)</span>, the integral image can be computed in one pass over the original image.</p>
<p>Using the integral image, any rectangular sum can be computed in four array references.</p>
<p><img src="/images/inteeg.png"></p>
<p><strong>Rectangle features</strong></p>
<p>The features for face detection are Haar-like functions. There are three kinds of features.</p>
<p>[1] Two-rectangle feature: The difference between the sum of the pixels within two rectangular regions.</p>
<p><img src="/images/recfea1.png"></p>
<p>[2] Three-rectangle feature: The feature is the sum within two outside rectangles subtracted from the sum in a center rectangle.</p>
<p><img src="/images/recfea2.png"></p>
<p>[3] Four-rectangle feature: The difference between diagonal pairs of rectangles.</p>
<p><img src="/images/recfea3.png"></p>
<p>The rectangle features are sensitive to the presence of edges, bars/lines, and other simple image structures in different scales and at different locations.</p>
<p>Given that the base resolution of the detector is 24 x 24 pixels, the exhaustive set of rectangle features is quite large, 160,000.</p>
<p>Given a feature set and a training set of positive and negative images, a classification function must be learned to classify a pattern into either face or non-face.</p>
<h2><span id="adaboost">Adaboost</span></h2>
<p>In this work, the classifier is designed based on the assumption that a very small number of features can be combined to form an effective classifier.</p>
<p>The <a href="http://en.wikipedia.org/wiki/AdaBoost" target="_blank" rel="noopener">AdaBoost</a> learning algorithm is used to boost the classification performance of a simple learning algorithm. The simple learning algorithm is applied to all rectangle features.</p>
<p>It does this by <strong>combining a collection of weak classification functions</strong> (weak classifiers with relatively high classification error) to form a stronger classifier. The final strong classifier takes the form of <strong>a weighted combination of weak classifiers followed by a threshold</strong>.</p>
<p>Weak classifier <span class="math inline">\(h_t\)</span> (each classifier compute one rectangle feature): <span class="math display">\[
h_t(\vec{x})=\begin{cases}
1\ \text{if }\vec{x}\text{ represents a face image }(f_t(\vec{x})&gt;\text{Threshold})\\
-1\ \text{otherwise}
\end{cases}\\
f_t(\vec{x})=\sum_{white} x-\sum_{black} x
\]</span> The strong classifier is <span class="math display">\[
H(\vec{x})=\text{sgn}\left(\sum_{t=1}^T\alpha_th_t(\vec{x})\right)
\]</span> where <span class="math inline">\(\alpha_t\)</span> is weight; and <span class="math inline">\(\text{sgn}(x)\)</span> is sign function: <span class="math display">\[
\text{sgn}(x)=\begin{cases} 
-1,  &amp; \mbox{if }x≤0 \\
1, &amp; \mbox{if }x&gt;0
\end{cases}
\]</span> <strong>Algorithm</strong></p>
<p>Given example images and classifications <span class="math inline">\((\vec{x}_i, y_i), i = 1, 2,..., N\)</span>, where <span class="math inline">\(N\)</span> is the total number of images.</p>
<p>Start with equal weights on each image <span class="math inline">\(\vec{x}_i\)</span>.</p>
<p>For <span class="math inline">\(t=1, ..., T\)</span>:</p>
<ul>
<li><p>Normalize all weights <span class="math inline">\(w_i = \frac{w_i}{\sum_{j=1}^Nw_j}\)</span> such that <span class="math inline">\(\sum_{i=1}^Nw_i=1\)</span>.</p></li>
<li><p>Select the weak classifier <span class="math inline">\(h_k\)</span> with minimum error: <span class="math display">\[
e_k=\sum_{i=1}^Nw_i\left(\frac{1-h_k(\vec{x}_i)y_i}{2}\right)
\]</span> where <span class="math inline">\(0≤e_k≤1\)</span>.</p></li>
<li><p>Set weight for selected weak classifier <span class="math display">\[
\alpha_t=\frac{1}{2}\ln\left(\frac{1-e_k}{e_k}\right)
\]</span></p></li>
<li><p>Reweight the examples (boosting) <span class="math display">\[
w_i=w_i\exp(-\alpha_iy_ih_k(\vec{x}_i))
\]</span></p></li>
</ul>
<p>For the last step, if the weak classifier classify example <span class="math inline">\(i\)</span> correctly, i.e. <span class="math inline">\(h_k(\vec{x}_i)=y_i\)</span>, then the example weight <span class="math inline">\(w_i=w_ie^{-\alpha_t}\)</span> will decrease; if the weak classifier classify example <span class="math inline">\(i\)</span> wrongly, the weight <span class="math inline">\(w_i=w_i^{\alpha_t}\)</span> will increase.</p>
<p>Values of <span class="math inline">\(T\)</span> can be 200 for <span class="math inline">\(N=10^8\)</span> images and 180,000 filters. Given the above strong classifier, a new image can classified as either face or non-face.</p>
<h2><span id="face-recognition-pca">Face Recognition (PCA)</span></h2>
<p>Images of faces often belong to a manifold of intrinsically low dimension. For example, if there are three 3x1 images (see below), then each image has three intensity values. If each intensity value is viewed as a coordinate in a 3D space, then each image can be viewed as a point in a 3D space.</p>
<p><img src="/images/imgspace.png"></p>
<p>To represent these points effectively, the number of dimensions can be reduced from three to one. It is the concept of <a href="http://en.wikipedia.org/wiki/Dimension_reduction" target="_blank" rel="noopener">dimensionality reduction</a>.</p>
<p><a href="http://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="noopener">Principal component analysis</a> (PCA) is a method for performing dimensionality reduction of high dimensional face images.</p>
<p><strong>Eigenfaces</strong></p>
<p>Let us consider a set of <span class="math inline">\(N\)</span> sample images (image vectors) with <span class="math inline">\(m\times n\)</span> dimensions:</p>
<p><img src="/images/eigenface1.png"></p>
<p>Each image is represented by a 1D vector with dimensions <span class="math inline">\((m\times n) \times 1\)</span>. The <strong>mean image vector</strong> is given by <span class="math display">\[
\vec{x}=\frac{1}{N}\sum_{i=1}^N\begin{bmatrix}
x_{i,1}      \\
\vdots \\
x_{i,mn}
\end{bmatrix}
\]</span> The <strong>scatter matrix</strong> is given by <span class="math display">\[
\vec{S}=[\vec{x_1}-\bar{x}\ \ \vec{x_2}-\bar{x}\ \dots\ \vec{x_N}-\bar{x}]\begin{bmatrix}
(\vec{x_1}-\bar{x})^T     \\
(\vec{x_2}-\bar{x})^T\\
\vdots \\
(\vec{x_N}-\bar{x})^T
\end{bmatrix}
\]</span> The corresponding <span class="math inline">\(t\)</span> eigenvectors with non-zero eigenvalues <span class="math inline">\(\lambda_i\)</span> are <span class="math display">\[
\vec{e}_1\ \ \vec{e}_2\ \ \dots\ \ \vec{e}_t
\]</span> where <span class="math inline">\(\lambda_1≥\lambda_2≥...≥\lambda_t\)</span>.</p>
<p>Then the origin image vector can be approximated by <span class="math display">\[
\vec{x}_j\approx\bar{x}+\sum_{i=1}^tg_{ji}\vec{e}_i
\]</span> where <span class="math inline">\(g_{ji}=(\vec{x}_j-\bar{x})\cdot\vec{e}_i\)</span>.</p>
<p><img src="/images/egface.png"></p>
<p>Since the eigenvectors <span class="math inline">\(e\)</span> have the same dimension as the image vectors, the eigenvectors are referred as <a href="http://en.wikipedia.org/wiki/Eigenface" target="_blank" rel="noopener">Eigenfaces</a>. The value of <span class="math inline">\(t\)</span> is usually much smaller than the value of <span class="math inline">\(mn\)</span>. Therefore, the number of dimensions can be reduced significantly.</p>
<p>For each image <span class="math inline">\(\vec{x}_i\)</span>, the dimension reduced representation is <span class="math display">\[
(g_{i1}, g_{i2}, ..., g_{it})
\]</span> To detect if the new image <span class="math inline">\(\vec{x}\)</span> with <span class="math inline">\(t\)</span> coefficients <span class="math inline">\((g_1, g_2, ..., g_t)\)</span> is a face: <span class="math display">\[
||\vec{x}-(\bar{x}+g_1\vec{e}_1+g_2\vec{e}_2+...+g_t\vec{e}_t)||&lt;\text{Threshold}
\]</span> If it is a face, find the closest labeled face based on the nearest neighbor in the <span class="math inline">\(t\)</span>-dimensional space.</p>
<p><strong>Near-infrared images for face recognition</strong></p>
<p>Most current face recognition systems are based on face images captured in the visible light spectrum. The infrared imaging system is able to produce face images of good condition regardless of visible lights in the environment.</p>
<p><img src="/images/infared.png"></p>
<p><img src="/images/infeared2.png"></p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/笔记/">笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/PCA/">PCA</a><a href="/tags/Recognition-System/">Recognition System</a><a href="/tags/Face-Recognition/">Face Recognition</a><a href="/tags/histogram-equalization/">histogram equalization</a><a href="/tags/Adaboost/">Adaboost</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>
<script type="text/javascript">
  var $config = {
    image: "icon.png",
  };
  socialShare('.social-share-cs', $config);
</script>



<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80MTI4MC8xNzgyOA==">
	<script type="text/javascript">
		(function (d, s) {
			var j, e = d.getElementsByTagName(s)[0];

			if (typeof LivereTower === 'function') {
				return;
			}

			j = d.createElement(s);
			j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
			j.async = true;

			e.parentNode.insertBefore(j, e);
		})(document, 'script');
	</script>
	<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->


      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2019
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>