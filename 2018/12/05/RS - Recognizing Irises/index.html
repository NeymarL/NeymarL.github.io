<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    Recognizing Irises | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="Recognizing Irises">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script>
  <script type="text/javascript" src="/js/search.js"></script>
  <script type="text/javascript" src="/js/jquery.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/search">搜索</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      Recognizing Irises</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/12/05/RS - Recognizing Irises/" rel="bookmark">
        <time class="entry-date published" datetime="2018-12-05T13:56:09.000Z">
          2018-12-05
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p>HKUST CSIT5401 Recognition System lecture notes 2. 识别系统复习笔记。</p>
<!-- toc -->
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#image-acquisition-systems">Image Acquisition Systems</a></li>
<li><a href="#iris-localization">Iris localization</a></li>
<li><a href="#pattern-matching">Pattern Matching</a>
<ul>
<li><a href="#alignment-registration">Alignment (Registration)</a></li>
<li><a href="#representation">Representation</a></li>
<li><a href="#goodness-of-match">Goodness of Match</a></li>
<li><a href="#decision-fld">Decision (FLD)</a></li>
</ul></li>
<li><a href="#hough-transform">Hough Transform</a></li>
</ul>
<!-- tocstop -->
<a id="more"></a>
<h2><span id="introduction">Introduction</span></h2>
<p>Face recognition and iris recognition are non-invasive method for verification and identification of people. In particular, the spatial patterns that are apparent in the human iris are highly distinctive to an individual.</p>
<p><img src="/images/iries.png"></p>
<p><strong>Schematic diagram of iris recognition</strong></p>
<p><img src="/images/iries_recog.png"></p>
<h2><span id="image-acquisition-systems">Image Acquisition Systems</span></h2>
<p>One of the major challenges of automated iris recognition is to capture a high-quality image of the iris while remaining non-invasive to the human operator.</p>
<p>There are three concerns while acquiring iris images:</p>
<ul>
<li>To support recognition, it is desirable to acquire images of the iris with sufficient resolution and sharpness.</li>
<li>It is important to have good contrast in the interior iris pattern without resorting to a level of illumination that annoys the operator, i.e., adequate intensity of source constrained by operator comfort with brightness.</li>
<li>These images must be well framed (i.e., centered) without unduly constraining the operator.</li>
</ul>
<p><strong>The Daugman system</strong></p>
<p>The Daugman system captures images with the iris diameter typically between 100 and 200 pixels from a distance of 15-46cm.</p>
<p>The system makes use of an LED-based point light source in conjunction with a standard video. By carefully positioning of the point source below the operator, reflections of the light source off eyeglasses can be avoided in the imaged iris.</p>
<p><img src="/images/daugman.png"></p>
<p>The Daugman system provides the operator with live video feedback via a tiny liquidcrystal display placed in line with the camera's optics via a beam splitter. This allows the operator to see what the camera is capturing and to adjust his position accordingly.</p>
<p><strong>The Wildes system</strong></p>
<p>The Wildes system images the iris with approximately 256 pixels across the diameter from 20cm. The system makes use of a diffused source and polarization in conjunction with a low-light level camera.</p>
<p>The use of matched <strong>circular polarizer</strong> at the light source and camera essentially eliminates the specular reflection of the light source.</p>
<p><img src="/images/wildes.png"></p>
<p>The coupling of a low light level camera with a diffused illumination allows for a level of illumination that is entirely unobjectionable to human operators.</p>
<p>The relative sizes and positions of the square contours are chosen so that when the eye is in an appropriate position, the squares overlap and appear as one to the operator.</p>
<h2><span id="iris-localization">Iris localization</span></h2>
<p><img src="/images/iries_loc.png"></p>
<p>Image acquisition will capture the iris as part of a larger image that also contains data derived from the immediately surrounding eye region. For example, eyelashes, upper eyelid, lower eyelid and sclera. Therefore, prior to performing iris pattern matching, it is important to <strong>localize</strong> that portion of the acquired image that corresponds to an iris.</p>
<p><strong>The Wildes system</strong> makes use of the <strong>first derivatives</strong> of image intensity to signal the location of edges that correspond to the borders of the iris.</p>
<ul>
<li>Step 1: The image intensity information is converted into binary edge-map.</li>
<li>Step 2: The edge points vote to particular contour parameter values.</li>
</ul>
<p><strong>Step 1</strong></p>
<p>The edge map is recovered via <strong>gradient-based edge detection</strong>. This operation consists of thresholding the magnitude of the image intensity gradient magnitude. <span class="math inline">\(I\)</span> is the intensity and (x, y) are the image coordinates. <span class="math display">\[
\text{Gradient magnitude }|\triangledown G(x, y)\ast I(x, y)|\\
\text{2D Gaussian function } G(x, y)=\frac{1}{2\pi\sigma^2}\exp(\frac{(x-x_0)^2+(y-y_0)^2}{2\sigma^2})
\]</span> <img src="/images/iris_edge.png"></p>
<p><strong>Step 2</strong></p>
<p>The voting procedure is realized via the <a href="#hough-transform">Hough transform</a>. For circular limbic or pupillary boundaries and a set of recovered edge points, a Hough transform is defined as follows.</p>
<p>Edge points <span class="math inline">\((x_j, y_j)\)</span> for <span class="math inline">\(j = 1, ..., n\)</span>: <span class="math display">\[
H(x_c, y_c, r)=\sum_{j=1}^nh(x_j,y_j,x_c,y_c,r)
\]</span> where <span class="math display">\[
h(x_j, y_j, x_c, y_c, r)=\begin{cases}
1, \text{ if }\ g(x_j, y_j, x_c, y_c, r)=0\\
0, \text{ otherwise}
\end{cases}\\
g(x_j, y_j, x_c, y_c, r)=(x_j-x_c)^2+(y_j-y_c)^2-r^2
\]</span> For every parameter triple <span class="math inline">\((x_c, y_c, r)\)</span> that represents a circle through the edge point <span class="math inline">\((x_j, y_j)\)</span>, <span class="math display">\[
g(x_j, y_j, x_c, y_c, r)=0
\]</span> <img src="/images/wildescir.png"></p>
<p>The parameter triple that <strong>maximizes</strong> the Hough space <span class="math inline">\(H\)</span> is common to the largest number of edge points and is a reasonable choice to represent the contour of interest.</p>
<hr>
<p>The limbus and pupil are modeled with <strong>circular contour models</strong>.</p>
<p><strong>The Daugman system</strong> fits the <strong>circular contours</strong> via gradient ascent on the parameters so as to maximize <span class="math display">\[
\left|\frac{\partial}{\partial r}G(r)\ast\oint_{x_c,y_c,r}\frac{I(x,y)}{2\pi r}ds\right|
\]</span> where <span class="math inline">\(G(r)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(r-r_0)^2}{2\sigma^2})\)</span>; <span class="math inline">\(r_0\)</span> is the center.</p>
<p>The first part of the equation is to perform Gaussian smoothing; while the second part is computing the average intensity along the circle.</p>
<p><img src="/images/IMG_66247EE67551-1.jpg"></p>
<p>In order to incorporate directional tuning of the image derivative, the arc of integration <span class="math inline">\(ds\)</span> is restricted to the left and right quadrants (i.e., near vertical edges) when fitting the <em>limbic boundary</em>.</p>
<p>This arc is considered over a fuller range when fitting the <em>pupillary boundary</em>.</p>
<h2><span id="pattern-matching">Pattern Matching</span></h2>
<p>Having localized the region of an acquired image that corresponds to the iris, the final task is to decide if this pattern matches a previously stored iris pattern.</p>
<p>There are four steps:</p>
<ol type="1">
<li>Alignment: bringing the newly acquired iris pattern into spatial alignment with a candidate data base entry.</li>
<li>Representation: choosing a representation of the aligned iris patterns that makes their distinctive patterns apparent.</li>
<li>Goodness of Match: evaluating the goodness of match between the newly acquired and data base representations.</li>
<li>Decision: deciding if the newly acquired data and the data base entry were derived from the same iris based on the goodness of match.</li>
</ol>
<h3><span id="alignment-registration">Alignment (Registration)</span></h3>
<p>To make a detailed comparison between two images, it is advantageous to establish a precise correspondence (or matching) between characteristic structures across the pair.</p>
<p>Both systems (Daugman and Wildes systems) compensate for image shift, scaling and rotation.</p>
<p><strong>The Daugman system for alignment</strong></p>
<p>The Daugman system uses <strong>radial scaling</strong> to compensate for overall size as well as a simple model pupil variation based on <strong>linear stretching</strong>.</p>
<p>The system maps the Cartesian image coordinates <span class="math inline">\((x, y)\)</span> to dimensionless polar image coordinates <span class="math inline">\((r, θ)\)</span> according to <span class="math display">\[
x(r,\theta)=(1-r)x_p(0,\theta)+rx_l(1,\theta)\\
y(r,\theta)=(1-r)y_p(0,\theta)+ry_l(1,\theta)
\]</span> <img src="/images/daugalign.png"></p>
<p><img src="/images/daugali.png"></p>
<p><strong>The Wildes system for alignment</strong></p>
<p>The Wildes system uses an <strong>image-registration</strong> technique to compensate for both scaling and rotation.</p>
<p>This approach geometrically warps a newly acquired image <span class="math inline">\(I_a (x, y)\)</span> into alignment with a selected data base image <span class="math inline">\(I_d (x, y)\)</span> according to a mapping function <span class="math inline">\((u(x, y), v(x, y))\)</span> such that for all <span class="math inline">\((x, y)\)</span>, the image intensity value at <span class="math inline">\((x, y) – (u(x, y), v(x, y))\)</span> is close to that at <span class="math inline">\((x, y)\)</span> at <span class="math inline">\(I_d\)</span>.</p>
<p>The mapping function is taken to minimize <span class="math display">\[
\int_x\int_y\left(I_d(x,y)-I_a(x-u, y-v)\right)^2dxdy
\]</span> under the constrains to capture similarity transformation of image coordinates <span class="math inline">\((x,y)\)</span> to <span class="math inline">\((x&#39;=x-u, y&#39;=y-v)\)</span>.</p>
<p><img src="/images/imgreg.jpg"></p>
<p><strong>Translation</strong> <span class="math display">\[
\vec{x}&#39;=\vec{x}+\vec{d}
\]</span> <img src="/images/imgtrans.png"></p>
<p><strong>Rotation</strong> <span class="math display">\[
\vec{x}&#39;=R_\theta\vec{x}\\
R_\theta=\begin{pmatrix}
\cos\theta &amp; -\sin\theta \\
\sin\theta &amp; \cos\theta
\end{pmatrix}
\]</span> <img src="/images/imgrot.png"></p>
<p><strong>Rotation + Translation</strong> <span class="math display">\[
\vec{x}&#39;=R\vec{x}+\vec{d}
\]</span> <strong>Scaling + Translation</strong> <span class="math display">\[
\vec{x}&#39;=S\vec{x}+\vec{d}
\]</span> <img src="/images/scatra.png"></p>
<p><strong>Shearing</strong> <span class="math display">\[
\vec{x}&#39;=K\vec{x}\\
K=\begin{bmatrix}
1      &amp; k_{xy}     \\
k_{yx}     &amp; 1
\end{bmatrix}
\]</span> <img src="/images/shearing.png"></p>
<p><strong>Affine</strong>: translation + rotation + scaling + shearing <span class="math display">\[
\vec{x}&#39;=R_\theta S K\vec{x}+\vec{d}
\]</span> Example: <span class="math display">\[
\begin{bmatrix}
x&#39; \\
y&#39;
\end{bmatrix}=\begin{bmatrix}
\cos\theta &amp; -\sin\theta \\
\sin\theta &amp; \cos\theta
\end{bmatrix}\begin{bmatrix}
s_x &amp; 0 \\
0 &amp; s_y
\end{bmatrix}\begin{bmatrix}
1 &amp; k_{xy} \\
k_{yx} &amp; 1
\end{bmatrix}\begin{bmatrix}
x \\
y
\end{bmatrix}+\begin{bmatrix}
d_x \\
d_y
\end{bmatrix}
\]</span></p>
<h3><span id="representation">Representation</span></h3>
<p>To represent the iris image for matching, both the Daugman and Wildes systems capture the multiscale information extracted from the image.</p>
<p>The Wildes system makes use of the Laplacian of Gaussian filters to construct a <strong>Laplacian pyramid</strong>.</p>
<p>The Laplacian of Gaussian (LoG) filter is given by <span class="math display">\[
-\frac{1}{\pi\sigma^4}\left(1-\frac{\rho^2}{2\sigma^2}\right)\exp(-\frac{\rho^2}{2\sigma^2})
\]</span> where <span class="math inline">\(\rho\)</span> is radial distance of a point from the filter's center; <span class="math inline">\(\sigma\)</span> is standard deviation.</p>
<p>A Laplacian pyramid is formed by collecting the LoG filtered images.</p>
<p><img src="/images/logpra.png"></p>
<h3><span id="goodness-of-match">Goodness of Match</span></h3>
<p>The Wildes system uses the <strong>normalized correlation</strong> between the acquired representation and data base representation. In discrete form, the normalized correlation can be defined as follows.</p>
<p>Let <span class="math inline">\(p_1[i, j]\)</span> and <span class="math inline">\(p_2[i, j]\)</span> be two image arrays of size <span class="math inline">\(n \times m\)</span>.</p>
<p><img src="/images/gom.png"></p>
<p>The normal correlation is <span class="math display">\[
NC=\frac{\sum_{i=1}^n\sum_{j=1}^m(p_1[i,j]-\mu_1)(p_2[i,j]-\mu_2)}{nm\sigma_1\sigma_2}
\]</span> <img src="/images/gompy.png"></p>
<h3><span id="decision-fld">Decision (FLD)</span></h3>
<p>The Wildes system combines four estimated normalized correlation values into a single <strong>accept/reject</strong> judgment.</p>
<p>In this application, the concept of <a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis" target="_blank" rel="noopener">Fisher's linear discriminant</a> is used for making binary decision. A <strong>weight vector</strong> is found such that <strong>the variance within a class of iris data is minimized</strong> while <strong>the variance between different classes of iris data is maximized</strong> for the transformed samples.</p>
<p>In iris recognition application, usually there are two classes: <strong>Authentic class (A)</strong> and <strong>Imposter class (I)</strong>.</p>
<p>To make a binary decision on a line, all points are projected onto the weight vector (or samples are transformed by using the weight vector).</p>
<p><img src="/images/fld1.png"></p>
<p>In iris recognition using the Wildes system, all samples are 4-dimensional vectors. Let there be n 4-dimensional samples.</p>
<p><img src="/images/fld2.png"></p>
<p>The total within class variance is <span class="math display">\[
\vec{S}_w=\vec{S}_i+\vec{S}_a
\]</span> Between class variance is <span class="math display">\[
\vec{S}_b=(\vec{\mu}_a-\vec{\mu}_i)(\vec{\mu}_a-\vec{\mu}_i)^T
\]</span> If all samples are transformed, the ratio of between class variance to total within class variance is <span class="math display">\[
\frac{\vec{w}^T\vec{S}_b\vec{w}}{\vec{w}^T\vec{S}_w\vec{w}}
\]</span> The ratio is maximized when <span class="math display">\[
\vec{w}=\vec{S}_w^{-1}(\vec{\mu}_a-\vec{\mu}_i)
\]</span> And the separation point for decision making is <span class="math display">\[
\frac{1}{2}\vec{w}^T(\vec{\mu}_a+\vec{\mu}_i)
\]</span> Therefore, values above this point will be taken as derived from class <span class="math inline">\(A\)</span>; values below this point will be taken as derived from class <span class="math inline">\(I\)</span>.</p>
<p><img src="/images/fld3.png"></p>
<h2><span id="hough-transform">Hough Transform</span></h2>
<p><strong>Detecting Lines</strong></p>
<p>Idea: if two edge points <span class="math inline">\((x_i, y_i)\)</span> and <span class="math inline">\((x_j, y_j)\)</span> lie on the same straight line, then they should have the same values of slope and y-intercepts on the xy-plane.</p>
<p><img src="/images/houghline.png"></p>
<p><strong>[1]</strong> For a point <span class="math inline">\((x_i,y_i)\)</span>, we set up a straight line equation: <span class="math display">\[
y_i=ax_i+b\Leftrightarrow b=(-x_i)a+y_i
\]</span> where <span class="math inline">\(a\)</span> = slope, <span class="math inline">\(b\)</span> = y-intercept, <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> are known and fixed.</p>
<p><strong>[2]</strong> We subdivide the a axis into <span class="math inline">\(K\)</span> increments between <span class="math inline">\([a_\min,a_\max]\)</span>. For each increment of <span class="math inline">\(a\)</span>, we evaluate the value of <span class="math inline">\(b\)</span>.</p>
<p><strong>[3]</strong> A relationship between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> can be plotted in a parameter space, i.e., ab-plane.</p>
<p><strong>[4]</strong> We partition the parameter space into a number of bins (accumulator cells), and increment the corresponding bin <span class="math inline">\(A(a,b)\)</span> by 1 (<span class="math inline">\(b\)</span> is rounded into the nearest integer).</p>
<p><img src="/images/houghbin.png"></p>
<p><strong>[5]</strong> For another point <span class="math inline">\((x_j,y_j)\)</span>, we set up another straight line equation: <span class="math display">\[
y_j=ax_j+b\Leftrightarrow b=(-x_j)a+y_j
\]</span> <strong>[6]</strong> Similarly, we subdivide the a axis into K increments between <span class="math inline">\([a_\min,a_\max]\)</span>. For each increment of <span class="math inline">\(a\)</span>, we evaluate the value of <span class="math inline">\(b\)</span>. We plot the relationship between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> in the same parameter space, and update bin values in the discrete parameter space.</p>
<p><strong>[7]</strong> The bin <span class="math inline">\(A(a,b)\)</span> having the highest count corresponds to the straight line passing through the points <span class="math inline">\((x_i,y_i)\)</span> and <span class="math inline">\((x_j,y_j)\)</span>.</p>
<p><img src="/images/hough2.png"></p>
<p><strong>[8]</strong> The same procedure can be applied to all points. The bin <span class="math inline">\(A(a,b)\)</span> having the <strong>highest count</strong> corresponds to the straight line passing through (or passing near) the largest number of points.</p>
<p>Problem: Values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> run from negative infinity to positive infinity. We need infinite number of bins!</p>
<p>Solution: use normal representation of a line: <span class="math display">\[
x\cos(\theta)+y\sin(\theta)=\rho
\]</span> <img src="/images/houghnormal.png"></p>
<p><span class="math inline">\(\theta\)</span> runs from <span class="math inline">\(–90^o\)</span> to <span class="math inline">\(90^o\)</span>. <span class="math inline">\(\rho\)</span> runs from <span class="math inline">\(-\sqrt{2}D\)</span> to <span class="math inline">\(\sqrt{2}D\)</span>, where <span class="math inline">\(D\)</span> is the distance between corners in the image (length and width).</p>
<p><strong>Circle Hough Transform (CHT)</strong></p>
<p>The Hough transform can be used to determine the parameters of a circle when a number of points that fall on the perimeter are known. A circle with radius <span class="math inline">\(R\)</span> and center <span class="math inline">\((a, b)\)</span> can be described with the parametric equations: <span class="math display">\[
x=a+R\cos(\theta)\\
y=b+R\sin(\theta)
\]</span> When the angle <span class="math inline">\(θ\)</span> sweeps through the full 360 degree range the points <span class="math inline">\((x, y)\)</span> trace the perimeter of a circle.</p>
<p>If the circles in an image are of <strong>known radius</strong> <span class="math inline">\(R\)</span>, then the search can be reduced to 2D. The objective is to find the <span class="math inline">\((a, b)\)</span> coordinates of the centers.</p>
<p><img src="/images/cht1.png"></p>
<p>If the <strong>radius is not known</strong>, then the locus of points in parameter space will fall on the surface of a <strong>cone</strong>. Each point <span class="math inline">\((x, y)\)</span> on the perimeter of a circle will produce a cone surface in parameter space. The triplet <span class="math inline">\((a, b, R)\)</span> will correspond to the accumulation cell where the largest number of cone surfaces intersect.</p>
<p><img src="/images/cone.png">The drawing above illustrates the generation of a conical surface in parameter space for one <span class="math inline">\((x, y)\)</span> point. A circle with a different radius will be constructed at each level, <span class="math inline">\(r\)</span>.</p>
<p>The search for circles with unknown radius can be conducted by using a three dimensional accumulation matrix.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Recognition-System/">Recognition System</a><a href="/tags/Iris/">Iris</a><a href="/tags/FLD/">FLD</a><a href="/tags/Daugman/">Daugman</a><a href="/tags/Wildes/">Wildes</a><a href="/tags/Hough-Transform/">Hough Transform</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>
<script type="text/javascript">
  var $config = {
    image: "icon.png",
  };
  socialShare('.social-share-cs', $config);
</script>



<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80MTI4MC8xNzgyOA==">
	<script type="text/javascript">
		(function (d, s) {
			var j, e = d.getElementsByTagName(s)[0];

			if (typeof LivereTower === 'function') {
				return;
			}

			j = d.createElement(s);
			j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
			j.async = true;

			e.parentNode.insertBefore(j, e);
		})(document, 'script');
	</script>
	<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->


      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2019
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>