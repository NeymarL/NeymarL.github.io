<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    Recognizing Fingerprints | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="Recognizing Fingerprints">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script>
  <script type="text/javascript" src="/js/search.js"></script>
  <script type="text/javascript" src="/js/jquery.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">博客</a></li>
      
        <li><a href="/categories">笔记</a></li>
      
        <li><a href="/archives">归档</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/search">搜索</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      Recognizing Fingerprints</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/12/07/RS - Recognizing Fingerprints/" rel="bookmark">
        <time class="entry-date published" datetime="2018-12-07T13:20:07.000Z">
          2018-12-07
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p>HKUST CSIT5401 Recognition System lecture notes 4. 识别系统复习笔记。</p>
<!-- toc -->
<ul>
<li><a href="#fingerprint-image-acquisition-systems">Fingerprint image acquisition systems</a></li>
<li><a href="#minutiae">Minutiae</a></li>
<li><a href="#fingerprint-enhancement">Fingerprint Enhancement</a>
<ul>
<li><a href="#normalization">Normalization</a></li>
<li><a href="#orientation-image-estimation">Orientation Image Estimation</a></li>
<li><a href="#ridge-frequency-estimation">Ridge Frequency Estimation</a></li>
<li><a href="#region-mask-estimation">Region mask estimation</a></li>
<li><a href="#gabor-filter">Gabor Filter*</a></li>
</ul></li>
<li><a href="#fingerprint-matching">Fingerprint Matching</a></li>
<li><a href="#fingercode">FingerCode</a></li>
</ul>
<!-- tocstop -->
<a id="more"></a>
<h2><span id="fingerprint-image-acquisition-systems">Fingerprint image acquisition systems</span></h2>
<p><a href="http://en.wikipedia.org/wiki/Fingerprint" target="_blank" rel="noopener">Fingerprint</a> matching (recognition) is the most popular biometric technique used in automatic personal identification. The main reason for the popularity of fingerprints as a form of identification is that the fingerprint of a person is unique and remains invariant with his or her age.</p>
<p>There are three kinds of sensing devices typically: <strong>optical sensors</strong>, <strong>solid-state sensors</strong> and <strong>ultrasound sensors</strong>.</p>
<p><strong>Optical sensors</strong></p>
<p>The finger touches the top side of a glass prism. The left side of the prism is illustrated through a diffused light. The light entering the prism is reflected at the valleys, and absorbed at the ridges. The lack of reflection allows the ridges to be discriminated from the valleys. The light rays exit from the right side of the prism and are focused through a lens onto an image sensor.</p>
<p><img src="/images/ridgeandvall.png"></p>
<p><strong>Solid-state sensors</strong></p>
<p>All silicon-based sensors consist of an array of pixels, each pixel being a tiny sensor itself. The user directly touches the surface of the silicon. The physical information is converted into <strong>electrical signals</strong> by using the capacitive sensor (other kinds of sensor can also be used, e.g., thermal, electric field and piezoelectric).</p>
<p>The capacitive sensor is a 2D array of micro-capacitor plates embedded in a chip. The other plate of each micro-capacitor is the finger skin itself.</p>
<p><img src="/images/ssdfinger.png"></p>
<p>Small electrical charges are created between the surface of the finger and each of the silicon plates when a finger is placed on the chip. The magnitude of these electrical charges depends on the distance between the fingerprint surface and the capacitance plates.</p>
<p><strong>Ultrasound sensors</strong></p>
<p>An ultrasound sensor is based on sending acoustic signals toward the fingertip and capturing the echo signal. The echo signal is used to compute the range image of the fingerprint and, subsequently, the ridge structure itself.</p>
<p><img src="/images/ultrasound.png"></p>
<h2><span id="minutiae">Minutiae</span></h2>
<p>An <strong>automatic fingerprint identification system</strong> (AFIS) consists of various processing stages.</p>
<p><img src="/images/afis.png"></p>
<p>In AFIS, the high-level structural features (<strong>ridges and valleys</strong>) are extracted from the fingerprint image for the purpose of representation and matching. The ridges and valleys in a fingerprint alternate, flowing in a local constant direction.</p>
<p><img src="/images/ridandval.png"></p>
<p>The ridges (or the valleys) exhibit anomalies of various kinds, such as ridge bifurcations, ridge endings, short ridges, and ridge crossovers. These features are called <a href="http://en.wikipedia.org/wiki/Minutiae" target="_blank" rel="noopener">minutiae</a>.</p>
<p>In a good quality rolled fingerprint image, there are about 70 to 80 minutia points and in a latent fingerprint the number of minutiae is much less (approximately 20 to 30 minutia points). Commercially available fingerprint identification systems typically use <strong>ridge bifurcations</strong> and <strong>ridge endings</strong> as features.</p>
<p><img src="/images/bifandending.png"></p>
<ul>
<li>A ridge ending is defined as the point where a ridge ends abruptly.</li>
<li>A ridge bifurcation is defined as the point where a ridge diverges into branch ridges.</li>
</ul>
<p>A critical step in fingerprint matching is to automatically and reliably <strong>extract minutiae</strong> from the input fingerprint images, which is a difficult task.</p>
<p><img src="/images/bifandend2.png"></p>
<h2><span id="fingerprint-enhancement">Fingerprint Enhancement</span></h2>
<p>Fingerprint images can be of very poor quality. An enhancement algorithm which can improve the clarity of the ridge structure is therefore necessary.</p>
<p>The flowchart of the fingerprint enhancement algorithm is shown below.</p>
<p><img src="/images/imgnorflow.png"></p>
<h3><span id="normalization">Normalization</span></h3>
<p>A gray-level fingerprint image <span class="math inline">\(I\)</span> is defined as an <span class="math inline">\(N \times N\)</span> matrix. At the <span class="math inline">\(i\)</span> th row and <span class="math inline">\(j\)</span> th column, the intensity of the pixel is <span class="math inline">\(I(i, j)\)</span>. It is assumed that the fingerprint images are scanned at a resolution of 500 dots per inch (dpi), which is the resolution recommended by FBI.</p>
<p>The mean and variance of a gray-level fingerprint image are defined as</p>
<p><img src="/images/imgnorma.png"></p>
<p>An input fingerprint image is normalized so that it has a pre-specified mean <span class="math inline">\(M_0\)</span> and variance <span class="math inline">\(\text{VAR}_0\)</span>.</p>
<p>The normalized image <span class="math inline">\(G(i,j)\)</span> is defined as <span class="math display">\[
G(i,j)=\begin{cases}
M_0+\sqrt{\frac{VAR_0(I(i,j)-M)^2}{VAR}} &amp;\text{if }I(i,j)&gt;M\\
M_0-\sqrt{\frac{VAR_0(I(i,j)-M)^2}{VAR}}&amp;\text{otherwise}\\
\end{cases}
\]</span> <img src="/images/imgnor2.png"></p>
<h3><span id="orientation-image-estimation">Orientation Image Estimation</span></h3>
<p>The orientation image (field) is estimated from the normalized input fingerprint image. By viewing a fingerprint image as an oriented image, a <strong>least-mean-square orientation estimation</strong> algorithm is used to estimate the local orientation.</p>
<p>The main steps are as follows.</p>
<p>[1] Divide the normalized image <span class="math inline">\(G\)</span> into blocks of size <span class="math inline">\(w \times w\)</span>.</p>
<p>[2] For each block, compute the gradients <span class="math inline">\(I_x\)</span> and <span class="math inline">\(I_y\)</span> at each pixel <span class="math inline">\((i , j)\)</span>.</p>
<p>[3] Estimate the local orientation of each block centered at pixel <span class="math inline">\((i, j)\)</span> using the following equations. <span class="math display">\[
\theta(i,j)=90^o+\frac{1}{2}\text{atan2}\left(\frac{V_1(i,j)}{V_2(i,j)}\right)
\]</span> where <span class="math display">\[
V_1(i,j)=\sum_{u=i-\frac{w}2}^{i+\frac{w}2}\sum_{v=j-\frac{w}2}^{j+\frac{w}2}2I_x(u,v)I_y(i,v)\\
V_2(i,j)=\sum_{u=i-\frac{w}2}^{i+\frac{w}2}\sum_{v=j-\frac{w}2}^{j+\frac{w}2}(I_x(u,v)^2-I_y(u,v)^2)\\
-180^o≤\text{atan2}(x)≤180^o
\]</span> [4] Due to the presence of noise, corrupted ridge and valley structures, minutiae, etc. in the input image, the estimated local ridge orientation may not always be correct. The local orientation image can be smoothed by using the low-pass smoothing filter and the concept of continuous vector field.</p>
<p><img src="/images/orienesti.png"></p>
<h3><span id="ridge-frequency-estimation">Ridge Frequency Estimation</span></h3>
<p>The gray levels along ridges and valleys can be modeled as a sinusoidal-shaped wave along a direction normal to the local ridge orientation.</p>
<p><img src="/images/ridgefreq.jpg"></p>
<p>Let <span class="math inline">\(G\)</span> be the normalized image and <span class="math inline">\(O\)</span> be the orientation image (field). For estimating the ridge frequency <span class="math inline">\(Ω\)</span> image,</p>
<ul>
<li><p>Step 1: divide <span class="math inline">\(G\)</span> into blocks of size <span class="math inline">\(w \times w\)</span>.</p></li>
<li><p>Step 2: for each block centered at pixel <span class="math inline">\((i, j)\)</span>, compute an oriented window of size <span class="math inline">\(w \times l\)</span> that is defined in the ridge coordinate system <span class="math inline">\((k, d)\)</span>. <img src="/images/ridgesys.png"></p></li>
<li><p>Step 3: for each block centered at pixel <span class="math inline">\((i, j)\)</span>, compute the x-signature, <span class="math inline">\(X[0], X[1], ..., X[l-1]\)</span>, of the ridges and valleys within the oriented window, where <span class="math display">\[
X[k]=\frac{1}w\sum_{d=0}^{w-1}G(u,v)\\
u=i+(d-\frac{w}2)\cos O(i,j)+(k+\frac{l}2)\sin O(i,j)\\
v=i+(d-\frac{w}2)\sin O(i,j)+(\frac{l}2-k)\cos O(i,j)
\]</span> <img src="/images/w2blabla.png"> <img src="/images/orientationdsa.png"></p></li>
</ul>
<p>The x-signature forms a discrete sinusoidal-shape wave, which has the same frequency as that of the ridges and valleys in the oriented window. Therefore, the <strong>frequency of ridges and valleys can be estimated from the x-signature</strong>.</p>
<p>Let <span class="math inline">\(T(i, j)\)</span> be the average number of pixels between two consecutive peaks in the x-signature, then the ridge frequency <span class="math inline">\(Ω(i, j)\)</span> is computed as <span class="math display">\[
\Omega(i,j)=\frac{1}{T(i,j)}
\]</span></p>
<h3><span id="region-mask-estimation">Region mask estimation</span></h3>
<p>A pixel (or a block) in an input fingerprint image can be either in a recoverable region or an unrecoverable region. Classification of pixels into recoverable and unrecoverable categories can be performed based on the assessment of the shape of the wave formed by the local ridges and valleys.</p>
<p><img src="/images/imgmaskest.png"></p>
<p>Three features are used to characterize the sinusoidal-shaped wave: amplitude, frequency, and variance.</p>
<p><img src="/images/imgmaskest2.png"></p>
<p>Typical fingerprint images where both recoverable and unrecoverable regions were manually labeled can be selected for region mask estimation. The above three features can be computed for each image.</p>
<p>Using the k-NN and clustering algorithms, each <span class="math inline">\(w \times w\)</span> block in an input fingerprint image can be classified into a recoverable or an unrecoverable block.</p>
<h3><span id="gabor-filter">Gabor Filter*</span></h3>
<p>The configurations of parallel ridges and valleys with well-defined frequency and orientation in a fingerprint image provide useful information which helps in removing undesired noise.</p>
<p>A special (bandpass) filter, namely <strong>Gabor filter</strong>, that is tuned to the corresponding frequency and orientation can efficiently remove the undesired noise and preserve the true ridge and valley structures.</p>
<p>Gabor filters have both frequency-selective and orientation-selective properties and are used for removing noise and preserving true ridge or valley structures.</p>
<p>The even-symmetric Gabor filter has the following general form:</p>
<p><img src="/images/gaborfilter.png"></p>
<p>The frequency of the filter <span class="math inline">\(f\)</span> is completely determined by the local ridge frequency and the Gabor filter orientation is determined by the local ridge orientation.</p>
<p><img src="/images/gabororien.png"></p>
<p>The ridge pixels are assigned a value '1' (white) and the remaining pixels are assigned a value '0' (black) in the resulting binary ridge image.</p>
<p><img src="/images/imgseg.png"></p>
<p>Once the ridges are located, <strong>directional smoothing</strong> is applied to smooth the ridges. A 3×7 mask is placed along the orientation field for each window. The mask containing all '1's enables us to count the number of '1's in the mask area.If the count of '1's is more than 25% of the total number of pixels, the ridge point is retained.</p>
<p><strong>Extracting minutiae</strong></p>
<p>Locating minutia points in the thinned image is relatively easy.</p>
<p>A count of the number of 'on' neighbors at a point of interest in a <span class="math inline">\(3\times 3\)</span> window is sufficient for this purpose.</p>
<ul>
<li>A ridge end point has only neighbor in the window.</li>
<li>A ridge bifurcation has at least three neighbors.</li>
</ul>
<p>Some post-processing can be performed to further improve detection quality.</p>
<p><img src="/images/endandbif3.jpg"></p>
<p><img src="/images/minuextra.png"></p>
<h2><span id="fingerprint-matching">Fingerprint Matching</span></h2>
<p>Matching a query fingerprint and a database fingerprint is equivalent to matching their minutia sets. Each database fingerprint minutia, <span class="math inline">\(p\)</span>, is examined to determine whether there is a corresponding query fingerprint minutia, <span class="math inline">\(q\)</span>.</p>
<p>There are three steps.</p>
<ol type="1">
<li>Registration</li>
<li>Minutia paring</li>
<li>Matching score computation</li>
</ol>
<p><strong>Registration via Hough Transform</strong></p>
<p>The input to the registration algorithm consists of two sets of minutia points <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>. <span class="math inline">\(|P|\)</span> and <span class="math inline">\(|Q|\)</span> represent the sizes of point sets <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> respectively. <span class="math display">\[
P=\{(p_x^1,p_y^1,\alpha^1),...,(p_x^{|P|},p_y^{|P|},\alpha^{|P|})\}\\
Q=\{(q_x^1,q_y^1,\beta^1),...,(q_x^{|Q|},q_y^{|Q|},\beta^{|Q|})\}
\]</span> Each minutia has three components: x-coordinate, y-coordinate and orientation of the minutia. Each minutia in <span class="math inline">\(P\)</span> is <strong>rotated, scaled and translated</strong> for matching against a minutia in <span class="math inline">\(Q\)</span>.</p>
<p>The usual <strong>Hough transform</strong> for line detection can be generalized for point matching.</p>
<p><img src="/images/regishoughtra.png"></p>
<p>The transform has maximum value of <span class="math inline">\(A\)</span> means that it can match as much points as possible.</p>
<p><img src="/images/imgregriscomp.png"></p>
<p><strong>Minutia pairing and score computation</strong></p>
<p>After minutia registration, the minutiae need to be paired. Two minutiae are said to be paired or matched if their components <span class="math inline">\((x, y,θ)\)</span> are equal with some tolerance after registration.</p>
<p><img src="/images/minutiamatch.png"></p>
<p>The matching algorithm is based on finding the number of paired minutiae between each database fingerprint and the query fingerprint.</p>
<p>In order to reduce the amount of computation, the matching algorithm takes into account only those minutiae that fall within a common bounding box. The common bounding box is the intersection of the bounding box for query and reference (database) fingerprints. Once the count of matching minutiae is obtained, a matching score is computed. The matching score is used for deciding the degree of match. Finally, a set of top ten scoring reference fingerprints is obtained as a result of matching.</p>
<h2><span id="fingercode">FingerCode</span></h2>
<p><em>FingerCode</em> is a new representation for the fingerprints which yields a <strong>relatively short, fixed length code</strong> suitable for matching as well as storage on a smartcard.</p>
<p>The matching reduces to finding the <strong>Euclidean distance</strong> between these <em>FingerCodes</em> and hence the matching is very fast and the representation is amenable to indexing.</p>
<p>The FingerCode partitions the region of interest of the given fingerprint image with respect to a reference point. A feature vector is composed of an ordered enumeration of features extracted from the information contained in each sector specified by the tessellation.</p>
<p><img src="/images/fingercode1.png"></p>
<p>The feature elements capture the local information by using the <strong>Gabor filterbank</strong>. The ordered enumeration of the tessellation captures the <strong>invariant global relationships</strong> among the local patterns.</p>
<p>These features capture both the global pattern of ridges and valleys and the local characteristics. Matching is based on the Euclidean distance between the FingerCodes.</p>
<p>There are four steps for extracting the FingerCode.</p>
<ol type="1">
<li>Determining the reference point for the fingerprint image.</li>
<li>Partitioning the region around the reference point.</li>
<li>Filtering the region of interest in eight different directions using a bank of Gabor filters.</li>
<li>Computing the <strong>average absolute deviation</strong> (AAD) from the mean of gray values in individual sectors in filtered images to define the <em>FingerCode</em> (the feature vector) for matching.</li>
</ol>
<p><strong>Determining the reference point</strong></p>
<p><img src="/images/fingercode2.png"></p>
<p>Given an input fingerprint image, there are seven steps for finding the reference point.</p>
<ol type="1">
<li><p>Estimate the orientation field <span class="math inline">\(O\)</span> using a window size of <span class="math inline">\(w\times w\)</span>.</p></li>
<li><p>Smooth the orientation field.</p></li>
<li><p>Compute the sine component <span class="math inline">\(E\)</span> of the orientation field <span class="math inline">\(O\)</span>.<br>
<span class="math display">\[
E(i,j)=\sin O(i,j)
\]</span></p></li>
<li><p>Initialize <span class="math inline">\(A\)</span>, a label image used to indicate the reference point.</p></li>
<li><p>For each pixel <span class="math inline">\(E(i, j)\)</span>, integrate pixel intensities in regions <span class="math inline">\(R_I\)</span> and <span class="math inline">\(R_{II}\)</span>, and assign the corresponding pixels in <span class="math inline">\(A\)</span> according to the value of their difference. <span class="math display">\[
A(i,j)=\sum_{R_I}E(i,j)-\sum_{R_{II}}E(i,j)
\]</span> <img src="/images/fingercode4.png"></p></li>
<li><p>Find the maximum value in <span class="math inline">\(A\)</span> and assign its coordinate to the reference point.</p></li>
<li><p>Repeat steps 1-6 by using a window size <span class="math inline">\(w&#39;\times w&#39;\)</span>, where <span class="math inline">\(w&#39; &lt; w\)</span>, and restrict the search for the reference point in step 6 in a local neighborhood of the detected reference point.</p></li>
</ol>
<p>The geometry of regions <span class="math inline">\(R_I\)</span> and <span class="math inline">\(R_{II}\)</span> is designed to capture the maximum curvature in concave ridges.</p>
<p><img src="/images/fingercode3.png"></p>
<p><strong>Partitioning the region around the reference point</strong></p>
<p>Given the detected reference point, the input fingerprint image is partitioned into 80 sectors.</p>
<p><img src="/images/fingercode5.png"></p>
<p><strong>Filtering the region of interest</strong></p>
<p>A minutia point can be viewed as an anomaly in locally parallel ridges and it is the information that is captured by using the Gabor filters.</p>
<p>Before filtering the fingerprint image, <strong>image normalization</strong> is performed separately for each sector with <span class="math inline">\(M_0\)</span> and <span class="math inline">\(VAR_0\)</span>.</p>
<p>An even symmetric Gabor filter is given <a href="#gabor-filter*">the Gabor filer setction</a>. The filter frequency <span class="math inline">\(f\)</span> can be set to the average ridge frequency. The average ridge frequency is the reciprocal of the average inter-ridge distance, which is around 10 pixels in a 500 dpi fingerprint image. Eight different values (<span class="math inline">\(0^o\)</span>, <span class="math inline">\(22.5^o\)</span>, <span class="math inline">\(45^o\)</span>, <span class="math inline">\(67.5^o\)</span>, <span class="math inline">\(90^o\)</span>, <span class="math inline">\(112.5^o\)</span>, <span class="math inline">\(135^o\)</span> and <span class="math inline">\(157.5^o\)</span>) are used for the direction θ with respect to the x-axis.</p>
<p>A fingerprint convolved with a <span class="math inline">\(0^o\)</span>-oriented filter accentuates those ridges which are parallel to the x-axis and smoothes the ridges in the other directions. These eight directional-sensitive filters capture <strong>most of the global ridge directionality information</strong> as well as the <strong>local ridge characteristics</strong> present in a fingerprint.</p>
<p><img src="/images/fingercode6.png"></p>
<p><strong>Compute AAD</strong></p>
<p>Let <span class="math inline">\(F_{iθ}(x, y)\)</span> be the θ-direction filtered image for sector <span class="math inline">\(S_i\)</span>. The feature value <span class="math inline">\(V_{iθ}\)</span> is the average absolute deviation (AAD) from the mean which is defined as <span class="math display">\[
V_{i\theta}=\frac{1}{n_i}\sum_{(x,y)\in S_i}|F_{i\theta}(x,y)-P_{i\theta}|
\]</span> where <span class="math inline">\(P_{i\theta}=\frac{1}{n_i}\sum_{(x,y)\in S_i}F_{i\theta}(x,y)\)</span>; <span class="math inline">\(n_i\)</span> is the number of pixels in <span class="math inline">\(S_i\)</span>.</p>
<p><img src="/images/fingercode7.png"></p>
<p>The average absolute deviation of each sector in each of the eight filtered images defines the components of the feature vector. Fingerprint matching is based on finding the <strong>Euclidean distance</strong> between the corresponding <em>FingerCodes</em>.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/笔记/">笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Recognition-System/">Recognition System</a><a href="/tags/fingerprints/">fingerprints</a><a href="/tags/fingercode/">fingercode</a><a href="/tags/hough-transform/">hough transform</a><a href="/tags/minutiae/">minutiae</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>
<script type="text/javascript">
  var $config = {
    image: "icon.png",
  };
  socialShare('.social-share-cs', $config);
</script>



<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80MTI4MC8xNzgyOA==">
	<script type="text/javascript">
		(function (d, s) {
			var j, e = d.getElementsByTagName(s)[0];

			if (typeof LivereTower === 'function') {
				return;
			}

			j = d.createElement(s);
			j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
			j.async = true;

			e.parentNode.insertBefore(j, e);
		})(document, 'script');
	</script>
	<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->


      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2019
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>