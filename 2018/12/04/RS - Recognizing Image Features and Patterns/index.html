<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    RS - Recognizing Image Features and Patterns | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="RS - Recognizing Image Features and Patterns">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script>
  <script type="text/javascript" src="/js/search.js"></script>
  <script type="text/javascript" src="/js/jquery.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/search">搜索</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      RS - Recognizing Image Features and Patterns</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/12/04/RS - Recognizing Image Features and Patterns/" rel="bookmark">
        <time class="entry-date published" datetime="2018-12-04T13:56:09.000Z">
          2018-12-04
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p>CSIT 5401 Recognition System Notes. 识别系统复习笔记。</p>
<!-- toc -->
<ul>
<li><a href="#laplacian-point-detector">Laplacian Point Detector</a></li>
<li><a href="#line-detector">Line Detector</a></li>
<li><a href="#edge-detectors">Edge Detectors</a>
<ul>
<li><a href="#gradient-operator">Gradient Operator</a></li>
<li><a href="#marr-hildreth-edge-detector">Marr-Hildreth Edge Detector</a></li>
</ul></li>
<li><a href="#scale-invariant-feature-transform-sift">Scale Invariant Feature Transform (SIFT)</a></li>
<li><a href="#visual-saliency-and-local-entropy">Visual Saliency and Local Entropy</a></li>
<li><a href="#corner-detector">Corner Detector</a></li>
</ul>
<!-- tocstop -->
<a id="more"></a>
<h2><span id="laplacian-point-detector">Laplacian Point Detector</span></h2>
<p>There are three different types of intensity discontinuities in a digital image:</p>
<ul>
<li>Point (Isolated Point)</li>
<li>Line</li>
<li>Edge (Ideal, Ramp and Roof)</li>
</ul>
<p>Intensity discountinuity is detected based on the mask response <span class="math inline">\(R\)</span> within a pre-defined window, e.g. <span class="math inline">\(3\times3\)</span>: <span class="math display">\[
R=\sum_{i=1}^9w_iz_i
\]</span> where <span class="math inline">\(w_i\)</span> represent weights within a pre-defined window; <span class="math inline">\(z_i\)</span> represent intensity values.</p>
<p>If <span class="math inline">\(|R|≥T\)</span> , then a point has been detected. This point is the location on which the mask is <strong>centred</strong>, where <span class="math inline">\(T\)</span> is a non-negative threshold.</p>
<p>The mask below is the <strong>Laplacian mask</strong> for detecting point. Sum of all weights is zero to make sure that there is no response at a flat region (constant intensity region).</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">1</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">-8</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>The Laplacian is given as (<span class="math inline">\(f\)</span> is input image): <span class="math display">\[
\triangledown^2f(x,y)=\frac{\partial^2f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}
\]</span> where <span class="math display">\[
\frac{\partial^2f}{\partial x^2}=f(x+1,y)-2f(x,y)+f(x-1,y)\\
\frac{\partial^2 f}{\partial y^2}=f(x,y+1)-2f(x,y)+f(x,y-1)
\]</span> The discrete implementation of the Laplacian operator is given as: <span class="math display">\[
\triangledown^2f(x,y)=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)
\]</span> Below are several Laplacian masks:</p>
<p><img src="/images/pd.png"></p>
<h2><span id="line-detector">Line Detector</span></h2>
<p>A line is detected when more than one aligned, connected points are detected or, the <strong>response</strong> of line mask <strong>is greater than some threshold</strong>. The below are line masks for detecting lines (1 pixel thick) in 4 different specific directions:</p>
<p><img src="/images/ld.png"></p>
<p>If we want to detect a line in a specified direction, then we should use the mask associated with that direction and threshold its output responses.</p>
<p>If 4 line masks are used, then the final response is equal to the <strong>largest</strong> response among the masks: <span class="math display">\[
R = \max\left(|R_{horizontal}|, |R_{45}|, |R_{vertical}|, |R_{-45}|\right)
\]</span> Example code (Matlab):</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">f = imread(<span class="string">'xxx.png'</span>); <span class="comment">% read image</span></span><br><span class="line">w = [<span class="number">2</span> <span class="number">-1</span> <span class="number">-1</span>; <span class="number">-1</span> <span class="number">2</span> <span class="number">-1</span>; <span class="number">-1</span> <span class="number">-1</span> <span class="number">2</span>]; <span class="comment">% mask</span></span><br><span class="line">g = <span class="built_in">abs</span>(imfilter(double(f),w)); <span class="comment">% mask responses</span></span><br></pre></td></tr></table></figure>
<h2><span id="edge-detectors">Edge Detectors</span></h2>
<p>Edge is the boundary of regions. The boundary has meaningful discontinuities in grey intensity level. There are three types of edges: ideal edge (left), ramp edge (middle) and roof edge (right).</p>
<p><img src="/images/3t.png"></p>
<p>For an <strong>ideal edge</strong> (step edge, left), an edge is a collection of connected pixels on the region boundary. Ideal edges can occur over the distance of 1 pixel.</p>
<p><strong>Roof edges</strong> (right) are models of lines through a region, with the base (width) of a roof edge being determined by the thickness and sharpness of the line. In the limit, when its base is 1 pixel wide, a roof edge becomes a 1 pixel thick line running through a region in an image. Roof edges can represent thin features, e.g., roads, line drawings, etc.</p>
<p>For a <strong>ramp edge</strong> (middle):</p>
<ul>
<li>edge point is any point contained in the ramp</li>
<li>edge length is determined by the length of the ramp</li>
<li>the slope of the ramp is inversely proportional to the degree of blurring in the edge</li>
<li>the <strong>first derivative</strong> of the intensity profile is positive at the points of transition into and out of the ramp (we move from left to right)</li>
<li>the <strong>second derivative</strong> of the intensity profile is positive at the transition associated with the dark side of the edge, and negative at the transition associated with the light side of the edge</li>
</ul>
<p><img src="/images/ramp.png"></p>
<p>The <strong>magnitude of the first derivative</strong> can be used to detect the presence of an edge. The <strong>sign of the second derivative</strong> can be used to determine whether an edge pixel lies on the dark or light side of an edge. The <strong>zero-crossing property</strong> of the second derivative is very useful for <strong>locating</strong> the centres of thick edge.</p>
<p>However, fairly little noise can have a significant impact on the first and second derivatives used for edge detection in images. <strong>Image smoothing</strong> is commonly used prior to the edge detection so that the estimations of the two derivatives can be more accurate.</p>
<h3><span id="gradient-operator">Gradient Operator</span></h3>
<p>The computation of the gradient of an image is based on obtaining the partial derivatives <span class="math inline">\(G_x = \partial f/\partial x\)</span> and <span class="math inline">\(G_y = \partial f/\partial y\)</span> at every pixel location (x,y). The gradient direction and gradient magnitude are <span class="math display">\[
\tan^{-1}\left(\frac{G_y}{G_x}\right)\\
|\triangledown f|\approx|G_x|+|G_y|
\]</span> <img src="/images/z.png"></p>
<p>The are several ways to approximate the partial derivatives:</p>
<ul>
<li>Roberts cross-gradient operators
<ul>
<li><span class="math inline">\(G_x = (z_9-z_5)\)</span></li>
<li><span class="math inline">\(G_y=(z_8-z_6)\)</span></li>
</ul></li>
<li>Prewitt operators
<ul>
<li><span class="math inline">\(G_x=(z_7+z_8+z_9)-(z_1+z_2+z_3)\)</span></li>
<li><span class="math inline">\(G_y=(z_3+z_6+z_9)-(z_1+z_4+z_7)\)</span></li>
</ul></li>
<li>Sobel operators
<ul>
<li><span class="math inline">\(G_x=(z_7+2z_8+z_9)-(z_1+2z_2+z_3)\)</span></li>
<li><span class="math inline">\(G_y=(z_3+2z_6+z_9)-(z_1+2z_4+z_7)\)</span></li>
</ul></li>
</ul>
<p><img src="/images/ed.png"></p>
<p>Below are diagonal edge masks for detecting discontinuities in the <strong>diagonal directions</strong>.</p>
<p><img src="/images/diag.png"></p>
<h3><span id="marr-hildreth-edge-detector">Marr-Hildreth Edge Detector</span></h3>
<p>The Laplacian of an image f(x,y) at location (x,y) is defined as <span class="math display">\[
\triangledown^2f(x,y)=\frac{\partial^2f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}
\]</span> There are two approximations: <span class="math display">\[
\triangledown^2f=4z_5-(z_2+z_4+z_6+z_8)\\
\triangledown^2f=8z_5-(z_1+z_2+z_3+z_4+z_6+z_7+z_8+z_9)
\]</span> The Laplacian generally is <strong>not</strong> used in its original form for edge detection (based on zero-crossing property) because it is <strong>unacceptably sensitive to noise</strong>. We smooth the image by using a Gaussian blurring function <span class="math inline">\(G(r)\)</span>, where <span class="math inline">\(r\)</span> = radius, before we apply the Laplacian operator: <span class="math display">\[
G(r)=\exp(\frac{-r^2}{2\sigma^2})
\]</span> The <strong>Laplacian of a Gaussian (LoG)</strong> operator is defined by <span class="math display">\[
\text{LoG}(f)=\triangledown^2(f\ast G)=f\ast(\triangledown^2G)
\]</span> Using the LoG, the location of edges can be detected reliably based on the zero-crossing values because image noise level is reduced by the Gaussian function.</p>
<p><strong>Marr-Hildreth algorithm</strong></p>
<ol type="1">
<li><p>Filter the input with an n-by-n Gaussian blurring filter <span class="math inline">\(G(r)\)</span>.</p></li>
<li><p>Compute the Laplacian of the image resulting from Step 1 using one of the following 3-by-3 masks.</p>
<p><img src="/images/mha.png"></p></li>
<li><p>Find the <strong>zero crossings</strong> of the image from Step 2.</p>
<ol type="1">
<li>A zero crossing at a pixel implies that the signs of at least two of its opposing neighboring pixels must be different.</li>
<li>There are four cases to test: left/right, up/down, and the two diagonals.</li>
<li>For testing, the signs of the two opposing neighboring pixels must be different and their absolute Laplacian values must be larger than or equal to some threshold.</li>
<li>If yes, we call the current pixel a zero-crossing pixel.</li>
</ol></li>
</ol>
<p><img src="/images/mhed.png"></p>
<h2><span id="scale-invariant-feature-transform-sift">Scale Invariant Feature Transform (SIFT)</span></h2>
<p>SIFT is useful for finding distinctive patches (<strong>keypoints</strong>) in images and transforming keypoints (locations) into <strong>features vectors</strong> for recognition tasks.</p>
<p>SIFT consists of four steps:</p>
<ol type="1">
<li>Scale-space extrema detection</li>
<li>Keypoint localization</li>
<li>Orientation assignment</li>
<li>Keypoint descriptor</li>
</ol>
<p>SIFT feature is</p>
<ul>
<li>invariant to image rotation and scale</li>
<li>partially invariant to change in illumination and 3D camera viewpoint</li>
</ul>
<p>The method is a cascade (one step followed by the other step) filtering approach, in which more computationally expensive operations are applied only at locations that pass an initial test.</p>
<p><strong>[1] Scale-space extrema detection</strong></p>
<p>Candidate locations are identified by searching for stable features across all scales in the scale space. The <strong>scale space</strong> of an image is defined as <span class="math display">\[
L(x,y,\sigma)=G(x,y,\sigma)\ast I(x,y)\\
G(x,y,\sigma)=\frac{1}{2\pi\sigma^2}\exp(-\frac{x^2+y^2}{2\sigma^2})
\]</span> , where <span class="math inline">\(\ast\)</span> is the convolution operator (filtering operation), the variable-scale Gaussian is <span class="math inline">\(G(x, y, \sigma)\)</span> and the image is <span class="math inline">\(I(s, y)\)</span>.</p>
<p>The difference between two nearby scales separated by a constant multiplicative factor <span class="math inline">\(k\)</span> is <span class="math display">\[
\begin{align}
D(x,y,\sigma)&amp;=L(x,y,k\sigma)-L(x,y,\sigma)\\
&amp;=\left(G(x,y,k\sigma)-G(x,y,\sigma)\right)*I(x,y)
\end{align}
\]</span> The DoG function provides a close and efficient approximation to the scale-normalized Laplacian of Gaussian (LoG).</p>
<p><img src="/images/dog.jpg"></p>
<p>Local <strong>extrema</strong> (maxima and minima) of <span class="math inline">\(D( x, y, σ)\)</span> are detected by comparing the values of its 26 neighbors, including 9 from the above image, 9 from bottom image and 8 from the current image.It is selected only if the center pixel is larger than <strong>all</strong> of these neighbors (26 neighbors) or smaller than all of them.</p>
<p><img src="/images/extra.png"></p>
<p><img src="/images/exdet.png"></p>
<p><strong>[2] Keypoint localization</strong></p>
<p>Once a keypoint candidate has been found by comparing a pixel to its neighbors, the next step is to determine whether the keypoint is selected based on <strong>local contrast and localization along edge</strong>. Therefore, the keypoints will be rejected if these points have low contrast.</p>
<p>All extrema are discarded if <span class="math inline">\(|D(x)|&lt;0.03\)</span> (minimum contrast), where <span class="math inline">\(x\)</span> represents a relative image position with maximum value of <span class="math inline">\(D\)</span>. The keypoints will be rejected if these points are poorly localized along an edge (determined based on the <strong>ratio of principle curvatures</strong>).</p>
<p><img src="/images/kl.png"></p>
<p><strong>[3] Orientation assignment</strong></p>
<p>Each corresponding keypoint in <span class="math inline">\(L\)</span> is assigned a dominant orientation. The keypoint descriptor can be represented relative to this orientation and therefore achieve invariance to image rotation.</p>
<p>The gradient magnitude and orientation are <span class="math display">\[
m(x,y)=\sqrt{(L(x+1,y)-L(x-1,y))^2+(L(x,y+1)-L(x,y-1))^2}\\
\theta(x,y)=\tan^{-1}\left(\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}\right)
\]</span> An <strong>orientation histogram</strong> is formed from the gradient orientations within a region around the keypoint. The orientation histogram has 36 bins covering the 360 degree range of orientations, 10 degrees per bin. Each sample added to the histogram is weighted by its gradient magnitude and by a Gaussian-weighted circular window (with σ that is 1.5 times that of the scale of the keypoint, and it is related to effective window size). Peak in the orientation histogram corresponds to dominant direction of the local gradients.</p>
<p><img src="/images/IMG_2E4DF1ED416A-1.jpg"></p>
<p><strong>[4] Descriptor for local image region</strong></p>
<p><img src="/images/kd.png"></p>
<p>For each detected keypoint, a local image descriptor is computed. It is partially invariant to change in illumination and 3D viewpoint.</p>
<p>In order to achieve orientation invariance, the coordinates and the gradient orientations are rotated relative to the keypoint orientation. A Gaussian weighting function with σ equal to one half the width of the descriptor window is used to assign a weight to the magnitude of each sample point.</p>
<p>Each subregion generates an orientation histogram with 8 orientation bins. Therefore, for each keypoint, if <span class="math inline">\(2\times2\)</span> descriptor is used, a feature vector can be formed with <span class="math inline">\(2\times2\times8 = 32\)</span> elements; if <span class="math inline">\(4\times4\)</span> descriptor is used, there will be <span class="math inline">\(4\times4\times8 = 128\)</span> elements in a feature vector. The <strong>optimal</strong> setting is 4x4 subregions and 8 orientation bins (the above picture).</p>
<p><strong>Feature matching</strong></p>
<p><img src="/images/IMG_043BB8EF0160-1.jpg"></p>
<p>Matlab Implementation</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% num = match(image1, image2)</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% This function reads two images, finds their SIFT features, and</span></span><br><span class="line"><span class="comment">%   displays lines connecting the matched keypoints.  A match is accepted</span></span><br><span class="line"><span class="comment">%   only if its distance is less than distRatio times the distance to the</span></span><br><span class="line"><span class="comment">%   second closest match.</span></span><br><span class="line"><span class="comment">% It returns the number of matches displayed.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% Example: match('scene.pgm','book.pgm');</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">num</span> = <span class="title">match</span><span class="params">(image1, image2)</span></span></span><br><span class="line"><span class="comment">% Find SIFT keypoints for each image</span></span><br><span class="line">[im1, des1, loc1] = sift(image1);</span><br><span class="line">[im2, des2, loc2] = sift(image2);</span><br><span class="line"><span class="comment">% For efficiency in Matlab, it is cheaper to compute dot products between</span></span><br><span class="line"><span class="comment">%  unit vectors rather than Euclidean distances.  Note that the ratio of</span></span><br><span class="line"><span class="comment">%  angles (acos of dot products of unit vectors) is a close approximation</span></span><br><span class="line"><span class="comment">%  to the ratio of Euclidean distances for small angles.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% distRatio: Only keep matches in which the ratio of vector angles from the</span></span><br><span class="line"><span class="comment">%   nearest to second nearest neighbor is less than distRatio.</span></span><br><span class="line">distRatio = <span class="number">0.6</span>;</span><br><span class="line"><span class="comment">% For each descriptor in the first image, select its match to second image.</span></span><br><span class="line">des2t = des2';</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">size</span>(des1,<span class="number">1</span>)</span><br><span class="line">    dotprods = des1(<span class="built_in">i</span>,:) * des2t; <span class="comment">% compute orientation of des1[i] and des2[j] for all j</span></span><br><span class="line">    [vals, indx] = <span class="built_in">sort</span>(<span class="built_in">acos</span>(dotprods)); <span class="comment">% Take inverse cosine and sort results</span></span><br><span class="line">    <span class="comment">% Check if nearest neighbor has angle less than distRatio times 2nd.</span></span><br><span class="line">    <span class="keyword">if</span> (vals(<span class="number">1</span>) &lt; distRatio * vals(<span class="number">2</span>))</span><br><span class="line">      match(<span class="built_in">i</span>) = indx(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      match(<span class="built_in">i</span>) = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/IMG_DAD55EC9C383-1.jpg"></p>
<h2><span id="visual-saliency-and-local-entropy">Visual Saliency and Local Entropy</span></h2>
<p>To be continued...</p>
<h2><span id="corner-detector">Corner Detector</span></h2>
<p>To be continued...</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Recognition-System/">Recognition System</a><a href="/tags/Prewitt/">Prewitt</a><a href="/tags/Sobel/">Sobel</a><a href="/tags/Marr-Hildreth-Edge-Detector/">Marr-Hildreth Edge Detector</a><a href="/tags/SIFT/">SIFT</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>
<script type="text/javascript">
  var $config = {
    image: "icon.png",
  };
  socialShare('.social-share-cs', $config);
</script>



<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80MTI4MC8xNzgyOA==">
	<script type="text/javascript">
		(function (d, s) {
			var j, e = d.getElementsByTagName(s)[0];

			if (typeof LivereTower === 'function') {
				return;
			}

			j = d.createElement(s);
			j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
			j.async = true;

			e.parentNode.insertBefore(j, e);
		})(document, 'script');
	</script>
	<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->


      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2018
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>