<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    Recognizing Irises | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="Recognizing Irises">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script>
  <script type="text/javascript" src="/js/search.js"></script>
  <script type="text/javascript" src="/js/jquery.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/search">搜索</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      Recognizing Irises</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/12/04/RS - Recognizing Irises/" rel="bookmark">
        <time class="entry-date published" datetime="2018-12-04T13:56:09.000Z">
          2018-12-04
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p>HKUST CSIT5401 Recognition System lecture notes. 识别系统复习笔记。</p>
<!-- toc -->
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#image-acquisition-systems">Image Acquisition Systems</a></li>
<li><a href="#iris-localization">Iris localization</a></li>
<li><a href="#pattern-matching">Pattern Matching</a><ul>
<li><a href="#alignment-registration">Alignment (Registration)</a></li>
<li><a href="#representation">Representation</a></li>
<li><a href="#goodness-of-match">Goodness of Match</a></li>
<li><a href="#decision-fld">Decision (FLD)</a></li>
</ul>
</li>
<li><a href="#hough-transform">Hough Transform</a></li>
</ul>
<!-- tocstop -->
<a id="more"></a>
<h2><span id="introduction">Introduction</span></h2><p>Face recognition and iris recognition are non-invasive method for verification and identification of people. In particular, the spatial patterns that are apparent in the human iris are highly distinctive to an individual.</p>
<p><img src="/images/iries.png" alt=""></p>
<p><strong>Schematic diagram of iris recognition</strong></p>
<p><img src="/images/iries_recog.png" alt=""></p>
<h2><span id="image-acquisition-systems">Image Acquisition Systems</span></h2><p>One of the major challenges of automated iris recognition is to capture a high-quality image of the iris while remaining non-invasive to the human operator.</p>
<p>There are three concerns while acquiring iris images:</p>
<ul>
<li>To support recognition, it is desirable to acquire images of the iris with sufficient resolution and sharpness.</li>
<li>It is important to have good contrast in the interior iris pattern without resorting to a level of illumination that annoys the operator, i.e., adequate intensity of source constrained by operator comfort with brightness.</li>
<li>These images must be well framed (i.e., centered) without unduly constraining the operator.</li>
</ul>
<p><strong>The Daugman system</strong></p>
<p>The Daugman system captures images with the iris diameter typically between 100 and 200 pixels from a distance of 15-46cm.</p>
<p>The system makes use of an LED-based point light source in conjunction with a standard video. By carefully positioning of the point source below the operator, reflections of the light source off eyeglasses can be avoided in the imaged iris.</p>
<p><img src="/images/daugman.png" alt=""></p>
<p>The Daugman system provides the operator with live video feedback via a tiny liquidcrystal display placed in line with the camera’s optics via a beam splitter. This allows the operator to see what the camera is capturing and to adjust his position accordingly.</p>
<p><strong>The Wildes system</strong></p>
<p>The Wildes system images the iris with approximately 256 pixels across the diameter from 20cm. The system makes use of a diffused source and polarization in conjunction with a low-light level camera.</p>
<p>The use of matched <strong>circular polarizer</strong> at the light source and camera essentially eliminates the specular reflection of the light source.</p>
<p><img src="/images/wildes.png" alt=""></p>
<p>The coupling of a low light level camera with a diffused illumination allows for a level of illumination that is entirely unobjectionable to human operators.</p>
<p>The relative sizes and positions of the square contours are chosen so that when the eye is in an appropriate position, the squares overlap and appear as one to the operator.</p>
<h2><span id="iris-localization">Iris localization</span></h2><p><img src="/images/iries_loc.png" alt=""></p>
<p>Image acquisition will capture the iris as part of a larger image that also contains data derived from the immediately surrounding eye region. For example, eyelashes, upper eyelid, lower eyelid and sclera. Therefore, prior to performing iris pattern matching, it is important to <strong>localize</strong> that portion of the acquired image that corresponds to an iris.</p>
<p><strong>The Wildes system</strong> makes use of the <strong>first derivatives</strong> of image intensity to signal the location of edges that correspond to the borders of the iris.</p>
<ul>
<li>Step 1: The image intensity information is converted into binary edge-map.</li>
<li>Step 2: The edge points vote to particular contour parameter values.</li>
</ul>
<p><strong>Step 1</strong></p>
<p>The edge map is recovered via <strong>gradient-based edge detection</strong>. This operation consists of thresholding the magnitude of the image intensity gradient magnitude. $I$ is the intensity and (x, y) are the image coordinates.<br>$$<br>\text{Gradient magnitude }|\triangledown G(x, y)\ast I(x, y)|\\<br>\text{2D Gaussian function } G(x, y)=\frac{1}{2\pi\sigma^2}\exp(\frac{(x-x_0)^2+(y-y_0)^2}{2\sigma^2})<br>$$<br><img src="/images/iris_edge.png" alt=""></p>
<p><strong>Step 2</strong></p>
<p>The voting procedure is realized via the <a href="#hough-transform">Hough transform</a>. For circular limbic or pupillary boundaries and a set of recovered edge points, a Hough transform is defined as follows.</p>
<p>Edge points $(x_j, y_j)$ for $j = 1, …, n$:<br>$$<br>H(x_c, y_c, r)=\sum_{j=1}^nh(x_j,y_j,x_c,y_c,r)<br>$$<br>where<br>$$<br>h(x_j, y_j, x_c, y_c, r)=\begin{cases}<br>1, \text{ if } g(x_j, y_j, x_c, y_c, r)=0\\<br>0, \text{ otherwise}<br>\end{cases}\\<br>g(x_j, y_j, x_c, y_c, r)=(x_j-x_c)^2+(y_j-y_c)^2-r^2<br>$$<br>For every parameter triple $(x_c, y_c, r)$ that represents a circle through the edge point $(x_j, y_j)$,<br>$$<br>g(x_j, y_j, x_c, y_c, r)=0<br>$$<br><img src="/images/wildescir.png" alt=""></p>
<p>The parameter triple that <strong>maximizes</strong> the Hough space $H$ is common to the largest number of edge points and is a reasonable choice to represent the contour of interest.</p>
<hr>
<p>The limbus and pupil are modeled with <strong>circular contour models</strong>. </p>
<p><strong>The Daugman system</strong> fits the <strong>circular contours</strong> via gradient ascent on the parameters so as to maximize<br>$$<br>\left|\frac{\partial}{\partial r}G(r)\ast\oint_{x_c,y_c,r}\frac{I(x,y)}{2\pi r}ds\right|<br>$$<br>where $G(r)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(r-r_0)^2}{2\sigma^2})$; $r_0$ is the center.</p>
<p>The first part of the equation is to perform Gaussian smoothing; while the second part is computing the average intensity along the circle.</p>
<p><img src="/images/IMG_66247EE67551-1.jpg" alt=""></p>
<p>In order to incorporate directional tuning of the image derivative, the arc of integration $ds$ is restricted to the left and right quadrants (i.e., near vertical edges) when fitting the <em>limbic boundary</em>.</p>
<p>This arc is considered over a fuller range when fitting the <em>pupillary boundary</em>.</p>
<h2><span id="pattern-matching">Pattern Matching</span></h2><p>Having localized the region of an acquired image that corresponds to the iris, the final task is to decide if this pattern matches a previously stored iris pattern.</p>
<p>There are four steps:</p>
<ol>
<li>Alignment: bringing the newly acquired iris pattern into spatial alignment with a candidate data base entry. </li>
<li>Representation: choosing a representation of the aligned iris patterns that makes their distinctive patterns apparent. </li>
<li>Goodness of Match: evaluating the goodness of match between the newly acquired and data base representations. </li>
<li>Decision: deciding if the newly acquired data and the data base entry were derived from the same iris based on the goodness of match. </li>
</ol>
<h3><span id="alignment-registration">Alignment (Registration)</span></h3><p>To make a detailed comparison between two images, it is advantageous to establish a precise correspondence (or matching) between characteristic structures across the pair.</p>
<p>Both systems (Daugman and Wildes systems) compensate for image shift, scaling and rotation.</p>
<p><strong>The Daugman system for alignment</strong></p>
<p>The Daugman system uses <strong>radial scaling</strong> to compensate for overall size as well as a simple model pupil variation based on <strong>linear stretching</strong>.</p>
<p>The system maps the Cartesian image coordinates $(x, y)$ to dimensionless polar image coordinates $(r, θ)$ according to<br>$$<br>x(r,\theta)=(1-r)x_p(0,\theta)+rx_l(1,\theta)\\<br>y(r,\theta)=(1-r)y_p(0,\theta)+ry_l(1,\theta)<br>$$<br><img src="/images/daugalign.png" alt=""></p>
<p><img src="/images/daugali.png" alt=""></p>
<p><strong>The Wildes system for alignment</strong></p>
<p>The Wildes system uses an <strong>image-registration</strong> technique to compensate for both scaling and rotation.</p>
<p>This approach geometrically warps a newly acquired image $I_a (x, y)$ into alignment with a selected data base image $I_d (x, y)$ according to a mapping function $(u(x, y), v(x, y))$ such that for all $(x, y)$, the image intensity value at $(x, y) – (u(x, y), v(x, y))$ is close to that at $(x, y)$ at $I_d$.</p>
<p>The mapping function is taken to minimize<br>$$<br>\int_x\int_y\left(I_d(x,y)-I_a(x-u, y-v)\right)^2dxdy<br>$$<br>under the constrains to capture similarity transformation of image coordinates $(x,y)$ to $(x’=x-u, y’=y-v)$.</p>
<p><img src="/images/imgreg.jpg" alt=""></p>
<p><strong>Translation</strong><br>$$<br>\vec{x}’=\vec{x}+\vec{d}<br>$$<br><img src="/images/imgtrans.png" alt=""></p>
<p><strong>Rotation</strong><br>$$<br>\vec{x}’=R_\theta\vec{x}\\<br>R_\theta=\begin{pmatrix}<br>\cos\theta &amp; -\sin\theta \\<br>\sin\theta &amp; \cos\theta<br>\end{pmatrix}<br>$$<br><img src="/images/imgrot.png" alt=""></p>
<p><strong>Rotation + Translation</strong><br>$$<br>\vec{x}’=R\vec{x}+\vec{d}<br>$$<br><strong>Scaling + Translation</strong><br>$$<br>\vec{x}’=S\vec{x}+\vec{d}<br>$$<br><img src="/images/scatra.png" alt=""></p>
<p><strong>Shearing</strong><br>$$<br>\vec{x}’=K\vec{x}\\<br>K=\begin{bmatrix}<br>1      &amp; k_{xy}     \\<br>k_{yx}     &amp; 1<br>\end{bmatrix}<br>$$<br><img src="/images/shearing.png" alt=""></p>
<p><strong>Affine</strong>: translation + rotation + scaling + shearing<br>$$<br>\vec{x}’=R_\theta S K\vec{x}+\vec{d}<br>$$<br>Example:<br>$$<br>\begin{bmatrix}<br>x’ \\<br>y’<br>\end{bmatrix}=\begin{bmatrix}<br>\cos\theta &amp; -\sin\theta \\<br>\sin\theta &amp; \cos\theta<br>\end{bmatrix}\begin{bmatrix}<br>s_x &amp; 0 \\<br>0 &amp; s_y<br>\end{bmatrix}\begin{bmatrix}<br>1 &amp; k_{xy} \\<br>k_{yx} &amp; 1<br>\end{bmatrix}\begin{bmatrix}<br>x \\<br>y<br>\end{bmatrix}+\begin{bmatrix}<br>d_x \\<br>d_y<br>\end{bmatrix}<br>$$</p>
<h3><span id="representation">Representation</span></h3><p>To represent the iris image for matching, both the Daugman and Wildes systems capture the multiscale information extracted from the image.</p>
<p>The Wildes system makes use of the Laplacian of Gaussian filters to construct a <strong>Laplacian pyramid</strong>.</p>
<p>The Laplacian of Gaussian (LoG) filter is given by<br>$$<br>-\frac{1}{\pi\sigma^4}\left(1-\frac{\rho^2}{2\sigma^2}\right)\exp(-\frac{\rho^2}{2\sigma^2})<br>$$<br>where $\rho$ is radial distance of a point from the filter’s center; $\sigma$ is standard deviation.</p>
<p>A Laplacian pyramid is formed by collecting the LoG filtered images.</p>
<p><img src="/images/logpra.png" alt=""></p>
<h3><span id="goodness-of-match">Goodness of Match</span></h3><p>The Wildes system uses the <strong>normalized correlation</strong> between the acquired representation and data base representation. In discrete form, the normalized correlation can be defined as follows. </p>
<p>Let $p_1[i, j]$ and $p_2[i, j]$ be two image arrays of size $n \times m$. </p>
<p><img src="/images/gom.png" alt=""></p>
<p>The normal correlation is<br>$$<br>NC=\frac{\sum_{i=1}^n\sum_{j=1}^m(p_1[i,j]-\mu_1)(p_2[i,j]-\mu_2)}{nm\sigma_1\sigma_2}<br>$$<br><img src="/images/gompy.png" alt=""></p>
<h3><span id="decision-fld">Decision (FLD)</span></h3><p>The Wildes system combines four estimated normalized correlation values into a single <strong>accept/reject</strong> judgment.</p>
<p>In this application, the concept of <a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis" target="_blank" rel="noopener">Fisher’s linear discriminant</a> is used for making binary decision. A <strong>weight vector</strong> is found such that <strong>the variance within a class of iris data is minimized</strong> while <strong>the variance between different classes of iris data is maximized</strong> for the transformed samples.</p>
<p>In iris recognition application, usually there are two classes: <strong>Authentic class (A)</strong> and <strong>Imposter class (I)</strong>.</p>
<p>To make a binary decision on a line, all points are projected onto the weight vector (or samples are transformed by using the weight vector).</p>
<p><img src="/images/fld1.png" alt=""></p>
<p>In iris recognition using the Wildes system, all samples are 4-dimensional vectors. Let there be n 4-dimensional samples.</p>
<p><img src="/images/fld2.png" alt=""></p>
<p>The total within class variance is<br>$$<br>\vec{S}_w=\vec{S}_i+\vec{S}_a<br>$$<br>Between class variance is<br>$$<br>\vec{S}_b=(\vec{\mu}_a-\vec{\mu}_i)(\vec{\mu}_a-\vec{\mu}_i)^T<br>$$<br>If all samples are transformed, the ratio of between class variance to total within class variance is<br>$$<br>\frac{\vec{w}^T\vec{S}_b\vec{w}}{\vec{w}^T\vec{S}_w\vec{w}}<br>$$<br>The ratio is maximized when<br>$$<br>\vec{w}=\vec{S}_w^{-1}(\vec{\mu}_a-\vec{\mu}_i)<br>$$<br>And the separation point for decision making is<br>$$<br>\frac{1}{2}\vec{w}^T(\vec{\mu}_a+\vec{\mu}_i)<br>$$<br>Therefore, values above this point will be taken as derived from class $A$; values below this point will be taken as derived from class $I$.</p>
<p><img src="/images/fld3.png" alt=""></p>
<h2><span id="hough-transform">Hough Transform</span></h2><p><strong>Detecting Lines</strong></p>
<p>Idea: if two edge points $(x_i, y_i)$ and $(x_j, y_j)$ lie on the same straight line, then they should have the same values of slope and y-intercepts on the xy-plane.</p>
<p><img src="/images/houghline.png" alt=""></p>
<p><strong>[1]</strong> For a point $(x_i,y_i)$, we set up a straight line equation:<br>$$<br>y_i=ax_i+b\Leftrightarrow b=(-x_i)a+y_i<br>$$<br>where $a$ = slope, $b$ = y-intercept, $x_i$ and $y_i$ are known and fixed.</p>
<p><strong>[2]</strong> We subdivide the a axis into $K$ increments between $[a_\min,a_\max]$. For each increment of $a$, we evaluate the value of $b$.</p>
<p><strong>[3]</strong> A relationship between $a$ and $b$ can be plotted in a parameter space, i.e., ab-plane.</p>
<p><strong>[4]</strong> We partition the parameter space into a number of bins (accumulator cells), and increment the corresponding bin $A(a,b)$ by 1 ($b$ is rounded into the nearest integer). </p>
<p><img src="/images/houghbin.png" alt=""></p>
<p><strong>[5]</strong> For another point $(x_j,y_j)$, we set up another straight line equation:<br>$$<br>y_j=ax_j+b\Leftrightarrow b=(-x_j)a+y_j<br>$$<br><strong>[6]</strong> Similarly, we subdivide the a axis into K increments between $[a_\min,a_\max]$. For each increment of $a$, we evaluate the value of $b$. We plot the relationship between $a$ and $b$ in the same parameter space, and update bin values in the discrete parameter space.</p>
<p><strong>[7]</strong> The bin $A(a,b)$ having the highest count corresponds to the straight line passing through the points $(x_i,y_i)$ and $(x_j,y_j)$.</p>
<p><img src="/images/hough2.png" alt=""></p>
<p><strong>[8]</strong> The same procedure can be applied to all points. The bin $A(a,b)$ having the <strong>highest count</strong> corresponds to the straight line passing through (or passing near) the largest number of points.</p>
<p>Problem: Values of $a$ and $b$ run from negative infinity to positive infinity. We need infinite number of bins!</p>
<p>Solution: use normal representation of a line:<br>$$<br>x\cos(\theta)+y\sin(\theta)=\rho<br>$$<br><img src="/images/houghnormal.png" alt=""></p>
<p>$\theta$ runs from $–90^o$ to $90^o$. $\rho$ runs from $-\sqrt{2}D$ to $\sqrt{2}D$, where $D$ is the distance between corners in the image (length and width).</p>
<p><strong>Circle Hough Transform (CHT)</strong></p>
<p>The Hough transform can be used to determine the parameters of a circle when a number of points that fall on the perimeter are known. A circle with radius $R$ and center $(a, b)$ can be described with the parametric equations:<br>$$<br>x=a+R\cos(\theta)\\<br>y=b+R\sin(\theta)<br>$$<br>When the angle $θ$ sweeps through the full 360 degree range the points $(x, y)$ trace the perimeter of a circle.</p>
<p>If the circles in an image are of <strong>known radius</strong> $R$, then the search can be reduced to 2D. The objective is to find the $(a, b)$ coordinates of the centers. </p>
<p><img src="/images/cht1.png" alt=""></p>
<p>If the <strong>radius is not known</strong>, then the locus of points in parameter space will fall on the surface of a <strong>cone</strong>. Each point $(x, y)$ on the perimeter of a circle will produce a cone surface in parameter space. The triplet $(a, b, R)$ will correspond to the accumulation cell where the largest number of cone surfaces intersect.</p>
<p><img src="/images/cone.png" alt="">The drawing above illustrates the generation of a conical surface in parameter space for one $(x, y)$ point. A circle with a different radius will be constructed at each level, $r$. </p>
<p>The search for circles with unknown radius can be conducted by using a three dimensional accumulation matrix.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Recognition-System/">Recognition System</a><a href="/tags/Iris/">Iris</a><a href="/tags/FLD/">FLD</a><a href="/tags/Daugman/">Daugman</a><a href="/tags/Wildes/">Wildes</a><a href="/tags/Hough-Transform/">Hough Transform</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>
<script type="text/javascript">
  var $config = {
    image: "icon.png",
  };
  socialShare('.social-share-cs', $config);
</script>



<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80MTI4MC8xNzgyOA==">
	<script type="text/javascript">
		(function (d, s) {
			var j, e = d.getElementsByTagName(s)[0];

			if (typeof LivereTower === 'function') {
				return;
			}

			j = d.createElement(s);
			j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
			j.async = true;

			e.parentNode.insertBefore(j, e);
		})(document, 'script');
	</script>
	<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->


      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2018
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>