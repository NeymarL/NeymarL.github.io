<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    RL - Proximal Policy Optimization (PPO) | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="RL - Proximal Policy Optimization (PPO)">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script>
  <script type="text/javascript" src="/js/search.js"></script>
  <script type="text/javascript" src="/js/jquery.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/search">搜索</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      RL - Proximal Policy Optimization (PPO)</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/11/25/RL - PPO/" rel="bookmark">
        <time class="entry-date published" datetime="2018-11-25T14:00:09.000Z">
          2018-11-25
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p><a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">Proximal Policy Optimization (PPO, PPO-Clip, PPO-Penalty)</a> 是由<a href="https://www.52coding.com.cn/2018/11/22/RL%20-%20TRPO/">TRPO</a>的作者Schulman等人于2017年提出的策略梯度类算法。PPO算法的思路和TRPO一致，都是想在优化时采取尽可能大的步幅但又不能太大以至于产生崩坏。相比于比TRPO，PPO实现起来更简单，泛化能力更强，可以使用随机梯度下降（SGD）进行优化。</p>
<a id="more"></a>
<h2><span id="背景">背景</span></h2>
<p>PPO的背景与<a href="https://www.52coding.com.cn/2018/11/22/RL%20-%20TRPO/#背景">TRPO的背景</a>一致，最终TRPO推导出如下的带约束优化问题： <span class="math display">\[
\max_{\theta}\mathbb{E}_t[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t]\\
\text{subject to }\mathbb{E}_t[\text{KL}[\pi_{\theta_{old}}(\cdot|s_t), \pi_\theta(\cdot|s_t)]]
\]</span> 令 <span class="math inline">\(r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\)</span> 为新旧策略的概率比（易知 <span class="math inline">\(r_t(\theta_{old}) = 1\)</span>）。TRPO最大化的替代目标（surrogate objective）可以写为如下形式： <span class="math display">\[
L^{CPI}(\theta)=\mathbb{E}_t[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t]=\mathbb{E}_t[r_t(\theta)A_t]
\]</span> 如果不加约束的话，直接优化该目标会产生巨大的更新，导致更新不稳定甚至崩溃。所以需要考虑一种惩罚方法，使 <span class="math inline">\(r_t(\theta)\)</span> 接近 <span class="math inline">\(1\)</span>。</p>
<h2><span id="ppo-clip">PPO-Clip</span></h2>
<p>PPO-Clip的目标函数为： <span class="math display">\[
L^{CLIP}(\theta)=\mathbb{E}_t[\min(r_t(\theta)A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t)]
\]</span> 其中 <span class="math inline">\(\epsilon\)</span> 为超参数控制截断率，取值通常比较小（0.2左右）。</p>
<p>上述目标函数的第一项与 <span class="math inline">\(L^{CPI}\)</span> 一致，第二项则是为了限制更新幅度（施加惩罚），控制 <span class="math inline">\(r_t(\theta) \in [1-\epsilon, 1+\epsilon]\)</span>。可见 <span class="math inline">\(L^{CLIP}(\theta)\)</span> 是 <span class="math inline">\(L^{CPI}(\theta)\)</span> 的一个下界。</p>
<p><img src="/images/lclip.png"></p>
<p>上图显示了 <span class="math inline">\(\text{clip}\)</span> 函数的工作方式：</p>
<ul>
<li>当 <span class="math inline">\(A &gt; 0\)</span> 时，如果想使目标函数取得更大的值，就需要增大 <span class="math inline">\(\pi_\theta(a_t|s_t)\)</span> 的值，也就是增大 <span class="math inline">\(r_t(\theta)\)</span> 。但是式中的 <span class="math inline">\(\min\)</span> 函数限制了 <span class="math inline">\(r_t(\theta)\)</span> 最大取到 <span class="math inline">\(1+\epsilon\)</span>，所以新策略再远离旧策略（<span class="math inline">\(r_t(\theta)\)</span> 继续增大）并不会带来更多地好处。</li>
<li>当 <span class="math inline">\(A &lt; 0\)</span> 时，如果想使目标函数取得更大的值，就需要减小 <span class="math inline">\(\pi_\theta(a_t|s_t)\)</span> 的值，也就是减小 <span class="math inline">\(r_t(\theta)\)</span> 。但是式中的 <span class="math inline">\(\min\)</span> 函数限制了 <span class="math inline">\(r_t(\theta)\)</span> 最小取到 <span class="math inline">\(1-\epsilon\)</span>，所以新策略再远离旧策略（<span class="math inline">\(r_t(\theta)\)</span> 继续减小）并不会带来更多地好处。</li>
</ul>
<p>在实现中，目标函数通常使用更简单的形式： <span class="math display">\[
L^{CLIP}(s, a, \theta_k, \theta)=\min(\frac{\pi_\theta(a|s)}{\pi_{\theta_{k}}(a|s)}A^{\pi_{\theta_k}}(s, a), g(\epsilon, A^{\pi_{\theta_k}}(s, a)))
\]</span> 其中， <span class="math display">\[
g(\epsilon, A^{\pi_{\theta_k}}(s, a))=\begin{cases} 
(1+\epsilon)A,  &amp; \mbox{if }A ≥0 \\
(1-\epsilon)A, &amp; \mbox{if }A&lt;0
\end{cases}
\]</span></p>
<blockquote>
<p>注：即便对 <span class="math inline">\(r_t(\theta)\)</span> 加上截断，新策略仍染有可能偏离旧策略很远，不过有很多trick来处理这个问题。其中一个特别简单的处理方式就是：如果新策略和旧策略的平均KL距离大于某个阈值，则停止进行更新（<strong>early stopping</strong>）。</p>
</blockquote>
<p>相比于TRPO，由于没有了KL约束，PPO可以用SGD来进行优化，实现简单很多。</p>
<p><strong>PPO-Clip算法</strong></p>
<p><img src="/images/ppo_algo.svg"></p>
<h2><span id="ppo-penalty">PPO-Penalty</span></h2>
<p>这种方法使用KL距离作为惩罚项，关键在于求出能够自适应多种任务的惩罚因子 <span class="math inline">\(\beta\)</span>。算法的逻辑为，在每次策略进行更新时：</p>
<ul>
<li><p>使用SGD优化目标函数： <span class="math display">\[
L^{KLPEN}(\theta)=\mathbb{E}_t\left[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t-\beta\cdot\text{KL}[\pi_{old}(a_t|s_t), \pi_\theta(a_t|s_t)]\right]
\]</span></p></li>
<li><p>计算 <span class="math inline">\(d = \mathbb{E}\left[\text{KL}[\pi_{\theta_{old}}(\cdot|s_t), \pi_\theta(\cdot|s_t)]\right]\)</span></p>
<ul>
<li>如果 <span class="math inline">\(d &lt; d_{targ}/1.5\)</span>，则 <span class="math inline">\(\beta \leftarrow \beta/2\)</span></li>
<li>如果 <span class="math inline">\(d &gt; d_{targ} \times 1.5\)</span>，则 <span class="math inline">\(\beta \leftarrow\beta \times 2\)</span></li>
</ul>
<p>其中，<span class="math inline">\(d_{targ}\)</span> 为超参数。</p></li>
</ul>
<blockquote>
<p>注：PPO-Penalty 没有 PPO-Clip 效果好。</p>
</blockquote>
<h2><span id="实验和总结">实验和总结</span></h2>
<p><strong>特点</strong></p>
<ul>
<li>训练稳定</li>
<li>通过限制 <span class="math inline">\(r_t(\theta)\)</span> 来找到尽可能大的并且合理的步长</li>
<li>on-policy 算法</li>
<li>可用于离散和连续的动作空间</li>
<li>相比于TRPO，PPO实现简单，效果更好</li>
</ul>
<p><strong>实验性能</strong></p>
<p>在大部分 MuJoCo 环境中强于其他策略梯度类算法，在Atari环境中，表现仅次于ACER，但是学习速度优于ACER。</p>
<p><img src="/images/ppo_mujoco.png"></p>
<p><img src="/images/ppo_atari.png"></p>
<h3><span id="代码">代码</span></h3>
<p>自己也实现了一下PPO-Clip算法，代码在<a href="https://github.com/NeymarL/Pacman-RL/blob/master/src/ppo.py" target="_blank" rel="noopener">这里</a>。下图显示了在 OpenAI <a href="https://gym.openai.com/" target="_blank" rel="noopener">Gym</a> 上的 <code>MsPacman-ram-v0</code> 环境上的测试结果：</p>
<p><img src="/images/ppo_pacman.png"></p>
<p><img src="/images/sample1.gif"></p>
<p><img src="/images/sample2.gif"></p>
<h2><span id="references">References</span></h2>
<p>[1] https://arxiv.org/abs/1707.06347</p>
<p>[2] https://spinningup.openai.com/en/latest/algorithms/ppo.html</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/Policy-Gradient/">Policy Gradient</a><a href="/tags/PPO/">PPO</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>
<script type="text/javascript">
  var $config = {
    image: "icon.png",
  };
  socialShare('.social-share-cs', $config);
</script>



<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80MTI4MC8xNzgyOA==">
	<script type="text/javascript">
		(function (d, s) {
			var j, e = d.getElementsByTagName(s)[0];

			if (typeof LivereTower === 'function') {
				return;
			}

			j = d.createElement(s);
			j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
			j.async = true;

			e.parentNode.insertBefore(j, e);
		})(document, 'script');
	</script>
	<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->


      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2018
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>