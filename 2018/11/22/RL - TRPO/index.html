<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    RL - Trust Region Policy Optimization (TRPO) | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="RL - Trust Region Policy Optimization (TRPO)">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      RL - Trust Region Policy Optimization (TRPO)</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/11/22/RL - TRPO/" rel="bookmark">
        <time class="entry-date published" datetime="2018-11-22T09:10:09.000Z">
          2018-11-22
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p><a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="noopener">Trust Region Policy Optimization (TRPO) </a>算法是由伯克利大学的Schulman等人于2015年提出的策略梯度（Policy Gradients）算法。TRPO通过最大化新策略相对于当前策略的优势来保证每次更新都是单调递增的（稳定），同时找到尽可能大的更新步幅。算法推导出的最终结果是在KL约束下最大化替代优势函数。</p>
<a id="more"></a>
<h2><span id="背景">背景</span></h2><p>考虑经典的 MDP$&lt;S, A, P, r, \rho_0, \gamma&gt;$，其中 $S$ 是所有状态（state）的集合，$A$ 是所有动作（action）的集合，$P: S\times A\times S \rightarrow \mathbb{R}$ 是转移概率分布，$r$ 是奖励（reward）函数，$\rho_0$ 是初始状态（$s_0$）分布，$\gamma$ 是折扣因子（ discount factor）。</p>
<p>定义 $\pi$ 为一个随机策略：$\pi: S\times A\rightarrow [0, 1]$，定义 $\eta(\pi)$ 来衡量策略 $\pi$ 的好坏：<br>$$<br>\eta(\pi)=\mathbb{E}_{s_0, a_0, …\sim\pi}[\sum_{t=0}^\infty\gamma^tr(s_t)]<br>$$<br>接着定义 state-action value function $Q_\pi$, value function $V_\pi$, 优势函数（advantage function）$A_\pi$:<br>$$<br>Q_\pi(s_t, a_t) = \mathbb{E}_{s_{t+1}, a_{t+1}, …\sim\pi}[\sum_{l=0}^\infty\gamma^lr_{s_{t+l}}]<br>$$</p>
<p>$$<br>V_\pi(s_t) =\mathbb{E}_{a_t, s_{t+1}, …\sim\pi}[\sum_{l=0}^\infty\gamma^lr_{s_{t+l}}]<br>$$</p>
<p>$$<br>A_\pi(s, a) = Q_\pi(s, a) - V_\pi(s)<br>$$</p>
<p>然后可以通过下式来衡量策略 $\tilde{\pi}$ 相对于策略 $\pi$ 的优势（证明详见论文）：<br>$$<br>\begin{align}<br>\eta(\tilde{\pi})&amp;=\eta(\pi)+\mathbb{E}_{s_0, a_0, …\sim\color{red}{\tilde{\pi}}}[\sum_{t=0}^\infty\gamma^tA_\pi(s_t,a_t)]\\<br>&amp;= \eta(\pi)+\sum_s\color{red}{\rho_\tilde{\pi}(s)}\sum_a\tilde{\pi}(a|s)A_\pi(s, a)<br>\end{align}<br>$$<br>其中 $\rho_\pi$ 为策略 $\pi$ 的折扣访问频率（discounted visitation frequency）：<br>$$<br>\rho_\pi(s) = P(s_0=s)+\gamma P(s_1=s) + \gamma^2 P(s_2=s)+…<br>$$<br>通过上式可知，只要每个状态 $s$ 的期望优势非负，即 $\sum_a\tilde{\pi}(a|s)A_\pi(s, a)&gt;0$，就可以保证更新是单调非递减的，这其实就是经典的<a href="https://www.52coding.com.cn/2017/12/07/RL%20-%20Planning%20by%20Dynamic%20Programming/">策略迭代（policy iteration）</a>的更新方式。然而，由于 $\rho_\tilde{\pi}(s)$ 的存在，导致直接优化上式很困难，所以引入一个<strong>替代优势</strong>（surrogate advantage）：<br>$$<br>\begin{align}<br>L_\pi(\tilde{\pi})&amp;=\eta(\pi)+\sum_s\color{red}{\rho_\pi(s)}\sum_a\tilde{\pi}(a|s)A_\pi(s, a)\\<br>\end{align}<br>$$<br>经过一系列推导，可以得到策略 $\tilde{\pi}$ 的优势下界：<br>$$<br>\eta(\tilde{\pi})≥L_\pi(\tilde{\pi})-C\cdot D_{KL}^\max(\pi, \tilde{\pi})<br>$$<br>其中，$C=\frac{4\epsilon\gamma}{(1-\gamma)^2}$，$D_{KL}^\max$ 是最大的KL散度。</p>
<p>基于上述公式，可以得到如下的类策略迭代的算法，即可保证每轮迭代都是单调的：</p>
<p><img src="/images/IMG_1925FD469BD9-1.jpeg" alt=""></p>
<h2><span id="trpo">TRPO</span></h2><p>由于Deep RL都是使用参数为 $\theta$ 的神经网络来拟合策略 $\pi_\theta$，为了使公式更简洁，把算法1中公式的 $\pi$ 替换成 $\theta$:<br>$$<br>\theta = \arg\max_{\theta}[L(\theta_{old}, \theta)-C\cdot D_{KL}^\max(\theta_{old}|| \theta)]<br>$$<br>其中，<br>$$<br>L(\theta_{old}, \theta) = \mathbb{E}_{s,a\sim\pi_{\theta_{old}}}[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)}A^{\pi_{\theta_{old}}}(s,a)]<br>$$<br>TRPO是算法1的近似，区别在于：TRPO没有使用惩罚项 $C$，而是使用 KL散度约束（i.e. trust region constraint）：<br>$$<br>\theta = \arg\max_\theta L(\theta_{old}, \theta)\\<br>\text{ s.t. }\bar{D}_{KL}(\theta||\theta_{old})≤\delta<br>$$<br>其中，$\bar{D}_{KL}$ 是平均KL散度：<br>$$<br>\bar{D}_{KL}(\theta||\theta_{old})=\mathbb{E}_{s\sim\pi_{\theta_{old}}}[D_{KL}(\pi_{\theta}(\cdot|s)||\pi_{\theta_{old}}(\cdot|s))]<br>$$<br>然而上面的带约束优化也并非容易，所以TRPO对上式进行了一些近似，对目标函数和约束进行一阶泰勒近似得：<br>$$<br>L(\theta_{old}, \theta) \approx g^T(\theta-\theta_{old})<br>$$</p>
<p>$$<br>\bar{D}_{KL}(\theta||\theta_{old})\approx \frac{1}{2}(\theta-\theta_{old})^TH(\theta-\theta_{old})<br>$$</p>
<p>其中，$g$ 是替代函数的梯度在 $\theta=\theta_{old}$ 处的值，凑巧的是，它和策略梯度的值正好相等：$g = \triangledown_\theta J(\pi_\theta)|_{\theta_{old}}$；$H$ 是对于 $\theta$ 的海森矩阵（Hessian matrix）。</p>
<p>于是得到如下的近似优化问题：<br>$$<br>\theta_{k+1}=\arg\max_\theta g^T(\theta-\theta_k)\\<br>\text{s.t. }\frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k)≤\delta<br>$$<br>通过拉格朗日法求解上述约束优化问题得：<br>$$<br>\theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g<br>$$<br>这个就是 <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf" target="_blank" rel="noopener">Natural Policy Gradient</a> 的更新公式。不过，由于泰勒近似引入了误差，上式的解可能不满足 KL 约束，所以 TRPO 增加了一个线性搜索（backtracking line search）：<br>$$<br>\theta_{k+1}=\theta_k+\alpha^j\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g<br>$$<br>其中，$\alpha\in(0,1)$ 是回溯系数（backtracking coefficient），$j$ 是最小的非负整数使得 $\pi_{\theta_{k+1}}$ 满足 KL 约束并且产生<strong>正</strong>的替代优势，这样就可以保证训练进步是单调的。</p>
<p>最后，计算和存储 $H^{-1}$ 的开销是很大的，尤其是神经网络的参数动不动就几M。TRPO 使用<a href="https://www.wikiwand.com/en/Conjugate_gradient_method" target="_blank" rel="noopener">共轭梯度法（conjugate gradient）</a>来解 $Hx = g$，这样就不用直接计算和存储 $H$。</p>
<p>最终的更新公式为：<br>$$<br>\theta_{k+1}=\theta_k+\alpha^j\sqrt{\frac{2\delta}{\hat{x}^TH\hat{x}}}\hat{x}<br>$$<br>其中，<br>$$<br>\begin{align}<br>\hat{x}&amp;\approx H^{-1}g &amp; \text{(using conjugate gradient)}<br>\end{align}<br>$$</p>
<p>$$<br>H\hat{x} = \triangledown_\theta((\triangledown_\theta\bar{D}_{KL}(\theta||\theta_k))^T\hat{x})<br>$$</p>
<p><strong>TRPO算法</strong></p>
<p><img src="/images/trpo.svg" alt=""></p>
<h2><span id="performance">Performance</span></h2><p><strong>TRPO的一些特点</strong></p>
<ul>
<li>保证每次更新在当前训练数据上都是进步的，训练过程更加稳定</li>
<li>通过满足KL约束来找尽可能大的步长</li>
<li>on-policy 算法</li>
<li>可用于离散和连续的动作空间</li>
<li>算法完全从理论推导，理解起来很烧脑</li>
</ul>
<p><strong>实验性能</strong></p>
<p>在模拟机器人走路、游泳等任务中取得了在当时不错的效果；在通过视频输入玩Atari游戏的任务中表现不如DQN等方法。</p>
<p><img src="/images/trpo_per.jpg" alt=""></p>
<p>下图是我在 OpenAI <a href="https://gym.openai.com/" target="_blank" rel="noopener">gym</a> 的 <code>Walker2d-v2</code> 和 <code>MsPacman-ram-v0</code> 中的结果。</p>
<p><img src="/images/walker.png" alt=""></p>
<p><img src="/images/pacman.png" alt=""></p>
<h2><span id="references">References</span></h2><p>[1] <a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="noopener">https://arxiv.org/abs/1502.05477</a></p>
<p>[2] <a href="https://spinningup.openai.com/en/latest/algorithms/trpo.html" target="_blank" rel="noopener">https://spinningup.openai.com/en/latest/algorithms/trpo.html</a></p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/Policy-Gradient/">Policy Gradient</a><a href="/tags/TRPO/">TRPO</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>
<script type="text/javascript">
  var $config = {
    image: "icon.png",
  };

  socialShare('.social-share-cs', $config);
</script>


	<section id="comment" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'Niuhe';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>



      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2018
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>