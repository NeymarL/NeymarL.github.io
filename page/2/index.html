<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    Page 2 | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script>
  <script type="text/javascript" src="/js/search.js"></script>
  <script type="text/javascript" src="/js/jquery.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/search">搜索</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        
  <article>

  
  
  <h3 class="article-title"><a href="/2018/05/15/AlphaGo and AlphaGo Zero/"><span>
        AlphaGo, AlphaGo Zero and AlphaZero</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/05/15/AlphaGo and AlphaGo Zero/" rel="bookmark">
        <time class="entry-date published" datetime="2018-05-15T07:55:19.000Z">
          2018-05-15
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2 id="go">Go</h2>
<p>围棋起源于古代中国，是世界上最古老的棋类运动之一。在宋代的《梦溪笔谈》中探讨了围棋的局数变化数目，作者沈括称“大约连书万字四十三个，即是局之大数”，意思是说变化数目要写43个万字。根据围棋规则，没有气的子不能存活，扣除这些状态后的合法状态约有 <span class="math inline">\(2.08×10^{170}\)</span> 种。Robertson 与 Munro 在1978年证得围棋是一种 PSPACE-hard 的问题，其必胜法之记忆计算量在<span class="math inline">\(10^{600}\)</span> 以上，这远远超过可观测宇宙的原子总数 <span class="math inline">\(10^{75}\)</span>，可见围棋对传统的搜索方法是非常有挑战的。
      
    </p></div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Deep-Learning/">Deep Learning</a><a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/AlphaGo/">AlphaGo</a><a href="/tags/AlphaZero/">AlphaZero</a><a href="/tags/增强学习/">增强学习</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2018/03/10/在没有人类知识的情况下掌握围棋/"><span>
        论文翻译：在没有人类知识的情况下掌握围棋</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/03/10/在没有人类知识的情况下掌握围棋/" rel="bookmark">
        <time class="entry-date published" datetime="2018-03-10T06:01:09.000Z">
          2018-03-10
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h4 id="前言">1. 前言</h4>
<p>​ 人工智能的一个长期目标是在一些有挑战的领域中从零开始学习出超人熟练程度的算法。最近，AlphaGo成为第一个在围棋比赛中击败世界冠军的程序。 AlphaGo中的树搜索使用深度神经网络评估位置和选定的移动。这些神经网络是通过监督学习来自人类专家的走法以及通过强化自我学习来进行训练的。这里我们只介绍一种基于强化学习的算法，没有超出游戏规则的人类数据，指导或领域知识。AlphaGo成为自己的老师：一个神经网络训练预测AlphaGo的移动选择和游戏的胜者。这个神经网络提高了树搜索的强度，在下一次迭代中拥有更高质量的移动选择和更强的自我学习。我们的新程序AlphaGo Zero从零开始学习，实现了超人的表现，与之前发布的夺冠冠军AlphaGo相比以100-0取胜。</p>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Deep-Learning/">Deep Learning</a><a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/AlphaGo/">AlphaGo</a><a href="/tags/AlphaZero/">AlphaZero</a><a href="/tags/增强学习/">增强学习</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2018/01/09/RL - Integrating Learning and Planning/"><span>
        RL - Integrating Learning and Planning</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/01/09/RL - Integrating Learning and Planning/" rel="bookmark">
        <time class="entry-date published" datetime="2018-01-09T13:11:09.000Z">
          2018-01-09
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2 id="introduction">Introduction</h2>
<p>In last lecture, we learn <strong>policy</strong> directly from experience. In previous lectures, we learn <strong>value function</strong> directly from experience. In this lecture, we will learn <strong>model</strong> directly from experience and use <strong>planning</strong> to construct a value function or policy. Integrate learning and planning into a single architecture.</p>
<p>Model-Based RL</p>
<ul>
<li>Learn a model from experience</li>
<li><strong>Plan</strong> value function (and/or policy) from model</li>
</ul>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/AlphaGo/">AlphaGo</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/MCTS/">MCTS</a><a href="/tags/TD-Search/">TD Search</a><a href="/tags/Dyna/">Dyna</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2018/01/06/RL - Policy Gradient/"><span>
        RL - Policy Gradient</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/01/06/RL - Policy Gradient/" rel="bookmark">
        <time class="entry-date published" datetime="2018-01-06T05:42:09.000Z">
          2018-01-06
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2 id="introduction">Introduction</h2>
<p>This lecture talks about methods that optimise policy directly. Instead of working with value function as we consider so far, we seek experience and use the experience to update our policy in the direction that makes it better.</p>
<p>In the last lecture, we approximated the value or action-value function using parameters <span class="math inline">\(\theta\)</span>, <span class="math display">\[
V_\theta(s)\approx V^\pi(s)\\
Q_\theta(s, a)\approx Q^\pi(s, a)
\]</span> A policy was generated directly from the value function using <span class="math inline">\(\epsilon\)</span>-greedy.</p>
<p>In this lecture we will directly parametrise the policy <span class="math display">\[
\pi_\theta(s, a)=\mathbb{P}[a|s, \theta]
\]</span> We will focus again on <span class="math inline">\(\color{red}{\mbox{model-free}}\)</span> reinforcement learning.</p>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/Policy-Gradient/">Policy Gradient</a><a href="/tags/REINFORCE/">REINFORCE</a><a href="/tags/Actor-Critic/">Actor-Critic</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2018/01/03/RL - Value Function Approximation/"><span>
        RL - Value Function Approximation</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/01/03/RL - Value Function Approximation/" rel="bookmark">
        <time class="entry-date published" datetime="2018-01-03T10:09:09.000Z">
          2018-01-03
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2 id="introduction">Introduction</h2>
<p>This lecture will introduce how to scale up our algorithm to real practical RL problems by value function approximation.</p>
<p>Reinforcement learning can be used to solve <em>large</em> problems, e.g.</p>
<ul>
<li>Backgammon: <span class="math inline">\(10^{20}\)</span> states</li>
<li>Computer Go: <span class="math inline">\(10^{170}\)</span> states</li>
<li>Helicopter: continuous state space</li>
</ul>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Deep-Learning/">Deep Learning</a><a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/DQN/">DQN</a><a href="/tags/Neural-Network/">Neural Network</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2017/12/21/RL - Model-Free Control/"><span>
        RL - Model-Free Control</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/12/21/RL - Model-Free Control/" rel="bookmark">
        <time class="entry-date published" datetime="2017-12-21T12:00:09.000Z">
          2017-12-21
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2 id="introduction">Introduction</h2>
<p>Last lecture:</p>
<ul>
<li>Model-free prediction</li>
<li><em>Estimate</em> the value function of an <em>unknown</em> MDP</li>
</ul>
<p>This lecture:</p>
<ul>
<li>Model-free control</li>
<li><strong>Optimise</strong> the value function of an unknown MDP</li>
</ul>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/Q-learning/">Q-learning</a><a href="/tags/Monte-Carlo-Control/">Monte-Carlo Control</a><a href="/tags/Sarsa/">Sarsa</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2017/12/16/RL - Model-Free Prediction/"><span>
        RL - Model-Free Prediction</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/12/16/RL - Model-Free Prediction/" rel="bookmark">
        <time class="entry-date published" datetime="2017-12-16T07:00:09.000Z">
          2017-12-16
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2 id="introduction">Introduction</h2>
<p>Last lecture, David taught us how to solve a <em>known</em> MDP, which is <em>planning by dynamic programming</em>. In this lecture, we will learn how to estimate the value function of an <strong>unknown</strong> MDP, which is <em>model-free prediction</em>. And in the next lecture, we will <em>optimise</em> the value function of an unknown MDP.</p>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/Model-Free/">Model-Free</a><a href="/tags/Monte-Carlo-Learning/">Monte-Carlo Learning</a><a href="/tags/TD/">TD</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2017/12/07/RL - Planning by Dynamic Programming/"><span>
        RL - Planning by Dynamic Programming</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/12/07/RL - Planning by Dynamic Programming/" rel="bookmark">
        <time class="entry-date published" datetime="2017-12-07T07:48:19.000Z">
          2017-12-07
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p><strong>Table of Contents</strong></p>
<!-- toc -->
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#policy-evaluation">Policy Evaluation</a></li>
<li><a href="#policy-iteration">Policy Iteration</a></li>
<li><a href="#value-iteration">Value Iteration</a></li>
<li><a href="#extentions-to-dynamic-programming">Extentions to Dynamic Programming</a></li>
<li><a href="#contraction-mapping">Contraction Mapping</a></li>
</ul>
<!-- tocstop -->
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/MDP/">MDP</a><a href="/tags/Dynamic-Programming/">Dynamic Programming</a><a href="/tags/Policy-Iteration/">Policy Iteration</a><a href="/tags/Value-Iteration/">Value Iteration</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2017/08/18/RL - Markov Decision Processes/"><span>
        RL - Markov Decision Processes</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/08/18/RL - Markov Decision Processes/" rel="bookmark">
        <time class="entry-date published" datetime="2017-08-18T07:02:19.000Z">
          2017-08-18
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p><strong>Table of Contents</strong></p>
<!-- toc -->
<ul>
<li><a href="#markov-processes">Markov Processes</a></li>
<li><a href="#markov-reward-process">Markov Reward Process</a></li>
<li><a href="#markov-decision-process">Markov Decision Process</a></li>
</ul>
<!-- tocstop -->
<h2 id="markov-processes">Markov Processes</h2>
<p>Basically, <strong>Markov decision processes</strong> formally describe an environment for reinforcement learning, where the environment is <strong>fully observable</strong>, which means the current state completely characterises the process.</p>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/MDP/">MDP</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2017/08/15/RL - Introduction to Reinforcement Learning/"><span>
        RL - Introduction to Reinforcement Learning</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/08/15/RL - Introduction to Reinforcement Learning/" rel="bookmark">
        <time class="entry-date published" datetime="2017-08-15T07:55:19.000Z">
          2017-08-15
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p>RL, especially DRL (Deep Reinforcement Learning) has been an fervent research area during these years. One of the most famous RL work would be AlphaGo, who has beat <a href="https://www.wikiwand.com/en/Lee_Sedol" target="_blank" rel="noopener">Lee Sedol</a>, one of the best players at Go, last year. And in this year (2017), AlphaGo won three games with Ke Jie, the world No.1 ranked player. Not only in Go, AI has defeated best human play in many games, which illustrates the powerful of the combination of Deep Learning and Reinfocement Learning. However, despite AI plays better games than human, AI takes more time, data and energy to train which cannot be said to be very intelligent. Still, there are numerous unexplored and unsolved problems in RL research, that's also why we want to learn RL.</p>
<p>This is the first note of David Silver's <a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">RL course</a>.</p>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/AlphaGo/">AlphaGo</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/DRL/">DRL</a>
      </span>
      

    </div>

    
  </div>
</article>



<nav class="pagination">
  
  <a href="/" class="pagination-prev">Prev</a>
  
  
  <a href="/page/3/" class="pagination-next">Next</a>
  
</nav>
      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2018
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>