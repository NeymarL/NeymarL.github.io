<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    Page 3 | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script>
  <script type="text/javascript" src="/js/search.js"></script>
  <script type="text/javascript" src="/js/jquery.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/search">搜索</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        
  <article>

  
  
  <h3 class="article-title"><a href="/2018/01/09/RL - Integrating Learning and Planning/"><span>
        RL - Integrating Learning and Planning</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/01/09/RL - Integrating Learning and Planning/" rel="bookmark">
        <time class="entry-date published" datetime="2018-01-09T13:11:09.000Z">
          2018-01-09
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2 id="introduction">Introduction</h2>
<p>In last lecture, we learn <strong>policy</strong> directly from experience. In previous lectures, we learn <strong>value function</strong> directly from experience. In this lecture, we will learn <strong>model</strong> directly from experience and use <strong>planning</strong> to construct a value function or policy. Integrate learning and planning into a single architecture.</p>
<p>Model-Based RL</p>
<ul>
<li>Learn a model from experience</li>
<li><strong>Plan</strong> value function (and/or policy) from model</li>
</ul>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/AlphaGo/">AlphaGo</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/MCTS/">MCTS</a><a href="/tags/TD-Search/">TD Search</a><a href="/tags/Dyna/">Dyna</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2018/01/06/RL - Policy Gradient/"><span>
        RL - Policy Gradient</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/01/06/RL - Policy Gradient/" rel="bookmark">
        <time class="entry-date published" datetime="2018-01-06T05:42:09.000Z">
          2018-01-06
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2 id="introduction">Introduction</h2>
<p>This lecture talks about methods that optimise policy directly. Instead of working with value function as we consider so far, we seek experience and use the experience to update our policy in the direction that makes it better.</p>
<p>In the last lecture, we approximated the value or action-value function using parameters <span class="math inline">\(\theta\)</span>, <span class="math display">\[
V_\theta(s)\approx V^\pi(s)\\
Q_\theta(s, a)\approx Q^\pi(s, a)
\]</span> A policy was generated directly from the value function using <span class="math inline">\(\epsilon\)</span>-greedy.</p>
<p>In this lecture we will directly parametrise the policy <span class="math display">\[
\pi_\theta(s, a)=\mathbb{P}[a|s, \theta]
\]</span> We will focus again on <span class="math inline">\(\color{red}{\mbox{model-free}}\)</span> reinforcement learning.</p>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/Policy-Gradient/">Policy Gradient</a><a href="/tags/REINFORCE/">REINFORCE</a><a href="/tags/Actor-Critic/">Actor-Critic</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2018/01/03/RL - Value Function Approximation/"><span>
        RL - Value Function Approximation</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/01/03/RL - Value Function Approximation/" rel="bookmark">
        <time class="entry-date published" datetime="2018-01-03T10:09:09.000Z">
          2018-01-03
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2 id="introduction">Introduction</h2>
<p>This lecture will introduce how to scale up our algorithm to real practical RL problems by value function approximation.</p>
<p>Reinforcement learning can be used to solve <em>large</em> problems, e.g.</p>
<ul>
<li>Backgammon: <span class="math inline">\(10^{20}\)</span> states</li>
<li>Computer Go: <span class="math inline">\(10^{170}\)</span> states</li>
<li>Helicopter: continuous state space</li>
</ul>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Deep-Learning/">Deep Learning</a><a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/DQN/">DQN</a><a href="/tags/Neural-Network/">Neural Network</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2017/12/21/RL - Model-Free Control/"><span>
        RL - Model-Free Control</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/12/21/RL - Model-Free Control/" rel="bookmark">
        <time class="entry-date published" datetime="2017-12-21T12:00:09.000Z">
          2017-12-21
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2 id="introduction">Introduction</h2>
<p>Last lecture:</p>
<ul>
<li>Model-free prediction</li>
<li><em>Estimate</em> the value function of an <em>unknown</em> MDP</li>
</ul>
<p>This lecture:</p>
<ul>
<li>Model-free control</li>
<li><strong>Optimise</strong> the value function of an unknown MDP</li>
</ul>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/Q-learning/">Q-learning</a><a href="/tags/Monte-Carlo-Control/">Monte-Carlo Control</a><a href="/tags/Sarsa/">Sarsa</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2017/12/16/RL - Model-Free Prediction/"><span>
        RL - Model-Free Prediction</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/12/16/RL - Model-Free Prediction/" rel="bookmark">
        <time class="entry-date published" datetime="2017-12-16T07:00:09.000Z">
          2017-12-16
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2 id="introduction">Introduction</h2>
<p>Last lecture, David taught us how to solve a <em>known</em> MDP, which is <em>planning by dynamic programming</em>. In this lecture, we will learn how to estimate the value function of an <strong>unknown</strong> MDP, which is <em>model-free prediction</em>. And in the next lecture, we will <em>optimise</em> the value function of an unknown MDP.</p>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/Model-Free/">Model-Free</a><a href="/tags/Monte-Carlo-Learning/">Monte-Carlo Learning</a><a href="/tags/TD/">TD</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2017/12/07/RL - Planning by Dynamic Programming/"><span>
        RL - Planning by Dynamic Programming</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/12/07/RL - Planning by Dynamic Programming/" rel="bookmark">
        <time class="entry-date published" datetime="2017-12-07T07:48:19.000Z">
          2017-12-07
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p><strong>Table of Contents</strong></p>
<!-- toc -->
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#policy-evaluation">Policy Evaluation</a></li>
<li><a href="#policy-iteration">Policy Iteration</a></li>
<li><a href="#value-iteration">Value Iteration</a></li>
<li><a href="#extentions-to-dynamic-programming">Extentions to Dynamic Programming</a></li>
<li><a href="#contraction-mapping">Contraction Mapping</a></li>
</ul>
<!-- tocstop -->
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/MDP/">MDP</a><a href="/tags/Dynamic-Programming/">Dynamic Programming</a><a href="/tags/Policy-Iteration/">Policy Iteration</a><a href="/tags/Value-Iteration/">Value Iteration</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2017/08/18/RL - Markov Decision Processes/"><span>
        RL - Markov Decision Processes</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/08/18/RL - Markov Decision Processes/" rel="bookmark">
        <time class="entry-date published" datetime="2017-08-18T07:02:19.000Z">
          2017-08-18
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p><strong>Table of Contents</strong></p>
<!-- toc -->
<ul>
<li><a href="#markov-processes">Markov Processes</a></li>
<li><a href="#markov-reward-process">Markov Reward Process</a></li>
<li><a href="#markov-decision-process">Markov Decision Process</a></li>
</ul>
<!-- tocstop -->
<h2 id="markov-processes">Markov Processes</h2>
<p>Basically, <strong>Markov decision processes</strong> formally describe an environment for reinforcement learning, where the environment is <strong>fully observable</strong>, which means the current state completely characterises the process.</p>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/MDP/">MDP</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2017/08/15/RL - Introduction to Reinforcement Learning/"><span>
        RL - Introduction to Reinforcement Learning</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/08/15/RL - Introduction to Reinforcement Learning/" rel="bookmark">
        <time class="entry-date published" datetime="2017-08-15T07:55:19.000Z">
          2017-08-15
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p>RL, especially DRL (Deep Reinforcement Learning) has been an fervent research area during these years. One of the most famous RL work would be AlphaGo, who has beat <a href="https://www.wikiwand.com/en/Lee_Sedol" target="_blank" rel="noopener">Lee Sedol</a>, one of the best players at Go, last year. And in this year (2017), AlphaGo won three games with Ke Jie, the world No.1 ranked player. Not only in Go, AI has defeated best human play in many games, which illustrates the powerful of the combination of Deep Learning and Reinfocement Learning. However, despite AI plays better games than human, AI takes more time, data and energy to train which cannot be said to be very intelligent. Still, there are numerous unexplored and unsolved problems in RL research, that's also why we want to learn RL.</p>
<p>This is the first note of David Silver's <a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">RL course</a>.</p>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/AlphaGo/">AlphaGo</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/DRL/">DRL</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2017/08/15/SAN for Image QA/"><span>
        Paper Reading - Stacked Attention Networks for Image QA</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/08/15/SAN for Image QA/" rel="bookmark">
        <time class="entry-date published" datetime="2017-08-15T01:30:19.000Z">
          2017-08-15
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <blockquote>
<p>Zichao Yang, Xiaodong He, Jianfeng Gao , Li Deng , Alex Smola <a href="https://arxiv.org/abs/1511.02274" target="_blank" rel="noopener">Stacked Attention Networks for Image Question Answering</a></p>
</blockquote>
<p>这篇文章发表在CVPR2016，作者把 attention 机制应用在 Visual QA，不但能理解神经网络生成答案的 multiple resoning，而且获得了当时最好的效果。</p>
<p>SAN总共由三部分组成：</p>
<ul>
<li>Image Model：用来编码图片信息</li>
<li>Question Moel：用来编码问题信息</li>
<li>Stacked Attention Networks：通过多层 attention layer 不断优化对问题的编码</li>
</ul>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Deep-Learning/">Deep Learning</a><a href="/tags/Attention/">Attention</a><a href="/tags/CV/">CV</a><a href="/tags/CNN/">CNN</a><a href="/tags/VQA/">VQA</a>
      </span>
      

    </div>

    
  </div>
</article>


  <article>

  
  
  <h3 class="article-title"><a href="/2017/08/14/Neural Machine Translation In Linear Time (ByteNet)/"><span>
        Paper Reading - Neural Machine Translation In Linear Time (ByteNet)</span></a></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/08/14/Neural Machine Translation In Linear Time (ByteNet)/" rel="bookmark">
        <time class="entry-date published" datetime="2017-08-14T01:30:19.000Z">
          2017-08-14
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p>ByteNet 可用于<strong>字符级</strong>的机器翻译模型并且有着很好的表现，它的特点在于可以在线性时间 (linear time) 完成翻译而且能够处理长距离依赖。它也采用编码器-解码器架构，并且编码器和解码器都由CNN组成。</p>
<p>ByteNet 之所以有上述的这些特性，是因为使用了如下一些技术：</p>
<ul>
<li>Dynamic Unfolding
<ul>
<li>解决了生成不同长度翻译的问题</li>
</ul></li>
<li>Dilated Convolution
<ul>
<li>缩短了依赖传播的距离</li>
</ul></li>
<li>Masked 1D Convolution
<ul>
<li>保证训练时只用过去的信息生成当前字符</li>
</ul></li>
<li>Residual Blocks
<ul>
<li>解决梯度消失问题</li>
</ul></li>
</ul>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Deep-Learning/">Deep Learning</a><a href="/tags/NLP/">NLP</a><a href="/tags/NMT/">NMT</a><a href="/tags/Deep-NLP/">Deep NLP</a><a href="/tags/CNN/">CNN</a><a href="/tags/ByteBet/">ByteBet</a>
      </span>
      

    </div>

    
  </div>
</article>



<nav class="pagination">
  
  <a href="/page/2/" class="pagination-prev">Prev</a>
  
  
  <a href="/page/4/" class="pagination-next">Next</a>
  
</nav>
      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2019
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>