<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    PyTorch源码浅析(2)：THC | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="PyTorch源码浅析(2)：THC">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script>
  <script type="text/javascript" src="/js/search.js"></script>
  <script type="text/javascript" src="/js/jquery.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">博客</a></li>
      
        <li><a href="/notes">笔记</a></li>
      
        <li><a href="/archives">归档</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/search">搜索</a></li>
      
        <li><a href="/about">关于</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      PyTorch源码浅析(2)：THC</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2019/05/05/PyTorch2/" rel="bookmark">
        <time class="entry-date published" datetime="2019-05-05T07:53:01.000Z">
          2019-05-05
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p>这篇主要看 Torch CUDA 部分，对应源码目录<code>aten/src/THC</code>，里面包含了许多C++和CUDA代码。这部分实现了操作 THCTensor 和 THCStorage 的接口，不过底层用的数据结构还是<code>TensorImpl</code>和<code>StorageImpl</code>。THC里的接口也是通过C语言范式实现的，但是Apply系列操作不再由宏来实现，而是使用了C++模板。其他的区别还有allocator不同，以及多了 THCState 结构。</p>
<a id="more"></a>
<p>记号：</p>
<ul>
<li>TH = TorcH</li>
<li>THC = TorcH Cuda</li>
</ul>
<p><strong>目录</strong></p>
<!-- toc -->
<ul>
<li><a href="#thcstate">THCState</a></li>
<li><a href="#thcallocator">THCAllocator</a>
<ul>
<li><a href="#thccachingallocator">THCCachingAllocator</a></li>
<li><a href="#thccachinghostallocator">THCCachingHostAllocator</a></li>
</ul></li>
<li><a href="#thcapply">THCApply</a></li>
<li><a href="#总结">总结</a></li>
</ul>
<!-- tocstop -->
<h2><span id="thcstate">THCState</span></h2>
<p>通过观察THC里面实现的接口不难发现，几乎每个接口都需要传入一个<code>THCState*</code>参数，而这是在TH中没有的，这个THCState的声明在<code>THCGeneral.hpp</code>中：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">THCState</span> &#123;</span></span><br><span class="line">  <span class="comment">// 貌似是用来生成随机数的</span></span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">THCRNGState</span>* <span class="title">rngState</span>;</span></span><br><span class="line">  <span class="comment">// 记录每个CUDA设备的cuBLAS句柄和cuSparse句柄</span></span><br><span class="line">  THCCudaResourcesPerDevice* resourcesPerDevice;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// cuda设备个数</span></span><br><span class="line">  <span class="keyword">int</span> numDevices; 	</span><br><span class="line"></span><br><span class="line">  at::Allocator* cudaHostAllocator;     <span class="comment">// 内存分配器</span></span><br><span class="line">  at::Allocator* cudaDeviceAllocator;   <span class="comment">// 显存分配器</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 二维数组，大小为 numDevices * numDevices</span></span><br><span class="line">  <span class="comment">// 数组中的每一项 i, j 记录了 CUDA_i 能否直接从 CUDA_j 复制数据</span></span><br><span class="line">  <span class="comment">// 如果值为 1 代表允许拷贝，0 代表不允许，-1 代表不知道（默认值）</span></span><br><span class="line">  <span class="keyword">int</span>** p2pAccessEnabled;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>THCState是一个全局CUDA状态，记录了CUDA设备的所有有用的信息，它的初始化在<code>THCGeneral.cpp</code>中，代码并不难理解。</p>
<blockquote>
<p>注 <strong>host和device</strong>：在CUDA编程中，<em>host</em> 指的是CPU和它的内存，而 <em>device</em> 指GPU及其显存。在 <em>host</em> 上运行的代码可以操控内存和显存，还可以启动 <em>kernels</em> 在GPU上计算。因为CUDA编程的这些特性，一个典型CUDA程序的执行过程为：</p>
<ol type="1">
<li><p>分配内存和显存空间</p></li>
<li><p>初始化内存数据（host）</p></li>
<li><p>把数据从内存 (host) 传送到显存 (device)</p></li>
<li><p>执行 kernels</p></li>
<li><p>把结果从显存 (device) 传回内存 (host)</p></li>
</ol>
</blockquote>
<h2><span id="thcallocator">THCAllocator</span></h2>
<p><br></p>
<h3><span id="thccachingallocator">THCCachingAllocator</span></h3>
<p>查看THCState初始化的代码不难发现，显存分配器<code>cudaDeviceAllocator</code>的类型为<code>CudaCachingAllocator</code>，它的实现在<code>c10/cuda/CUDACachingAllocator.cpp</code>中。进一步观察后发现，<code>CudaCachingAllocator</code>调用的分配内存函数实际上是<code>THCCachingAllocator</code>实现的，<code>THCCachingAllocator</code>不光向系统请求分配显存，还实现了缓存管理系统，我们主要来看一下<code>THCCachingAllocator</code>的实现。</p>
<p>首先声明一个内存区块的结构体，里面存储设备、stream、区块大小等信息：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Block</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span>           device;      <span class="comment">// gpu</span></span><br><span class="line">  cudaStream_t  stream;      <span class="comment">// allocation stream</span></span><br><span class="line">  stream_set    stream_uses; <span class="comment">// streams on which the block was used</span></span><br><span class="line">  <span class="keyword">size_t</span>        size;        <span class="comment">// block size in bytes</span></span><br><span class="line">  <span class="keyword">char</span>*         ptr;         <span class="comment">// memory address</span></span><br><span class="line">  <span class="keyword">bool</span>          allocated;   <span class="comment">// in-use flag      </span></span><br><span class="line">  <span class="keyword">int</span>           event_count; <span class="comment">// number of outstanding CUDA events</span></span><br><span class="line">  <span class="comment">// prev block if split from a larger allocation</span></span><br><span class="line">  Block*        prev;</span><br><span class="line">  <span class="comment">// next block if split from a larger allocation</span></span><br><span class="line">  Block*        next;</span><br><span class="line">  </span><br><span class="line">  Block(<span class="keyword">int</span> device, cudaStream_t stream, <span class="keyword">size_t</span> size) :</span><br><span class="line">      device(device), stream(stream), stream_uses(), size(size),</span><br><span class="line">      ptr(ptr), allocated(<span class="number">0</span>), prev(<span class="literal">NULL</span>), next(<span class="literal">NULL</span>),</span><br><span class="line">      event_count(<span class="number">0</span>) &#123; &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>为了方便比较查找，为<code>Block</code>定义比较大小的方法：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">bool</span> <span class="title">BlockComparator</span><span class="params">(<span class="keyword">const</span> Block* a, <span class="keyword">const</span> Block* b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">// 首先比较设备ID</span></span><br><span class="line">  <span class="keyword">if</span> (a-&gt;device != b-&gt;device) &#123;</span><br><span class="line">    <span class="keyword">return</span> a-&gt;device &lt; b-&gt;device;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 再比较stream id</span></span><br><span class="line">  <span class="keyword">if</span> (a-&gt;stream != b-&gt;stream) &#123;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">uintptr_t</span>)a-&gt;stream &lt; (<span class="keyword">uintptr_t</span>)b-&gt;stream;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 再比较区块大小</span></span><br><span class="line">  <span class="keyword">if</span> (a-&gt;size != b-&gt;size) &#123;</span><br><span class="line">    <span class="keyword">return</span> a-&gt;size &lt; b-&gt;size;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 最后比较内存地址大小</span></span><br><span class="line">  <span class="keyword">return</span> (<span class="keyword">uintptr_t</span>)a-&gt;ptr &lt; (<span class="keyword">uintptr_t</span>)b-&gt;ptr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用此比较函数的话<code>Block(device, NULL, 0)</code>就是设备device上区块大小的下限，而<code>Block(device + 1, NULL, 0)</code>则是设备device上的区块大小上限。</p>
<p>接下来看<code>THCCachingAllocator</code>定义的属性：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">THCCachingAllocator</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">typedef</span> <span class="title">bool</span> <span class="params">(*Comparison)</span><span class="params">(<span class="keyword">const</span> Block*, <span class="keyword">const</span> Block*)</span></span>;</span><br><span class="line">  <span class="keyword">typedef</span> <span class="built_in">std</span>::<span class="built_in">set</span>&lt;Block*, Comparison&gt; FreeBlocks;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// device statistics</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;DeviceStats&gt; device_stats;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// lock around all operations</span></span><br><span class="line">  <span class="built_in">std</span>::mutex mutex;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// lock around calls to cudaFree (to prevent deadlocks with NCCL)</span></span><br><span class="line">  <span class="built_in">std</span>::mutex cuda_free_mutex;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// cached blocks larger than 1 MB</span></span><br><span class="line">  FreeBlocks large_blocks;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// cached blocks 1 MB or smaller</span></span><br><span class="line">  FreeBlocks small_blocks;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// allocated blocks by device pointer</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">unordered_map</span>&lt;<span class="keyword">void</span>*, Block*&gt; allocated_blocks;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// outstanding cuda events</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">deque</span>&lt;<span class="built_in">std</span>::pair&lt;cudaEvent_t, Block*&gt;&gt; cuda_events;</span><br><span class="line"></span><br><span class="line">  THCCachingAllocator() :</span><br><span class="line">      large_blocks(BlockComparator),</span><br><span class="line">      small_blocks(BlockComparator) &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个属性是干什么的注释已经写的很清楚了，注意到<code>FreeBlocks</code>类型是Block的有序集合，曾经分配过但是用完了的内存会被缓存起来，大于1M的分块会进入<code>large_blocks</code>，小于等于1M的分块进入<code>small_blocks</code>。之后再向分配器申请内存时会先从缓存里面分配，分配策略为Best-fit，即返回大于等于所需大小的最小分块（使用<code>set::lower_bound</code>实现）。如果缓存中的所有分块都小于目标大小，那么会尝试<code>cudaMalloc</code>分配目标大小的内存，如果还是失败的话就把缓存release了再试<code>cudaMalloc</code>，如果还是分配失败就会报内（显）存不足的错误。</p>
<p>如果上面某一步分配内存成功了，由于分配的块很有可能比实际需要的size大，所以还要进行切割操作。切割操作就是判断多余的大小是否大于1M，如果大于1M就插入到<code>large_blocks</code>，否则插入到<code>small_blocks</code>中，然后再设置一下<code>prev</code>和<code>next</code>指针即可。</p>
<p>分配内存的（简略）实现如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment"> * allocates a block which is safe to use from the provided stream </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">void</span> THCCachingAllocator::<span class="built_in">malloc</span>(<span class="keyword">void</span>** devPtr, <span class="keyword">size_t</span> size, cudaStream_t stream)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">// 加锁，函数返回时自动释放</span></span><br><span class="line">  <span class="built_in">std</span>::lock_guard&lt;<span class="built_in">std</span>::mutex&gt; lock(mutex);</span><br><span class="line">  <span class="comment">// 获取设备ID</span></span><br><span class="line">  <span class="keyword">int</span> device;</span><br><span class="line">  C10_CUDA_CHECK(cudaGetDevice(&amp;device));</span><br><span class="line">  <span class="comment">// 类似四舍五入</span></span><br><span class="line">  size = round_size(size);</span><br><span class="line">  <span class="keyword">bool</span> small = size &lt;= kSmallAlloc;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 搜索目标</span></span><br><span class="line">  <span class="function">Block <span class="title">search_key</span><span class="params">(device, stream, size)</span></span>;</span><br><span class="line">  <span class="comment">// 判断应该从哪个集合分配</span></span><br><span class="line">  <span class="keyword">auto</span>&amp; free_blocks = small ? small_blocks : large_blocks;</span><br><span class="line"></span><br><span class="line">  Block* block = <span class="literal">NULL</span>;</span><br><span class="line">  Block* remaining = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 搜索大于等于目标的第一个块</span></span><br><span class="line">  <span class="keyword">auto</span> it = free_blocks.lower_bound(&amp;search_key);</span><br><span class="line">  <span class="keyword">if</span> (it != free_blocks.end() &amp;&amp; (*it)-&gt;device == device &amp;&amp; (*it)-&gt;stream == stream) &#123;</span><br><span class="line">    <span class="comment">// 如果device和stream和目标相同的话就分配该块内存</span></span><br><span class="line">    block = *it;</span><br><span class="line">    free_blocks.erase(it);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">void</span>* ptr;</span><br><span class="line">    <span class="keyword">size_t</span> alloc_size = small ? kSmallAlloc : size;</span><br><span class="line">    <span class="comment">// 尝试向系统申请内存</span></span><br><span class="line">    cudaError_t err = cuda_malloc_retry(device, &amp;ptr, alloc_size);</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess) &#123;</span><br><span class="line">      <span class="keyword">if</span> (err == cudaErrorMemoryAllocation) &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">auto</span>&amp; stats = get_stats_for_device(device);</span><br><span class="line">        <span class="comment">// 报错：分配内存失败！</span></span><br><span class="line">        AT_ERROR(<span class="string">"CUDA out of memory. Tried to allocate"</span>);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        C10_CUDA_CHECK(err);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    block = <span class="keyword">new</span> Block(device, stream, alloc_size, (<span class="keyword">char</span>*)ptr);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (block-&gt;size - size &gt;= (small? kRoundSmall : kSmallAlloc+<span class="number">1</span>)) &#123;</span><br><span class="line">    remaining = block;</span><br><span class="line">    <span class="comment">// 切割多余内存块</span></span><br><span class="line">    block = <span class="keyword">new</span> Block(device, stream, size, block-&gt;ptr);</span><br><span class="line">    block-&gt;prev = remaining-&gt;prev;</span><br><span class="line">    <span class="keyword">if</span> (block-&gt;prev) &#123;</span><br><span class="line">      block-&gt;prev-&gt;next = block;</span><br><span class="line">    &#125;</span><br><span class="line">    block-&gt;next = remaining;</span><br><span class="line"></span><br><span class="line">    remaining-&gt;prev = block;</span><br><span class="line">    remaining-&gt;ptr += size;</span><br><span class="line">    remaining-&gt;size -= size;</span><br><span class="line">    <span class="comment">// 把剩余块插入缓存</span></span><br><span class="line">    free_blocks.insert(remaining);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  block-&gt;allocated = <span class="literal">true</span>;</span><br><span class="line">  allocated_blocks[block-&gt;ptr] = block;</span><br><span class="line"></span><br><span class="line">  *devPtr = (<span class="keyword">void</span>*)block-&gt;ptr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>释放内存的时候不会把内存直接还给系统，而是缓存起来根据块大小插入<code>large_blocks</code>或<code>small_blocks</code>，在插入之前还会检查能否和之前被切割的部分合并。</p>
<blockquote>
<p>注 <strong>CUDA架构</strong>：从硬件上看，CUDA设备含有 SP (Streaming Processor) 和 SM (Streaming Multiprocessor)；从软件上看，有 thread, block, grid, 和 warp 等概念。</p>
<ul>
<li><p>SP：最基本的处理单元，最后具体的指令都是在SP上处理的。</p></li>
<li><p>SM：多个SP加上其他资源，如：warp scheduler, register, shared memory等组成的大核，相当于CPU的核心。</p></li>
<li><p>thread：普通的线程，运行在SP上。</p></li>
<li><p>block：多个线程会组成一个block，同一个block中的线程可以通过shared memory通信。同一个block中的线程在同一个SM中执行。</p></li>
<li><p>grid：多个block构成一个grid。</p></li>
<li><p>warp：调度和运行的基本单元，包含多个线程。每个线程执行相同指令，但是数据不同（SIMT）。一个warp需要占用一个SM运行，多个warps需要轮流进入SM。</p></li>
<li><p>stream：一个GPU操作队列，该队列中的操作将以添加到流中的先后顺序而依次执行。</p></li>
</ul>
<p><img src="/images/pytorch/cuda.png"></p>
</blockquote>
<h3><span id="thccachinghostallocator">THCCachingHostAllocator</span></h3>
<p><code>THCState</code>中还有一个内存分配器<code>cudaHostAllocator</code>用来分配main memory，这个分配器指针指向<code>HostAllocator</code>，实现在<code>aten/src/THC/THCCachingHostAllocator.cpp</code>中。这个分配器的实现和上面的类似，也提供了缓存功能。值得注意的是，它向系统分配内存是用的<code>cudaHostAlloc()</code>函数，这是cuda库函数，它与<code>malloc()</code>不同。<code>malloc()</code>分配的标准的、可分页的主机内存，而<code>cudaHostAlloc()</code>分配的是页锁定的主机内存，也称作固定内存 (pinned memory)。它的一个重要特点是操作系统将不会对这块内存分页并交换到磁盘上，从而保证了内存始终驻留在物理内存中。由于GPU知道内存的物理地址，因此就可以使用DMA技术来在GPU和CPU之间复制数据，加快复制速度。</p>
<p><img src="/images/pytorch/cpugpu.png"></p>
<h2><span id="thcapply">THCApply</span></h2>
<p>和TH中一样，THC也有Apply系列函数，不同的是THC中的Apply不再用宏实现，而是用C++模板函数实现，代码在<code>src/THC/THCApply.cuh</code>。</p>
<p>首先来看直接运行在GPU上的kernel的实现：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Op,</span><br><span class="line">          <span class="keyword">typename</span> Ta,</span><br><span class="line">          <span class="keyword">typename</span> IndexType,</span><br><span class="line">          <span class="keyword">int</span> ADims&gt;</span><br><span class="line">__global__ <span class="keyword">void</span></span><br><span class="line">kernelPointwiseApply1(<span class="keyword">const</span> OffsetInfo&lt;Ta, IndexType, ADims&gt; a,</span><br><span class="line">                      IndexType totalElements,</span><br><span class="line">                      Op op) &#123;</span><br><span class="line">  <span class="keyword">for</span> (IndexType linearIndex = (IndexType) blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">       linearIndex &lt; totalElements;</span><br><span class="line">       linearIndex += (IndexType) gridDim.x * blockDim.x) &#123;</span><br><span class="line">    op(a.get(linearIndex));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的kernel实现了对Tensor的一元操作，也就是对Tensor中的每一个都数据都执行<code>op</code>操作，二元和三元操作与一元操作代码类似，就不列出了。这段理解起来不难，只有三点需要特殊说明一下：</p>
<p><strong>函数对象</strong></p>
<p>模板参数中的<code>Op</code>是<a href="https://www.wikiwand.com/zh-hans/%E5%87%BD%E6%95%B0%E5%AF%B9%E8%B1%A1" target="_blank" rel="noopener">函数对象</a>类型，它实现了<code>operator()</code>方法，这样的话它的实例对象就可以直接当函数来用，如：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">IntComparator</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> &amp;a, <span class="keyword">const</span> <span class="keyword">int</span> &amp;b)</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a &lt; b;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>这就是一个很常用的用来比较整数大小的函数对象类型，它可以这样用：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">IntComparator op;</span><br><span class="line">op(<span class="number">1</span>, <span class="number">2</span>);							<span class="comment">// true</span></span><br></pre></td></tr></table></figure>
<p>函数对象类似于函数指针，但有两个优点：第一是编译器可以内联执行函数对象的调用；第二是函数对象内部可以保持状态。</p>
<p><strong>循环下标</strong></p>
<p>循环的下标每次增加<code>gridDim.x * blockDim.x</code>，而不是通常的<code>1</code>，这就涉及到kernel是如何执行的了。首先kernel被分配到多个Grid上执行，每个Grid里有多个Block，每个Block里有多个Thread，这些线程(Thread)都执行相同的代码，也就是kernel。为了让这些线程分工合作，每个线程都记录了<code>gridDim</code> <code>blockDim</code>,<code>blockIdx</code>, 和<code>threadIdx</code>，分别代表Grid维度，Block维度，Block ID，和Thread ID。在这里，Grid和Block都是一维的，所以当前线程的全局ID可以通过</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blockIdx.x * blockDim.x + threadIdx.x</span><br></pre></td></tr></table></figure>
<p>得到，其中<code>blockIdx.x</code>表示第几个Block，<code>blockDim.x</code>是一个Block内有多少线程，<code>threadIdx.x</code>是该线程在Block内的ID。</p>
<p>再看下标递增的间隔<code>gridDim.x * blockDim.x</code>实际上是执行这个kernel的线程个数，即一个Grid内的Block数 <span class="math inline">\(\times\)</span> 一个Block内线程数。这样的好处是，如果不同线程要读取的内存是连续的，则这些内存读取可以捆绑到一起进行，加快了内存读取速度，如下图。</p>
<p><img src="/images/pytorch/coalesced.png"></p>
<p><strong>OffsetInfo</strong></p>
<p>注意到循环体里只有一句代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">op(a.get(linearIndex));</span><br></pre></td></tr></table></figure>
<p>意思对第<code>linearIndex</code>个数据进行<code>op</code>操作。由于<code>linearIndex</code>是线性地址，并不是数据真正的内存偏移，所以 <code>a.get()</code>的作用是把线性地址转换为实际数据的存储地址，而<code>a</code>的类型是<code>OffsetInfo&lt;Ta, IndexType, Dims&gt;</code>。</p>
<p>转换的算法其实很简单，线性地址就是第几个数据，比如<code>x[0][0]</code>是第0个数据，线性地址就是0，<code>x[0][1]</code>的线性地址就是1，<code>x[i][j]</code>的线性地址就是<code>i * size[0] + j</code>。而且我们知道<code>x[i][j]</code>的内存偏移为<code>i * strides[0] + j * strides[1]</code>，那么转换算法要做的就是把线性地址转换为<code>ijk</code>下标，然后再把下标转化为内存地址：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> IndexType&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">IndexToOffset</span>&lt;T, IndexType, -1&gt; &#123;</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">inline</span> __host__ __<span class="function">device__ IndexType <span class="title">get</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    IndexType linearId,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> TensorInfo&lt;T, IndexType&gt;&amp; info)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    IndexType offset = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从最外围下标开始计算</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = info.dims - <span class="number">1</span>; i &gt; <span class="number">0</span>; --i) &#123;</span><br><span class="line">    	<span class="comment">// 求出第i维下标</span></span><br><span class="line">      IndexType curDimIndex = linearId % info.sizes[i];</span><br><span class="line">      <span class="comment">// 转换为内存偏移</span></span><br><span class="line">      IndexType curDimOffset = curDimIndex * info.strides[i];</span><br><span class="line">      <span class="comment">// 和总偏移相加</span></span><br><span class="line">      offset += curDimOffset;</span><br><span class="line">      <span class="comment">// 计算第i-1维的线性地址</span></span><br><span class="line">      linearId /= info.sizes[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> offset + linearId * info.strides[<span class="number">0</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>细心的读者可能注意到了，上面的函数是<code>IndexToOffset</code>的方法，并不是<code>OffsetInfo</code>的，实际上后者对非负整数除法和取余进行了优化，把除法转换为一系列乘法，从而加快计算速度。</p>
<p>大致思想是这样的，假设我们要计算 <span class="math inline">\(\lfloor\frac{n}{d}\rfloor\)</span>，对任意N位非负整数<span class="math inline">\(d\)</span>，总可以找到一个 magic number <span class="math inline">\(m\)</span> 和 <span class="math inline">\(s\)</span>，使得： <span class="math display">\[
\lfloor\frac{n}{d}\rfloor=\lfloor\frac{m\times n}{2^{N+s}}\rfloor
\]</span> 这样就把除法操作转化为乘法和右移了。</p>
<p>那么怎么找 <span class="math inline">\(m\)</span> 和 <span class="math inline">\(s\)</span> 呢，非常简单： <span class="math display">\[
s=\lceil\log_2 d\rceil \\
m = \lfloor 2^N\times\frac{(2^s-d)}{d}\rfloor+2^N+1
\]</span> 对于固定的除数 <span class="math inline">\(d\)</span>，这两个参数是不会变的，如果需要多次除以 <span class="math inline">\(d\)</span>，则可以提前计算好 <span class="math inline">\(m\)</span> 和 <span class="math inline">\(s\)</span>，在计算时加快速度。注意到计算 <span class="math inline">\(m\)</span> 时也是需要除法运算的，所以如果这个除数只用一次的话，那么用快速除法是没意义的（除非<span class="math inline">\(m\)</span>是在编译期计算的）。关于这个式子的证明和推导看<a href="http://ridiculousfish.com/blog/posts/labor-of-division-episode-i.html" target="_blank" rel="noopener">这里</a>。</p>
<p>代码中，<code>OffsetInfo</code>的实现实际上是跟模板参数<code>Dims</code>相关的，如果维度能在编译期给出的话（值非-1），则在生成<code>OffsetInfo</code>对象时计算 <span class="math inline">\(m\)</span> 和 <span class="math inline">\(s\)</span>，以备之后使用：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> IndexType, <span class="keyword">int</span> Dims&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">OffsetInfo</span> &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">OffsetInfo</span><span class="params">(<span class="keyword">const</span> TensorInfo&lt;T, IndexType&gt;&amp; tinfo)</span> </span>&#123;</span><br><span class="line">    assert(tinfo.dims == Dims);</span><br><span class="line">    data = tinfo.data;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; Dims; ++i) &#123;</span><br><span class="line">      <span class="comment">// 提前计算 m, s (实现在 IntDivider 类中)</span></span><br><span class="line">      sizes[i] = IntDivider&lt;IndexType&gt;(tinfo.sizes[i]);</span><br><span class="line">      strides[i] = tinfo.strides[i];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  __host__ __<span class="function">device__ T* <span class="title">get</span><span class="params">(IndexType linearIndex)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    IndexType offset = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = Dims - <span class="number">1</span>; i &gt; <span class="number">0</span>; --i) &#123;</span><br><span class="line">      <span class="comment">// 使用快速除法</span></span><br><span class="line">      DivMod&lt;IndexType&gt; divmod = sizes[i].divmod(linearIndex);</span><br><span class="line">      linearIndex = divmod.div;</span><br><span class="line">      offset += divmod.mod * strides[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &amp;data[offset + linearIndex * strides[<span class="number">0</span>]];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  T* data;</span><br><span class="line">  <span class="comment">// Dims 是编译期常量，所以 sizes 和 strides 是静态分配的</span></span><br><span class="line">  IntDivider&lt;IndexType&gt; sizes[Dims];</span><br><span class="line">  IndexType strides[Dims];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>如果创建<code>OffsetInfo</code>时，<code>Dims</code>的值为-1的话，代表Tensor的大小不是固定的，这样的话<code>OffsetInfo::sizes</code>和<code>OffsetInfo::strides</code>是动态分配的，就会触发NVCC的一个bug：<em>if a kernel argument contains an array that is dynamically accessed, the whole array is first copied into the local memory. Pre-computation makes it worse because now we have more data to copy.</em></p>
<p>所以对于大小不固定（编译期不能给出具体维度）的Tensor采用之前的办法计算，这里使用了特化模板匹配<code>Dims</code>为-1的情况：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> IndexType&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">OffsetInfo</span>&lt;T, IndexType, -1&gt; &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">OffsetInfo</span><span class="params">(<span class="keyword">const</span> TensorInfo&lt;T, IndexType&gt;&amp; tinfo)</span></span></span><br><span class="line">    : tinfo(tinfo) &#123; &#125;</span><br><span class="line"></span><br><span class="line">  __host__ __<span class="function">device__ T* <span class="title">get</span><span class="params">(IndexType linearIndex)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 直接调用之前列出的 IndexToOffset::get 方法</span></span><br><span class="line">    IndexType offset = IndexToOffset&lt;T, IndexType, <span class="number">-1</span>&gt;::get(linearIndex, tinfo);</span><br><span class="line">    <span class="keyword">return</span> &amp;tinfo.data[offset];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  TensorInfo&lt;T, IndexType&gt; tinfo;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h2><span id="总结">总结</span></h2>
<p>总结一下，THC中的API的声明和实现都在<code>generic</code>目录下，API的形式是C风格范式：<code>THCTensor_(xxx)(...)</code>。这些函数几乎都会调用 apply 系列函数来在GPU中实现具体功能，而 apply 函数的核心在于传入的OP，这些OP都定义在<code>THC/</code>根目录下。</p>
<p>举个例子，来看一下<code>THCTensor_fill(state, self, value)</code>是怎么执行的，它的功能是把<code>self</code>指向的Tensor的每个元素赋值为<code>value</code>。这个API的定义在<code>THC/generic/THCTensorMath.cu</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THCTensor_</span><span class="params">(fill)</span><span class="params">(THCState* state, THCTensor *self_, <span class="keyword">scalar_t</span> value)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  THCAssertSameGPU(THCTensor_(checkGPU)(state, <span class="number">1</span>, self_));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!THC_pointwiseApply1&lt;<span class="keyword">scalar_t</span>&gt;(</span><br><span class="line">        state, self_, TensorFillOp&lt;<span class="keyword">scalar_t</span>&gt;(value))) &#123;</span><br><span class="line">    THArgCheck(<span class="literal">false</span>, <span class="number">1</span>, CUTORCH_DIM_WARNING);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  THCudaCheck(cudaGetLastError());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>核心代码为：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">THC_pointwiseApply1&lt;<span class="keyword">scalar_t</span>&gt;(state, self_, TensorFillOp&lt;<span class="keyword">scalar_t</span>&gt;(value))</span><br></pre></td></tr></table></figure>
<p>这句话调用一元 apply 函数，传入的参数为 THCState, 要操作的Tensor和相应OP。<code>THC_pointwiseApply1()</code>会进行一些特殊情况的处理和优化，最后调用之前列出的apply kernel执行相应OP。</p>
<p><code>TensorFillOp</code>定义在<code>THC/THCTensorMath.cu</code>中，</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TensorFillOp</span> &#123;</span></span><br><span class="line">  TensorFillOp(T v) : val(v) &#123;&#125;</span><br><span class="line">  __device__ __<span class="function">forceinline__ <span class="keyword">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(T* v)</span> </span>&#123; *v = val; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">const</span> T val;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>注意到填入Tensor的值被保存为类的常量，而不是作为<code>operator()</code>的参数，这样才能统一接口，才能被<code>kernelPointwiseApply1()</code>直接调用。</p>
<p>调用过程的示意图如下：</p>
<p><img src="/images/pytorch/THCTensor_fill.png"></p>
<p>つづく</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">上一篇：<a href="https://www.52coding.com.cn/2019/05/05/PyTorch1/">THTensor</a></th>
<th style="text-align: right;">下一篇：<a href="https://www.52coding.com.cn/2019/05/05/PyTorch3/">NN</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/博客/">博客</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/PyTorch/">PyTorch</a><a href="/tags/Tensor/">Tensor</a><a href="/tags/CUDA/">CUDA</a><a href="/tags/THC/">THC</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>
<script type="text/javascript">
  var $config = {
    image: "icon.png",
  };
  socialShare('.social-share-cs', $config);
</script>



<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80MTI4MC8xNzgyOA==">
	<script type="text/javascript">
		(function (d, s) {
			var j, e = d.getElementsByTagName(s)[0];

			if (typeof LivereTower === 'function') {
				return;
			}

			j = d.createElement(s);
			j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
			j.async = true;

			e.parentNode.insertBefore(j, e);
		})(document, 'script');
	</script>
	<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->


      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2019
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>