<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    PyTorch源码浅析(1)：THTensor | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="PyTorch源码浅析(1)：THTensor">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script>
  <script type="text/javascript" src="/js/search.js"></script>
  <script type="text/javascript" src="/js/jquery.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">博客</a></li>
      
        <li><a href="/notes">笔记</a></li>
      
        <li><a href="/archives">归档</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/search">搜索</a></li>
      
        <li><a href="/about">关于</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      PyTorch源码浅析(1)：THTensor</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2019/05/05/PyTorch1/" rel="bookmark">
        <time class="entry-date published" datetime="2019-05-05T07:51:01.000Z">
          2019-05-05
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p>PyTorch中Tensor的存储和表示分开，多个THTensor可能共享一个THStorage，每个THTensor可能拥有不同的view（e.g. size, stride）。这样设计的好处是，有时看起来不一样的数据底层是共享的，比如矩阵与矩阵的转置、二维矩阵与二维矩阵变成一维时的矩阵。这部分的主要实现在<code>pytorch/aten</code>文件夹中，这里面既实现了底层的Tensor操作库，也封装了名为 <strong>ATen</strong> 的 C++11接口。</p>
<a id="more"></a>
<p><code>aten/src</code>里有几个重要的文件夹，它们实现的内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">src</span><br><span class="line">├── ATen        # Tensor操作的C++11接口</span><br><span class="line">├── TH          # CPU Tensor 的底层实现（本篇内容）</span><br><span class="line">├── THC         # GPU Tensor (CUDA) 的底层实现（在下一篇讲）</span><br><span class="line">├── THCUNN      # CUDA版底层神经网络实现(下下篇讲)</span><br><span class="line">└── THNN        # CPU版底层神经网络实现(下下篇讲)</span><br></pre></td></tr></table></figure>
<p>这篇讲的主要代码也都在TH文件夹内。</p>
<p><strong>目录</strong></p>
<!-- toc -->
<ul>
<li><a href="#thtensor-thstorage">THTensor &amp; THStorage</a>
<ul>
<li><a href="#tensorimpl">TensorImpl</a></li>
<li><a href="#storageimpl">StorageImpl</a></li>
</ul></li>
<li><a href="#智能指针-intrusive_ptr">智能指针 Intrusive_ptr</a></li>
<li><a href="#tensor-apply-dynamic-dispatch">Tensor Apply &amp; Dynamic Dispatch</a></li>
</ul>
<!-- tocstop -->
<h2><span id="thtensor-amp-thstorage">THTensor &amp; THStorage</span></h2>
<p>TH里面的核心类型就是<code>THTensor</code>和<code>THStorage</code>了，前者是Tensor的view，后者是Tensor数据的存储地址。由于Tensor的数据类型可以是多种多样的，而每种类型的API都一致，所以需要用到范型来减少重复代码，TH中是使用宏实现范型功能（因为Torch一开始是用C实现的），不了解的读者可以先参考一下<a href="https://zhuanlan.zhihu.com/p/34496542" target="_blank" rel="noopener">这篇知乎专栏</a>。</p>
<p><code>THTensor</code>的定义在<code>aten/src/TH/generic/THTensor.h</code>中：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THTensor at::TensorImpl</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THFloatTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THDoubleTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THHalfTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THByteTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THCharTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THShortTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THIntTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THLongTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THBoolTensor THTensor</span></span><br></pre></td></tr></table></figure>
<p>同样的，<code>THStorage</code>的定义在<code>Aten/src/TH/generic/THStorage.h</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THStorage at::StorageImpl</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THFloatStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THDoubleStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THHalfStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THByteStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THCharStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THShortStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THIntStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THLongStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THBoolStorage THStorage</span></span><br></pre></td></tr></table></figure>
<p>在<code>THTensor.h</code>中有很多类似这样的API声明：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TH_API THStorage* <span class="title">THTensor_</span><span class="params">(storage)</span><span class="params">(<span class="keyword">const</span> THTensor *self)</span></span>;</span><br></pre></td></tr></table></figure>
<p>其中，<code>THTensor_</code>的宏定义为</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THTensor_(NAME)   TH_CONCAT_4(TH,Real,Tensor_,NAME)</span></span><br></pre></td></tr></table></figure>
<p>上面的<code>TH_CONCAT_4</code>宏就是把它的四个参数连接起来，其中<code>Real</code>会被定义为实际类型（Float, Bool等），所以上面的API会被展开成：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">at::<span class="function">StorageImpl <span class="title">THFloatTensor_storage</span><span class="params">(<span class="keyword">const</span> at::TensorImpl *self)</span></span>;</span><br><span class="line">at::<span class="function">StorageImpl <span class="title">THBoolTensor_storage</span><span class="params">(<span class="keyword">const</span> at::TensorImpl *self)</span></span>;</span><br><span class="line">at::<span class="function">StorageImpl <span class="title">THLongTensor_storage</span><span class="params">(<span class="keyword">const</span> at::TensorImpl *self)</span></span>;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>在这些API中还会用到<code>scalar_t</code>类型，这个也是一个宏，特化的时候会传入具体的类型（如用来确保<code>THFloatTensor</code>里的<code>StorageImpl</code>存储的float类型）。</p>
<p>不难发现，所有的THTensor类型最终都会替换成<code>at::TensorImpl</code>，所有的THStorage类型也都会替换成<code>at::StorageImpl</code>，前者的实现在<code>c10/core/TensorImpl.h</code>中，后者的实现在<code>c10/core/StorageImpl.h</code>中。这两个类型的实现在c10中，也就是说Tensor类型的实现转移到了c10中，但API的实现依然在TH中。</p>
<h3><span id="tensorimpl">TensorImpl</span></h3>
<p>接着具体来看一下<code>TensorImpl</code>的实现，首先来看一下它的声明和属性：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">TensorImpl</span> :</span> <span class="keyword">public</span> c10::intrusive_ptr_target &#123;</span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">  <span class="comment">// 指向实际数据存储位置，也就是指向StorageImpl</span></span><br><span class="line">  Storage storage_;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// 用于自动微分，只对Variable适用</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;c10::AutogradMetaInterface&gt; autograd_meta_ = <span class="literal">nullptr</span>;</span><br><span class="line"></span><br><span class="line">  SmallVector&lt;<span class="keyword">int64_t</span>,<span class="number">5</span>&gt; sizes_;      <span class="comment">// Tensor的实际大小</span></span><br><span class="line">  SmallVector&lt;<span class="keyword">int64_t</span>,<span class="number">5</span>&gt; strides_;    <span class="comment">// 各个维度直接的间隔</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">int64_t</span> storage_offset_ = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int64_t</span> numel_ = <span class="number">1</span>; <span class="comment">// Tensor中元素个数，也就是sizes_数组中元素的乘积</span></span><br><span class="line"></span><br><span class="line">  caffe2::TypeMeta data_type_;        <span class="comment">// 数据类型</span></span><br><span class="line"></span><br><span class="line">  TensorTypeId type_id_;</span><br><span class="line">  <span class="keyword">bool</span> is_contiguous_ = <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">bool</span> is_variable_ = <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">bool</span> is_wrapped_number_ = <span class="literal">false</span>;</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">bool</span> allow_tensor_metadata_change_ = <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">bool</span> reserved_ = <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>TensorImpl</code>继承自<code>intrusive_ptr_target</code>，后者是为了通过使用<code>intrusive_ptr&lt;T&gt;</code>实现引用计数而设计的基类，需要实现引用计数的类只需继承它即可。<code>TensorImpl</code>中的每个成员是干什么的基本都有注释，其中<code>strides_</code>是用来实现内存寻址的，即某个ijk脚标对应的内存地址为：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">storage_offset_ + i * strides_[<span class="number">0</span>] + j * strides_[<span class="number">1</span>] + k * strides_[<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<p>正常情况下用<code>sizes_</code>代替<code>strides_</code>可以实现同样的功能，但是有的Tensor是由较大的Tensor分割而来，维度之间的间隔不是<code>sizes_</code>，所以需要用另一个数组<code>strides_</code>存储维度间隔。</p>
<p><code>TensorImpl</code>中的方法较多，就不一一列出了，实现了对Tensor（和Variable）的基本操作，Aten中的API也是基于这些基本操作实现的。</p>
<p><strong><code>Variable</code>与<code>Tensor</code>的合并</strong></p>
<p>在早期的PyTorch版本中，Variable与Tensor是不同的类，Variable用来保存需要计算梯度的Tensor，但Variable的实现并不美好：一方面<code>Variable::Impl</code>是Tensor的子类，而它的成员里又拥有一个Tensor（存储数据），这违反了<a href="https://www.wikiwand.com/zh-hans/%E9%87%8C%E6%B0%8F%E6%9B%BF%E6%8D%A2%E5%8E%9F%E5%88%99" target="_blank" rel="noopener">里氏替换原则</a>，而且让Tensor的实现变得很复杂。而在现版本中，已经把Variable变成Tensor了，把一些<code>Variable</code>特有的方法（e.g.<code>requires_grad</code>）移到了<code>TensorImpl</code>里。</p>
<h3><span id="storageimpl">StorageImpl</span></h3>
<p><code>StorageImpl</code>的声明和属性如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">StorageImpl</span> <span class="title">final</span> :</span> <span class="keyword">public</span> c10::intrusive_ptr_target &#123;</span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  caffe2::TypeMeta data_type_;  <span class="comment">// 数据类型</span></span><br><span class="line">  DataPtr data_ptr_;            <span class="comment">// 指向存储数据的内存块</span></span><br><span class="line">  <span class="keyword">int64_t</span> numel_;               <span class="comment">// 数据总数</span></span><br><span class="line">  <span class="keyword">bool</span> resizable_;</span><br><span class="line">  Allocator* allocator_;        <span class="comment">// 内存分配器</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中，<code>caffe2::TypeMeta data_type_</code>存储了数据类型信息，包括：类型id、大小、类型名字等；<code>DataPtr</code>其实就是 <em>unique_ptr</em>，但指针类型固定 <code>void*</code>。</p>
<p><strong>分配内存</strong></p>
<p>在<code>Allocator.cpp</code>中定义了全局变量<code>allocator_array</code>来存储所有的 allocator，每个设备类型对应一个：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">C10_API at::Allocator* allocator_array[at::COMPILE_TIME_MAX_DEVICE_TYPES];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">SetAllocator</span><span class="params">(at::DeviceType t, at::Allocator* alloc)</span> </span>&#123;</span><br><span class="line">  allocator_array[<span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(t)] = alloc;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">at::<span class="function">Allocator* <span class="title">GetAllocator</span><span class="params">(<span class="keyword">const</span> at::DeviceType&amp; t)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span>* alloc = allocator_array[<span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(t)];</span><br><span class="line">  AT_ASSERTM(alloc, <span class="string">"Allocator for "</span>, t, <span class="string">" is not set."</span>);</span><br><span class="line">  <span class="keyword">return</span> alloc;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同时配备了<code>SetAllocator</code>和<code>GetAllocator</code>来设置和获取相应的分配器。</p>
<p><code>Allocator</code>是一个虚基类，它的声明如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">Allocator</span> &#123;</span></span><br><span class="line">  <span class="keyword">virtual</span> ~Allocator() = <span class="keyword">default</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> DataPtr <span class="title">allocate</span><span class="params">(<span class="keyword">size_t</span> n)</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 重载这个函数很关键，用来释放申请到的内存</span></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> DeleterFnPtr <span class="title">raw_deleter</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span>* <span class="title">raw_allocate</span><span class="params">(<span class="keyword">size_t</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> dptr = allocate(n);</span><br><span class="line">    AT_ASSERT(dptr.get() == dptr.get_context());</span><br><span class="line">    <span class="keyword">return</span> dptr.release_context();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">raw_deallocate</span><span class="params">(<span class="keyword">void</span>* ptr)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> d = raw_deleter();</span><br><span class="line">    AT_ASSERT(d);</span><br><span class="line">    d(ptr);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>这个分配器有两种使用方法，第一种就是直接调用<code>raw_allocate</code>和<code>raw_deallocate</code>分配和释放内存。第二种方法是调用<code>Allocator::allocate</code>，这个方法将返回一个<code>DataPtr</code>类型的指针，也就是 <em>unique_ptr</em>，由于deleter存储在指针中，在释放指针的时候会释放相应的内存。不过两种方法的正确性都依赖于<code>Allocator::raw_deleter</code>能正确返回相应的释放器（deleter），否则两种方法都不能正确释放内存。这里需要注意的是，释放器（deleter）未必就是C库函数<code>free</code>：根据操作系统的不同，也可能是<code>_aligned_free</code>；根据设备的不同也可能是其他函数。</p>
<p>下面是在CPU上内存分配的具体实现：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">DefaultCPUAllocator</span> <span class="title">final</span> :</span> at::Allocator &#123;</span><br><span class="line">  ...</span><br><span class="line">      </span><br><span class="line">  at::<span class="function">DataPtr <span class="title">allocate</span><span class="params">(<span class="keyword">size_t</span> nbytes)</span> <span class="keyword">const</span> override </span>&#123;</span><br><span class="line">    <span class="keyword">void</span>* data = alloc_cpu(nbytes);</span><br><span class="line">    <span class="keyword">if</span> (FLAGS_caffe2_report_cpu_memory_usage &amp;&amp; nbytes &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      getMemoryAllocationReporter().New(data, nbytes);</span><br><span class="line">      <span class="keyword">return</span> &#123;data, data, &amp;ReportAndDelete, </span><br><span class="line">              at::Device(at::DeviceType::CPU)&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 这四个参数用来构造一个DataPtr</span></span><br><span class="line">    <span class="keyword">return</span> &#123;data, data, &amp;free_cpu, </span><br><span class="line">            at::Device(at::DeviceType::CPU)&#125;;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  at::<span class="function">DeleterFnPtr <span class="title">raw_deleter</span><span class="params">()</span> <span class="keyword">const</span> override </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (FLAGS_caffe2_report_cpu_memory_usage) &#123;</span><br><span class="line">      <span class="keyword">return</span> &amp;ReportAndDelete;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> &amp;free_cpu;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">alloc_cpu</span><span class="params">(<span class="keyword">size_t</span> nbytes)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (nbytes == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">void</span>* data;</span><br><span class="line">  <span class="comment">// 分配64字节对齐的连续内存块</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> __ANDROID__</span></span><br><span class="line">  data = memalign(gAlignment, nbytes);	<span class="comment">// gAlignment = 64</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">elif</span> defined(_MSC_VER)</span></span><br><span class="line">  data = _aligned_malloc(nbytes, gAlignment);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">  CAFFE_ENFORCE_EQ(posix_memalign(&amp;data, gAlignment, nbytes), <span class="number">0</span>);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 移动数据到线程的NUMA节点中</span></span><br><span class="line">  NUMAMove(data, nbytes, GetCurrentNUMANode());</span><br><span class="line">	<span class="comment">// 填充内存</span></span><br><span class="line">  <span class="keyword">if</span> (FLAGS_caffe2_cpu_allocator_do_zero_fill) &#123;</span><br><span class="line">    <span class="built_in">memset</span>(data, <span class="number">0</span>, nbytes);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (FLAGS_caffe2_cpu_allocator_do_junk_fill) &#123;</span><br><span class="line">    memset_junk(data, nbytes);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> data;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">free_cpu</span><span class="params">(<span class="keyword">void</span>* data)</span> </span>&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> _MSC_VER</span></span><br><span class="line">  _aligned_free(data);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">  <span class="built_in">free</span>(data);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>分配时使用<code>memalign/_aligned_malloc/posix_memalign</code>函数确保内存是64字节对齐的。</p>
<h2><span id="智能指针-intrusive_ptr">智能指针 Intrusive_ptr</span></h2>
<p>PyTorch中使用intrusive_ptr来管理THTensor和THStorage的引用计数，其中引用分为强引用和弱引用（弱引用为了解决循环引用问题），对应的类名 <code>intrusive_ptr</code>和<code>weak_intrusive_ptr</code>。由于弱引用和强引用的实现类似，为了简单起见，我把弱引用的代码都去掉了，简化intrusive_ptr的实现如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">intrusive_ptr_target</span> &#123;</span></span><br><span class="line">  <span class="keyword">mutable</span> <span class="built_in">std</span>::atomic&lt;<span class="keyword">size_t</span>&gt; refcount_;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 声明友元类使得只能指针可以访问 refcount_</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="keyword">friend</span> <span class="class"><span class="keyword">class</span> <span class="title">intrusive_ptr</span>;</span></span><br><span class="line"></span><br><span class="line"> <span class="keyword">protected</span>:</span><br><span class="line">  <span class="comment">// 隐藏析构函数，防止直接析构对象</span></span><br><span class="line">  <span class="keyword">virtual</span> ~intrusive_ptr_target() &#123;&#125;</span><br><span class="line"></span><br><span class="line">  constexpr intrusive_ptr_target() noexcept : refcount_(0) &#123;&#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="comment">// 在摧毁对象时会调用次函数释放资源</span></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">release_resources</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">TTarget</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">intrusive_ptr</span> <span class="title">final</span> &#123;</span></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  TTarget* target_;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 增加引用计数</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">retain_</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (target_ != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">      <span class="keyword">size_t</span> new_refcount = ++target_-&gt;refcount_;</span><br><span class="line">      AT_ASSERTM(new_refcount != <span class="number">1</span>,</span><br><span class="line">                 <span class="string">"intrusive_ptr:Cannot increase refcount after it"</span></span><br><span class="line">                 <span class="string">"reached zero."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 销毁智能指针，减少引用计数，当引用计数为0时摧毁对象</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">reset_</span><span class="params">()</span> <span class="keyword">noexcept</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (target_ != <span class="literal">nullptr</span> &amp;&amp; --target_-&gt;refcount_ == <span class="number">0</span>) &#123;</span><br><span class="line">      target_-&gt;release_resources();</span><br><span class="line">      <span class="keyword">delete</span> target_;</span><br><span class="line">    &#125;</span><br><span class="line">    target_ = <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 隐藏通过普通指针初始化的构造函数（只能通过make_intrusive调用）</span></span><br><span class="line">  explicit intrusive_ptr(TTarget* target) noexcept : target_(target) &#123;&#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  intrusive_ptr() <span class="keyword">noexcept</span> : intrusive_ptr(<span class="literal">nullptr</span>) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 通过其他智能指针初始化，需增加引用计数</span></span><br><span class="line">  intrusive_ptr(<span class="keyword">const</span> intrusive_ptr&amp; rhs) : target_(rhs.target_) &#123;</span><br><span class="line">  	retain_(); </span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ~intrusive_ptr() <span class="keyword">noexcept</span> &#123; reset_(); &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">TTarget* <span class="title">get</span><span class="params">()</span> <span class="keyword">const</span> <span class="keyword">noexcept</span> </span>&#123; <span class="keyword">return</span> target_; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 重载运算符使其能当作正常指针使用</span></span><br><span class="line">  <span class="keyword">const</span> TTarget&amp; <span class="keyword">operator</span>*() <span class="keyword">const</span> <span class="keyword">noexcept</span> &#123; <span class="keyword">return</span> *target_; &#125;</span><br><span class="line">  TTarget&amp; <span class="keyword">operator</span>*() <span class="keyword">noexcept</span> &#123; <span class="keyword">return</span> *target_; &#125;</span><br><span class="line">  <span class="keyword">const</span> TTarget* <span class="keyword">operator</span>-&gt;() <span class="keyword">const</span> <span class="keyword">noexcept</span> &#123; <span class="keyword">return</span> target_; &#125;</span><br><span class="line">  TTarget* <span class="keyword">operator</span>-&gt;() <span class="keyword">noexcept</span> &#123; <span class="keyword">return</span> target_; &#125;</span><br><span class="line">  <span class="function"><span class="keyword">operator</span> <span class="title">bool</span><span class="params">()</span> <span class="keyword">const</span> <span class="keyword">noexcept</span> </span>&#123; <span class="keyword">return</span> target_ != <span class="literal">nullptr</span>; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获取当前的引用计数</span></span><br><span class="line">  <span class="keyword">size_t</span> use_count() <span class="keyword">const</span> <span class="keyword">noexcept</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (target_ == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> target_-&gt;refcount_.load();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 把普通指针转换为智能指针（增加引用计数）</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span>... <span class="title">Args</span>&gt;</span></span><br><span class="line"><span class="class">  <span class="title">static</span> <span class="title">intrusive_ptr</span> <span class="title">make</span>(<span class="title">Args</span>&amp;&amp;... <span class="title">args</span>) &#123;</span></span><br><span class="line">    <span class="keyword">auto</span> result = intrusive_ptr(<span class="keyword">new</span> TTarget(<span class="built_in">std</span>::forward&lt;Args&gt;(args)...));</span><br><span class="line">    ++result.target_-&gt;refcount_;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 把智能指针转换为普通指针</span></span><br><span class="line">  <span class="function">TTarget* <span class="title">release</span><span class="params">()</span> <span class="keyword">noexcept</span> </span>&#123;</span><br><span class="line">    TTarget* result = target_;</span><br><span class="line">    target_ = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 把普通指针转换为智能指针（不增加引用计数）</span></span><br><span class="line">  <span class="function"><span class="keyword">static</span> intrusive_ptr <span class="title">reclaim</span><span class="params">(TTarget* owning_ptr)</span> </span>&#123;</span><br><span class="line">    AT_ASSERTM(</span><br><span class="line">        owning_ptr == <span class="literal">nullptr</span> || owning_ptr-&gt;refcount_.load() &gt; <span class="number">0</span>,</span><br><span class="line">        <span class="string">"intrusive_ptr: Can only intrusive_ptr::reclaim() owning pointers that were created using intrusive_ptr::release()."</span>);</span><br><span class="line">    <span class="keyword">return</span> intrusive_ptr(owning_ptr);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 用于构造智能指针</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">TTarget</span>, <span class="title">class</span>... <span class="title">Args</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">inline</span> <span class="title">intrusive_ptr</span>&lt;TTarget&gt; <span class="title">make_intrusive</span>(<span class="title">Args</span>&amp;&amp;... <span class="title">args</span>) &#123;</span></span><br><span class="line">  <span class="keyword">return</span> intrusive_ptr&lt;TTarget&gt;::make(<span class="built_in">std</span>::forward&lt;Args&gt;(args)...);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>intrusive_ptr_target</code>是被引用对象的父类，<code>TensorImpl</code>和<code>StorageImpl</code>都继承自它，主要工作是声明了一个引用计数<code>refcount_</code>，并且把智能指针类声明为友元类，这样在<code>intrusive_ptr</code>里面就可以直接操作<code>refcount_</code>了。</p>
<p>再来看<code>intrusive_ptr</code>的实现，它有一个私有成员<code>TTarget* target_</code>，是被引用对象的普通指针；还有两个私有方法<code>retain_</code>和<code>reset_</code>，分别用来增加和减少引用计数。需要注意的是：通过普通指针初始化<code>intrusive_ptr</code>的构造函数也是私有的，不能被外部调用，只能通过静态函数<code>make</code>来调用（详见下文）；在通过其他智能指针初始化的时候需要增加引用计数。</p>
<p>最后还有两个方法<code>release</code>和<code>reclaim</code>，它们实现了普通指针和智能指针间的相互转换，用于C风格的API中（在Aten中经常用到）。除此之外，<code>intrusive_ptr</code>里还重载了许多运算符，让它可以像普通指针一样使用，还有许多其他方法就不一一介绍了。</p>
<p><strong>创建Tensor</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">THTensor *<span class="title">THTensor_</span><span class="params">(<span class="keyword">new</span>)</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> c10::make_intrusive&lt;at::TensorImpl&gt;(</span><br><span class="line">    <span class="comment">// 下面三个参数将通过intrusive_ptr::make传给TensorImpl的构造函数，然后</span></span><br><span class="line">    <span class="comment">// 通过TensorImpl的指针构造intrusive_ptr</span></span><br><span class="line">    c10::intrusive_ptr&lt;at::StorageImpl&gt;::reclaim(THStorage_(<span class="keyword">new</span>)()),										<span class="comment">// Storage&amp;&amp; storage</span></span><br><span class="line">    at::CPUTensorId(),	<span class="comment">// TensorTypeId type_id</span></span><br><span class="line">    <span class="literal">false</span>								<span class="comment">// bool is_variable</span></span><br><span class="line">  ).release();	<span class="comment">// release返回普通指针，TensorImpl*</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">THStorage* <span class="title">THStorage_</span><span class="params">(<span class="keyword">new</span>)</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> THStorage_new(caffe2::TypeMeta::Make&lt;<span class="keyword">scalar_t</span>&gt;());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">THStorage* <span class="title">THStorage_new</span><span class="params">(caffe2::TypeMeta data_type)</span> </span>&#123;</span><br><span class="line">  THStorage* storage = c10::make_intrusive&lt;at::StorageImpl&gt;(</span><br><span class="line">      <span class="comment">// 同理下面四个参数将通过intrusive_ptr::make传给StorageImpl的构造函</span></span><br><span class="line">      <span class="comment">// 数，然后通过StorageImpl的指针构造intrusive_ptr</span></span><br><span class="line">      data_type,</span><br><span class="line">      <span class="number">0</span>,</span><br><span class="line">      getTHDefaultAllocator(),</span><br><span class="line">      <span class="literal">true</span></span><br><span class="line">  ).release();</span><br><span class="line">  <span class="keyword">return</span> storage;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面是新建一个tensor的过程，通过<code>c10::make_instrusive</code>构造智能指针，然后调用<code>release</code>返回普通指针。传入的三个参数：storage智能指针，tensortype，is_variable会被转发到<code>intrusive_ptr::make</code>函数中，然后用这三个参数构造一个<code>TensorImpl</code>对象：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TensorImpl(Storage&amp;&amp; storage, TensorTypeId type_id, <span class="keyword">bool</span> is_variable);</span><br></pre></td></tr></table></figure>
<p>再用该对象的指针初始化智能指针：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">intrusive_ptr(TTarget* target);</span><br></pre></td></tr></table></figure>
<p>同时增加引用计数，最后<code>make_intrusive</code>返回智能指针。THStorage的构造过程同理。</p>
<h2><span id="tensor-apply-amp-dynamic-dispatch">Tensor Apply &amp; Dynamic Dispatch</span></h2>
<p>TH中的Tensor Apply就相当于map函数，把一个函数应用到tensor的每个数据中。举个例子来看一下<code>THTensor_(cadd)</code>的具体实现（简化过的）：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THTensor_</span><span class="params">(cadd)</span><span class="params">(THTensor *r_, THTensor *t, <span class="keyword">scalar_t</span> value, THTensor *src)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  THTensor_(resizeAs)(r_, t);</span><br><span class="line">  <span class="keyword">int64_t</span> r_Size = THTensor_(nElement)(r_);</span><br><span class="line">  <span class="keyword">int64_t</span> srcSize = THTensor_(nElement)(src);</span><br><span class="line">  <span class="keyword">int</span> r_Contig = THTensor_(isContiguous)(r_);</span><br><span class="line">  <span class="keyword">int</span> tContig = THTensor_(isContiguous)(t);</span><br><span class="line">  <span class="keyword">int</span> srcContig = THTensor_(isContiguous)(src);</span><br><span class="line">  <span class="keyword">int</span> serial_path = <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (srcSize == r_Size)&#123;</span><br><span class="line">    <span class="keyword">if</span> (r_Contig &amp;&amp; tContig &amp;&amp; srcContig) &#123;</span><br><span class="line">      TH_TENSOR_APPLY3_CONTIG(<span class="keyword">scalar_t</span>, r_, <span class="keyword">scalar_t</span>, t, <span class="keyword">scalar_t</span>, src, THVector_(cadd)(r__data, t_data, src_data, value, r__len););</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      serial_path = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    serial_path = <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (serial_path) &#123;</span><br><span class="line">    TH_TENSOR_APPLY3(</span><br><span class="line">      <span class="keyword">scalar_t</span>, r_, <span class="keyword">scalar_t</span>, t, <span class="keyword">scalar_t</span>, src, </span><br><span class="line">      *r__data = *t_data + value * *src_data;</span><br><span class="line">    );</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数实现的功能很简单，就是<code>*r_ = *t + value * *src</code>，把<code>src</code>的值乘以<code>value</code>然后和<code>t</code>相加，最后赋值给<code>r_</code>，其中<code>r_, t, src</code>都是THTensor。函数首先获取元素个数和是否连续等信息，如果连续的话调用<code>TH_TENSOR_APPLY3_CONTIG</code>来处理，否则调用<code>TH_TENSOR_APPLY3</code>处理。后者太复杂了，我们主要看前者的实现：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> _OPENMP</span></span><br><span class="line"><span class="comment">// 多线程加速</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> TH_TENSOR_APPLY3_CONTIG(TYPE1, TENSOR1, TYPE2, TENSOR2, TYPE3, TENSOR3, CODE) \ </span></span><br><span class="line">&#123; \ </span><br><span class="line">  <span class="keyword">int</span> inOmp = omp_in_parallel(); \</span><br><span class="line">  <span class="keyword">ptrdiff_t</span> TH_TENSOR_size = THTensor_(nElement)(TENSOR1); \ </span><br><span class="line">  # 启动 OpenMP 多线程</span><br><span class="line">  PRAGMA(omp parallel <span class="keyword">if</span> ((TH_TENSOR_size &gt; TH_OMP_OVERHEAD_THRESHOLD) &amp;&amp; (!inOmp))) \</span><br><span class="line">  &#123; \</span><br><span class="line">    <span class="keyword">size_t</span> num_threads = omp_get_num_threads(); \</span><br><span class="line">    <span class="comment">// 获取线程数</span></span><br><span class="line">    <span class="keyword">size_t</span> tid = omp_get_thread_num(); \</span><br><span class="line">    <span class="comment">// 计算开始和结尾</span></span><br><span class="line">    <span class="keyword">ptrdiff_t</span> TH_TENSOR_offset = tid*(TH_TENSOR_size/num_threads);\</span><br><span class="line">    <span class="keyword">ptrdiff_t</span> TH_TENSOR_end =(tid==num_threads<span class="number">-1</span>)?TH_TENSOR_size: \</span><br><span class="line">      TH_TENSOR_offset + TH_TENSOR_size / num_threads; \</span><br><span class="line">    <span class="comment">// 每个线程负责 TH_TENSOR_offset 到 TH_TENSOR_end 之间的数据</span></span><br><span class="line">    <span class="keyword">ptrdiff_t</span> TENSOR1##_len = TH_TENSOR_end - TH_TENSOR_offset; \</span><br><span class="line">    <span class="comment">// 获取Tensor的数据</span></span><br><span class="line">    TYPE1 *TENSOR1##_data = TENSOR1-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;() + TH_TENSOR_offset; \</span><br><span class="line">    TYPE2 *TENSOR2##_data = TENSOR2-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;() + TH_TENSOR_offset; \</span><br><span class="line">    TYPE3 *TENSOR3##_data = TENSOR3-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;() + TH_TENSOR_offset; \</span><br><span class="line">    CODE \</span><br><span class="line">  &#125; \</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line"><span class="comment">// 普通实现</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> TH_TENSOR_APPLY3_CONTIG(TYPE1, TENSOR1, TYPE2, TENSOR2, TYPE3, TENSOR3, CODE) \ </span></span><br><span class="line">&#123; \</span><br><span class="line">  <span class="comment">// 获取Tensor的数据</span></span><br><span class="line">  TYPE1 *TENSOR1##_data = TENSOR1-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;(); \</span><br><span class="line">  TYPE2 *TENSOR2##_data = TENSOR2-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;(); \</span><br><span class="line">  TYPE3 *TENSOR3##_data = TENSOR3-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;(); \</span><br><span class="line">  <span class="keyword">ptrdiff_t</span> TENSOR1##_len = THTensor_(nElement)(TENSOR1); \</span><br><span class="line">  <span class="comment">// 执行传入的代码</span></span><br><span class="line">  CODE \</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure>
<p>上半部分的实现为OPENMP的多线程加速版，下面是普通版。这个宏接收7个参数：三个Tensor和对应类型，还有要执行的操作。<code>THTensor_(cadd)</code>中传入的操作是<code>THVector_(cadd)(r__data, t_data, src_data, value, r__len);</code>，它的定义为：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// THVectorDispatch.cpp</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THVector_</span><span class="params">(cadd)</span><span class="params">(<span class="keyword">scalar_t</span> *z, <span class="keyword">const</span> <span class="keyword">scalar_t</span> *x, <span class="keyword">const</span> <span class="keyword">scalar_t</span> *y, <span class="keyword">const</span> <span class="keyword">scalar_t</span> c, <span class="keyword">const</span> <span class="keyword">ptrdiff_t</span> n)</span> </span>&#123;</span><br><span class="line">  THVector_(cadd_DISPATCHPTR)(z, x, y, c, n);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 动态派发函数指针</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="title">void</span> <span class="params">(*THVector_(cadd_DISPATCHPTR))</span><span class="params">(<span class="keyword">scalar_t</span> *, <span class="keyword">const</span> <span class="keyword">scalar_t</span> *, <span class="keyword">const</span> <span class="keyword">scalar_t</span> *, <span class="keyword">const</span> <span class="keyword">scalar_t</span>, <span class="keyword">const</span> <span class="keyword">ptrdiff_t</span>)</span> </span>= &amp;THVector_(cadd_DEFAULT);</span><br><span class="line"><span class="comment">// 对各种向量化指令的支持</span></span><br><span class="line">static FunctionDescription THVector_(cadd_DISPATCHTABLE)[] = &#123;</span><br><span class="line">  <span class="meta">#<span class="meta-keyword">if</span> defined(__NEON__)</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">if</span> defined(TH_REAL_IS_FLOAT)</span></span><br><span class="line">      FUNCTION_IMPL(THVector_(cadd_NEON), SIMDExtension_NEON),</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">if</span> defined(USE_AVX2)</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">if</span> defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT)</span></span><br><span class="line">      FUNCTION_IMPL(THVector_(cadd_AVX2), SIMDExtension_AVX2),</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">if</span> defined(USE_AVX)</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">if</span> defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT)</span></span><br><span class="line">      FUNCTION_IMPL(THVector_(cadd_AVX), SIMDExtension_AVX),</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">  FUNCTION_IMPL(THVector_(cadd_DEFAULT), SIMDExtension_DEFAULT)</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// THVectorDefault.cpp</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THVector_</span><span class="params">(cadd_DEFAULT)</span><span class="params">(<span class="keyword">scalar_t</span> *z, <span class="keyword">const</span> <span class="keyword">scalar_t</span> *x, <span class="keyword">const</span> <span class="keyword">scalar_t</span> *y, <span class="keyword">const</span> <span class="keyword">scalar_t</span> c, <span class="keyword">const</span> <span class="keyword">ptrdiff_t</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">ptrdiff_t</span> i = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span>(; i&lt;n<span class="number">-4</span>; i+=<span class="number">4</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    z[i] = x[i] + c * y[i];</span><br><span class="line">    z[i+<span class="number">1</span>] = x[i+<span class="number">1</span>] + c * y[i+<span class="number">1</span>];</span><br><span class="line">    z[i+<span class="number">2</span>] = x[i+<span class="number">2</span>] + c * y[i+<span class="number">2</span>];</span><br><span class="line">    z[i+<span class="number">3</span>] = x[i+<span class="number">3</span>] + c * y[i+<span class="number">3</span>];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span>(; i&lt;n; i++)</span><br><span class="line">    z[i] = x[i] + c * y[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里涉及到对SIMD向量化指令的支持和动态派发，可以看到<code>THVector_(cadd)</code>函数里调用的是<code>THVector_(cadd_DISPATCHPTR)</code>，而它是一个函数指针，默认指向<code>THVector_(cadd_DEFAULT)</code>，这个是不支持向量化指令的实现。代码中间部分的数组<code>FunctionDescription THVector_(cadd_DISPATCHTABLE)[]</code>记录各种向量化指令的实现，最后是默认的实现。</p>
<p>动态派发的实现在<code>TH/vector/simd.h</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INIT_DISPATCH_PTR(OP)                                     \ </span></span><br><span class="line">  <span class="keyword">do</span> &#123;                                                            \ </span><br><span class="line">    <span class="keyword">size_t</span> i;                                                     \ </span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; <span class="keyword">sizeof</span>(THVector_(OP ## _DISPATCHTABLE)) /     \ </span><br><span class="line">                         <span class="keyword">sizeof</span>(FunctionDescription); ++i) &#123;      \</span><br><span class="line">      THVector_(OP ## _DISPATCHPTR) =                             \</span><br><span class="line">        <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">decltype</span>(THVector_(OP ## _DISPATCHPTR))&gt; \</span><br><span class="line">        (THVector_(OP ## _DISPATCHTABLE)[i].function);            \</span><br><span class="line">      <span class="keyword">if</span> (THVector_(OP ## _DISPATCHTABLE)[i].supportedSimdExt     \</span><br><span class="line">          &amp; hostSimdExts) &#123;                                       \</span><br><span class="line">        <span class="keyword">break</span>;                                                    \</span><br><span class="line">      &#125;                                                           \</span><br><span class="line">    &#125;                                                             \</span><br><span class="line">  &#125; <span class="keyword">while</span>(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">FunctionDescription</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="keyword">void</span> *function;</span><br><span class="line">  <span class="keyword">uint32_t</span> supportedSimdExt;</span><br><span class="line">&#125; FunctionDescription;</span><br></pre></td></tr></table></figure>
<p><code>INIT_DISPATCH_PTR</code>这个宏做的事就是遍历<code>THVector_(cadd_DISPATCHTABLE)</code>数组，循环内把动态派发指针（<code>THVector_(cadd_DISPATCHPTR)</code>）赋给当前数组元素所对应的函数指针，最后判断一下当前架构是否支持该指令集，如果支持就退出循环；如果向量化指令集都不支持的话，最后还会指向默认实现的函数指针。</p>
<p>つづく</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">上一篇：<a href="https://www.52coding.com.cn/2019/05/05/PyTorch0/">简介</a></th>
<th style="text-align: right;">下一篇：<a href="https://www.52coding.com.cn/2019/05/05/PyTorch2/">THC</a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/博客/">博客</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/PyTorch/">PyTorch</a><a href="/tags/Tensor/">Tensor</a><a href="/tags/THTensor/">THTensor</a><a href="/tags/THStorage/">THStorage</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>
<script type="text/javascript">
  var $config = {
    image: "icon.png",
  };
  socialShare('.social-share-cs', $config);
</script>



<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80MTI4MC8xNzgyOA==">
	<script type="text/javascript">
		(function (d, s) {
			var j, e = d.getElementsByTagName(s)[0];

			if (typeof LivereTower === 'function') {
				return;
			}

			j = d.createElement(s);
			j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
			j.async = true;

			e.parentNode.insertBefore(j, e);
		})(document, 'script');
	</script>
	<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->


      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2019
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>