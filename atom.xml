<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>NIUHE</title>
  
  <subtitle>Be Myself Enjoy My Life</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.52coding.com.cn/"/>
  <updated>2018-11-07T09:40:49.637Z</updated>
  <id>http://www.52coding.com.cn/</id>
  
  <author>
    <name>NIUHE</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ä¸­å›½è±¡æ£‹ZeroæŠ€æœ¯è¯¦è§£</title>
    <link href="http://www.52coding.com.cn/2018/11/07/CCZero/"/>
    <id>http://www.52coding.com.cn/2018/11/07/CCZero/</id>
    <published>2018-11-07T09:27:47.000Z</published>
    <updated>2018-11-07T09:40:49.637Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://cczero.org/" target="_blank" rel="noopener">ä¸­å›½è±¡æ£‹Zeroï¼ˆCCZeroï¼‰</a>æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼ŒæŠŠ<a href="https://arxiv.org/abs/1712.01815" target="_blank" rel="noopener">AlphaZero</a>çš„ç®—æ³•åº”ç”¨åˆ°äº†ä¸­å›½è±¡æ£‹ä¸Šï¼Œæ—¨åœ¨å€ŸåŠ©å¹¿å¤§è±¡æ£‹çˆ±å¥½è€…ä¹‹åŠ›ä¸€èµ·è®­ç»ƒå‡ºä¸€ä¸ªå¯ä»¥æ‰“è´¥æ—‹é£åæ‰‹çš„â€œè±¡æ£‹ä¹‹ç¥â€ã€‚å› ä¸ºç§ç§åŸå› å§ï¼Œè¿™ä¸ªç›®æ ‡åˆ°ç›®å‰ï¼ˆ2018/11/07ï¼‰ä¸ºæ­¢æœªèƒ½å®ç°ï¼Œæˆ–è€…è¯´è¿˜å·®å¾—è¿œï¼Œè€Œè·‘è°±çš„äººä¹Ÿè¶Šæ¥è¶Šå°‘äº†ï¼Œå¾ˆå¯èƒ½åšæŒä¸äº†å¤šä¹…äº†ã€‚</p><p>è™½ç„¶æœªèƒ½å®ç°ç›®æ ‡ï¼Œä½†åœ¨æŠ€æœ¯ä¸Šè¿˜æ˜¯æœ‰ä¸€å®šæ„ä¹‰çš„ï¼Œ<a href="https://github.com/NeymarL/ChineseChess-AlphaZero" target="_blank" rel="noopener">GitHub</a>ä¸Šä¹Ÿæ˜¯ä¸æ˜¯æœ‰äººè¯¢é—®æŠ€æœ¯ç»†èŠ‚ï¼Œåœ¨æ­¤æ€»ç»“ä¸€ä¸‹ï¼Œè®°å½•ä¸€äº›å‘ä»¥åä¸è¦å†è¸©ã€‚</p><a id="more"></a><h2><span id="æ¨¡å—">æ¨¡å—</span></h2><p>ç¨‹åºä¸»è¦åˆ†ä¸ºä¸‰å¤§æ¨¡å—ï¼ˆæ¯ä¸ªæ¨¡å—å¯¹åº”ä¸€ä¸ªç›®å½•ï¼‰ï¼š</p><ul><li><code>agents</code>ï¼šæ ¸å¿ƒæ¨¡å—ï¼Œå†³å®šå¦‚ä½•ä¸‹æ£‹<ul><li><code>model.py</code>ï¼šç¥ç»ç½‘ç»œæ¨¡å‹</li><li><code>player.py</code>ï¼šMCTSï¼Œè¾“å‡ºèµ°æ³•</li><li><code>api.py</code>ï¼šä¾›å¤–ç•Œè°ƒç”¨model</li></ul></li><li><code>envrionment</code>ï¼šè±¡æ£‹è§„åˆ™<ul><li>è®­ç»ƒï¼ˆè·‘è°±ï¼‰ä½¿ç”¨<code>static_env.py</code>ï¼Œé€Ÿåº¦å¿«ä¸€äº›</li><li>ç”¨è‡ªå¸¦GUIä¸‹æ£‹æ—¶ä½¿ç”¨çš„æ˜¯<code>env.py</code>, <code>chessboard.py</code>è¿™äº›ï¼Œå¯ä»¥è¾“å‡ºPNGæ ¼å¼çš„æ£‹è°±</li></ul></li><li><code>worker</code>ï¼šæŠŠagentå’Œenvrionmentä¸²è”èµ·æ¥çš„è„šæœ¬<ul><li><code>self_play.py</code>ï¼šè‡ªæˆ‘åšå¼ˆ</li><li><code>compute_elo.py</code>ï¼šè¯„æµ‹å¹¶ä¸Šä¼ ç»“æœåˆ°å®˜ç½‘</li><li><code>optimize.py</code>ï¼šè®­ç»ƒæ£‹è°±</li><li><code>_windows</code>åç¼€è¡¨ç¤ºæ˜¯åœ¨Windowså¹³å°ä¸Šè¿è¡Œçš„ç›¸åº”åŠŸèƒ½ï¼Œä¹‹æ‰€ä»¥åˆ†å¼€æ˜¯å› ä¸ºä¸¤ä¸ªå¤šè¿›ç¨‹çš„å¯åŠ¨æ–¹å¼ä¸åŒï¼Œå¯¼è‡´ä»£ç ç»“æ„ä¹Ÿè¦å‘ç”Ÿä¸€äº›å˜åŒ–ï¼Œè¯¦è§<a href="#è‡ªæˆ‘åšå¼ˆ">è‡ªæˆ‘åšå¼ˆ</a>ã€‚</li></ul></li></ul><h3><span id="ç¥ç»ç½‘ç»œæ¨¡å‹">ç¥ç»ç½‘ç»œæ¨¡å‹</span></h3><p><strong>ç½‘ç»œè¾“å…¥</strong>ï¼š<span class="math inline">\(14\times10\times9\)</span></p><ul><li><span class="math inline">\(10 \times 9\)</span> æ˜¯ä¸­å›½è±¡æ£‹æ£‹ç›˜çš„å¤§å°</li><li><span class="math inline">\(14\)</span> æ˜¯æ‰€æœ‰æ£‹å­ç§ç±»ï¼ˆçº¢/é»‘ç®—ä¸åŒç§ç±»ï¼‰</li><li>æ•´ä½“çš„è¾“å…¥å°±æ˜¯14ä¸ªæ£‹ç›˜å †å åœ¨ä¸€èµ·ï¼Œæ¯ä¸ªæ£‹ç›˜è¡¨ç¤ºä¸€ç§æ£‹å­çš„ä½ç½®ï¼šæ£‹å­æ‰€åœ¨çš„ä½ç½®ä¸º1ï¼Œå…¶ä½™ä½ç½®ä¸º0ã€‚</li></ul><p><strong>ç½‘ç»œè¾“å‡º</strong></p><ul><li>ç­–ç•¥å¤´ï¼ˆpolicy headï¼‰è¾“å‡ºï¼š<span class="math inline">\(2086\)</span><ul><li><span class="math inline">\(2086\)</span> æ˜¯è¡ŒåŠ¨ç©ºé—´çš„å¤§å°ã€‚è¡ŒåŠ¨ç©ºé—´å°±æ˜¯è¯´æ ¹æ®ä¸­å›½è±¡æ£‹çš„è§„åˆ™ï¼Œä»»æ„æ£‹å­åœ¨ä»»æ„ä½ç½®çš„èµ°æ³•é›†åˆã€‚</li></ul></li><li>ä»·å€¼å¤´ï¼ˆvalue headï¼‰è¾“å‡ºï¼š<span class="math inline">\(1\)</span><ul><li>ä»·å€¼å¤´è¾“å‡ºä¸€ä¸ªæ ‡é‡è¡¡é‡å½“å‰å±€åŠ¿ <span class="math inline">\(v\in[-1, 1]\)</span>ï¼šå½“ <span class="math inline">\(v\)</span> æ¥è¿‘1æ—¶ï¼Œå±€åŠ¿å¤§å¥½ï¼›æ¥è¿‘0ä¸ºå‡åŠ¿ï¼›æ¥è¿‘-1ä¸ºè´¥åŠ¿ã€‚</li></ul></li></ul><p>é™„ï¼šæ£‹å­ç¼–å·è¡¨</p><table><thead><tr class="header"><th>æ£‹å­</th><th>ç¼–å·</th></tr></thead><tbody><tr class="odd"><td>å…µ/å’</td><td>0</td></tr><tr class="even"><td>ç‚®</td><td>1</td></tr><tr class="odd"><td>è½¦</td><td>2</td></tr><tr class="even"><td>é©¬</td><td>3</td></tr><tr class="odd"><td>ç›¸/è±¡</td><td>4</td></tr><tr class="even"><td>ä»•/å£«</td><td>5</td></tr><tr class="odd"><td>å¸…/å°†</td><td>6</td></tr></tbody></table><p><strong>ç½‘ç»œç»“æ„</strong></p><p>ç½‘ç»œä¸»ä½“æ˜¯ ResNetï¼Œè¾“å‡ºéƒ¨åˆ†åˆ†å‡ºä¸¤ä¸ªå¤´ï¼Œåˆ†åˆ«è¾“å‡º policy å’Œ valueã€‚ç°åœ¨çš„æ¶æ„æ˜¯ä¸­é—´æœ‰10ä¸ªæ®‹å‰å—ï¼ˆResidual Blockï¼‰ï¼Œæ¯ä¸ªå—é‡Œé¢æœ‰ä¸¤ä¸ªCNNï¼šå·ç§¯æ ¸å¤§å°ä¸º <span class="math inline">\(3 \times 3\)</span>ï¼Œè¿‡æ»¤å™¨ä¸ªæ•°ä¸º192ã€‚</p><h3><span id="è’™ç‰¹å¡æ´›æ ‘æœç´¢">è’™ç‰¹å¡æ´›æ ‘æœç´¢</span></h3><p><img src="/images/mcts0.png"></p><p>æœç´¢æ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½åŒ…å«æ‰€æœ‰åˆæ³•ç§»åŠ¨ a âˆˆ A(s) çš„è¾¹(sï¼Œa)ã€‚ æ¯æ¡è¾¹å­˜å‚¨ä¸€ç»„ç»Ÿè®¡æ•°æ®ï¼Œ <span class="math display">\[\{N(s,a) ,W(s,a) ,Q(s,a) ,P(s,a)\}\]</span> å…¶ä¸­ <span class="math inline">\(N(s,a)\)</span> æ˜¯è®¿é—®è®¡æ•°ï¼Œ<span class="math inline">\(W(s,a)\)</span> æ˜¯æ€»åŠ¨ä½œä»·å€¼ï¼Œ<span class="math inline">\(Q(s,a)\)</span> æ˜¯å¹³å‡åŠ¨ä½œä»·å€¼ï¼Œ<span class="math inline">\(P(s,a)\)</span> æ˜¯é€‰æ‹©è¯¥è¾¹çš„å…ˆéªŒæ¦‚ç‡ã€‚ å¤šä¸ªæ¨¡æ‹Ÿåœ¨å•ç‹¬çš„æœç´¢çº¿ç¨‹ä¸Šå¹¶è¡Œæ‰§è¡Œã€‚</p><ol type="1"><li><p>é€‰æ‹©</p><p>æ¯ä¸ªæ¨¡æ‹Ÿçš„ç¬¬ä¸€ä¸ª in-tree é˜¶æ®µå¼€å§‹äºæœç´¢æ ‘çš„æ ¹èŠ‚ç‚¹ <span class="math inline">\(s_0\)</span>ï¼Œå¹¶ä¸”åœ¨æ¨¡æ‹Ÿæ—¶åˆ» L å¤„åˆ°è¾¾å¶èŠ‚ç‚¹ <span class="math inline">\(s_L\)</span> æ—¶ç»“æŸã€‚åœ¨æ¯ä¸ªè¿™äº›æ—¶åˆ» <span class="math inline">\(t &lt; L\)</span> å¤„ï¼Œæ ¹æ®æœç´¢æ ‘ä¸­çš„ç»Ÿè®¡é‡é€‰æ‹©ä¸€ä¸ªç§»åŠ¨: <span class="math inline">\(a_t = \arg\max_a(Q(s_t,a) + U(s_t,a))\)</span>ï¼Œå…¶ä¸­ <span class="math inline">\(U(s_t,a)\)</span> ä½¿ç”¨PUCTç®—æ³•çš„å˜ä½“å¾—åˆ° <span class="math display">\[U(s,a)=c_{puct}P(s,a)\frac{\sqrt{\sum_bN(s,b)}}{1+N(s,a)}\]</span> å…¶ä¸­ <span class="math inline">\(c_{puct}â€‹\)</span> æ˜¯ä¸€ä¸ªå†³å®šæ¢ç´¢ç¨‹åº¦çš„å¸¸æ•°; è¿™ç§æœç´¢æ§åˆ¶ç­–ç•¥æœ€åˆå€¾å‘äºå…·æœ‰é«˜å…ˆéªŒæ¦‚ç‡å’Œä½è®¿é—®æ¬¡æ•°çš„è¡Œä¸ºï¼Œä½†åæœŸå€¾å‘äºå…·æœ‰é«˜åŠ¨ä½œä»·å€¼çš„è¡Œä¸ºã€‚</p></li><li><p>æ‰©å±•å’Œè¯„ä¼°</p><p>å¶å­ç»“ç‚¹ <span class="math inline">\(s_L\)</span> è¢«åŠ å…¥åˆ°ç­‰å¾…è¯„ä¼°é˜Ÿåˆ—è¿›è¡Œè¯„ä¼°: <span class="math inline">\((d_i(p),v)=f_\theta(d_i(s_L))\)</span>ï¼Œå…¶ä¸­ <span class="math inline">\(d_i\)</span>æ˜¯æ—‹è½¬æˆ–åå°„æ“ä½œã€‚ç¥ç»ç½‘ç»œä¸€æ¬¡è¯„ä¼°é˜Ÿåˆ—é‡Œçš„ 8 ä¸ªç»“ç‚¹;æœç´¢è¿›ç¨‹ç›´åˆ°è¯„ä¼°å®Œæ¯•æ‰èƒ½ç»§ç»­å·¥ä½œã€‚æ¯ä¸ªå¶å­ç»“ç‚¹å’Œæ¯æ¡è¾¹ <span class="math inline">\((s_L,a)\)</span> çš„ç»Ÿè®¡å€¼è¢«åˆå§‹åŒ–ä¸º <span class="math inline">\(\{N(s_L,a) = 0,W(s_L,a) = 0,Q(s_L,a) =0, P(s_L, a) = p_a\}\)</span>ï¼Œç„¶åä»·å€¼ v å¼€å§‹å›æº¯ã€‚</p></li><li><p>å›æº¯</p><p>æ¯æ¡è¾¹çš„ç»Ÿè®¡å€¼å»¶è·¯å¾„åå‘æ›´æ–°ï¼šè®¿é—®è®¡æ•°é€’å¢ <span class="math inline">\(N(s_t,ğ‘_t) = N(s_t,ğ‘_t) +1\)</span>ï¼Œç§»åŠ¨ä»·å€¼æ›´æ–°ä¸ºå¹³å‡å€¼ <span class="math inline">\(W(s_t,a_t)=W(s_t,a_t)+v\)</span>, <span class="math inline">\(Q(s_t,a_t)=\frac{W(s_t,a_t)}{N(s_t,a_t)}\)</span>ã€‚</p></li><li><p>ä¸‹æ£‹</p><p>åœ¨æœç´¢ç»“æŸæ—¶ï¼ŒAlphaGo Zero åœ¨æ ¹ä½ç½® <span class="math inline">\(s_0\)</span> é€‰æ‹©ç§»åŠ¨ aï¼Œä¸å…¶æŒ‡æ•°è®¿é—®è®¡æ•°æˆæ¯”ä¾‹ï¼Œ<span class="math inline">\(\pi(a|s_0) = \frac{N(s_0,a)^{1/\tau}}{\sum_bN(s,b)^{1/\tau}}\)</span>ï¼Œå…¶ä¸­ <span class="math inline">\(Ï„\)</span> æ˜¯æ§åˆ¶æ¢ç´¢æ°´å¹³çš„å‚æ•°ã€‚æœç´¢æ ‘å¯ä»¥åœ¨åé¢çš„æ—¶åˆ»é‡ç”¨ï¼šä¸æ‰€é€‰æ‹©çš„ç§»åŠ¨å¯¹åº”çš„å­èŠ‚ç‚¹æˆä¸ºæ–°çš„æ ¹èŠ‚ç‚¹; åœ¨è¿™ä¸ªèŠ‚ç‚¹ä¸‹é¢çš„å­æ ‘è¢«ä¿ç•™ä»¥åŠå®ƒçš„æ‰€æœ‰ç»Ÿè®¡æ•°æ®ï¼Œè€Œæ ‘çš„å…¶ä½™éƒ¨åˆ†è¢«ä¸¢å¼ƒã€‚</p></li></ol><h4><span id="å®ç°ç»†èŠ‚">å®ç°ç»†èŠ‚</span></h4><p><strong>åœ¨é€‰æ‹©çš„è¿‡ç¨‹ä¸­ï¼Œå‘ç°å½“å‰stateåœ¨historyä¸­å‡ºç°è¿‡ï¼ˆå½¢æˆå¾ªç¯ï¼‰æ€ä¹ˆåŠï¼Ÿ</strong></p><ul><li>æ ¹æ®æ¯”èµ›è§„åˆ™ï¼šé—²ç€å¾ªç¯3æ¬¡åˆ¤å’Œï¼›è¿è§„ï¼ˆé•¿æ‰ã€é•¿å°†ç­‰ï¼‰åˆ¤è´Ÿï¼›å¯¹æ–¹è¿è§„åˆ¤èƒœã€‚</li></ul><p><strong>Virtual Loss</strong></p><ul><li><p>å¤šçº¿ç¨‹æœç´¢æ—¶ï¼Œå½“æŸä¸€çº¿ç¨‹é€‰æ‹©äº†æŸä¸ªactionæ—¶ï¼Œä¸ºäº†é¼“åŠ±å…¶ä»–çº¿ç¨‹é€‰æ‹©å…¶ä»–actionï¼Œåº”è¯¥é™ä½è¯¥actionçš„ä»·å€¼ï¼ˆæ–½åŠ virtual lossï¼‰</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.tree[state].sum_n += <span class="number">1</span></span><br><span class="line">action_state = self.tree[state].a[sel_action]</span><br><span class="line">action_state.n += virtual_loss</span><br><span class="line">action_state.w -= virtual_loss</span><br><span class="line">action_state.q = action_state.w / action_state.n</span><br></pre></td></tr></table></figure></li><li><p>åœ¨å›æº¯æ—¶ï¼Œæ›´æ–°valueè¦è€ƒè™‘åˆ°virtual lossçš„å½±å“</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">node = self.tree[state]</span><br><span class="line">action_state = node.a[action]</span><br><span class="line">action_state.n += <span class="number">1</span> - virtual_loss</span><br><span class="line">action_state.w += v + virtual_loss</span><br><span class="line">action_state.q = action_state.w / action_state.n</span><br></pre></td></tr></table></figure></li></ul><p><strong>stateè¡¨ç¤º</strong></p><p>è™½ç„¶å¯¹äºç¥ç»ç½‘ç»œæ¥è¯´stateå°±æ˜¯<span class="math inline">\(14\times10\times9\)</span>çš„tensorï¼Œä½†æ˜¯å¯¹äºæœç´¢æ ‘æ¥è¯´ï¼Œæ˜¾ç„¶ä¸èƒ½ç”¨å®ƒæ¥è¡¨ç¤ºæ¯ä¸ªå±€é¢ã€‚</p><p>åœ¨åˆå§‹ç‰ˆæœ¬ä¸­ï¼Œè±¡æ£‹ç¯å¢ƒï¼ˆ<code>environment/chessboard.py</code>ï¼‰é‡Œæ˜¯ç”¨æ•°ç»„æ¥è¡¨ç¤ºæ£‹ç›˜çš„ï¼Œæ‰€ä»¥åœ¨æœç´¢ä¸­ä¹Ÿä½¿ç”¨ç›¸åº”çš„æ•°ç»„è¡¨ç¤ºstateï¼Œè¿™æ ·åšè™½ç„¶æ²¡ä»€ä¹ˆé—®é¢˜ï¼Œä½†æ˜¯åœ¨æœç´¢çš„è¿‡ç¨‹ä¸­éœ€è¦å¤§é‡çš„æ·±æ‹·è´æ“ä½œï¼ˆå› ä¸ºéœ€è¦å›æº¯ï¼‰ï¼Œå¢åŠ äº†è®¸å¤šå¼€é”€ã€‚</p><p>åæ¥ç‰ˆæœ¬è¿›è¡Œäº†æ”¹è¿›ï¼Œä½¿ç”¨<a href="https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation" target="_blank" rel="noopener">FEN string</a>ä½œä¸ºstateçš„è¡¨ç¤ºï¼Œé™ä½äº†æ‹·è´æ“ä½œçš„å¼€é”€ï¼›åŒæ—¶ä¹Ÿä¼˜åŒ–äº†è±¡æ£‹ç¯å¢ƒï¼ˆ<code>environment/static_env.py</code>ï¼‰ï¼Œå¯ä»¥ç›´æ¥å¯¹FENè¿›è¡Œæ“ä½œï¼Œæ— éœ€è®°å½•å¤æ‚çš„æ•°ç»„ã€‚</p><blockquote><p><strong>Forsythâ€“Edwards Notation</strong> (<strong>FEN</strong>) is a standard <a href="https://www.wikiwand.com/en/Chess_notation" target="_blank" rel="noopener">notation</a> for describing a particular board position of a <a href="https://www.wikiwand.com/en/Chess" target="_blank" rel="noopener">chess</a> game. The purpose of FEN is to provide all the necessary information to restart a game from a particular position.</p></blockquote><h3><span id="è‡ªæˆ‘åšå¼ˆ">è‡ªæˆ‘åšå¼ˆ</span></h3><p>ä¸ºäº†æé«˜CPU/GPUåˆ©ç”¨ç‡ï¼Œè¿™é‡Œä½¿ç”¨äº†å¤šè¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹å„è‡ªè¿›è¡Œè‡ªæˆ‘åšå¼ˆã€‚Pythonçš„å¤šè¿›ç¨‹æœ‰ä¸‰ä¸ªå®ç°æ–¹å¼ï¼š<code>fork</code>, <code>spawn</code>, <code>forkserver</code>ã€‚</p><blockquote><p>On Windows only <code>'spawn'</code> is available. On Unix <code>'fork'</code> and <code>'spawn'</code> are always supported, with <code>'fork'</code> being the default.</p></blockquote><p>ç”±äºæˆ‘è‡ªå·±åœ¨macOS/Linuxä¸Šå¼€å‘å’Œæµ‹è¯•ï¼Œæ‰€ä»¥é¦–å…ˆå®ç°çš„æ˜¯åŸºäº<code>fork</code>çš„å¤šè¿›ç¨‹ï¼Œè€Œå½“æˆ‘åœ¨ç¨‹åºåŠ äº†<code>mp.set_start_method('spawn')</code>çš„æ—¶å€™ï¼Œç¨‹åºå°±è·‘ä¸äº†äº†ï¼Œä¼šæŠ¥pickling errorï¼ˆè²Œä¼¼æ˜¯å› ä¸ºä¼ ç»™å­è¿›ç¨‹çš„å‚æ•°é‡Œä¸èƒ½å‡ºç°queueçš„æ•°æ®ç»“æ„ï¼‰ï¼Œäºæ˜¯åªèƒ½æ¢ç§æ–¹å¼å®ç°æ¥ç»•è¿‡è¿™ä¸ªé—®é¢˜ã€‚</p><h2><span id="åˆ†å¸ƒå¼">åˆ†å¸ƒå¼</span></h2><p>èµ·åˆæˆ‘æ˜¯æ²¡æ‰“ç®—åšæˆåˆ†å¸ƒå¼çš„ï¼Œå®ç°å®Œä¸Šé¢è¯´è¿°æ¨¡å—ä¹‹åæˆ‘ç”¨å®éªŒå®¤çš„K80è¿›è¡Œè®­ç»ƒï¼Œç»ƒäº†å‡ å¤©ä¹‹åå‘ç°è¿›æ­¥å¹¶ä¸æ˜æ˜¾ï¼Œå‡ ä¹è¿˜æ˜¯éšæœºä¸‹ï¼Œå¾ˆå¼±æ™ºï¼Œè¿™æ˜¯æˆ‘æ‰æ„è¯†åˆ°å³ä½¿æŠŠå®ƒè®­ç»ƒåˆ°ä¸€ä¸ªä¸šä½™ç©å®¶çš„æ°´å¹³ä¹Ÿéœ€è¦å·¨å¤§çš„ç®—åŠ›ã€‚</p><p><img src="/images/issueouashd.png"></p><p>åæ¥æœ‰ä¸€å¤©æœ‰äººåœ¨GitHubä¸Šæäº†ä¸€ä¸ªissueè¯´ä½ å¯ä»¥æŠŠå®ƒåšæˆåˆ†å¸ƒå¼çš„ï¼ŒåƒLeelaZeroé‚£æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å¸®ä½ ä¸€èµ·è®­ç»ƒã€‚<a href="https://zero.sjeng.org/" target="_blank" rel="noopener">LeelaZero</a>æ˜¯å›½å¤–ä¸€ä¸ªå¼€å‘è€…å¤ç°AlphaGoè®ºæ–‡æçš„å›´æ£‹AIï¼Œå› ä¸ºDeepMindå¹¶æ²¡æœ‰å…¬å¼€ç¨‹åºæˆ–ä»£ç ï¼Œæ‰€ä»¥ä»–æƒ³è®­ç»ƒå‡ºä¸€ä¸ªå…¬å¼€çš„å›´æ£‹ä¹‹ç¥ï¼Œç„¶åå°±é‚€è¯·ç½‘å‹å¸®ä»–ä¸€èµ·è®­ç»ƒï¼Œå…·ä½“çš„æ–¹æ³•å°±æ˜¯ï¼šç½‘å‹ä»¬åœ¨è‡ªå·±çš„æœºå™¨ä¸Šè¿›è¡Œè‡ªæˆ‘åšå¼ˆï¼Œç„¶åæŠŠåšå¼ˆçš„æ£‹è°±ä¸Šä¼ åˆ°ä»–çš„æœåŠ¡å™¨ä¸Šï¼Œç„¶åä»–æ”’å¤Ÿä¸€å®šæ£‹è°±ä¹‹åè¿›è¡Œè®­ç»ƒç¥ç»ç½‘ç»œï¼Œè®­ç»ƒå¥½ä¹‹ååˆ†å‘æ–°çš„æƒé‡ã€‚</p><p>åœ¨å›½å†…ä¹Ÿæœ‰å¾ˆå¤šäººå¸®ä»–è®­ç»ƒï¼ˆè·‘è°±ï¼‰ï¼Œç»™æˆ‘æissueçš„é‚£ä¸ªäººä¹Ÿæ˜¯å¸®LeelaZeroè®­ç»ƒä¸­çš„ä¸€å‘˜ã€‚å½“æ—¶æ­£å¥½ç¨‹åºå†™å®Œäº†æ²¡ä»€ä¹ˆäº‹åšï¼Œæ¯å¤©å°±åªèƒ½ç­‰è®­ç»ƒç»“æœï¼Œç„¶åå°±å†³å®šå°è¯•ä¸€ä¸‹è¿™ä¸ªæ¨¡å¼ã€‚å› ä¸ºä¹‹å‰æœ‰è¿‡Webå¼€å‘çš„ç»éªŒï¼Œæ‰€ä»¥æœåŠ¡å™¨å¾ˆå¿«å°±æ­å¥½äº†ï¼Œæµ‹è¯•åŸºæœ¬æ²¡é—®é¢˜ä¹‹åå°±å¼€å§‹è¿è¡Œã€‚</p><p>åœ¨ç»´æŠ¤è¿™ä¸ªé¡¹ç›®æ­£å¸¸è¿è¡Œçš„è¿‡ç¨‹ä¸­é‡åˆ°å¾ˆå¤š<strong>å‘</strong>ï¼Œç¨‹åºä¹Ÿåšäº†å¾ˆå¤šæ”¹è¿›ï¼š</p><ol type="1"><li>é¦–å…ˆæ˜¯å¸®å¿™è·‘è°±çš„å¤§å¤šéƒ½æ˜¯è±¡æ£‹çˆ±å¥½è€…ï¼Œå¹¶éå¼€å‘è€…ï¼Œæ‰€ä»¥æˆ‘è¦æŠŠPythonä»£ç æ‰“åŒ…æˆexeæ–‡ä»¶åˆ†å‘ç»™ä»–ä»¬ä¸€é”®æ‰§è¡Œï¼Œæœ€ç»ˆä½¿ç”¨<a href="https://www.pyinstaller.org/" target="_blank" rel="noopener">PyInstaller</a>æ‰“åŒ…æˆåŠŸï¼Œè¿™å…¶ä¸­é‡åˆ°äº†å¾ˆå¤šå‘ï¼š<ul><li>å¸è½½cytoolzï¼›pandasçš„ç‰ˆæœ¬å¿…é¡»ä¸º0.20.3</li><li>ä»£ç é‡ŒåŠ ä¸Š<code>mp.freeze_support()</code>ï¼Œå¦åˆ™å¤šè¿›ç¨‹ä¸ä¼šæ­£å¸¸å·¥ä½œ</li></ul></li><li>æœåŠ¡å™¨å¸¦å®½æœ‰é™ï¼Œå®¢æˆ·ç«¯ä¸‹è½½æƒé‡å¤ªæ…¢ï¼Œè§£å†³åŠæ³•ï¼šæŠŠæƒé‡æ”¾åˆ°äº‘å­˜å‚¨æœåŠ¡ä¸­ï¼Œå¦‚è…¾è®¯äº‘/ä¸ƒç‰›äº‘çš„å¯¹è±¡å­˜å‚¨æœåŠ¡ã€‚</li><li>ä¸­å›½è±¡æ£‹æ£‹è§„çš„å®Œå–„ã€‚å¹¶ä¸æ˜¯è¯´åŸºç¡€çš„é©¬èµ°æ—¥è±¡èµ°ç”°è¿™ç§è§„åˆ™ï¼Œè€Œæ˜¯åƒé•¿å°†ã€é•¿æ‰ç­‰è¿™ç§æ¯”èµ›è§„åˆ™ï¼Œè¿™ä¸ªç®—æ˜¯å‘æœ€å¤§çš„ä¸€ä¸ªï¼Œç›´åˆ°ç°åœ¨è§„åˆ™è¿˜å­˜åœ¨å°‘è®¸é—®é¢˜ã€‚</li><li>éƒ¨åˆ†æ”¯æŒäº†UCIåè®®ã€‚è¿™æ ·å°±å¯ä»¥ä½¿ç”¨å…¶ä»–çš„è±¡æ£‹ç•Œé¢åŠ è½½è¿™ä¸ªå¼•æ“ï¼Œå¹¶ä¸”èƒ½å’Œå…¶ä»–å¼•æ“å¯¹å¼ˆã€‚</li><li>å› ä¸ºâ€œåŒè¡Œç«äº‰â€ï¼Œæˆ‘çš„æœåŠ¡å™¨åœ¨ä»Šå¹´æš‘å‡æœŸé—´æˆ‘çš„æœåŠ¡å™¨ç»å¸¸é­å—DDosæ”»å‡»ï¼Œç”±äºä¹°ä¸èµ·è…¾è®¯äº‘çš„é«˜é˜²æœåŠ¡ï¼Œåªèƒ½å°è¯•å…¶ä»–åŠæ³•ï¼ŒåŒ…æ‹¬é…ç½®å¼¹æ€§IPã€é…ç½®é˜²ç«å¢™ã€Cloudfare CDNç­‰ï¼Œä½†éƒ½ä¸å¥½ç”¨ã€‚æœ€ç»ˆæŠŠæœåŠ¡è½¬ç§»åˆ°<a href="https://www.ovh.com/" target="_blank" rel="noopener">OVH</a>æä¾›çš„VPSä¸Šæ‰è§£å†³äº†é—®é¢˜ï¼ˆOVHæä¾›å…è´¹çš„DDosé˜²æŠ¤ï¼‰ã€‚</li></ol><p>æš‚æ—¶æƒ³åˆ°è¿™ä¹ˆå¤šï¼Œä»¥åæƒ³èµ·å•¥äº†å†æ›´æ–°ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://cczero.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ä¸­å›½è±¡æ£‹Zeroï¼ˆCCZeroï¼‰&lt;/a&gt;æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼ŒæŠŠ&lt;a href=&quot;https://arxiv.org/abs/1712.01815&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AlphaZero&lt;/a&gt;çš„ç®—æ³•åº”ç”¨åˆ°äº†ä¸­å›½è±¡æ£‹ä¸Šï¼Œæ—¨åœ¨å€ŸåŠ©å¹¿å¤§è±¡æ£‹çˆ±å¥½è€…ä¹‹åŠ›ä¸€èµ·è®­ç»ƒå‡ºä¸€ä¸ªå¯ä»¥æ‰“è´¥æ—‹é£åæ‰‹çš„â€œè±¡æ£‹ä¹‹ç¥â€ã€‚å› ä¸ºç§ç§åŸå› å§ï¼Œè¿™ä¸ªç›®æ ‡åˆ°ç›®å‰ï¼ˆ2018/11/07ï¼‰ä¸ºæ­¢æœªèƒ½å®ç°ï¼Œæˆ–è€…è¯´è¿˜å·®å¾—è¿œï¼Œè€Œè·‘è°±çš„äººä¹Ÿè¶Šæ¥è¶Šå°‘äº†ï¼Œå¾ˆå¯èƒ½åšæŒä¸äº†å¤šä¹…äº†ã€‚&lt;/p&gt;
&lt;p&gt;è™½ç„¶æœªèƒ½å®ç°ç›®æ ‡ï¼Œä½†åœ¨æŠ€æœ¯ä¸Šè¿˜æ˜¯æœ‰ä¸€å®šæ„ä¹‰çš„ï¼Œ&lt;a href=&quot;https://github.com/NeymarL/ChineseChess-AlphaZero&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;ä¸Šä¹Ÿæ˜¯ä¸æ˜¯æœ‰äººè¯¢é—®æŠ€æœ¯ç»†èŠ‚ï¼Œåœ¨æ­¤æ€»ç»“ä¸€ä¸‹ï¼Œè®°å½•ä¸€äº›å‘ä»¥åä¸è¦å†è¸©ã€‚&lt;/p&gt;
    
    </summary>
    
      <category term="è¸©å‘ç°åœº" scheme="http://www.52coding.com.cn/categories/%E8%B8%A9%E5%9D%91%E7%8E%B0%E5%9C%BA/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="AlphaZero" scheme="http://www.52coding.com.cn/tags/AlphaZero/"/>
    
      <category term="MCTS" scheme="http://www.52coding.com.cn/tags/MCTS/"/>
    
      <category term="CCZero" scheme="http://www.52coding.com.cn/tags/CCZero/"/>
    
      <category term="ä¸­å›½è±¡æ£‹" scheme="http://www.52coding.com.cn/tags/%E4%B8%AD%E5%9B%BD%E8%B1%A1%E6%A3%8B/"/>
    
  </entry>
  
  <entry>
    <title>åšå®¢è¿ç§»è¸©å‘è®°å½•</title>
    <link href="http://www.52coding.com.cn/2018/11/06/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E8%AE%B0%E5%BD%95/"/>
    <id>http://www.52coding.com.cn/2018/11/06/åšå®¢è¿ç§»è®°å½•/</id>
    <published>2018-11-06T02:41:47.000Z</published>
    <updated>2018-11-06T11:17:46.893Z</updated>
    
    <content type="html"><![CDATA[<p>åšå®¢è¿ç§»è¿™ä¸ªäº‹æ—©å°±æƒ³åšäº†ï¼Œä½†åˆ°ç°åœ¨æ‰æœ‰æ—¶é—´å’Œç²¾åŠ›æ¥å®Œæˆã€‚ä¸ºä»€ä¹ˆè¦è¿ç§»å‘¢ï¼Ÿä¸»è¦æœ‰å‡ ä¸ªåŸå› ï¼š</p><ol type="1"><li>åŸåšå®¢æ›´æ–°ã€ç»´æŠ¤è¾ƒéº»çƒ¦ã€‚åŸåšå®¢æ˜¯è‡ªå·±ç”¨<a href="https://www.52coding.com.cn/2015/12/21/%E8%AE%B0%E5%BD%95%EF%BC%9A%E7%94%A8PHP%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95web%E6%A1%86%E6%9E%B6/">PHPæ­çš„</a>ï¼Œä¹‹å‰çš„å†™ä½œæ–¹å¼æ˜¯ç”¨Markdownå†™å¥½å¯¼å‡ºHTMLï¼Œå†ä¿®æ”¹HTMLä»£ç ä½¿å¾—é™æ€èµ„æºï¼ˆå›¾ç‰‡ç­‰ï¼‰åŠ è½½æ­£ç¡®ï¼Œè¿™å°±ä½¿å¾—ä¿®æ”¹åšå®¢å¾ˆéº»çƒ¦ï¼›æ›´æ¢ä¸»é¢˜ä¹Ÿå¾ˆéº»çƒ¦ï¼Œåšå®¢çš„ä¸»é¢˜å’ŒMarkdownçš„ä¸»é¢˜é€šå¸¸ä¼šæœ‰å†²çªï¼Œæ‰€ä»¥æƒ³æ¢ä¸ªæ ·å¼å°±è¦æ”¹åŠå¤©CSSã€‚</li><li>è§‰å¾—UIæœ‰äº›éš¾çœ‹ï¼Œæƒ³è¦ç®€æ´ä¸€äº›ï¼›</li><li>å®‰å…¨é—®é¢˜ã€‚</li></ol><p>ç°åœ¨çš„è§£å†³æ–¹æ¡ˆæ˜¯<a href="https://pages.github.com/" target="_blank" rel="noopener">Github Pages</a> + <a href="https://hexo.io/zh-cn/index.html" target="_blank" rel="noopener">Hexo</a>ï¼Œä¸»é¢˜é€‰çš„æ˜¯<a href="https://github.com/CodeDaraW/Hacker" target="_blank" rel="noopener">Hacker</a>ï¼Œè¿ç§»äº†ä¸¤å¤©ç»ˆäºæå®Œäº†ï¼Œåœ¨æ­¤ç®€å•è®°å½•ä¸€ä¸‹é‡åˆ°çš„å‘ã€‚</p><a id="more"></a><h3><span id="æ•°å­¦å…¬å¼æ¸²æŸ“">æ•°å­¦å…¬å¼æ¸²æŸ“</span></h3><p>ç”±äºè¿™æ¬¾ä¸»é¢˜å¹¶ä¸æ˜¯åŸç”Ÿæ”¯æŒæ•°å­¦å…¬å¼çš„ï¼Œæ‰€ä»¥è¦æ·»åŠ äº›ä»£ç æ¥ä½¿å…¶æ”¯æŒMathjaxï¼Œå‚è€ƒhttp://searene.me/2016/10/01/Let-hexo-support-mathjax/ã€‚</p><p>é¦–å…ˆåœ¨ä¸»é¢˜çš„<code>layout</code>ç›®å½•ä¸‹æ–°å»º<code>mathjax.ejs</code>ï¼Œæ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;% if (theme.mathjax.enable)&#123; %&gt;</span><br><span class="line">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;</span><br><span class="line">  MathJax.Hub.Config(&#123;</span><br><span class="line">      tex2jax: &#123;</span><br><span class="line">        inlineMath: [ [&apos;$&apos;,&apos;$&apos;], [&quot;\\(&quot;,&quot;\\)&quot;] ],</span><br><span class="line">        processEscapes: true</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;</span><br><span class="line">  MathJax.Hub.Config(&#123;</span><br><span class="line">        tex2jax: &#123;</span><br><span class="line">          skipTags: [&apos;script&apos;, &apos;noscript&apos;, &apos;style&apos;, &apos;textarea&apos;, &apos;pre&apos;, &apos;code&apos;]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;</span><br><span class="line">  MathJax.Hub.Queue(function() &#123;</span><br><span class="line">          var all = MathJax.Hub.getAllJax(), i;</span><br><span class="line">          for(i=0; i &lt; all.length; i += 1) &#123;</span><br><span class="line">              all[i].SourceElement().parentNode.className += &apos; has-jax&apos;;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;</span><br><span class="line">&lt;/script&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure><p><strong>å‘1</strong>ï¼šä¹‹å‰åœ¨ç½‘ä¸ŠæŸ¥åˆ°çš„ä»£ç ç»™çš„MathJax.jsçš„é“¾æ¥å¤šæ˜¯è¿‡æœŸçš„ï¼Œå¦‚<code>https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML</code>ï¼Œç„¶åå°±è¢«å‘äº†ã€‚</p><p>ç„¶ååœ¨<code>layout.ejs</code>ä¸­åŠ ä¸Š<code>&lt;%- partial('mathjax') %&gt;</code>ï¼Œæ–‡ä»¶æ•´ä½“å†…å®¹ä¸ºï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;!DOCTYPE HTML&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;%- partial(&apos;components/head&apos;) %&gt;</span><br><span class="line"></span><br><span class="line">&lt;body&gt;</span><br><span class="line">  &lt;div class=&quot;blog&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;content&quot;&gt;</span><br><span class="line"></span><br><span class="line">      &lt;%- partial(&apos;components/header&apos;) %&gt;</span><br><span class="line"></span><br><span class="line">      &lt;main class=&quot;site-main posts-loop&quot;&gt;</span><br><span class="line">        &lt;%- body %&gt;</span><br><span class="line">      &lt;/main&gt;</span><br><span class="line"></span><br><span class="line">      &lt;%- partial(&apos;components/footer&apos;) %&gt;</span><br><span class="line">      &lt;%- partial(&apos;components/googleanalytics&apos;) %&gt;</span><br><span class="line">      &lt;!-- æ–°åŠ çš„ --&gt;</span><br><span class="line">      &lt;%- partial(&apos;mathjax&apos;) %&gt; </span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line"></span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><p>æœ€ååœ¨ä¸»é¢˜çš„<code>_config.yml</code>ä¸­åŠ ä¸Šï¼š <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mathjax:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><p>å¦‚æœæ²¡æœ‰å®‰è£…MathJaxæ’ä»¶çš„è¯éœ€è¦å®‰è£…ä¸€ä¸‹ï¼š</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-math --save</span><br></pre></td></tr></table></figure><p>é‡æ–°ç”Ÿæˆä¸€ä¸‹åº”è¯¥å°±å¯ä»¥æ¸²æŸ“æ•°å­¦å…¬å¼äº†ã€‚</p><p>ä¸è¿‡è¿˜æœ‰äº›é—®é¢˜ï¼Œå°±æ˜¯ä½ å†™åœ¨æ•°å­¦å…¬å¼é‡Œçš„ä¸‹åˆ’çº¿(<code>_</code>)ã€åæ–œæ (<code>\</code>)ã€å’Œæ˜Ÿå·(<code>*</code>)ä¼šè¢«å½“ä½œæ™®é€šMarkdownæ¥å¤„ç†ï¼Œæ¯”å¦‚æŠŠä¸‹åˆ’çº¿(<code>_</code>)å’Œæ˜Ÿå·(<code>*</code>)æ›¿æ¢æˆ<code>&lt;em&gt;</code>æ ‡ç­¾ç­‰å¯¼è‡´å…¬å¼æ¸²æŸ“é”™è¯¯ã€‚</p><p>è§£å†³æ–¹æ¡ˆæ¥è‡ªhttps://zhuanlan.zhihu.com/p/33857596ï¼Œæ‰“å¼€<code>nodes_modules/marked/lib/marked.js</code>:</p><ol type="1"><li><p>æ‰¾åˆ°ä¸‹é¢çš„ä»£ç :</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([\\`*&#123;&#125;\[\]()# +\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure><p>æ”¹ä¸ºï¼š</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*&#123;&#125;\[\]()# +\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure></li><li><p>æ‰¾åˆ°emçš„ç¬¦å·:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em: <span class="regexp">/^\b((?:[^]|_)+?)\b|^*((?:**|[\s\S])+?)*(?!*)/</span>,</span><br></pre></td></tr></table></figure><p>æ”¹ä¸ºï¼š</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em:<span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure></li></ol><p>è¿™æ ·å°±å»æ‰äº†<code>_</code>çš„æ–œä½“å«ä¹‰ï¼Œåœ¨å…¬å¼é‡Œä½¿ç”¨<code>_</code>å°±æ²¡æœ‰é—®é¢˜äº†ï¼Œä¸è¿‡è¦ä½¿ç”¨<code>*</code>çš„è¯è¦ç”¨<code>\ast</code>æ›¿ä»£ã€‚</p><h3><span id="è¯„è®º">è¯„è®º</span></h3><p>Hackerè¿™æ¬¾ä¸»é¢˜æ”¯æŒä¸¤ç§è¯„è®ºæ–¹å¼ï¼Œåˆ†åˆ«æ˜¯<a href="https://github.com/imsun/gitment" target="_blank" rel="noopener">Gitment</a>å’Œ<a href="https://disqus.com/" target="_blank" rel="noopener">Disqus</a>ã€‚ä¸€å¼€å§‹è¯•äº†è¯•Gitmentï¼Œé…ç½®å¥½ä¹‹åå‘ç°ä¸èƒ½ç”¨ï¼Œå…¶åŸå› æ˜¯æœ‰ä¸€ä¸ªæœåŠ¡è¿‡æœŸäº†è€Œä½œè€…ä¹Ÿå¼ƒå‘äº†æ²¡äººç®¡ï¼Œæˆ‘ä¹Ÿæ‡’å¾—æŠ˜è…¾å°±è½¬å‘äº†Disqusï¼Œæ³¨å†Œäº†ä¹‹åå°±å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œååˆ†æ–¹ä¾¿ï¼ˆä¸è¿‡å¥½åƒè¦ç¿»å¢™æ‰èƒ½è®¿é—®= =ï¼‰ã€‚</p><h3><span id="å…¶ä»–">å…¶ä»–</span></h3><p><strong>åˆ†å‰²çº¿</strong></p><p>Hexoä¸­å†™ä½œä¸èƒ½ä½¿ç”¨<code>___</code>æ¥å®ç°åˆ†å‰²çº¿ï¼Œç”¨äº†çš„è¯ä¼šgenerateå¤±è´¥ï¼Œè€Œä¸”æç¤ºçš„é”™è¯¯å¾ˆè¿·ï¼Œæ›¾ç»å›°æ‰°äº†æˆ‘å¾ˆä¹…ã€‚</p><p><strong>ä¿®æ”¹ç½‘ç«™Icon</strong></p><p>åœ¨ä¸»é¢˜ä¸­æ‰¾åˆ°<code>head.ejs</code>æ–‡ä»¶ï¼Œå…¶ä¸­æœ‰ä¸€è¡Œï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;link href=&quot;&lt;%- config.root %&gt;favicon.ico&quot; rel=&quot;icon&quot;&gt;;</span><br></pre></td></tr></table></figure><p>æŒ‰ç†æ¥è¯´åªè¦å¾€æ ¹ç›®å½•ï¼ˆ<code>source</code>ï¼‰ä¸‹æ”¾ä¸€ä¸ª<code>favicon.ico</code>çš„æ–‡ä»¶å³å¯ã€‚</p><p>å¯æ˜¯æˆ‘çš„å°±ä¸è¡Œï¼Œä¸çŸ¥é“ä»€ä¹ˆåŸå› ï¼ŒæŠŠæ–‡ä»¶åæ¢äº†å°±å¯ä»¥äº†ï¼Œæ‰€ä»¥æˆ‘æ”¹æˆäº†ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;link href=&quot;&lt;%- config.root %&gt;icon.png&quot; type=&quot;image/png&quot; rel=&quot;icon&quot;&gt;</span><br></pre></td></tr></table></figure><p>ç„¶åå¾€æ ¹ç›®å½•ä¸‹æ”¾ä¸€ä¸ª<code>icon.png</code>ï¼Œè§£å†³ã€‚</p><p><strong>åˆ†äº«</strong></p><p>åˆ†äº«æ¥å£ä½¿ç”¨<a href="https://github.com/overtrue/share.js" target="_blank" rel="noopener">Share.js</a>ï¼Œåªéœ€å¼•å…¥ç›¸åº”çš„csså’Œjsæ–‡ä»¶ï¼Œç…§æ–‡æ¡£ä½¿ç”¨å³å¯ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;åšå®¢è¿ç§»è¿™ä¸ªäº‹æ—©å°±æƒ³åšäº†ï¼Œä½†åˆ°ç°åœ¨æ‰æœ‰æ—¶é—´å’Œç²¾åŠ›æ¥å®Œæˆã€‚ä¸ºä»€ä¹ˆè¦è¿ç§»å‘¢ï¼Ÿä¸»è¦æœ‰å‡ ä¸ªåŸå› ï¼š&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;åŸåšå®¢æ›´æ–°ã€ç»´æŠ¤è¾ƒéº»çƒ¦ã€‚åŸåšå®¢æ˜¯è‡ªå·±ç”¨&lt;a href=&quot;https://www.52coding.com.cn/2015/12/21/%E8%AE%B0%E5%BD%95%EF%BC%9A%E7%94%A8PHP%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95web%E6%A1%86%E6%9E%B6/&quot;&gt;PHPæ­çš„&lt;/a&gt;ï¼Œä¹‹å‰çš„å†™ä½œæ–¹å¼æ˜¯ç”¨Markdownå†™å¥½å¯¼å‡ºHTMLï¼Œå†ä¿®æ”¹HTMLä»£ç ä½¿å¾—é™æ€èµ„æºï¼ˆå›¾ç‰‡ç­‰ï¼‰åŠ è½½æ­£ç¡®ï¼Œè¿™å°±ä½¿å¾—ä¿®æ”¹åšå®¢å¾ˆéº»çƒ¦ï¼›æ›´æ¢ä¸»é¢˜ä¹Ÿå¾ˆéº»çƒ¦ï¼Œåšå®¢çš„ä¸»é¢˜å’ŒMarkdownçš„ä¸»é¢˜é€šå¸¸ä¼šæœ‰å†²çªï¼Œæ‰€ä»¥æƒ³æ¢ä¸ªæ ·å¼å°±è¦æ”¹åŠå¤©CSSã€‚&lt;/li&gt;
&lt;li&gt;è§‰å¾—UIæœ‰äº›éš¾çœ‹ï¼Œæƒ³è¦ç®€æ´ä¸€äº›ï¼›&lt;/li&gt;
&lt;li&gt;å®‰å…¨é—®é¢˜ã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ç°åœ¨çš„è§£å†³æ–¹æ¡ˆæ˜¯&lt;a href=&quot;https://pages.github.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github Pages&lt;/a&gt; + &lt;a href=&quot;https://hexo.io/zh-cn/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;ï¼Œä¸»é¢˜é€‰çš„æ˜¯&lt;a href=&quot;https://github.com/CodeDaraW/Hacker&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hacker&lt;/a&gt;ï¼Œè¿ç§»äº†ä¸¤å¤©ç»ˆäºæå®Œäº†ï¼Œåœ¨æ­¤ç®€å•è®°å½•ä¸€ä¸‹é‡åˆ°çš„å‘ã€‚&lt;/p&gt;
    
    </summary>
    
      <category term="è¸©å‘ç°åœº" scheme="http://www.52coding.com.cn/categories/%E8%B8%A9%E5%9D%91%E7%8E%B0%E5%9C%BA/"/>
    
    
      <category term="Mathjax" scheme="http://www.52coding.com.cn/tags/Mathjax/"/>
    
      <category term="Hexo" scheme="http://www.52coding.com.cn/tags/Hexo/"/>
    
      <category term="Disqus" scheme="http://www.52coding.com.cn/tags/Disqus/"/>
    
      <category term="Gitment" scheme="http://www.52coding.com.cn/tags/Gitment/"/>
    
      <category term="Github Pages" scheme="http://www.52coding.com.cn/tags/Github-Pages/"/>
    
  </entry>
  
  <entry>
    <title>Interdependence and the Gains from Trade</title>
    <link href="http://www.52coding.com.cn/2018/11/03/Interdependence%20and%20the%20Gains%20from%20Trade/"/>
    <id>http://www.52coding.com.cn/2018/11/03/Interdependence and the Gains from Trade/</id>
    <published>2018-11-03T12:10:47.000Z</published>
    <updated>2018-11-06T07:10:39.499Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#a-parable-for-the-modern-economy">A Parable for the Modern Economy</a><ul><li><a href="#production-possibilities">Production Possibilities</a></li><li><a href="#specialization-and-trade">Specialization and Trade</a></li></ul></li><li><a href="#comparative-advance-the-driving-force-of-specialization">Comparative Advance: The Driving Force of Specialization</a><ul><li><a href="#absolute-advantage">Absolute Advantage</a></li><li><a href="#opportunity-cost-and-comparative-advantage">Opportunity Cost and Comparative Advantage</a></li><li><a href="#comparative-advantage-and-trade">Comparative Advantage and Trade</a></li><li><a href="#the-price-and-the-trade">The Price and The Trade</a></li></ul></li><li><a href="#applications-of-comparative-advantage">Applications of Comparative Advantage</a><ul><li><a href="#should-tiger-woods-mow-his-own-lawn">Should Tiger Woods Mow His Own Lawn?</a></li><li><a href="#should-the-united-states-trade-with-other-countries">Should the United States Trade With Other Countries?</a></li></ul></li><li><a href="#summary">Summary</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="a-parable-for-the-modern-economy">A Parable for the Modern Economy</span></h2><h3><span id="production-possibilities">Production Possibilities</span></h3><p>The graph shows the various mixes of output that an economy can produce and illustrate that people face trade-offs.</p><p><img src="/images/IMG_CBD807794C2B-1.png"> If the farmer and rancher do not trade, the production possibilities frontier is also the consumption possibilities frontier. However, the frontier shows trade-offs but do not show what they will choose to do. So letâ€™s suppose the choose the combinations identified in the graph by points A and B.</p><h3><span id="specialization-and-trade">Specialization and Trade</span></h3><p><img src="/images/IMG_FA9EF67DD902-1.png"> The farmer and rancher can both benefit because trade allows each of them to specialize in doing what they do best. The farmer will spend more time growing potatoes and less time raising cattle. The rancher will spend more time raising cattle and less time growing potatoes. As a result of specialization and trade, each of them can consume more meat and more potatoes without working any more hours.</p><h2><span id="comparative-advance-the-driving-force-of-specialization">Comparative Advance: The Driving Force of Specialization</span></h2><p>The puzzle is why the rancher does better in both fields, he still gain from trade? To solve this puzzle, we should answer first who has a lower cost to produce potatoes?</p><h3><span id="absolute-advantage">Absolute Advantage</span></h3><p>We can measure the cost through <em>absolute advantage</em> which is the ability to produce a good using fewer inputs than another producer. In our example, the only input is time. Because the rancher need fewer time to produce both items, he has the absolute advantage in producing both goods. Base on this the rancher has a lower cost to produce potatoes.</p><h3><span id="opportunity-cost-and-comparative-advantage">Opportunity Cost and Comparative Advantage</span></h3><p>Recall the <em>opportunity cost</em> is whatever must be given up to obtain some item. In our example, the opportunity cost of the rancher to produce 1 ounce potatoes is 1/2 ounce meat and the opportunity cost of him to produce 1 ounce meat is 2 ounce potatoes. Similarly, we can compute the opportunity cost for farmer which summarize in table 1. <img src="/images/IMG_857CB49E31D3-1.png"></p><p>We can also measure the cost through <em>comparative advantage</em>, which is the ability to produce a good at a lower opportunity cost than another producer. Through table 1 we can find out that farmer has comparative advantage in producing potatoes and rancher has comparative advantage in producing meat. Thatâ€™s why the rancher can gain from trade.</p><p>Although it is possible for one person to have an absolute advantage in both goods, it is impossible for one to have a comparative advantage in both goods. Because the opportunity cost of one good is inverse of the opportunity cost of the other.</p><h3><span id="comparative-advantage-and-trade">Comparative Advantage and Trade</span></h3><p>The gains from specialization and trade based on comparative advantage. By specialization in what he has a comparative advantage, the total production of the society raises which means increase the size of economic pie.</p><p>Also, the price of the goods should lower than their opportunity cost of producing it. For example, the farmer exchange 15 ounce potatoes for 5 ounce meat. The price of meat for the farmer is 3 ounce potatoes which is lower than his opportunity cost of producing meat (4 ounce potatoes). Similarly, for the rancher, the price of 1 ounce potatoes is 1/3 ounce meat which is also lower than his opportunity cost of producing potatoes (1/2 ounce meat).</p><p>Conclude: <strong>Trade can benefit everyone in society because it allows people to specialize in activities in which they have a comparative advantage</strong>.</p><h3><span id="the-price-and-the-trade">The Price and The Trade</span></h3><p><strong>For both parties to gain from trade, the price at which they trade must lie between the two opportunity costs</strong>.</p><h2><span id="applications-of-comparative-advantage">Applications of Comparative Advantage</span></h2><h3><span id="should-tiger-woods-mow-his-own-lawn">Should Tiger Woods Mow His Own Lawn?</span></h3><p>Say Tiger Woods can mow his own lawn in 2 hours while, Forrest, the boy next door, can mow the lawn in 4 hours. Should Tiger Woods mow his own lawn?</p><p>Clearly, Tiger Woods has an absolute advantage in mowing the lawn but he has a higher opportunity cost in doing it because he could spend 2 hours filming a commercial advertisement earning $10000 while Forrest can only earn $20 in 4 hours. So Tiger should hire Forrest to mow the lawn and both of them will better off as long as the payment is between $20 and $10000.</p><h3><span id="should-the-united-states-trade-with-other-countries">Should the United States Trade With Other Countries?</span></h3><p>International trade can make some individuals worse off, even as it makes the country as a whole better off. When the US exports food and imports cars, the impact on an American farmer is not the same as the impact on an American autoworker. Yet, international trade is not like war, in which some countries win and others lose. <em>Trade allows all countries to achieve greater prosperity</em>.</p><h2><span id="summary">Summary</span></h2><ul><li>Each person consumes goods and services produced by many other people both in the United States and around the world. Interdependence and trade are desirable because they allow everyone to enjoy a greater <strong>quantity and variety</strong> of goods and services.</li><li>There are two ways to compare the ability of two people in producing a good. The person who can produce the good with the smaller quantity of inputs is said to have an <em>absolute advantage</em> in producing the good. The person who has the smaller <em>opportunity cost</em> of producing the good is said to have a <em>comparative advantage</em>. The gains from trade are based on comparative advantage, not absolute advantage.</li><li>Trade makes everyone better off because it allows people to specialize in those activities in which they have a comparative advantage.</li><li>The principle of comparative advantage applies to countries as well as to people. Economists use the principle of comparative advantage to advocate free trade among countries.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#a-parable-for-the-modern-economy&quot;&gt;A Parable for the Modern Economy&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#production-possibilities&quot;&gt;Production Possibilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#specialization-and-trade&quot;&gt;Specialization and Trade&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#comparative-advance-the-driving-force-of-specialization&quot;&gt;Comparative Advance: The Driving Force of Specialization&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#absolute-advantage&quot;&gt;Absolute Advantage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#opportunity-cost-and-comparative-advantage&quot;&gt;Opportunity Cost and Comparative Advantage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#comparative-advantage-and-trade&quot;&gt;Comparative Advantage and Trade&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#the-price-and-the-trade&quot;&gt;The Price and The Trade&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#applications-of-comparative-advantage&quot;&gt;Applications of Comparative Advantage&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#should-tiger-woods-mow-his-own-lawn&quot;&gt;Should Tiger Woods Mow His Own Lawn?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#should-the-united-states-trade-with-other-countries&quot;&gt;Should the United States Trade With Other Countries?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="è¯»ä¹¦ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="å¾®è§‚ç»æµå‹åŸç†" scheme="http://www.52coding.com.cn/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%9E%8B%E5%8E%9F%E7%90%86/"/>
    
      <category term="trade" scheme="http://www.52coding.com.cn/tags/trade/"/>
    
      <category term="comparative advantage" scheme="http://www.52coding.com.cn/tags/comparative-advantage/"/>
    
  </entry>
  
  <entry>
    <title>Unityå­¦ä¹ ç¬”è®°</title>
    <link href="http://www.52coding.com.cn/2018/11/01/Unity%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://www.52coding.com.cn/2018/11/01/Unityå­¦ä¹ ç¬”è®°/</id>
    <published>2018-11-01T02:41:47.000Z</published>
    <updated>2018-11-06T04:01:05.679Z</updated>
    
    <content type="html"><![CDATA[<p><strong>è®°å½•ä¸€äº›å°åŠŸèƒ½çš„å®ç°</strong></p><!-- toc --><ul><li><a href="#å®ç°ç›¸æœºè·Ÿéš">å®ç°ç›¸æœºè·Ÿéš</a></li><li><a href="#æ‹–åŠ¨å›¾æ ‡åœ¨åœºæ™¯ç”Ÿæˆç‰©ä½“">æ‹–åŠ¨å›¾æ ‡åœ¨åœºæ™¯ç”Ÿæˆç‰©ä½“</a></li><li><a href="#æŠ€èƒ½å†·å´æ•ˆæœ">æŠ€èƒ½å†·å´æ•ˆæœ</a></li><li><a href="#é¼ æ ‡ç‚¹å‡»é€‰ä¸­åœºæ™¯ä¸­çš„ç‰©ä½“">é¼ æ ‡ç‚¹å‡»é€‰ä¸­åœºæ™¯ä¸­çš„ç‰©ä½“</a></li><li><a href="#2däººç‰©æœå·¦æœå³">2Däººç‰©æœå·¦æœå³</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="å®ç°ç›¸æœºè·Ÿéš">å®ç°ç›¸æœºè·Ÿéš</span></h2><ul><li>æ–¹æ³•ä¸€<ul><li>æŠŠç›¸æœºè®¾ç½®ä¸ºç›®æ ‡çš„Child</li></ul></li><li>æ–¹æ³•äºŒ<ul><li>è®¾ç½®å¥½è·ç›®æ ‡çš„è·ç¦»å’Œè§’åº¦ï¼Œæ ¹æ®æ•°å­¦å…³ç³»è®¡ç®—å‡ºç›¸æœºä½ç½®</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">float distance = 15;// è·ç¦»</span><br><span class="line">float rot = 0;// æ¨ªå‘è§’åº¦</span><br><span class="line">GameObject target;// ç›®æ ‡ç‰©ä½“</span><br><span class="line">float roll = 30f * Mathf.PI * 2 / 360; // çºµå‘è§’åº¦</span><br><span class="line"></span><br><span class="line">void LateUpdate () &#123;</span><br><span class="line">Vector3 targetPos = target.transform.position;</span><br><span class="line">Vector3 cameraPos;</span><br><span class="line">float d = distance * Mathf.Cos (roll);</span><br><span class="line">float height = distance * Mathf.Sin (roll);</span><br><span class="line">cameraPos.x = targetPos.x + d * Mathf.Cos (rot);</span><br><span class="line">cameraPos.z = targetPos.z + d * Mathf.Sin (rot);</span><br><span class="line">cameraPos.y = targetPos.y + height;</span><br><span class="line">Camera.main.transform.position = cameraPos;</span><br><span class="line">Camera.main.transform.LookAt (target.transform);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>ç›¸æœºéšé¼ æ ‡æ—‹è½¬</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">void Rotate()</span><br><span class="line">&#123;</span><br><span class="line"> float w = Input.GetAxis (&quot;Mouse X&quot;) * rotSpeed;</span><br><span class="line">rot -= w;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void Roll()</span><br><span class="line">&#123;</span><br><span class="line">float w = Input.GetAxis (&quot;Mouse Y&quot;) * rollSpeed * 0.5f;</span><br><span class="line">roll -= w;</span><br><span class="line">if (roll &gt; maxRoll) &#123;</span><br><span class="line">roll = maxRoll;</span><br><span class="line">&#125;</span><br><span class="line">if (roll &lt; minRoll) &#123;</span><br><span class="line">roll = minRoll;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void LateUpdate () &#123;</span><br><span class="line">Rotate();</span><br><span class="line">  Roll();</span><br><span class="line">....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="æ‹–åŠ¨å›¾æ ‡åœ¨åœºæ™¯ç”Ÿæˆç‰©ä½“">æ‹–åŠ¨å›¾æ ‡åœ¨åœºæ™¯ç”Ÿæˆç‰©ä½“</span></h2><p><strong>æ‹–åŠ¨UI</strong></p><p>æ–°å»º<code>Drag</code>ç±»ï¼Œç»§æ‰¿<code>IBeginDragHandler, IDragHandler, IEndDragHandler</code>ï¼Œå®ç°æ‹–åŠ¨UIåŠŸèƒ½æœ‰ä¸‰ä¸ªæ¥å£ï¼š</p><ul><li><code>public void OnBeginDrag (PointerEventData eventData)</code></li><li><code>public void OnDrag (PointerEventData eventData)</code></li><li><code>public void OnEndDrag (PointerEventData eventData)</code></li></ul><p>åœ¨<code>Drag</code>ç±»é‡Œå®ç°è¿™ä¸‰ä¸ªæ¥å£å³å¯å®ç°æƒ³è¦çš„æ‹–åŠ¨æ•ˆæœï¼Œæœ€åä¸ç”¨å¿˜äº†æŠŠ<code>Drag</code>è„šæœ¬æ·»åŠ åˆ°æƒ³è¦è¢«æ‹–åŠ¨çš„UIç‰©ä½“ä¸Šã€‚</p><p><strong>åœ¨åœºæ™¯ä¸­ç”Ÿæˆç‰©ä½“</strong></p><p>è¦å®ç°è¿™ä¸ªåŠŸèƒ½:</p><ul><li>é¦–å…ˆåœ¨<code>OnBeginDrag</code>ä¸­ç”Ÿæˆæ–°çš„<code>GameObject</code>ï¼›</li><li>ç„¶ååœ¨<code>OnDrag</code>ä¸­ï¼Œæ ¹æ®é¼ æ ‡åœ¨åœºæ™¯é‡Œçš„ä½ç½®è°ƒæ•´<code>GameObject</code>çš„ä½ç½®ï¼Œå†æ£€æµ‹<code>GameObject</code>çš„collideræœ‰æ— å’Œå…¶ä»–ç‰©ä½“ç¢°æ’ï¼›</li><li>æœ€ååœ¨<code>OnEndDrag</code>ä¸­ï¼Œå¦‚æœ<code>GameObject</code>çš„æœ€ç»ˆä½ç½®åˆæ³•ï¼Œåˆ™ä¸å†ç§»åŠ¨ï¼›å¦åˆ™é”€æ¯ç‰©ä½“ï¼Œç”Ÿæˆå¤±è´¥ã€‚</li></ul><h2><span id="æŠ€èƒ½å†·å´æ•ˆæœ">æŠ€èƒ½å†·å´æ•ˆæœ</span></h2><p><strong>å®šæ—¶å™¨</strong></p><p>å®ç°å†·å´æ•ˆæœè®¡æ—¶å™¨å¿…ä¸å¯å°‘ï¼Œå®ç°æ–¹æ³•ä¹Ÿå¾ˆç®€å•ï¼Œåªéœ€ä¸¤ä¸ªå˜é‡ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bool timerStarted = false;</span><br><span class="line">float remain = 10f;</span><br></pre></td></tr></table></figure><p>ç„¶ååœ¨<code>Update</code>ä¸­ä½œå¦‚ä¸‹æ›´æ–°ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">void Update ()</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">    if (timerStarted) &#123;</span><br><span class="line">remain -= Time.deltaTime;</span><br><span class="line">        if (remain &lt;= 0) &#123;</span><br><span class="line">            CloseTimer();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>UIæ•ˆæœ</strong></p><p>Buttonçš„å±‚æ¬¡å¦‚ä¸‹ï¼š</p><ul><li><p><code>Button</code></p><ul><li><p><code>Image</code>ï¼šæŒ‰é’®æ˜¾ç¤ºçš„å›¾æ ‡</p></li><li><p><code>Mask</code>ï¼šå¯ä»¥ç”¨æŒ‰é’®çš„é»˜è®¤èƒŒæ™¯ï¼›è°ƒæ•´é¢œè‰²å’Œé€æ˜åº¦ï¼›ImageTypeä¸ºfilledï¼›é€šè¿‡è°ƒæ•´Fill Amountæ¥å®ç°è½¬åŠ¨æ•ˆæœ</p><p><img src="/images/Screen%20Shot%202018-10-30%20at%204.09.00%20PM.png"></p></li><li><p><code>CD Text</code>ï¼šæ˜¾ç¤ºå‰©ä½™å†·å´æ—¶é—´</p></li></ul></li></ul><p><strong>Note</strong>ï¼šåœ¨å¼€å§‹å†·å´çš„åŒæ—¶ï¼Œåº”æŠŠè®¾ç½®<code>btn.interactable = false;</code>ï¼Œå¦åˆ™æŒ‰é’®å¯ä»¥åœ¨å†·å´è¿‡ç¨‹ä¸­å†æ¬¡è¢«ç‚¹å‡»ã€‚</p><p>è¿™é‡ŒButtonçš„<code>OnClick</code>ç»‘å®šäº†ä¸¤ä¸ªå‡½æ•°ï¼Œåˆ†åˆ«ç»™<code>CharacterController</code>å®ç°æŠ€èƒ½æ•ˆæœï¼Œå’Œç»™<code>UIManager</code>å®ç°UIåŠ¨æ•ˆï¼š</p><p><img src="/images/btnclick.png"></p><h2><span id="é¼ æ ‡ç‚¹å‡»é€‰ä¸­åœºæ™¯ä¸­çš„ç‰©ä½“">é¼ æ ‡ç‚¹å‡»é€‰ä¸­åœºæ™¯ä¸­çš„ç‰©ä½“</span></h2><p>æ€è·¯ï¼šä»ç‚¹å‡»ä½ç½®å‘åœºæ™¯å‘å°„å°„çº¿ï¼Œæ£€æµ‹æ˜¯å¦å‡»ä¸­ç‰©ä½“</p><p>å®ç°ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">void MousePick () &#123;</span><br><span class="line">    if (Input.GetMouseButtonUp (0)) &#123;</span><br><span class="line">        // å‘å°„å°„çº¿</span><br><span class="line">        Ray myRay = Camera.main.ScreenPointToRay (Input.mousePosition);</span><br><span class="line">        // é€‰æ‹©æƒ³è¢«é€‰ä¸­çš„layer</span><br><span class="line">        int layerMask = LayerMask.GetMask (&quot;Building&quot;);</span><br><span class="line">        // æ£€æµ‹ç¢°æ’</span><br><span class="line">        RaycastHit2D hit = Physics2D.Raycast (new Vector2 (myRay.origin.x, myRay.origin.y),</span><br><span class="line">            Vector2.down, Mathf.Infinity, layerMask);</span><br><span class="line">        if (hit.collider) &#123;</span><br><span class="line">            // æ£€æµ‹åˆ°ç¢°æ’ï¼Œé€‰ä¸­è¯¥ç‰©ä½“</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="2däººç‰©æœå·¦æœå³">2Däººç‰©æœå·¦æœå³</span></h2><p>æ€è·¯ï¼šå¦‚æœåŸspriteæœå³ï¼Œé‚£ä¹ˆåªè¦æŠŠtransformçš„<code>scale.x</code>å˜æˆ<code>-1</code>å°±æ˜¯æœå·¦äº†ã€‚</p><p><img src="/images/facingside.png"></p><p>å®ç°ï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">void LateUpdate () &#123;</span><br><span class="line">    Vector3 localScale = _transform.localScale;</span><br><span class="line"></span><br><span class="line">    if (_vx &gt; 0) &#123; // moving right so face right</span><br><span class="line">        _facingRight = true;</span><br><span class="line">    &#125; else if (_vx &lt; 0) &#123; // moving left so face left</span><br><span class="line">        _facingRight = false;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // check to see if scale x is right for the player</span><br><span class="line">    // if not, multiple by -1 which is an easy way to flip a sprite</span><br><span class="line">    if ((_facingRight) &amp;&amp; (localScale.x &lt; 0) || </span><br><span class="line">        ((localScale.x &gt; 0)) &#123;</span><br><span class="line">        localScale.x *= -1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // update the scale</span><br><span class="line">    _transform.localScale = localScale;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>æœªå®Œå¾…ç»­</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;è®°å½•ä¸€äº›å°åŠŸèƒ½çš„å®ç°&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#å®ç°ç›¸æœºè·Ÿéš&quot;&gt;å®ç°ç›¸æœºè·Ÿéš&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#æ‹–åŠ¨å›¾æ ‡åœ¨åœºæ™¯ç”Ÿæˆç‰©ä½“&quot;&gt;æ‹–åŠ¨å›¾æ ‡åœ¨åœºæ™¯ç”Ÿæˆç‰©ä½“&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#æŠ€èƒ½å†·å´æ•ˆæœ&quot;&gt;æŠ€èƒ½å†·å´æ•ˆæœ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#é¼ æ ‡ç‚¹å‡»é€‰ä¸­åœºæ™¯ä¸­çš„ç‰©ä½“&quot;&gt;é¼ æ ‡ç‚¹å‡»é€‰ä¸­åœºæ™¯ä¸­çš„ç‰©ä½“&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#2däººç‰©æœå·¦æœå³&quot;&gt;2Däººç‰©æœå·¦æœå³&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="è¸©å‘ç°åœº" scheme="http://www.52coding.com.cn/categories/%E8%B8%A9%E5%9D%91%E7%8E%B0%E5%9C%BA/"/>
    
    
      <category term="Unity" scheme="http://www.52coding.com.cn/tags/Unity/"/>
    
      <category term="C#" scheme="http://www.52coding.com.cn/tags/C/"/>
    
      <category term="Game Dev" scheme="http://www.52coding.com.cn/tags/Game-Dev/"/>
    
  </entry>
  
  <entry>
    <title>Thinking Like an Economist</title>
    <link href="http://www.52coding.com.cn/2018/10/03/Thinking%20Like%20an%20Economist/"/>
    <id>http://www.52coding.com.cn/2018/10/03/Thinking Like an Economist/</id>
    <published>2018-10-03T12:10:47.000Z</published>
    <updated>2018-11-06T06:39:17.839Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#the-economist-as-scientist">The Economist as Scientist</a><ul><li><a href="#the-scientific-method-observation-theory-and-more-observation">The Scientific Method: Observation, Theory, and More Observation</a></li><li><a href="#the-role-of-assumptions">The Role of Assumptions</a></li><li><a href="#economic-models">Economic Models</a></li><li><a href="#our-first-model-the-circular-flow-diagram">Our First Model: The Circular-Flow Diagram</a></li><li><a href="#our-second-model-the-production-possibilities-frontier">Our Second Model: The Production Possibilities Frontier</a></li><li><a href="#microeconomics-and-macroeconomics">Microeconomics and Macroeconomics</a></li></ul></li><li><a href="#the-economist-as-policy-adviser">The Economist as Policy Adviser</a><ul><li><a href="#positive-versus-normative-analysis">Positive versus Normative Analysis</a></li><li><a href="#economists-in-washington">Economists in Washington</a></li><li><a href="#why-economists-advice-is-not-always-followed">Why Economistsâ€™ Advice Is Not Always Followed</a></li></ul></li><li><a href="#why-economists-disagree">Why Economists Disagree</a><ul><li><a href="#differences-in-scientific-judgments">Differences in Scientific Judgments</a></li><li><a href="#difference-in-values">Difference in Values</a></li><li><a href="#perception-versus-reality">Perception versus Reality</a></li></ul></li></ul><!-- tocstop --><a id="more"></a><h2><span id="the-economist-as-scientist">The Economist as Scientist</span></h2><h3><span id="the-scientific-method-observation-theory-and-more-observation">The Scientific Method: Observation, Theory, and More Observation</span></h3><p>Invention an economic theory is just like in other scientific fields, which is <em>observation, summary to a theory and then collect data to test it</em>. However, it is often <em>difficult or impossible</em> for economists to <em>collect data</em> because you cannot change policies just for experiments. <strong>Therefore, economists often pay attention to the natural experiments offered by history</strong> which can not only give insight into the economy of the past, but also allow to illustrate and evaluate economic theories of the present.</p><h3><span id="the-role-of-assumptions">The Role of Assumptions</span></h3><p><strong>Make assumptions can simplify the question.</strong> e.g. once we understood the international trade in the simplified imaginary world, we are in a better position to understand international trader in the more complex world.</p><p>Also, the art in scientific thinking is <strong>deciding which assumptions to make</strong>. e.g. study short-run effect or long-run effect make different assumptions.</p><h3><span id="economic-models">Economic Models</span></h3><p>Economic models composed with graphs and equations which omit a lot of details to emphases the essence of economy. Each economic models make assumptions to simplify reality so as to improve our understanding of it.</p><h3><span id="our-first-model-the-circular-flow-diagram">Our First Model: The Circular-Flow Diagram</span></h3><p><img src="/images/IMG_9A38B6F35EC5-1.jpeg.jpg"></p><p>e.g. money in your wallet -&gt; buy coffee in markets for goods and services (local Starbucks) -&gt; revenue of the company -&gt; pay rental or wage -&gt; someoneâ€™s wallet</p><p>Because of its simplicity, this circular-flow diagram is useful to keep in mind when thinking about <strong>how the pieces of the economy fit together</strong>.</p><h3><span id="our-second-model-the-production-possibilities-frontier">Our Second Model: The Production Possibilities Frontier</span></h3><p><img src="/images/IMG_2122EF5B828A-1.jpeg.jpg"> The production possibilities frontier shows the <em>efficiency</em> of the society. Because resources are <em>scarce</em>, not every conceivable outcome is feasible. Points <strong>on</strong> the production possibilities frontier represent <em>efficient levels</em> of production.</p><p>It also reveals <em>trade-off</em> and <em>opportunity costs</em>: if produce more computers, means have to produce less cars. The <em>opportunity cost</em> is measured by the <strong>slope</strong> of the production possibilities frontier, which means point Fâ€™s opportunity cost of a car is lower and point Eâ€™ opportunity cost of producing a car is higher. Thatâ€™s because when at point E, the society has let all of car engineers to produce cars. Producing one more car means moving some of the best computer technicians out of the computer industry and making them autoworkers.</p><p>The production possibilities frontier also change with time, which shows <em>economic growth</em>. <img src="/images/IMG_0EA219852402-1.jpeg.jpg"></p><h3><span id="microeconomics-and-macroeconomics">Microeconomics and Macroeconomics</span></h3><p>Economics is studied on various levels, which is traditionally divided into two broad subfields:</p><ul><li><strong>Microeconomics</strong> is the study of how households and firms make decisions and how they interact in specific markets.</li><li><strong>Macroeconomics</strong> is the study of economy-wide phenomena, including inflation, unemployment, and economic growth.</li></ul><h2><span id="the-economist-as-policy-adviser">The Economist as Policy Adviser</span></h2><h3><span id="positive-versus-normative-analysis">Positive versus Normative Analysis</span></h3><p><strong>positive statements</strong>: claims that attempt to describe the world as it is <strong>normative statements</strong>: claims that attempt to prescribe how the world should be</p><p>Normative statements comes from positive statements as well as value judgements. Deciding what is good or bad policy is not just a matter of science. It also involves our views on ethics, religion, and political philosophy.</p><h3><span id="economists-in-washington">Economists in Washington</span></h3><p>Economists in Whitehouse also face trade-offs. The influence of economists on policy goes beyond their role as advisers: Their research and writings often affect policy indirectly.</p><h3><span id="why-economists-advice-is-not-always-followed">Why Economistsâ€™ Advice Is Not Always Followed</span></h3><p>Economists offer crucial input into the policy process, but their advice is only one ingredient of a complex recipe.</p><h2><span id="why-economists-disagree">Why Economists Disagree</span></h2><h3><span id="differences-in-scientific-judgments">Differences in Scientific Judgments</span></h3><p><strong>Economic is a young science and there is still much to be learned.</strong> They disagree because they have different hunches about the validity of alternative theories or about the size of important parameters that measure how economic variables are related.</p><h3><span id="difference-in-values">Difference in Values</span></h3><p>Economists give conflicting advice sometimes because they have different values.</p><h3><span id="perception-versus-reality">Perception versus Reality</span></h3><p>Why do policies such as rent control persist if the experts are united in their opposition? It may be that the realities of the <strong>political process</strong> stand as immovable obstacles. But it also may be that economists have <strong>not yet convinced</strong> enough of the public that these policies are undesirable.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#the-economist-as-scientist&quot;&gt;The Economist as Scientist&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#the-scientific-method-observation-theory-and-more-observation&quot;&gt;The Scientific Method: Observation, Theory, and More Observation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#the-role-of-assumptions&quot;&gt;The Role of Assumptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#economic-models&quot;&gt;Economic Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#our-first-model-the-circular-flow-diagram&quot;&gt;Our First Model: The Circular-Flow Diagram&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#our-second-model-the-production-possibilities-frontier&quot;&gt;Our Second Model: The Production Possibilities Frontier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#microeconomics-and-macroeconomics&quot;&gt;Microeconomics and Macroeconomics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#the-economist-as-policy-adviser&quot;&gt;The Economist as Policy Adviser&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#positive-versus-normative-analysis&quot;&gt;Positive versus Normative Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#economists-in-washington&quot;&gt;Economists in Washington&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#why-economists-advice-is-not-always-followed&quot;&gt;Why Economistsâ€™ Advice Is Not Always Followed&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#why-economists-disagree&quot;&gt;Why Economists Disagree&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#differences-in-scientific-judgments&quot;&gt;Differences in Scientific Judgments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#difference-in-values&quot;&gt;Difference in Values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#perception-versus-reality&quot;&gt;Perception versus Reality&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="è¯»ä¹¦ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="å¾®è§‚ç»æµå‹åŸç†" scheme="http://www.52coding.com.cn/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%9E%8B%E5%8E%9F%E7%90%86/"/>
    
      <category term="Production Possibilities Frontier" scheme="http://www.52coding.com.cn/tags/Production-Possibilities-Frontier/"/>
    
  </entry>
  
  <entry>
    <title>Ten Principles of Economics</title>
    <link href="http://www.52coding.com.cn/2018/09/16/Chapter%201-%20Ten%20Principles%20of%20Economics/"/>
    <id>http://www.52coding.com.cn/2018/09/16/Chapter 1- Ten Principles of Economics/</id>
    <published>2018-09-16T02:41:47.000Z</published>
    <updated>2018-11-06T06:23:18.037Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#how-people-make-decisions">How People Make Decisions</a><ul><li><a href="#principle-1-people-face-trade-offs">Principle 1: People Face Trade-offs</a></li><li><a href="#principle-2-the-cost-of-something-is-what-you-give-up-to-get-it">Principle 2: The Cost of Something Is What You Give Up to Get It</a></li><li><a href="#principle-3-rational-people-think-at-the-margin">Principle 3: Rational People Think at the Margin</a></li><li><a href="#principle-4-people-respond-to-incentives">Principle 4: People Respond to Incentives</a></li></ul></li><li><a href="#how-people-interact">How People Interact</a><ul><li><a href="#principle-5-trade-can-make-everyone-better-off">Principle 5: Trade Can Make Everyone Better Off</a></li><li><a href="#principle-6-markets-are-usually-a-good-way-to-organize-economic-activity">Principle 6: Markets Are Usually a Good Way to Organize Economic Activity</a></li><li><a href="#principle-7-governments-can-sometimes-improve-market-outcomes">Principle 7: Governments Can Sometimes Improve Market Outcomes</a></li></ul></li><li><a href="#how-the-economy-as-a-whole-works">How the Economy as a Whole Works</a><ul><li><a href="#principle-8-a-countrys-standard-of-living-depends-on-its-ability-to-produce-goods-and-services">Principle 8: A Countryâ€™s Standard of Living Depends on Its Ability to Produce Goods and Services</a></li><li><a href="#principle-9-prices-rise-when-the-government-prints-too-much-money">Principle 9: Prices Rise When the Government Prints Too Much Money</a></li><li><a href="#principle-10-society-faces-a-short-run-trade-off-between-inflation-and-unemployment">Principle 10: Society Faces a Short-Run Trade-off between Inflation and Unemployment</a></li></ul></li><li><a href="#summary">Summary</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="how-people-make-decisions">How People Make Decisions</span></h2><p><strong>scarcity</strong>: the limited nature of societyâ€™s resources <strong>economics</strong>: the study of how society manages its <em>scarce</em> resources</p><h3><span id="principle-1-people-face-trade-offs">Principle 1: People Face Trade-offs</span></h3><p>To get one thing we like, we usually have to give up another thing that we like. <strong>Making decisions</strong> requires trading-off one goal against another.</p><ul><li>student cannot learn two or more things at the same time</li><li>how to spend family income</li><li>guns (defense) and butter (living conditions)</li></ul><p><strong>Efficiency</strong> and <strong>Â Equality</strong></p><ul><li><em>Efficiency</em> means the property of society getting the most it can from its scarce resources.</li><li><em>Equality</em> means the property of distributing economic prosperity uniformly among the members of society.</li><li>In other words, <em>efficiency</em> refers to the size of the economic pie, and <em>equality</em> refers to how the pie is divided into individual slices.</li><li>When government tries to cut the economic pie into more equal slices, the pie get smaller.</li></ul><p>Nonetheless, people are likely to make good decisions only if they understand the options they have available. Our study of economics, therefore, starts by acknowledging lifeâ€™s trade-offs.</p><h3><span id="principle-2-the-cost-of-something-is-what-you-give-up-to-get-it">Principle 2: The Cost of Something Is What You Give Up to Get It</span></h3><p><strong>Opportunity cost</strong>: whatever must be given up to obtain some item. When making any decision, decision makers should be aware of the opportunity costs that accompany each possible action.</p><h3><span id="principle-3-rational-people-think-at-the-margin">Principle 3: Rational People Think at the Margin</span></h3><p><strong>Rational people</strong> systematically and purposefully do the best they can to achieve their objectives, given the available opportunities. <strong>Â Marginal change</strong>: a small incremental adjustment to a plan of action. e.g. when exam around, study one more hour instead of playing games.</p><p>Rational people often make decisions by comparing <em>marginal benefits</em> and <em>marginal costs</em>.</p><ul><li>airline ticket</li><li>why is water so cheap, while diamonds are so expensive?<ul><li>water is plentiful -&gt; margin benefit is small</li><li>diamonds are so rare -&gt; margin benefit is large A rational decision maker takes an action if and only if the <em>marginal benefit</em> of the action <strong>exceeds</strong> the <em>marginal cost</em>.</li></ul></li></ul><h3><span id="principle-4-people-respond-to-incentives">Principle 4: People Respond to Incentives</span></h3><p>An <strong>incentive</strong> is something that induces a person to act, such as the prospect of a punishment or a reward. <em>People respond to incentives, the rest is commentary.</em></p><p>Auto safety</p><ul><li>1950s, no seat belt, accident is costly -&gt; seat belt law -&gt; accident is not that costly -&gt; people drive faster (cost less time) -&gt; few deaths per accident but more accidents -&gt; little change in driver deaths and an increase in the number of pedestrian deaths.</li></ul><p>When analyzing any policy, we must consider not only the direct effects but also the less obvious indirect effects that work through incentives. If the policy changes incentives, it will cause people to alter their behavior.</p><p><em>Incentive Pay</em> Chicago buses do not take the shortcut when around congestion, because they have no incentive to do so. If they are paid by passengers like taxi rather than by bus company, they will choose the shortcuts to get more passengers like other cars do. It will increase the bus driverâ€™s productivity but also increase the risk of having accidents.</p><h2><span id="how-people-interact">How People Interact</span></h2><h3><span id="principle-5-trade-can-make-everyone-better-off">Principle 5: Trade Can Make Everyone Better Off</span></h3><p><strong>Trade</strong> between two countries is not like a sports contest in which one side wins and the other side loses. In fact, the opposite is true: <em>Trade between two countries can make each country better off</em>.</p><p>Trade allows countries to specialize in what they do best and to enjoy a greater variety of goods and services.</p><h3><span id="principle-6-markets-are-usually-a-good-way-to-organize-economic-activity">Principle 6: Markets Are Usually a Good Way to Organize Economic Activity</span></h3><p><em>Communist</em> countries worked on the premise that government officials were in the best position to allocate the economyâ€™s scarce resources. The theory behind <em>central planning</em> was that only the government could organize economic activity in a way that promoted <em>economic well-being for the country as a whole</em>. <em>Central planners</em> failed because they tried to run the economy with one hand tied behind their backs â€” the invisible hand of the marketplace.</p><p>In a <strong>market economy</strong>, the decisions of a central planner are replaced by the decisions of millions of firms and households.</p><blockquote><p>Households and firms interacting in markets act as if they are guided by an â€œinvisible handâ€ that leads them to desirable market outcomes. â€” Adam Smith</p></blockquote><p>In any market, buyers look at the price when determining how much to demand, and sellers look at the price when deciding how much to supply. As a result of the decisions that buyers and sellers make, <em>market prices</em> reflect both <em>the value of a good to society</em> and <em>the cost to society of making the good</em>. Smithâ€™s great insight was that <strong>prices</strong> adjust to <strong>guide</strong> these individual buyers and sellers to reach outcomes that, in many cases, <em>maximize the well-being of society as a whole</em>.</p><h3><span id="principle-7-governments-can-sometimes-improve-market-outcomes">Principle 7: Governments Can Sometimes Improve Market Outcomes</span></h3><p><strong>property right</strong>: the ability of an individual to own and exercise control over scarce resources. <strong>market failure</strong>: a situation in which a market left on its own fails to allocate resources efficiently. <strong>externality</strong>: the impact of one personâ€™s actions on the well-being of a bystander. <strong>market power</strong>: the ability of a single economic actor (or a small group of actors) to have a substantial influence on market prices.</p><p><em>The invisible hand is powerful, but it is not omnipotent.</em> The economy needs the government to</p><ul><li>enforce the rules and maintain the institutions that are key to a market economy</li><li>enforce <strong>property right</strong><ul><li>We all rely on government-provided police and courts to enforce our rights over the things we produce â€” and the <em>invisible hand</em> counts on our ability to enforce our rights.</li></ul></li><li>promote <strong>efficiency</strong><ul><li><em>market failure</em> because <strong>externality</strong> (e.g. pollution) and <strong>market power</strong> (e.g. monopoly)</li></ul></li><li>promote <strong>equality</strong></li></ul><h2><span id="how-the-economy-as-a-whole-works">How the Economy as a Whole Works</span></h2><h3><span id="principle-8-a-countrys-standard-of-living-depends-on-its-ability-to-produce-goods-and-services">Principle 8: A Countryâ€™s Standard of Living Depends on Its Ability to Produce Goods and Services</span></h3><p>Why the differences in living standards among countries and over time are so large? Almost all variation in living standards is attributable to differences in countriesâ€™ <strong>productivity</strong> â€” that is, the amount of goods and services produced from each unit of labor input. When thinking about how any policy will affect our living standards, the key question is <em>how it will affect our ability to produce goods and services</em>.</p><h3><span id="principle-9-prices-rise-when-the-government-prints-too-much-money">Principle 9: Prices Rise When the Government Prints Too Much Money</span></h3><p><strong>inflation</strong>: an increase in the overall level of prices in the economy</p><p>What cause inflation? In almost all cases of large or persistent inflation, the culprit is <em>growth in the quantity of money</em>.</p><blockquote><p>The broken window fallacy Some teenagers, being the little beasts that they are, toss a brick through a bakery window. A crown gathers and laments, â€œWhat a shameâ€. But before you know it, someone suggests a silver lining to the situation: Now the baker will have to spend money to have the window repaired. This will add to the income of the repairman, who will spend his additional income, which will add to another sellerâ€™s income, and so on. The chain of spending will multiply and generate higher income and employment. If the broken window is large enough, it might produce an economic boom! But if the baker hadnâ€™t spent his money on window repair, he would have spent it on the new suit he was saving to buy. Then the tailor would have the new income to spend, and so on. <em>The broken window didnâ€™t create new spending; it just diverted spending from somewhere else.</em></p></blockquote><h3><span id="principle-10-society-faces-a-short-run-trade-off-between-inflation-and-unemployment">Principle 10: Society Faces a Short-Run Trade-off between Inflation and Unemployment</span></h3><p>Short-run effects of monetary injections as follows:</p><ul><li>Increasing the amount of money in the economy stimulates the overall level of spending and thus the demand for goods and services</li><li>Higher demand many over time cause firms to raise their prices, but in the meantime, it also encourage them to hire more workers and produce a larger quantity of goods and services.</li><li>More hiring means lower unemployment.</li></ul><p><strong>business cycle</strong>: fluctuations in economic activity, such as employment and production.</p><p>Case: 2008 deep economic downturn -&gt; Barack Obama: <em>stimulus package of reduced taxes and increased government spending</em> -&gt; Federal Reserve: <em>increased the supply of money</em> -&gt; <strong>reduce unemployment</strong> -&gt; might over time lead to an <strong>excessive level of inflation</strong>.</p><h2><span id="summary">Summary</span></h2><ol type="1"><li>The fundamental lessons about individual decision making are that people face trade-offs among alternative goals, that the cost of any action is measured in terms of forgone opportunities, that rational people make decisions by comparing marginal costs and marginal benefits, and that people change their behavior in response to the incentives they face.</li><li>The fundamental lessons about interactions among people are that trade and interdependence can be mutually beneficial, that markets are usually a good way of coordinating economic activity among people, and that the government can potentially improve market outcomes by remedying a market failure or by promoting greater economic equality.</li><li>The fundamental lessons about the economy as a whole are that productivity is the ultimate source of living standards, that growth in the quantity of money is the ultimate source of inflation, and that society faces a short-run trade-off between inflation and unemployment.</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#how-people-make-decisions&quot;&gt;How People Make Decisions&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#principle-1-people-face-trade-offs&quot;&gt;Principle 1: People Face Trade-offs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-2-the-cost-of-something-is-what-you-give-up-to-get-it&quot;&gt;Principle 2: The Cost of Something Is What You Give Up to Get It&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-3-rational-people-think-at-the-margin&quot;&gt;Principle 3: Rational People Think at the Margin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-4-people-respond-to-incentives&quot;&gt;Principle 4: People Respond to Incentives&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#how-people-interact&quot;&gt;How People Interact&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#principle-5-trade-can-make-everyone-better-off&quot;&gt;Principle 5: Trade Can Make Everyone Better Off&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-6-markets-are-usually-a-good-way-to-organize-economic-activity&quot;&gt;Principle 6: Markets Are Usually a Good Way to Organize Economic Activity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-7-governments-can-sometimes-improve-market-outcomes&quot;&gt;Principle 7: Governments Can Sometimes Improve Market Outcomes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#how-the-economy-as-a-whole-works&quot;&gt;How the Economy as a Whole Works&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#principle-8-a-countrys-standard-of-living-depends-on-its-ability-to-produce-goods-and-services&quot;&gt;Principle 8: A Countryâ€™s Standard of Living Depends on Its Ability to Produce Goods and Services&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-9-prices-rise-when-the-government-prints-too-much-money&quot;&gt;Principle 9: Prices Rise When the Government Prints Too Much Money&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-10-society-faces-a-short-run-trade-off-between-inflation-and-unemployment&quot;&gt;Principle 10: Society Faces a Short-Run Trade-off between Inflation and Unemployment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="è¯»ä¹¦ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="å¾®è§‚ç»æµå‹åŸç†" scheme="http://www.52coding.com.cn/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%9E%8B%E5%8E%9F%E7%90%86/"/>
    
      <category term="inflation" scheme="http://www.52coding.com.cn/tags/inflation/"/>
    
      <category term="marginal benefit" scheme="http://www.52coding.com.cn/tags/marginal-benefit/"/>
    
  </entry>
  
  <entry>
    <title>AlphaGo, AlphaGo Zero and AlphaZero</title>
    <link href="http://www.52coding.com.cn/2018/05/15/AlphaGo%20and%20AlphaGo%20Zero/"/>
    <id>http://www.52coding.com.cn/2018/05/15/AlphaGo and AlphaGo Zero/</id>
    <published>2018-05-15T07:55:19.000Z</published>
    <updated>2018-11-06T03:43:57.031Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="go">Go</span></h2><p>å›´æ£‹èµ·æºäºå¤ä»£ä¸­å›½ï¼Œæ˜¯ä¸–ç•Œä¸Šæœ€å¤è€çš„æ£‹ç±»è¿åŠ¨ä¹‹ä¸€ã€‚åœ¨å®‹ä»£çš„ã€Šæ¢¦æºªç¬”è°ˆã€‹ä¸­æ¢è®¨äº†å›´æ£‹çš„å±€æ•°å˜åŒ–æ•°ç›®ï¼Œä½œè€…æ²ˆæ‹¬ç§°â€œå¤§çº¦è¿ä¹¦ä¸‡å­—å››åä¸‰ä¸ªï¼Œå³æ˜¯å±€ä¹‹å¤§æ•°â€ï¼Œæ„æ€æ˜¯è¯´å˜åŒ–æ•°ç›®è¦å†™43ä¸ªä¸‡å­—ã€‚æ ¹æ®å›´æ£‹è§„åˆ™ï¼Œæ²¡æœ‰æ°”çš„å­ä¸èƒ½å­˜æ´»ï¼Œæ‰£é™¤è¿™äº›çŠ¶æ€åçš„åˆæ³•çŠ¶æ€çº¦æœ‰ <span class="math inline">\(2.08Ã—10^{170}\)</span> ç§ã€‚Robertson ä¸ Munro åœ¨1978å¹´è¯å¾—å›´æ£‹æ˜¯ä¸€ç§ PSPACE-hard çš„é—®é¢˜ï¼Œå…¶å¿…èƒœæ³•ä¹‹è®°å¿†è®¡ç®—é‡åœ¨<span class="math inline">\(10^{600}\)</span> ä»¥ä¸Šï¼Œè¿™è¿œè¿œè¶…è¿‡å¯è§‚æµ‹å®‡å®™çš„åŸå­æ€»æ•° <span class="math inline">\(10^{75}\)</span>ï¼Œå¯è§å›´æ£‹å¯¹ä¼ ç»Ÿçš„æœç´¢æ–¹æ³•æ˜¯éå¸¸æœ‰æŒ‘æˆ˜çš„ã€‚ <a id="more"></a></p><p><img src="/images/go1.png"></p><h2><span id="alphago">AlphaGo</span></h2><p><img src="/images/alphago_ori.png"></p><p>AlphaGoæ˜¯ç¬¬ä¸€ä¸ªæ‰“è´¥äººç±»å† å†›çš„ç”µè„‘ç¨‹åºã€‚</p><p><strong>ç½‘ç»œç»“æ„</strong></p><p>å®ƒç”±ä¸¤ä¸ªå·ç§¯ç¥ç»ç½‘ç»œç»„æˆï¼Œåˆ†åˆ«æ˜¯ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œã€‚</p><p><img src="/images/policynet.png"></p><p>ç­–ç•¥ç½‘ç»œ P æ¨èä¸‹ä¸€æ­¥æ€ä¹ˆèµ°ï¼›å®ƒçš„è¾“å…¥å°±æ˜¯æ£‹ç›˜çš„çŸ©é˜µï¼šç™½æ£‹å’Œé»‘æ£‹çš„ä½ç½®ã€‚è¿™ä¸ªç½‘ç»œç”±è®¸å¤šå·ç§¯å±‚ç»„æˆï¼Œé€æ¸å­¦ä¹ å›´æ£‹çŸ¥è¯†ï¼Œæœ€ç»ˆè¾“å‡ºè¡ŒåŠ¨ï¼ˆactionï¼‰çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ¥æ¨èä¸‹ä¸€æ­¥æ€ä¹ˆèµ°ã€‚</p><p><img src="/images/valuenet.png"></p><p>ä»·å€¼ç½‘ç»œä¹Ÿç”±å·ç§¯ç¥ç»ç½‘ç»œç»„æˆï¼Œå®ƒæ˜¯ç”¨æ¥é¢„æµ‹è¿™ç›˜æ£‹çš„èƒœè€…ã€‚å®ƒçš„è¾“å…¥ä¹Ÿæ˜¯æ£‹ç›˜çŸ©é˜µï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªå±äº <span class="math inline">\([-1, +1]\)</span> çš„æ ‡é‡ï¼Œ-1ä»£è¡¨AlphaGoä¸€å®šä¼šè¾“ï¼Œ+1ä»£è¡¨ä¸€å®šä¼šèµ¢ã€‚</p><p><strong>è®­ç»ƒæµç¨‹</strong></p><p><img src="/images/alphago_train.png"></p><p>é¦–å…ˆæ˜¯ç›‘ç£å­¦ä¹ ï¼Œè®©ç­–ç•¥ç½‘ç»œå­¦ä¹ äººç±»ä¸“å®¶çš„æ•°æ®é›†ï¼šæ¯ä¸€ä¸ªæ£‹é¢éƒ½æœ‰ä¸€ä¸ªæ ‡ç­¾ï¼Œå¯¹åº”äººç±»ä¸“å®¶çš„ä¸‹æ³•ï¼Œè®©AlphaGoé¦–å…ˆå­¦ä¹ ä¸“å®¶çš„èµ°æ³•ã€‚ç„¶åä½¿ç”¨ç­–ç•¥ç½‘ç»œè¿›è¡Œè‡ªæˆ‘åšå¼ˆï¼Œç”±äºæ¯å±€éƒ½ä¼šäº§ç”Ÿèƒœè€…ï¼Œç”¨è¿™äº›æ•°æ®æ¥è®­ç»ƒä»·å€¼ç½‘ç»œã€‚</p><p><strong>æœç´¢ç®—æ³•</strong></p><p><img src="/images/rebredth.png"></p><p>ä½¿ç”¨ç­–ç•¥ç½‘ç»œå‡å°‘æœç´¢å®½åº¦ï¼Œåªè€ƒè™‘ç½‘ç»œæ¨èçš„ä¸‹æ³•ã€‚</p><p><img src="/images/red_val.png"></p><p>è¿˜å¯ä»¥ä½¿ç”¨ä»·å€¼ç½‘ç»œæ¥é™ä½æœç´¢æ ‘çš„æ·±åº¦ï¼Œå¯ä»¥æŠŠæœç´¢å­æ ‘æ›¿æ¢ä¸ºä¸€ä¸ªå€¼æ¥è¡¨æ˜è¿™ä¸ªå±€é¢èµ¢çš„æ¦‚ç‡ã€‚</p><p><img src="/images/mcts_go.png"></p><p>ä¸è¿‡å®é™…ä¸Šè¿˜æ˜¯ç”¨çš„è’™ç‰¹å¡æ´›æœç´¢æ ‘ã€‚å®ƒåˆ†ä¸ºä¸‰æ­¥ï¼š</p><ol type="1"><li><p>é€‰æ‹©</p><p>é¦–å…ˆä»æ ‘æ ¹å‘ä¸‹éå†ï¼Œæ¯æ¬¡é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„èµ°æ³•ï¼Œç›´åˆ°å¶èŠ‚ç‚¹ã€‚ç½®ä¿¡åº¦æ˜¯ç”±æ¯ä¸ªèŠ‚ç‚¹ä¸­å­˜å‚¨çš„ Q-value å’Œç­–ç•¥ç½‘ç»œç»™çš„å…ˆéªŒæ¦‚ç‡ P ç»„æˆã€‚</p></li><li><p>æ‰©å±•å’Œè¯„ä¼°</p><p>åˆ°äº†å¶èŠ‚ç‚¹ä¹‹åå°±è¦æ‰©å±•è¿™é¢—æ ‘ï¼Œç”¨ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œåˆ†åˆ«è¯„ä¼°å½“å‰å±€é¢ï¼ŒæŠŠæ¦‚ç‡æœ€å¤§çš„èŠ‚ç‚¹åŠ å…¥æœç´¢æ ‘ã€‚</p></li><li><p>å›æº¯</p><p>æŠŠæ–°åŠ å…¥èŠ‚ç‚¹çš„ä»·å€¼ v å›æº¯åˆ°è·¯å¾„ä¸Šçš„æ¯ä¸€ä¸ªèŠ‚ç‚¹çš„ Q-value ä¸Šã€‚</p></li></ol><p>è¿™å°±æ˜¯åˆå§‹ç‰ˆæœ¬çš„AlphaGoï¼Œè¿™ä¸ªç‰ˆæœ¬èµ¢äº†ä¸–ç•Œå† å†›æä¸–çŸ³ã€‚</p><p><img src="/images/leesd.png"></p><h2><span id="alphago-zero">AlphaGo Zero</span></h2><p>AlphaGo Zero é™¤äº†å›´æ£‹è§„åˆ™æœ¬èº«ä»¥å¤–å®Œå…¨ç§»é™¤äº†äººç±»çš„å›´æ£‹çŸ¥è¯†ï¼Œå®ƒä¸AlphaGoçš„ä¸»è¦åŒºåˆ«å¦‚ä¸‹ï¼š</p><ul><li>æ— äººç±»æ•°æ®<ul><li>å®Œå…¨ä»è‡ªæˆ‘åšå¼ˆä¸­å­¦ä¹ </li></ul></li><li>æ— æ‰‹åŠ¨ç¼–ç çš„ç‰¹å¾<ul><li>è¾“å…¥åªæ˜¯æ£‹ç›˜æœ¬èº«</li></ul></li><li>å•ä¸€çš„ç¥ç»ç½‘ç»œ<ul><li>ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œåˆäºŒä¸ºä¸€ï¼Œå¹¶ä¸”ç»“æ„æ”¹è¿›ä¸ºResNet</li><li>è¾“å‡ºéƒ¨åˆ†åˆ†ä¸ºä¸¤å¤´ï¼Œåˆ†åˆ«è¾“å‡º policy å’Œ value</li></ul></li><li>æ›´ç®€å•çš„æœç´¢<ul><li>æ›´ç®€å•çš„MCTSï¼Œæ— éšæœºçš„å¿«é€Ÿèµ°å­ï¼Œåªç”¨ç¥ç»ç½‘ç»œè¿›è¡Œè¯„ä¼°</li></ul></li></ul><p><strong>å¢å¼ºå­¦ä¹ ç®—æ³•</strong></p><p><img src="/images/rl_zero.png"></p><p>ç›®æ ‡ï¼šä½¿ç”¨é«˜è´¨é‡ï¼ˆreally really high qualityï¼‰æ•°æ®æ¥è®­ç»ƒç¥ç»ç½‘ç»œï¼Œè€Œæœ€å¥½çš„æ•°æ®æ¥æºå°±æ˜¯AlphaGoè‡ªæˆ‘åšå¼ˆã€‚</p><p>æ‰€ä»¥æµç¨‹å°±æ˜¯è¿™æ ·çš„ï¼š</p><ol type="1"><li>è¾“å…¥å½“å‰çš„æ£‹å±€ï¼Œä½¿ç”¨å½“å‰çš„ç¥ç»ç½‘ç»œæ¥æŒ‡å¯¼è¿›è¡Œè’™ç‰¹å¡æ´›æœç´¢ï¼Œç„¶åä¸‹æœç´¢å‡ºçš„é‚£æ­¥æ£‹ï¼Œæ¥ç€è¾“å…¥åé¢çš„æ£‹å±€ã€æœç´¢â€¦.ç›´åˆ°ä¸€ç›˜æ£‹ç»“æŸã€‚</li></ol><p><img src="/images/train_zero.png"></p><ol start="2" type="1"><li>ä¸‹ä¸€æ­¥å°±æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œï¼Œä½¿ç”¨ä¹‹å‰è‡ªæˆ‘å¯¹å±€çš„æ•°æ®ï¼Œè®­ç»ƒç­–ç•¥çš„æ•°æ®çš„ç‰¹å¾å°±æ˜¯ä»»ä¸€æ£‹å±€ï¼Œæ ‡ç­¾å°±æ˜¯è’™ç‰¹å¡æ´›æœç´¢çš„ç»“æœï¼Œå³ç­–ç•¥æ›´è´´è¿‘äºAlphaGoå®é™…ä¸‹çš„ç­–ç•¥ï¼ˆMCTSçš„æœç´¢ç»“æœï¼‰</li></ol><p><img src="/images/train_zero_val.png"></p><ol start="3" type="1"><li>ä¸æ­¤åŒæ—¶ï¼Œä½¿ç”¨æ¯ç›˜å¯¹å±€çš„èƒœè€…è®­ç»ƒä»·å€¼ç½‘ç»œéƒ¨åˆ†ã€‚</li></ol><p><img src="/images/zero_iterate.png"></p><ol start="4" type="1"><li>æœ€åï¼Œç»è¿‡è®­ç»ƒçš„ç¥ç»ç½‘ç»œåˆå¯ä»¥ç»§ç»­è¿›è¡Œè‡ªæˆ‘åšå¼ˆï¼Œäº§ç”Ÿæ›´é«˜è´¨é‡çš„æ•°æ®ï¼Œç„¶åç”¨è¿™ä¸ªæ•°æ®ç»§ç»­è®­ç»ƒâ€¦. å¾ªç¯å¾€å¤ï¼Œå¾ªç¯çš„å…³é”®åœ¨äºï¼Œç»è¿‡æ¯ä¸ªå¾ªç¯ï¼Œæˆ‘ä»¬éƒ½ä¼šå¾—åˆ°æ›´å¼ºçš„æ£‹æ‰‹ï¼ˆç¥ç»ç½‘ç»œï¼‰ï¼Œæ‰€ä»¥ç»§ç»­ä¼šå¾—åˆ°æ›´é«˜è´¨é‡çš„æ•°æ®ã€‚æœ€åå°±äº§ç”Ÿäº†éå¸¸å¼ºçš„æ£‹æ‰‹ã€‚</li></ol><p><img src="/images/rl_policy_ite.png"></p><p>è¿™ä¸ªç®—æ³•å¯ä»¥è¢«çœ‹ä½œæ˜¯å¢å¼ºå­¦ä¹ é‡Œçš„ç­–ç•¥è¿­ä»£ï¼ˆPolicy Iterationï¼‰ç®—æ³•ï¼š</p><ul><li>Search-Based Policy Improvement ï¼ˆç­–ç•¥å¢å¼ºï¼‰<ul><li>ç”¨å½“å‰çš„ç½‘ç»œè¿›è¡ŒMCTS</li><li>MCTSæœç´¢å‡ºæ¥çš„ç»“æœ &gt; ç¥ç»ç½‘ç»œç›´æ¥é€‰æ‹©çš„ç»“æœï¼ˆå› ä¸ºæœç´¢çš„ç»“æœç»“åˆäº†å‰ç»ï¼‰</li></ul></li><li>Search-Based Policy Evaluation ï¼ˆç­–ç•¥è¯„ä¼°ï¼‰<ul><li>ä½¿ç”¨æœç´¢ç®—æ³•å’Œç¥ç»ç½‘ç»œè¿›è¡Œè‡ªæˆ‘åšå¼ˆ</li><li>è¯„ä¼°æ”¹è¿›åçš„ç­–ç•¥</li></ul></li></ul><p><strong>å­¦ä¹ æ›²çº¿</strong></p><p><img src="/images/gozero_curve.png"></p><p><strong>å®åŠ›</strong></p><p><img src="/images/gozero_rating.png"></p><h2><span id="alphazero">AlphaZero</span></h2><p><img src="/images/alphazero.png"></p><p>AlphaZeroä½¿ç”¨åŒä¸€ç§ç®—æ³•å­¦ä¹ ä¸‰ç§ä¸åŒçš„æ£‹ç±»ï¼Œå¹¶éƒ½å–å¾—äº†è¶…äººçš„æ°´å¹³ã€‚</p><p>æ£‹ç±»AIç ”ç©¶æƒ…å†µæ€»ç»“</p><ul><li>åœ¨AIçš„å†å²ä¸Šå¾ˆæ—©å°±å¼€å§‹ç ”ç©¶æ£‹ç±»ï¼Œå¦‚å›¾çµã€é¦™å†œã€å†¯è¯ºä¼Šæ›¼ç­‰</li><li>ä¸“ä¸€ç³»ç»Ÿæ›¾åœ¨å›½é™…è±¡æ£‹ä¸ŠæˆåŠŸè¿‡<ul><li>æ·±è“åœ¨1997å¹´å‡»è´¥å¡æ°</li><li>ç°åœ¨çš„è±¡æ£‹ç¨‹åºäººç±»å·²æ— æ³•å‡»è´¥</li></ul></li><li>å°†æ£‹ï¼ˆæ—¥æœ¬è±¡æ£‹ï¼‰æ¯”å›½é™…è±¡æ£‹æ›´éš¾<ul><li>æ›´å¤§çš„æ£‹ç›˜å’Œè¡ŒåŠ¨ç©ºé—´</li><li>åªæœ‰æœ€è¿‘çš„ç¨‹åºæ‰è¾¾åˆ°äº†é¾™ç‹çš„æ°´å¹³</li></ul></li><li>æœ€å‰æ²¿çš„å¼•æ“éƒ½æ˜¯æ ¹æ® alpha-beta æœç´¢<ul><li>äººç±»å¤§å¸ˆæ‰‹å·¥ä¼˜åŒ–çš„è¯„ä¼°å‡½æ•°</li><li>æœç´¢åŸŸé’ˆå¯¹ä¸åŒæ£‹ç±»ç–¯ç‹‚ä¼˜åŒ–</li></ul></li></ul><p><img src="/images/gochess.png"></p><p>ç”±ä¸Šå›¾å¯è§å›´æ£‹ä¸å°†æ£‹å’Œè±¡æ£‹è¿˜æ˜¯æœ‰å¾ˆå¤§ä¸åŒçš„ï¼Œä½†æ˜¯AlphaZeroçš„ä¸»è¦ç®—æ³•å’ŒAlphaGo Zeroä¸€æ ·ï¼Œéƒ½æ˜¯è‡ªæˆ‘åšå¼ˆçš„å¢å¼ºå­¦ä¹ ï¼Œåªæ˜¯æŠŠä¸€äº›åªé’ˆå¯¹å›´æ£‹çš„ç»†èŠ‚å»æ‰äº†ï¼ˆæ¯”å¦‚é€šè¿‡æ—‹è½¬è¿›è¡Œæ•°æ®å¢å¼ºï¼Œå› ä¸ºå›´æ£‹æ˜¯å¯¹ç§°çš„ï¼‰å’Œè¾“å…¥è¾“å‡ºç»´åº¦è¿›è¡Œäº†æ”¹å˜ã€‚</p><p>å®ƒçš„å­¦ä¹ æ›²çº¿å¦‚ä¸‹ï¼Œå‡è¾¾åˆ°äº†é¡¶å°–æ°´å¹³ï¼š</p><p><img src="/images/zero_curve2.png"></p><h2><span id="summary">Summary</span></h2><p>ç°åœ¨æ£‹ç±»äººå·¥æ™ºèƒ½ç®—æ³•çš„å‘å±•è¶‹åŠ¿æ˜¯è¶Šæ¥è¶Šæ³›åŒ–ï¼Œè¶‹å‘äºå¤šåŠŸèƒ½ã€‚ä» AlphaGo çš„å­¦ä¹ äººç±»ä¸“å®¶çš„æ£‹è°±åˆ° AlphaGo Zero çš„ä»é›¶å¼€å§‹æ— éœ€äººç±»çŸ¥è¯†çš„è‡ªæˆ‘åšå¼ˆå­¦ä¹ å†åˆ° AlphaZero çš„åŒä¸€ç®—æ³•é€‚åº”ä¸åŒæ£‹ç±»å¹¶ä¸”éƒ½å–å¾—è¶…äººæ°´å¹³ã€‚å¯è§äººå·¥æ™ºèƒ½è¶Šæ¥è¶Šå‘é€šç”¨æ™ºèƒ½å‘å±•ï¼Œè™½ç„¶é•¿è·¯æ¼«æ¼«ï¼Œç°åœ¨çš„ç®—æ³•è¿œä¸å¤Ÿæ³›åŒ–ï¼Œä½†æ˜¯å¾ˆå¤šä¸œè¥¿ï¼Œæ¯”å¦‚ç¥ç»ç½‘ç»œç»“æ„éƒ½æ˜¯å¯ä»¥ç”¨åˆ°ä¸åŒé¢†åŸŸçš„ã€‚AlphaGo ç³»åˆ—çš„ä½œè€…ä¹‹ä¸€ David Silver æ›¾è¯´:â€œæ¯æ¬¡ä½ ä¸“é—¨åŒ–ä¸€äº›ä¸œè¥¿éƒ½ä¼šä¼¤å®³ä½ çš„æ³›åŒ–èƒ½åŠ›â€ (Every time you specialize something you hurt your generalization ability.)ã€‚äº‹å®ä¹Ÿçš„ç¡®å¦‚æ­¤ï¼ŒAlphaGo ç³»åˆ—æ¶æ„è¶Šæ¥è¶Šç®€å•ï¼Œè€Œå…¶æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›å´è¶Šæ¥è¶Šå¼ºå¤§ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;go&quot;&gt;Go&lt;/h2&gt;
&lt;p&gt;å›´æ£‹èµ·æºäºå¤ä»£ä¸­å›½ï¼Œæ˜¯ä¸–ç•Œä¸Šæœ€å¤è€çš„æ£‹ç±»è¿åŠ¨ä¹‹ä¸€ã€‚åœ¨å®‹ä»£çš„ã€Šæ¢¦æºªç¬”è°ˆã€‹ä¸­æ¢è®¨äº†å›´æ£‹çš„å±€æ•°å˜åŒ–æ•°ç›®ï¼Œä½œè€…æ²ˆæ‹¬ç§°â€œå¤§çº¦è¿ä¹¦ä¸‡å­—å››åä¸‰ä¸ªï¼Œå³æ˜¯å±€ä¹‹å¤§æ•°â€ï¼Œæ„æ€æ˜¯è¯´å˜åŒ–æ•°ç›®è¦å†™43ä¸ªä¸‡å­—ã€‚æ ¹æ®å›´æ£‹è§„åˆ™ï¼Œæ²¡æœ‰æ°”çš„å­ä¸èƒ½å­˜æ´»ï¼Œæ‰£é™¤è¿™äº›çŠ¶æ€åçš„åˆæ³•çŠ¶æ€çº¦æœ‰ &lt;span class=&quot;math inline&quot;&gt;\(2.08Ã—10^{170}\)&lt;/span&gt; ç§ã€‚Robertson ä¸ Munro åœ¨1978å¹´è¯å¾—å›´æ£‹æ˜¯ä¸€ç§ PSPACE-hard çš„é—®é¢˜ï¼Œå…¶å¿…èƒœæ³•ä¹‹è®°å¿†è®¡ç®—é‡åœ¨&lt;span class=&quot;math inline&quot;&gt;\(10^{600}\)&lt;/span&gt; ä»¥ä¸Šï¼Œè¿™è¿œè¿œè¶…è¿‡å¯è§‚æµ‹å®‡å®™çš„åŸå­æ€»æ•° &lt;span class=&quot;math inline&quot;&gt;\(10^{75}\)&lt;/span&gt;ï¼Œå¯è§å›´æ£‹å¯¹ä¼ ç»Ÿçš„æœç´¢æ–¹æ³•æ˜¯éå¸¸æœ‰æŒ‘æˆ˜çš„ã€‚
    
    </summary>
    
      <category term="å­¦ä¹ ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="AlphaGo" scheme="http://www.52coding.com.cn/tags/AlphaGo/"/>
    
      <category term="AlphaZero" scheme="http://www.52coding.com.cn/tags/AlphaZero/"/>
    
      <category term="å¢å¼ºå­¦ä¹ " scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>è®ºæ–‡ç¿»è¯‘ï¼šåœ¨æ²¡æœ‰äººç±»çŸ¥è¯†çš„æƒ…å†µä¸‹æŒæ¡å›´æ£‹</title>
    <link href="http://www.52coding.com.cn/2018/03/10/%E5%9C%A8%E6%B2%A1%E6%9C%89%E4%BA%BA%E7%B1%BB%E7%9F%A5%E8%AF%86%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E6%8E%8C%E6%8F%A1%E5%9B%B4%E6%A3%8B/"/>
    <id>http://www.52coding.com.cn/2018/03/10/åœ¨æ²¡æœ‰äººç±»çŸ¥è¯†çš„æƒ…å†µä¸‹æŒæ¡å›´æ£‹/</id>
    <published>2018-03-10T06:01:09.000Z</published>
    <updated>2018-11-06T03:48:59.304Z</updated>
    
    <content type="html"><![CDATA[<h4><span id="1-å‰è¨€">1. å‰è¨€</span></h4><p>â€‹ äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé•¿æœŸç›®æ ‡æ˜¯åœ¨ä¸€äº›æœ‰æŒ‘æˆ˜çš„é¢†åŸŸä¸­ä»é›¶å¼€å§‹å­¦ä¹ å‡ºè¶…äººç†Ÿç»ƒç¨‹åº¦çš„ç®—æ³•ã€‚æœ€è¿‘ï¼ŒAlphaGoæˆä¸ºç¬¬ä¸€ä¸ªåœ¨å›´æ£‹æ¯”èµ›ä¸­å‡»è´¥ä¸–ç•Œå† å†›çš„ç¨‹åºã€‚ AlphaGoä¸­çš„æ ‘æœç´¢ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¯„ä¼°ä½ç½®å’Œé€‰å®šçš„ç§»åŠ¨ã€‚è¿™äº›ç¥ç»ç½‘ç»œæ˜¯é€šè¿‡ç›‘ç£å­¦ä¹ æ¥è‡ªäººç±»ä¸“å®¶çš„èµ°æ³•ä»¥åŠé€šè¿‡å¼ºåŒ–è‡ªæˆ‘å­¦ä¹ æ¥è¿›è¡Œè®­ç»ƒçš„ã€‚è¿™é‡Œæˆ‘ä»¬åªä»‹ç»ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç®—æ³•ï¼Œæ²¡æœ‰è¶…å‡ºæ¸¸æˆè§„åˆ™çš„äººç±»æ•°æ®ï¼ŒæŒ‡å¯¼æˆ–é¢†åŸŸçŸ¥è¯†ã€‚AlphaGoæˆä¸ºè‡ªå·±çš„è€å¸ˆï¼šä¸€ä¸ªç¥ç»ç½‘ç»œè®­ç»ƒé¢„æµ‹AlphaGoçš„ç§»åŠ¨é€‰æ‹©å’Œæ¸¸æˆçš„èƒœè€…ã€‚è¿™ä¸ªç¥ç»ç½‘ç»œæé«˜äº†æ ‘æœç´¢çš„å¼ºåº¦ï¼Œåœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­æ‹¥æœ‰æ›´é«˜è´¨é‡çš„ç§»åŠ¨é€‰æ‹©å’Œæ›´å¼ºçš„è‡ªæˆ‘å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–°ç¨‹åºAlphaGo Zeroä»é›¶å¼€å§‹å­¦ä¹ ï¼Œå®ç°äº†è¶…äººçš„è¡¨ç°ï¼Œä¸ä¹‹å‰å‘å¸ƒçš„å¤ºå† å† å†›AlphaGoç›¸æ¯”ä»¥100-0å–èƒœã€‚</p><a id="more"></a><h4><span id="2-æ¦‚è¿°">2. æ¦‚è¿°</span></h4><p>â€‹ å›´æ£‹ç¨‹åºåœ¨äººå·¥æ™ºèƒ½æ–¹é¢å·²ç»å–å¾—äº†å¾ˆå¤§çš„è¿›å±•ï¼Œä½¿ç”¨ç»è¿‡è®­ç»ƒçš„ç›‘ç£å­¦ä¹ ç³»ç»Ÿæ¥å¤åˆ¶äººç±»ä¸“å®¶çš„å†³å®šã€‚ä½†æ˜¯ï¼Œä¸“å®¶æ•°æ®é›†é€šå¸¸å¾ˆæ˜‚è´µï¼Œä¸å¯é æˆ–æ ¹æœ¬æ— æ³•ä½¿ç”¨ã€‚å³ä½¿æœ‰å¯é çš„æ•°æ®é›†ï¼Œå®ƒä»¬ä¹Ÿå¯èƒ½ä¼šå¯¹ä»¥è¿™ç§æ–¹å¼åŸ¹è®­çš„ç³»ç»Ÿçš„æ€§èƒ½æ–½åŠ ä¸Šé™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¼ºåŒ–å­¦ä¹ ç³»ç»Ÿæ˜¯æ ¹æ®ä»–ä»¬è‡ªå·±çš„ç»éªŒè¿›è¡Œå­¦ä¹ çš„ï¼ŒåŸåˆ™ä¸Šå…è®¸ä»–ä»¬è¶…è¶Šäººç±»èƒ½åŠ›ï¼Œå¹¶åœ¨ç¼ºä¹äººåŠ›ä¸“ä¸šçŸ¥è¯†çš„é¢†åŸŸè¿ä½œã€‚æœ€è¿‘ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œæœç€è¿™ä¸ªç›®æ ‡å¿«é€Ÿå‘å±•ã€‚è¿™äº›ç³»ç»Ÿåœ¨è®¡ç®—æœºæ¸¸æˆä¸­èƒœè¿‡äººç±»ï¼Œå¦‚Atariæ¸¸æˆå’Œ3Dè™šæ‹Ÿç¯å¢ƒã€‚ç„¶è€Œï¼Œåœ¨äººç±»æ™ºåŠ›æ–¹é¢æœ€å…·æŒ‘æˆ˜æ€§çš„é¢†åŸŸ - æ¯”å¦‚è¢«å¹¿æ³›è®¤ä¸ºæ˜¯äººå·¥æ™ºèƒ½çš„å·¨å¤§æŒ‘æˆ˜çš„å›´æ£‹æ¸¸æˆ - åœ¨å¹¿é˜”çš„æœç´¢ç©ºé—´ä¸­éœ€è¦ç²¾ç¡®å’Œå¤æ‚çš„æœç´¢ã€‚ä»¥å‰çš„æ–¹æ³•æ²¡æœ‰åœ¨è¿™äº›é¢†åŸŸå®ç°è¾¾åˆ°äººç±»æ°´å¹³çš„è¡¨ç°ã€‚</p><p>â€‹ AlphaGo æ˜¯ç¬¬ä¸€ä¸ªåœ¨å›´æ£‹ä¸­å®ç°è¶…äººè¡¨ç°çš„ç¨‹åºã€‚ä¹‹å‰å‘å¸ƒçš„ç‰ˆæœ¬ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºAlphaGo Fanï¼Œäº2015å¹´10æœˆå‡»è´¥äº†æ¬§æ´²å† å†›èŒƒè¾‰ã€‚AlphaGo Fan ä½¿ç”¨äº†ä¸¤ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œï¼šè¾“å‡ºç§»åŠ¨æ¦‚ç‡çš„ç­–ç•¥ç½‘ç»œå’Œè¾“å‡ºä½ç½®è¯„ä¼°çš„ä»·å€¼ç½‘ç»œã€‚ç­–ç•¥ç½‘ç»œæœ€åˆæ˜¯é€šè¿‡ç›‘ç£å­¦ä¹ æ¥å‡†ç¡®åœ°é¢„æµ‹äººç±»ä¸“å®¶çš„è¡Œä¸ºï¼Œéšåé€šè¿‡ç­–ç•¥å‡çº§å¼ºåŒ–å­¦ä¹ è¿›è¡Œäº†æ”¹è¿›ã€‚ä»·å€¼ç½‘ç»œç»è¿‡è®­ç»ƒï¼Œå¯ä»¥é¢„æµ‹æ¸¸æˆçš„èƒœè€…ã€‚ä¸€æ—¦å¼€å§‹è®­ç»ƒï¼Œè¿™äº›ç½‘ç»œå°±ä¼šä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç»“åˆä½¿ç”¨ï¼Œä»è€Œæä¾›å…ˆè¡Œæœç´¢ï¼Œä½¿ç”¨ç­–ç•¥ç½‘ç»œå°†æœç´¢èŒƒå›´ç¼©å°ä¸ºé«˜æ¦‚ç‡ç§»åŠ¨ï¼Œå¹¶ä½¿ç”¨ä»·å€¼ç½‘ç»œæ¥è¯„ä¼°æ ‘ä¸­çš„ä½ç½®ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸º AlphaGo Lee çš„åç»­ç‰ˆæœ¬ä½¿ç”¨äº†ç±»ä¼¼çš„æ–¹æ³•ï¼Œå¹¶äº2016å¹´3æœˆå‡»è´¥äº†è·å¾—18ä¸ªå›½é™…å† å†›çš„Lee Sedolã€‚</p><p>â€‹ AlphaGo Zero ä¸ AlphaGo Fan å’Œ AlphaGo Lee åœ¨å‡ ä¸ªé‡è¦æ–¹é¢ä¸åŒã€‚é¦–å…ˆï¼Œå®ƒåªæ˜¯é€šè¿‡è‡ªæˆ‘å¢å¼ºå¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä»éšæœºæ¯”èµ›å¼€å§‹ï¼Œæ²¡æœ‰ä»»ä½•ç›‘ç£æˆ–ä½¿ç”¨äººç±»æ•°æ®ã€‚å…¶æ¬¡ï¼Œå®ƒåªä½¿ç”¨é»‘ç™½æ£‹ä½ç½®ä½œä¸ºè¾“å…¥ã€‚ç¬¬ä¸‰ï¼Œå®ƒä½¿ç”¨å•ä¸€çš„ç¥ç»ç½‘ç»œï¼Œè€Œä¸æ˜¯å•ç‹¬çš„ç­–ç•¥å’Œä»·å€¼ç½‘ç»œã€‚æœ€åï¼Œå®ƒä½¿ç”¨æ›´ç®€å•çš„æœç´¢æ ‘ï¼Œè¯¥æœç´¢ä¾èµ–äºè¿™ä¸ªå•ä¸€çš„ç¥ç»ç½‘ç»œæ¥è¯„ä¼°ä½ç½®å’Œç§»åŠ¨ï¼Œè€Œæ— éœ€æ‰§è¡Œä»»ä½• Monte Carlo å›æº¯ã€‚ä¸ºäº†å®ç°è¿™äº›ç»“æœï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨è®­ç»ƒç¯å†…éƒ¨ç»“åˆäº†å‰ç»æœç´¢ï¼Œä»è€Œå®ç°äº†å¿«é€Ÿæ”¹è¿›å’Œç²¾ç¡®è€Œç¨³å®šçš„å­¦ä¹ ã€‚åœ¨æ–¹æ³•ä¸€æ ä¸­ä¸­æè¿°äº†æœç´¢ç®—æ³•ï¼Œè®­ç»ƒè¿‡ç¨‹å’Œç½‘ç»œä½“ç³»ç»“æ„ä¸­çš„å…¶ä»–æŠ€æœ¯å·®å¼‚ã€‚</p><h4><span id="3-alphago-zero-ä¸­çš„å¢å¼ºå­¦ä¹ ">3. AlphaGo Zero ä¸­çš„å¢å¼ºå­¦ä¹ </span></h4><p>â€‹ æˆ‘ä»¬çš„æ–°æ–¹æ³•ä½¿ç”¨å‚æ•°ä¸º <span class="math inline">\(\theta\)</span> çš„æ·±åº¦ç¥ç»ç½‘ç»œ <span class="math inline">\(f(\theta)\)</span>ã€‚è¯¥ç¥ç»ç½‘ç»œå°†ä½ç½®åŠå…¶å†å²çš„åŸå§‹å¹³é¢è¡¨ç¤º s ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºç§»åŠ¨æ¦‚ç‡å’Œä»·å€¼ <span class="math inline">\((p,v)=f_\theta(s)\)</span>ã€‚ ç§»åŠ¨æ¦‚ç‡ p çš„å‘é‡è¡¨ç¤ºé€‰æ‹©æ¯ä¸ªç§»åŠ¨ a çš„æ¦‚ç‡ï¼Œ<span class="math inline">\(P_a=Pr(a|s)\)</span> ã€‚ä»·å€¼ v æ˜¯ä¸€ä¸ªæ ‡é‡è¯„ä¼°ï¼Œç”¨äºä¼°è®¡å½“å‰ç©å®¶ä»ä½ç½® s è·èƒœçš„æ¦‚ç‡ã€‚è¿™ä¸ªç¥ç»ç½‘ç»œå°†ç­–ç•¥ç½‘ç»œå’Œä»·å€¼ç½‘ç»œç»“åˆåˆ°ä¸€ä¸ªç½‘ç»œä¸­ã€‚ç¥ç»ç½‘ç»œç”±å·ç§¯å±‚ï¼Œè®¸å¤šæ®‹å·®å—ç»„æˆï¼Œæ‰¹é‡å½’ä¸€åŒ–å’Œéçº¿æ€§æ•´æµå™¨ï¼ˆå‚è§æ–¹æ³•ï¼‰ç»„æˆã€‚</p><p>â€‹ AlphaGo Zero ä¸­çš„ç¥ç»ç½‘ç»œæ˜¯é€šè¿‡ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä»è‡ªæˆ‘åšå¼ˆçš„æ¸¸æˆä¸­è®­ç»ƒå‡ºæ¥çš„ã€‚åœ¨æ¯ä¸ªä½ç½® <span class="math inline">\(s\)</span>ï¼Œæ‰§è¡Œ MCTS æœç´¢ï¼Œç”±ç¥ç»ç½‘ç»œ <span class="math inline">\(f(\theta)\)</span> æŒ‡å¯¼ã€‚MCTS æœç´¢è¾“å‡ºæ¯æ¬¡ç§»åŠ¨çš„æ¦‚ç‡ <span class="math inline">\(Ï€\)</span>ã€‚è¿™äº›æœç´¢æ¦‚ç‡é€šå¸¸é€‰æ‹©æ¯”ç¥ç»ç½‘ç»œçš„åŸå§‹ç§»åŠ¨æ¦‚ç‡ <span class="math inline">\(p\)</span> æ›´åŠ å¼ºå¤§;å› æ­¤ï¼ŒMCTS å¯è¢«è§†ä¸ºç­–ç•¥æ”¹è¿›çš„æ“ä½œã€‚ä½¿ç”¨æœç´¢è¿›è¡Œè‡ªæˆ‘åšå¼ˆ - ä½¿ç”¨æ”¹è¿›çš„åŸºäº MCTS çš„ç­–ç•¥æ¥é€‰æ‹©æ¯ä¸ªåŠ¨ä½œï¼Œç„¶åä½¿ç”¨æ¸¸æˆè·èƒœè€… <span class="math inline">\(z\)</span> ä½œä¸ºä»·å€¼çš„æ ·æœ¬ - å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªå¼ºå¤§çš„ç­–ç•¥è¯„ä¼°æ“ä½œã€‚æˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ä¸»è¦æ€æƒ³æ˜¯åœ¨ç­–ç•¥è¿­ä»£è¿‡ç¨‹ä¸­é‡å¤ä½¿ç”¨è¿™äº›æ“ä½œï¼šæ›´æ–°ç¥ç»ç½‘ç»œçš„å‚æ•°ä»¥ä½¿ç§»åŠ¨æ¦‚ç‡å’Œå€¼ <span class="math inline">\((p,v)=f_\theta(s)\)</span> æ›´ç´§å¯†åŒ¹é…æ”¹è¿›çš„æœç´¢æ¦‚ç‡å’Œè·èƒœè€… <span class="math inline">\((\pi,z)\)</span>ï¼›è¿™äº›æ–°å‚æ•°å°†ç”¨äºä¸‹ä¸€æ¬¡è‡ªæˆ‘åšå¼ˆï¼Œä»¥ä½¿æœç´¢æ›´åŠ å¼ºå¤§ã€‚å›¾ 1è¯´æ˜äº†è‡ªæˆ‘åšå¼ˆè®­ç»ƒæµç¨‹ã€‚</p><p><img src="/images/selfplay.png"></p><p>â€‹ <em>å›¾1 AlphaGo Zeroä¸­çš„è‡ªæˆ‘åšå¼ˆä¸å¢å¼ºå­¦ä¹ è®­ç»ƒæµç¨‹</em></p><p>â€‹ MCTS ä½¿ç”¨ç¥ç»ç½‘ç»œ <span class="math inline">\(f(\theta)\)</span> æ¥æŒ‡å¯¼å…¶æ¨¡æ‹Ÿï¼ˆè§å›¾ 2ï¼‰ã€‚æœç´¢æ ‘ä¸­çš„æ¯ä¸ªè¾¹ <span class="math inline">\((s,a)\)</span> å­˜å‚¨å…ˆéªŒæ¦‚ç‡ <span class="math inline">\(P(s,a)\)</span>ï¼Œè®¿é—®è®¡æ•° <span class="math inline">\(N(s,a)\)</span> å’ŒåŠ¨ä½œä»·å€¼ <span class="math inline">\(Q(s,a)\)</span> ã€‚æ¯ä¸ªæ¨¡æ‹Ÿä»æ ¹çŠ¶æ€å¼€å§‹ï¼Œå¹¶ä¸”è¿­ä»£åœ°é€‰æ‹©ä½¿ç½®ä¿¡ä¸Šé™ <span class="math inline">\(Q(s,a)+U(s,a)\)</span> æœ€å¤§åŒ–çš„ç§»åŠ¨ï¼Œå…¶ä¸­<span class="math inline">\(U\propto \frac{P(s,a)}{1+N(s,a)}\)</span>ï¼Œç›´åˆ°é‡åˆ°å¶èŠ‚ç‚¹ <span class="math inline">\(s&#39;\)</span>ã€‚è¯¥å¶å­ä½ç½®è¢«ç½‘ç»œæ‰©å±•å’Œè¯„ä¼°ä¸€æ¬¡ï¼Œä»¥äº§ç”Ÿå…ˆéªŒæ¦‚ç‡å’Œè¯„ä¼°ï¼Œ<span class="math inline">\((P(s&#39;, \cdot), v(s&#39;))=f_\theta(s&#39;)\)</span>ã€‚åœ¨æ¨¡æ‹Ÿä¸­éå†çš„æ¯ä¸ªè¾¹ <span class="math inline">\((s,a)\)</span> è¢«æ›´æ–°ä»¥å¢åŠ å…¶è®¿é—®è®¡æ•° <span class="math inline">\(N(s,a)\)</span>ï¼Œå¹¶ä¸”å°†å…¶åŠ¨ä½œä»·å€¼æ›´æ–°ä¸ºåœ¨è¿™äº›æ¨¡æ‹Ÿä¸Šçš„å¹³å‡è¯„ä¼° <span class="math inline">\(Q(s,a) = \frac{1}{N(s,a)}\sum_{s&#39;|s,a\rightarrow s&#39;}V(s&#39;)\)</span>ï¼Œ å…¶ä¸­ <span class="math inline">\(s,a\rightarrow s&#39;\)</span> è¡¨ç¤ºåœ¨ä»ä½ç½® <span class="math inline">\(s\)</span> æ‰§è¡Œè¡ŒåŠ¨ <span class="math inline">\(a\)</span> åæ¨¡æ‹Ÿæœ€ç»ˆè¾¾åˆ°ä½ç½® <span class="math inline">\(s&#39;\)</span>ã€‚</p><p><img src="/images/mcts0.png"></p><p>â€‹ <em>å›¾2 AlphaGo Zeroä¸­çš„è’™ç‰¹å¡æ´›æœç´¢æ ‘</em></p><p>â€‹ MCTS å¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ç§è‡ªæˆ‘åšå¼ˆç®—æ³•ï¼Œåœ¨ç»™å®šç¥ç»ç½‘ç»œå‚æ•° <span class="math inline">\(Î¸\)</span> å’Œæ ¹ä½ç½® <span class="math inline">\(s\)</span> çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—æ¨èç§»åŠ¨çš„æœç´¢æ¦‚ç‡çŸ¢é‡ï¼Œ<span class="math inline">\(\pi = a_\theta(s)\)</span>ï¼Œä¸æ¯æ¬¡ç§»åŠ¨çš„è®¿é—®è®¡æ•°çš„æŒ‡æ•°æˆæ¯”ä¾‹ï¼Œ<span class="math inline">\(\pi_a\propto N(s,a)^{1/\tau}\)</span>ï¼Œå…¶ä¸­ <span class="math inline">\(Ï„\)</span> æ˜¯æ¸©åº¦å‚æ•°ã€‚</p><p>â€‹ ç¥ç»ç½‘ç»œé€šè¿‡ä½¿ç”¨ MCTS é€‰æ‹©æ¯ä¸ªåŠ¨ä½œçš„è‡ªæˆ‘åšå¼ˆå¢å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚é¦–å…ˆï¼Œç¥ç»ç½‘ç»œè¢«åˆå§‹åŒ–ä¸ºéšæœºæƒé‡ <span class="math inline">\(\theta_0\)</span>ã€‚åœ¨éšåçš„æ¯æ¬¡è¿­ä»£ <span class="math inline">\(iâ‰¥1\)</span> æ—¶ï¼Œäº§ç”Ÿè‡ªæˆ‘åšå¼ˆçš„æ•°æ®ï¼ˆå›¾ 1ï¼‰ã€‚åœ¨æ¯ä¸ªæ—¶åˆ» <span class="math inline">\(t\)</span>ï¼Œä½¿ç”¨å…ˆå‰çš„ç¥ç»ç½‘ç»œè¿­ä»£ <span class="math inline">\(f_{\theta_{i-1}}\)</span> æ‰§è¡Œ MCTS æœç´¢ï¼Œå¹¶ä¸”é€šè¿‡å¯¹æœç´¢æ¦‚ç‡ <span class="math inline">\(\pi_t\)</span> è¿›è¡Œé‡‡æ ·æ¥æ‰§è¡Œç§»åŠ¨ã€‚å½“ä¸¤ä¸ªç©å®¶éƒ½æ— è·¯å¯èµ°æ—¶æˆ–è€…å½“æœç´¢å€¼ä¸‹é™åˆ°ä½äºé˜ˆå€¼æˆ–å½“æ¸¸æˆè¶…è¿‡æœ€å¤§é•¿åº¦æ—¶ï¼Œæ¸¸æˆåœ¨æ—¶åˆ» <span class="math inline">\(T\)</span> ç»ˆæ­¢;ç„¶åå¯¹æ¸¸æˆè¿›è¡Œè¯„åˆ†ä»¥ç»™å‡º <span class="math inline">\(r_T\in\{-1,+1\}\)</span> çš„æœ€ç»ˆå¥–åŠ±ï¼ˆè¯¦è§æ–¹æ³•ï¼‰ã€‚æ¯ä¸ªæ—¶åˆ» <span class="math inline">\(t\)</span> çš„æ•°æ®å­˜å‚¨ä¸º <span class="math inline">\((s_t,\pi_t,z_t)\)</span>ï¼Œå…¶ä¸­ <span class="math inline">\(z_t = \pm r_T\)</span> æ˜¯æ—¶åˆ» <span class="math inline">\(t\)</span> ä»å½“å‰ç©å®¶è§’åº¦å‡ºå‘çš„æ¸¸æˆè·èƒœè€…ã€‚åŒæ—¶ï¼ˆå¦‚å›¾ 1ï¼‰ï¼Œæ–°çš„ç½‘ç»œå‚æ•° <span class="math inline">\(\theta_i\)</span> ä»æœ€åä¸€æ¬¡è‡ªæˆ‘åšå¼ˆçš„æ‰€æœ‰æ—¶é—´ä¸­ç»Ÿä¸€é‡‡æ ·çš„æ•°æ® <span class="math inline">\((s,\pi,t)\)</span> è¿›è¡Œè®­ç»ƒã€‚è°ƒæ•´ç¥ç»ç½‘ç»œ <span class="math inline">\((p,v)=f_{\theta_i}(s)\)</span> ä»¥æœ€å°åŒ–é¢„æµ‹å€¼ <span class="math inline">\(v\)</span> ä¸å®é™…èµ¢å¾—è€… <span class="math inline">\(z\)</span>ä¹‹é—´çš„è¯¯å·®ï¼Œå¹¶ä½¿ç¥ç»ç½‘ç»œç§»åŠ¨æ¦‚ç‡ <span class="math inline">\(p\)</span> ä¸æœç´¢æ¦‚ç‡ <span class="math inline">\(Ï€\)</span> çš„ç›¸ä¼¼æ€§æœ€å¤§åŒ–ã€‚å…·ä½“è€Œè¨€ï¼Œå‚æ•° <span class="math inline">\(Î¸\)</span> é€šè¿‡æ¢¯åº¦ä¸‹é™åœ¨æŸå¤±å‡½æ•° <span class="math inline">\(l\)</span> ä¸Šè¿›è¡Œè°ƒæ•´ï¼Œæ‰€è¿°æŸå¤±å‡½æ•° <span class="math inline">\(l\)</span> åˆ†åˆ«å¯¹å‡æ–¹è¯¯å·®å’Œäº¤å‰ç†µè¯¯å·®è¿›è¡Œæ±‚å’Œï¼š <span class="math display">\[(p,v)=f_\theta(s) \mbox{ and }l=(z-v)^2-\pi^T\log p+c||\theta||^2\]</span> å…¶ä¸­ <span class="math inline">\(c\)</span> æ˜¯æ§åˆ¶ L2 æ­£åˆ™åŒ–ç¨‹åº¦çš„è¶…å‚æ•°ï¼ˆä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ã€‚</p><h4><span id="4-alphago-zero-çš„å®éªŒåˆ†æ">4. AlphaGo Zero çš„å®éªŒåˆ†æ</span></h4><p>â€‹ æˆ‘ä»¬ä½¿ç”¨ä¸Šè¿°å¼ºåŒ–å­¦ä¹ æµç¨‹æ¥è®­ç»ƒ AlphaGo Zeroã€‚è®­ç»ƒä»å®Œå…¨éšæœºçš„è¡Œä¸ºå¼€å§‹ï¼ŒæŒç»­çº¦ä¸‰å¤©ä¸”æ— äººä¸ºå¹²é¢„ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ª MCTS ä½¿ç”¨ 1,600 æ¬¡æ¨¡æ‹Ÿï¼Œæ¯æ¬¡ç§»åŠ¨çš„æ€è€ƒæ—¶é—´å¤§çº¦ä¸º 0.4sï¼Œä»è€Œäº§ç”Ÿäº† 490 ä¸‡å±€è‡ªæˆ‘åšå¼ˆã€‚ å‚æ•°ä» 700,000 ä¸ªåŒ…å« 2048 ä¸ªçŠ¶æ€çš„æ‰¹é‡ä¸­æ›´æ–°ã€‚ç¥ç»ç½‘ç»œåŒ…å« 20 ä¸ªæ®‹ä½™å—ã€‚</p><p>â€‹ å›¾ 3 æ˜¾ç¤ºäº† AlphaGo Zero åœ¨è‡ªæˆ‘åšå¼ˆè¿‡ç¨‹ä¸­çš„è¡¨ç°ï¼Œæ¨ªåæ ‡ä¸ºè®­ç»ƒæ—¶é—´ï¼Œçºµåæ ‡ä¸º Elo é‡ã€‚æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹è¿›å±•é¡ºåˆ©ï¼Œå¹¶ä¸”æ²¡æœ‰é­å—å…ˆå‰æ–‡çŒ®ä¸­æå‡ºçš„æŒ¯è¡æˆ–ç¾éš¾æ€§é—å¿˜ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒAlphaGo Zero ä»…ä»… 36 å°æ—¶å°±èµ¢äº†AlphaGo Leeã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒAlphaGo Lee è®­ç»ƒäº†å‡ ä¸ªæœˆã€‚åœ¨ 72 å°æ—¶åï¼Œæˆ‘ä»¬æ ¹æ®åœ¨é¦–å°”äººæœºæ¯”èµ›ä¸­ä½¿ç”¨çš„ç›¸åŒçš„ 2 å°æ—¶æ—¶é—´æ§åˆ¶å’ŒåŒ¹é…æ¡ä»¶ï¼Œå¯¹ AlphaGo Zero ä¸ AlphaGo Lee çš„ç¡®åˆ‡ç‰ˆæœ¬è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯¥ç‰ˆæœ¬å‡»è´¥äº† Lee Sedolã€‚AlphaGo Zero ä½¿ç”¨å¸¦æœ‰4ä¸ªå¼ é‡å¤„ç†å•å…ƒï¼ˆTPUï¼‰çš„å•å°æœºå™¨ï¼Œè€Œ AlphaGo Lee åˆ†å¸ƒåœ¨å¤šå°æœºå™¨ä¸Šå¹¶ä½¿ç”¨ 48 ä¸ªTPUã€‚AlphaGo Zero å°† AlphaGo Lee ä»¥ 100 æ¯” 0 å‡»è´¥ã€‚</p><p>â€‹ ä¸ºäº†è¯„ä¼°è‡ªæˆ‘å¼ºåŒ–å­¦ä¹ çš„ä¼˜ç‚¹ï¼Œä¸ä»äººç±»æ•°æ®ä¸­å­¦ä¹ ç›¸æ¯”ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ç¬¬äºŒä¸ªç¥ç»ç½‘ç»œï¼ˆä½¿ç”¨ç›¸åŒçš„ä½“ç³»ç»“æ„ï¼‰æ¥é¢„æµ‹ KGS æœåŠ¡å™¨æ•°æ®é›†ä¸­çš„ä¸“å®¶åŠ¨ä½œ; ä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼Œè¿™å®ç°äº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ ç›‘ç£å¼å­¦ä¹ çš„åˆå§‹è¡¨ç°æ›´å¥½ï¼Œå¹¶ä¸”æ›´å¥½åœ°é¢„æµ‹äººç±»èŒä¸šåŠ¨ä½œï¼ˆå›¾ 3ï¼‰ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡ç›‘ç£å­¦ä¹ è·å¾—äº†æ›´é«˜çš„ç§»åŠ¨é¢„æµ‹å‡†ç¡®åº¦ï¼Œä½†è‡ªå­¦è€…çš„æ•´ä½“è¡¨ç°æ›´å¥½ï¼Œåœ¨è®­ç»ƒçš„å‰ 24 å°æ—¶å†…å‡»è´¥äº†è®­ç»ƒæœ‰ç´ çš„é€‰æ‰‹ã€‚è¿™è¡¨æ˜ AlphaGo Zero å¯èƒ½æ­£åœ¨å­¦ä¹ ä¸€ç§ä¸äººç±»ä¸‹æ£‹ä¸åŒçš„ç­–ç•¥ã€‚</p><p><img src="/images/ag0em.png"></p><p>â€‹ <em>å›¾3 AlphaGo Zeroçš„å®éªŒè¯„ä¼°</em></p><p>â€‹ ä¸ºäº†åˆ†ç¦»æ¶æ„å’Œç®—æ³•çš„è´¡çŒ®ï¼Œæˆ‘ä»¬å°† AlphaGo Zero ä¸­çš„ç¥ç»ç½‘ç»œæ¶æ„çš„æ€§èƒ½ä¸ AlphaGo Lee ä¸­ä½¿ç”¨çš„ä»¥å‰çš„ç¥ç»ç½‘ç»œæ¶æ„è¿›è¡Œäº†æ¯”è¾ƒï¼ˆè§å›¾ 4ï¼‰ã€‚ æ–°è®­ç»ƒçš„AlphaGo Zero æœ‰å››ä¸ªç‰ˆæœ¬çš„ç¥ç»ç½‘ç»œï¼Œåˆ†åˆ«æ˜¯ï¼šä½¿ç”¨ AlphaGo Lee çš„å·ç§¯ç½‘ç»œæ¶æ„ï¼›AlphaGo Zero çš„å‰©ä½™ç½‘ç»œæ¶æ„ï¼›ä½¿ç”¨ AlphaGo Zero çš„å·ç§¯ç½‘ç»œæ¶æ„ï¼›ä½¿ç”¨AlphaGo Lee çš„å‰©ä½™ç½‘ç»œæ¶æ„ã€‚æ¯ä¸ªç½‘ç»œéƒ½ç»è¿‡è®­ç»ƒï¼Œä»¥æœ€å°åŒ–ç›¸åŒçš„æŸå¤±å‡½æ•°ï¼Œä½¿ç”¨ç”± AlphaGo Zero åœ¨è‡ªæˆ‘è®­ç»ƒ 72 å°æ—¶åäº§ç”Ÿçš„å›ºå®šæ•°æ®é›†ã€‚ä½¿ç”¨å‰©ä½™ç½‘ç»œæ›´å‡†ç¡®ï¼Œå®ç°äº†æ›´ä½çš„è¯¯å·®ï¼ŒAlphaGo çš„æ€§èƒ½æé«˜äº† 600 å¤š Eloã€‚å°†ç­–ç•¥å’Œä»·å€¼ç»„åˆåœ¨ä¸€èµ·æˆä¸ºä¸€ä¸ªç½‘ç»œï¼Œç•¥å¾®é™ä½äº†ç§»åŠ¨é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œä½†æ˜¯å°†é™ä½äº† AlphaGo çš„ä»·å€¼è¯¯å·®å’Œæé«˜äº†åšå¼ˆæ€§èƒ½çº¦ 600 ä¸ª Eloã€‚éƒ¨åˆ†åŸå› åœ¨äºæé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œä½†æ›´é‡è¦çš„æ˜¯ï¼ŒåŒé‡ç›®æ ‡å°†ç½‘ç»œæ­£åˆ™åŒ–ä¸ºæ”¯æŒå¤šç§ç”¨ä¾‹çš„è¡¨ç¤ºã€‚</p><p><img src="/images/ag02.png"></p><p>â€‹ <em>å›¾4 AlphaGo Zeroå’ŒAlphaGo Leeçš„ç¥ç»ç½‘ç»œç»“æ„æ¯”è¾ƒ</em></p><h4><span id="5-alphago-zero-å­¦ä¹ åˆ°çš„å›´æ£‹çŸ¥è¯†">5. AlphaGo Zero å­¦ä¹ åˆ°çš„å›´æ£‹çŸ¥è¯†</span></h4><p>â€‹ AlphaGoZeroåœ¨å…¶è‡ªæˆ‘åšå¼ˆè®­ç»ƒè¿‡ç¨‹ä¸­å‘ç°äº†éå‡¡çš„å›´æ£‹çŸ¥è¯†æ°´å¹³ã€‚è¿™ä¸ä»…åŒ…æ‹¬äººç±»å›´æ£‹çŸ¥è¯†çš„åŸºæœ¬è¦ç´ ï¼Œè¿˜åŒ…æ‹¬è¶…å‡ºä¼ ç»Ÿå›´æ£‹çŸ¥è¯†èŒƒå›´çš„éæ ‡å‡†ç­–ç•¥ã€‚</p><p>â€‹ å›¾ 5æ˜¾ç¤ºäº†ä¸€ä¸ªæ—¶é—´çº¿ï¼Œè¡¨æ˜ä½•æ—¶å‘ç°äº†ä¸“ä¸š josekiï¼ˆè§’ç‚¹åºåˆ—ï¼‰;æœ€ç»ˆAlphaGo Zero æ›´å–œæ¬¢å…ˆå‰æœªçŸ¥çš„æ–°çš„ joseki å˜ä½“ï¼ˆå›¾5b ï¼‰ã€‚å›¾5c æ˜¾ç¤ºäº†å‡ ç§åœ¨ä¸åŒè®­ç»ƒé˜¶æ®µè¿›è¡Œçš„å¿«é€Ÿè‡ªæˆ‘åšå¼ˆã€‚åœ¨æ•´ä¸ªè®­ç»ƒä¸­å®šæœŸè¿›è¡Œçš„é”¦æ ‡èµ›é•¿åº¦æ¯”èµ›åœ¨è¡¥å……ä¿¡æ¯ä¸­æ˜¾ç¤ºã€‚ AlphaGo Zero ä»å®Œå…¨éšæœºçš„ç§»åŠ¨è¿‡æ¸¡åˆ°å¯¹å›´æ£‹æ¦‚å¿µçš„å¤æ‚ç†è§£ï¼ŒåŒ…æ‹¬fusekiï¼ˆå¼€åœºï¼‰ï¼Œtesujiï¼ˆæˆ˜æœ¯ï¼‰ï¼Œç”Ÿä¸æ­»ï¼Œkoï¼ˆé‡å¤æ£‹å±€ï¼‰ï¼Œyoseï¼ˆç»ˆå±€ï¼‰ï¼Œæ•æ‰æ¯”èµ›ï¼Œsenteï¼ˆå€¡è®®ï¼‰ï¼Œå½¢çŠ¶ï¼Œå½±å“åŠ›å’Œé¢†åœŸï¼Œéƒ½æ˜¯ä»æœ€åˆçš„åŸåˆ™å‘ç°çš„ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒShocho - äººç±»å­¦ä¹ çš„å›´æ£‹çŸ¥è¯†çš„ç¬¬ä¸€è¦ç´ ä¹‹ä¸€ - åªæœ‰åœ¨ AlphaGo Zero çš„è®­ç»ƒä¸­æ‰èƒ½è¢«ç†è§£ã€‚</p><p><img src="/images/ag05.png"></p><p>â€‹ <em>å›¾5 AlphaGo Zeroå­¦åˆ°çš„å›´æ£‹çŸ¥è¯†</em></p><h4><span id="6-alphago-zero-çš„æœ€ç»ˆæ°´å¹³">6. AlphaGo Zero çš„æœ€ç»ˆæ°´å¹³</span></h4><p>â€‹ éšåæˆ‘ä»¬ä½¿ç”¨æ›´å¤§çš„ç¥ç»ç½‘ç»œå’Œæ›´é•¿çš„æŒç»­æ—¶é—´å°†æˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ ç®¡é“åº”ç”¨äºAlphaGo Zeroçš„ç¬¬äºŒä¸ªå®ä¾‹ã€‚å†æ¬¡è®­ç»ƒä»å®Œå…¨éšæœºè¡Œä¸ºå¼€å§‹å¹¶æŒç»­å¤§çº¦40å¤©ã€‚</p><p>â€‹ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œäº§ç”Ÿäº† 2900 ä¸‡æ¬¡è‡ªæˆ‘åšå¼ˆã€‚å‚æ•°ä»æ¯ä¸ª 2,048 ä¸ªä½ç½®çš„ 310 ä¸‡ä¸ªå°å‹æ‰¹é‡ä¸­æ›´æ–°ã€‚ç¥ç»ç½‘ç»œåŒ…å« 40 ä¸ªæ®‹ä½™å—ã€‚å­¦ä¹ æ›²çº¿å¦‚å›¾ 6a æ‰€ç¤ºã€‚åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸè¿›è¡Œçš„æ¯”èµ›æ˜¾ç¤ºåœ¨è¡¥å……ä¿¡æ¯ä¸­ã€‚</p><p><img src="/images/ag06.png"></p><p>â€‹ <em>å›¾6 AlphaGo Zeroçš„è¯„ä¼°</em></p><p>â€‹ æˆ‘ä»¬ä½¿ç”¨ AlphaGo Fanï¼ŒAlphaGo Lee å’Œä¹‹å‰çš„å‡ ä¸ªå›´æ£‹ç¨‹åºçš„å†…éƒ¨æ¯”èµ›è¯„ä¼°äº†è®­ç»ƒæœ‰ç´ çš„ AlphaGo Zeroã€‚æˆ‘ä»¬è¿˜ä¸æœ€å¼ºå¤§çš„ç°æœ‰ç¨‹åº AlphaGo Master è¿›è¡Œäº†æ¸¸æˆï¼Œè¯¥ç¨‹åºåŸºäºæœ¬æ–‡æä¾›çš„ç®—æ³•å’Œä½“ç³»ç»“æ„ï¼Œä½†ä½¿ç”¨äº†äººç±»æ•°æ®å’Œç‰¹å¾ï¼ˆè¯·å‚é˜…æ–¹æ³•ï¼‰ - å®ƒåœ¨ 60-0 åœ¨çº¿æ¸¸æˆä¸­å‡»è´¥äº†æœ€å¼ºçš„äººç±»èŒä¸šç©å®¶ã€‚åœ¨æˆ‘ä»¬çš„è¯„ä¼°ä¸­ï¼Œæ‰€æœ‰ç¨‹åºéƒ½å…è®¸æ¯ä¸ªåŠ¨ä½œæœ‰5ç§’çš„æ€è€ƒæ—¶é—´; AlphaGo Zero å’Œ AlphaGo Master æ¯å°åœ¨å¸¦æœ‰ 4 ä¸ª TPU çš„å•å°æœºå™¨ä¸Šåšå¼ˆ; AlphaGo Fan å’Œ AlphaGo Lee åˆ†åˆ«åˆ†å¸ƒæœ‰ 176 ä¸ªGPUå’Œ 48 ä¸ªTPUã€‚æˆ‘ä»¬è¿˜åŒ…æ‹¬ä¸€ä¸ªå®Œå…¨åŸºäº AlphaGo Zero åŸå§‹ç¥ç»ç½‘ç»œçš„é€‰æ‰‹; è¯¥é€‰æ‰‹åªæ˜¯ä»¥æœ€å¤§çš„æ¦‚ç‡é€‰æ‹©ç§»åŠ¨ï¼ˆä¸è¿›è¡Œ MCTS æœç´¢ï¼‰ã€‚</p><p>â€‹ å›¾ 6bæ˜¾ç¤ºäº†æ¯ä¸ªç¨‹åºåœ¨Eloè§„æ¨¡ä¸Šçš„è¡¨ç°ã€‚æœªä½¿ç”¨ä»»ä½•é¢„æµ‹çš„åŸå§‹ç¥ç»ç½‘ç»œå®ç°äº†3,055çš„Eloè¯„çº§ã€‚ AlphaGo Zero è·å¾—äº†5,185çš„è¯„åˆ†ï¼Œè€Œ AlphaGo Master çš„4,858ï¼ŒAlphaGo Lee çš„ 3,739å’Œ AlphaGo Fan çš„3,144ã€‚</p><p>â€‹ æœ€åï¼Œæˆ‘ä»¬è¯„ä¼°äº† AlphaGo Zero å¯¹é˜µ AlphaGo Masterï¼Œåœ¨æ¯åœº2å°æ—¶çš„æ—¶é—´é™å®šå†…è¿›è¡Œäº†100åœºæ¯”èµ›ï¼ŒAlphaGo Zero èµ¢å¾—äº†å…¶ä¸­çš„89åœºã€‚</p><h4><span id="7-ç»“è®º">7. ç»“è®º</span></h4><p>â€‹ æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå…¨é¢è¯æ˜ï¼Œå³ä½¿åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„é¢†åŸŸä¸­ï¼Œçº¯ç²¹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¹Ÿæ˜¯å®Œå…¨å¯è¡Œçš„ï¼šåœ¨ä¸è¶…å‡ºåŸºæœ¬è§„åˆ™çš„æƒ…å†µä¸‹ï¼Œæ²¡æœ‰å…³äºé¢†åŸŸçš„çŸ¥è¯†ï¼Œå°±å¯ä»¥è®­ç»ƒåˆ°è¶…äººçš„æ°´å¹³ï¼Œæ²¡æœ‰äººç±»çš„ä¾‹å­æˆ–æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œä¸ç”¨äººç±»ä¸“å®¶æ•°æ®è®­ç»ƒçš„ç¨‹åºç›¸æ¯”ï¼Œçº¯ç²¹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åªéœ€è¦å‡ ä¸ªå°æ—¶çš„è®­ç»ƒæ—¶é—´å°±èƒ½è¾¾åˆ°æ›´å¥½çš„æ€§èƒ½ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼ŒAlphaGo Zero å¤§å¹…åº¦å‡»è´¥äº†ä½¿ç”¨äººå·¥æ•°æ®è®­ç»ƒçš„ AlphaGo æœ€å¼ºå¤§çš„å…ˆå‰ç‰ˆæœ¬ã€‚</p><p>â€‹ äººç±»å·²ç»ç§¯ç´¯äº†å‡ åƒå¹´æ¥çš„å›´æ£‹çŸ¥è¯†ï¼Œå‘å±•æˆå›ºå®šçš„æ¨¡å¼ï¼Œæ€»ç»“æˆè°šè¯­å’Œä¹¦ç±ã€‚åœ¨å‡ å¤©çš„æ—¶é—´é‡Œï¼ŒAlphaGo Zero ä»é›¶å­¦èµ·ï¼Œå°±èƒ½å¤Ÿé‡æ–°å‘ç°è®¸å¤šçš„å›´æ£‹çŸ¥è¯†ï¼Œå¹¶èƒ½ä¸ºè¿™ä¸ªå¤è€çš„æ¸¸æˆæä¾›æ–°è§è§£ã€æ–°ç­–ç•¥ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;å‰è¨€&quot;&gt;1. å‰è¨€&lt;/h4&gt;
&lt;p&gt;â€‹ äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé•¿æœŸç›®æ ‡æ˜¯åœ¨ä¸€äº›æœ‰æŒ‘æˆ˜çš„é¢†åŸŸä¸­ä»é›¶å¼€å§‹å­¦ä¹ å‡ºè¶…äººç†Ÿç»ƒç¨‹åº¦çš„ç®—æ³•ã€‚æœ€è¿‘ï¼ŒAlphaGoæˆä¸ºç¬¬ä¸€ä¸ªåœ¨å›´æ£‹æ¯”èµ›ä¸­å‡»è´¥ä¸–ç•Œå† å†›çš„ç¨‹åºã€‚ AlphaGoä¸­çš„æ ‘æœç´¢ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¯„ä¼°ä½ç½®å’Œé€‰å®šçš„ç§»åŠ¨ã€‚è¿™äº›ç¥ç»ç½‘ç»œæ˜¯é€šè¿‡ç›‘ç£å­¦ä¹ æ¥è‡ªäººç±»ä¸“å®¶çš„èµ°æ³•ä»¥åŠé€šè¿‡å¼ºåŒ–è‡ªæˆ‘å­¦ä¹ æ¥è¿›è¡Œè®­ç»ƒçš„ã€‚è¿™é‡Œæˆ‘ä»¬åªä»‹ç»ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç®—æ³•ï¼Œæ²¡æœ‰è¶…å‡ºæ¸¸æˆè§„åˆ™çš„äººç±»æ•°æ®ï¼ŒæŒ‡å¯¼æˆ–é¢†åŸŸçŸ¥è¯†ã€‚AlphaGoæˆä¸ºè‡ªå·±çš„è€å¸ˆï¼šä¸€ä¸ªç¥ç»ç½‘ç»œè®­ç»ƒé¢„æµ‹AlphaGoçš„ç§»åŠ¨é€‰æ‹©å’Œæ¸¸æˆçš„èƒœè€…ã€‚è¿™ä¸ªç¥ç»ç½‘ç»œæé«˜äº†æ ‘æœç´¢çš„å¼ºåº¦ï¼Œåœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­æ‹¥æœ‰æ›´é«˜è´¨é‡çš„ç§»åŠ¨é€‰æ‹©å’Œæ›´å¼ºçš„è‡ªæˆ‘å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–°ç¨‹åºAlphaGo Zeroä»é›¶å¼€å§‹å­¦ä¹ ï¼Œå®ç°äº†è¶…äººçš„è¡¨ç°ï¼Œä¸ä¹‹å‰å‘å¸ƒçš„å¤ºå† å† å†›AlphaGoç›¸æ¯”ä»¥100-0å–èƒœã€‚&lt;/p&gt;
    
    </summary>
    
      <category term="å­¦ä¹ ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="AlphaGo" scheme="http://www.52coding.com.cn/tags/AlphaGo/"/>
    
      <category term="AlphaZero" scheme="http://www.52coding.com.cn/tags/AlphaZero/"/>
    
      <category term="å¢å¼ºå­¦ä¹ " scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>RL - Integrating Learning and Planning</title>
    <link href="http://www.52coding.com.cn/2018/01/09/RL%20-%20Integrating%20Learning%20and%20Planning/"/>
    <id>http://www.52coding.com.cn/2018/01/09/RL - Integrating Learning and Planning/</id>
    <published>2018-01-09T13:11:09.000Z</published>
    <updated>2018-11-06T03:47:24.502Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="introduction">Introduction</span></h2><p>In last lecture, we learn <strong>policy</strong> directly from experience. In previous lectures, we learn <strong>value function</strong> directly from experience. In this lecture, we will learn <strong>model</strong> directly from experience and use <strong>planning</strong> to construct a value function or policy. Integrate learning and planning into a single architecture.</p><p>Model-Based RL</p><ul><li>Learn a model from experience</li><li><strong>Plan</strong> value function (and/or policy) from model</li></ul><a id="more"></a><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#model-based-reinforcement-learning">Model-Based Reinforcement Learning</a></li><li><a href="#integrated-architectures">Integrated Architectures</a></li><li><a href="#simulation-based-search">Simulation-Based Search</a></li></ul><!-- tocstop --><h2><span id="model-based-reinforcement-learning">Model-Based Reinforcement Learning</span></h2><p><img src="/images/mbrl.png"></p><p>Advantages of Model-Based RL</p><ul><li>Can efficiently learn model by supervised learning methods</li><li>Can reason about model uncertainty</li></ul><p>Disadvantages</p><ul><li>First learn a model, then construct a value function -&gt; two source of approximation error</li></ul><p><strong>What is a Model?</strong></p><p>A model <span class="math inline">\(\mathcal{M}\)</span> is a representation of an MDP <span class="math inline">\(&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}&gt;\)</span> parametrized by <span class="math inline">\(\eta\)</span>.</p><p>We will assume state space <span class="math inline">\(\mathcal{S}\)</span> and action space <span class="math inline">\(\mathcal{A}\)</span> are known. So a model <span class="math inline">\(\mathcal{M}=&lt;\mathcal{P}_, \eta\mathcal{R}_\eta&gt;\)</span> represents state transitions <span class="math inline">\(\mathcal{P}_\eta \approx \mathcal{P}\)</span> and rewards <span class="math inline">\(\mathcal{R}_\eta\approx \mathcal{R}\)</span>. <span class="math display">\[S_{t+1}\sim\mathcal{P}_\eta(S_{t+1}|S_t, A_t)\\R_{t+1}=\mathcal{R}_\eta(R_{t+1}|S_t, A_t)\]</span> Typically assume conditional independence between state transitions and rewards.</p><p>Goal: estimate model <span class="math inline">\(\mathcal{M}_\eta\)</span> from experience <span class="math inline">\(\{S_1, A_1, R_2, â€¦, S_T\}\)</span>.</p><p>This is a supervised learning problem: <span class="math display">\[S_1, A_1 \rightarrow R_2, S_2 \\S_2, A_2 \rightarrow R_3, S_3 \\...\\S_{T-1}, A_{T-1} \rightarrow R_T, S_T \\\]</span> Learning <span class="math inline">\(s, a\rightarrow r\)</span> is a <em>regression</em> problem; learning <span class="math inline">\(s, a\rightarrow s&#39;\)</span> is a <em>density</em> estimation problem. Pick loss function, e.g. mean-squared error, KL divergence, â€¦ Find parameters <span class="math inline">\(\eta\)</span> that minimise empirical loss.</p><p>Examples of Models</p><ul><li>Table Lookup Model</li><li>Linear Expectation Model</li><li>Linear Gaussian Model</li><li>Gaussian Process Model</li><li>Deep Belief Network Model</li></ul><p><strong>Table Lookup Model</strong></p><p>Model is an explicit MDP. Count visits <span class="math inline">\(N(s, a)\)</span> to each state action pair: <span class="math display">\[\hat{\mathcal{P}}^a_{s,s&#39;}=\frac{1}{N(s,a)}\sum^T_{t=1}1(S_t,A_t,S_{t+1}=s, a, s&#39;)\\\hat{\mathcal{R}}^a_{s,s&#39;}=\frac{1}{N(s,a)}\sum^T_{t=1}1(S_t,A_t=s, a)R_t\]</span> Alternatively, at each time-step <span class="math inline">\(t\)</span>, record experience tuple <span class="math inline">\(&lt;S_t, A_t, R_{t+1}, S_{t+1}&gt;\)</span>. To sample model, randomly pick tuple matching <span class="math inline">\(&lt;s, a, \cdot, \cdot&gt;\)</span>.</p><p><strong>AB Example</strong></p><p><img src="/images/ab2.png"></p><p>We have contrusted a <strong>table lookup model</strong> from the experience. Next step, we will planning with a model.</p><p><strong>Planning with a model</strong></p><p>Given a model <span class="math inline">\(\mathcal{M}_\eta=&lt;\mathcal{P}_\eta, \mathcal{R}_\eta&gt;\)</span>, solve the MDP <span class="math inline">\(&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}_\eta, \mathcal{R}_\eta&gt;\)</span> using favorite planning algorithms</p><ul><li>Value iteration</li><li>Policy iteration</li><li>Tree search</li><li>â€¦.</li></ul><p><strong>Sample-Based Planning</strong></p><p>A simple but powerful approach to planning is to use the model <strong>only</strong> to generate samples.</p><p><strong>Sample</strong> experience from model <span class="math display">\[S_{t+1}\sim\mathcal{P}_\eta(S_{t+1}|S_t,A_t)\\R_{t+1}=\mathcal{R}_\eta(R_{t+1}|S_t,A_t)\]</span> Apply <strong>model-free</strong> RL to samples, e.g.:</p><ul><li>Monte-Carlo control</li><li>Sarsa</li><li>Q-learning</li></ul><p>Sample-based planning methods are often more efficient.</p><p><strong>Back to AB Example</strong></p><p><img src="/images/ab3.png"></p><p>We can use our model to sample more experience and apply model-free RL to them.</p><p><strong>Planning with an Inaccurate Model</strong></p><p>Given an imperfect model <span class="math inline">\(&lt;\mathcal{P}_\eta, \mathcal{R}_\eta&gt; â‰  &lt;\mathcal{P}, \mathcal{R}&gt;\)</span>. Performance of model-based RL is limited to optimal policy for approximate MDP <span class="math inline">\(&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}_\eta, \mathcal{R}_\eta&gt;\)</span> i.e. Model-based RL is only as good as the estimated model.</p><p>When the model is inaccurate, planning process will compute a suboptimal policy.</p><ul><li>Solution1: when model is wrong, use model-free RL</li><li>Solution2: reason explicitly about model uncertainty</li></ul><h2><span id="integrated-architectures">Integrated Architectures</span></h2><p>We consider two sources of experience:</p><ul><li><p>Real experience: Sampled from environment (true MDP) <span class="math display">\[S&#39;\sim \mathcal{P}^a_{s,s&#39;}\\R=\mathcal{R}^a_s\]</span></p></li><li><p>Simulated experience: Sampled from model (approximate MDP) <span class="math display">\[S&#39;\sim \mathcal{P}_\eta(S&#39;|S, A)\\R=\mathcal{R}_\eta(R|S, A)\]</span></p></li></ul><p><strong>Integrating Learning and Planning</strong></p><p>Dyna Architecture</p><ul><li>Learn a model from real experience</li><li>Learn and plan value function (and/or policy) from real and simulated experience</li></ul><p><img src="/images/dyna.png"></p><p>The simplest dyna algorithm is <em>Dyna-Q Algorithm</em>:</p><p><img src="/images/dynaq.png"></p><p><img src="/images/dynaqres.png"></p><p>From the experiments, we can see that using planning is more efficient than direct RL only.</p><p><strong>Dyna-Q with an Inaccurate Model</strong></p><p>The changed envrionment is <strong>harder</strong>:</p><p><img src="/images/dynaqhard.png"></p><p>There is a <strong>easier</strong> change:</p><p><img src="/images/dynaqeasy.png"></p><h2><span id="simulation-based-search">Simulation-Based Search</span></h2><p>Let's back to planning problems. Simulation-based search is another approach to solve MDP.</p><p><strong>Forward Search</strong></p><p>Forward search algorithms select the best action by <strong>lookahead</strong>. They build a <strong>search tree</strong> with the current state <span class="math inline">\(s_t\)</span> at the root using a model of the MDP to look ahead.</p><p><img src="/images/ftree.png"></p><p>We don't need to solve the whole MDP, just sub-MDP starting from <strong>now</strong>.</p><p><strong>Simulation-Based Search</strong></p><p>Simulation-based search is forward search paradigm using sample-based planning. Simulate episodes of experience from <strong>now</strong> with the model. Apply <strong>model-free</strong> RL to simulated episodes.</p><p><img src="/images/sbsearch.png"></p><p>Simulate episodes of experience from <strong>now</strong> with the model: <span class="math display">\[\{s_t^k, A^k_t,R^k_{t+1}, ..., S^k_T\}^K_{k=1}\sim\mathcal{M}_v\]</span> Apply <strong>model-free</strong> RL to simulated episodes</p><ul><li>Monte-Carlo control <span class="math inline">\(\rightarrow\)</span> Monte-Carlo search</li><li>Sarsa <span class="math inline">\(\rightarrow\)</span> TD search</li></ul><p><strong>Simple Monte-Carlo Search</strong></p><p>Given a model <span class="math inline">\(\mathcal{M}_v\)</span> and a simulation policy <span class="math inline">\(\pi\)</span>.</p><p>For each action <span class="math inline">\(a\in\mathcal{A}\)</span></p><ul><li><p>Simulate <span class="math inline">\(K\)</span> episodes from current (real) state <span class="math inline">\(s_t\)</span> <span class="math display">\[\{s_t, a, R^k_{t+1},S^k_{t+1},A^k_{t+1}, ..., S^k_T \}^K_{k=1}\sim \mathcal{M}_v, \pi\]</span></p></li><li><p>Evaluate actions by mean return (<strong>Monte-Carlo evaluation</strong>) <span class="math display">\[Q(s_t, a)=\frac{1}{K}\sum^k_{k=1}G_t\rightarrow q_\pi(s_t, a)\]</span></p></li></ul><p>Select current (real) action with maximum value <span class="math display">\[a_t=\arg\max_{a\in\mathcal{A}}Q(s_t, a)\]</span> <strong>Monte-Carlo Tree Search</strong></p><p>Given a model <span class="math inline">\(\mathcal{M}_v\)</span>. Simulate <span class="math inline">\(K\)</span> episodes from current state <span class="math inline">\(s_t\)</span> using current simulation policy <span class="math inline">\(\pi\)</span>. <span class="math display">\[\{s_t, A_t^k, R^k_{t+1},S^k_{t+1},A^k_{t+1}, ..., S^k_T \}^K_{k=1}\sim \mathcal{M}_v, \pi\]</span> Build a search tree containing visited states and actions. <strong>Evaluate</strong> states <span class="math inline">\(Q(s, a)\)</span> by mean return of episodes from <span class="math inline">\(s, a\)</span>: <span class="math display">\[Q(s, a)=\frac{1}{N(s,a)}\sum^K_{k=1}\sum^T_{u=t}1(S_u, A_u=s,a)G_u \to q_\pi(s,a)\]</span> After search is finished, select current (real) action with maximum value in search tree: <span class="math display">\[a_t=\arg\max_{a\in\mathcal{A}}Q(s_t, a)\]</span> In MCTS, the simulation policy <span class="math inline">\(\pi\)</span> <strong>improves</strong>.</p><p>Each simulation consists of two phases (in-tree, out-of-tree)</p><ul><li><strong>Tree policy</strong> (improves): pick actions to maximise <span class="math inline">\(Q(S,A)\)</span></li><li><strong>Default policy</strong> (fixed): pick actions randomly</li></ul><p>Repeat (each simulation)</p><ul><li><span class="math inline">\(\color{red}{\mbox{Evaluate}}\)</span> states <span class="math inline">\(Q(S,A)\)</span> by Monte-Carlo evaluation</li><li><span class="math inline">\(\color{red}{\mbox{Improve}}\)</span> tree policy, e.g. by <span class="math inline">\(\epsilon\)</span>-greedy(Q)</li></ul><p>MCTS is <strong>Monte-Carlo control</strong> applied to <strong>simulated experience</strong>.</p><p>Converges on the optimal search tree, <span class="math inline">\(Q(S, A) \to q_*(S, A)\)</span>.</p><p><strong>Case Study: the Game of Go</strong></p><p><img src="/images/go_2.png"></p><p><em>Rules of Go</em></p><ul><li>Usually played on 19$<span class="math inline">\(19, also 13\)</span><span class="math inline">\(13 or 9\)</span>$9 board</li><li>Black and white place down stones alternately</li><li>Surrounded stones are captured and removed</li><li>The player with more territory wins the game</li></ul><p><img src="/images/ruleofgo.png"></p><p><em>Position Evaluation in Go</em></p><p>The key problem is how good is a position <span class="math inline">\(s\)</span>?</p><p>So the reward function is if Black wins, the reward of the final position is 1, otherwise 0: <span class="math display">\[R_t = 0 \mbox{ for all non-terminal steps } t&lt;T\\R_T=\begin{cases} 1,  &amp; \mbox{if }\mbox{ Black wins} \\0, &amp; \mbox{if }\mbox{ White wins}\end{cases}\]</span> Policy <span class="math inline">\(\pi=&lt;\pi_B,\pi_W&gt;\)</span> selects moves for both players.</p><p>Value function (how good is position <span class="math inline">\(s\)</span>): <span class="math display">\[v_\pi(s)=\mathbb{E}_\pi[R_T|S=s]=\mathbb{P}[Black wins|S=s]\\v_*(s)=\max_{\pi_B}\min_{\pi_w}v_\pi(s)\]</span> <em>Monte Carlo Evaluation in Go</em></p><p><img src="/images/mcego.png"></p><p><img src="/images/amcts1.png"></p><p><img src="/images/amcts2.png"></p><p><img src="/images/amcts3.png"></p><p><img src="/images/amcts4.png"></p><p><img src="/images/amcts5.png"></p><p>So, MCTS will expand the tree towards the node that is most promising and ignore the useless parts.</p><p><strong>Advantages of MC Tree Search</strong></p><ul><li>Highly selective best-first search</li><li>Evaluates states <em>dynamically</em></li><li>Uses sampling to break curse of dimensionality</li><li>Works for &quot;black-box&quot; models (only requires samples)</li><li>Computationally efficient, anytime, parallelisable</li></ul><p><strong>Temporal-Difference Search</strong></p><ul><li>Simulation-based search</li><li>Using TD instead of MC (bootstrapping)</li><li>MC tree search applies MC control to sub-MDP from now</li><li>TD search applies Sarsa to sub-MDP from now</li></ul><p><strong>MC vs. TD search</strong></p><p>For model-free reinforcement learning, bootstrapping is helpful</p><ul><li>TD learning reduces variance but increase bias</li><li>TD learning is usually more efficient than MC</li><li>TD(<span class="math inline">\(\lambda\)</span>) can be much more efficient than MC</li></ul><p>For simulation-based search, bootstrapping is also helpful</p><ul><li>TD search reduces variance but increase bias</li><li>TD search is usually more efficient than MC search</li><li>TD(<span class="math inline">\(\lambda\)</span>) search can be much more efficient than MC search</li></ul><p><strong>TD Search</strong></p><p>Simulate episodes from the current (real) state <span class="math inline">\(s_t\)</span>. Estimate action-value function <span class="math inline">\(Q(s, a)\)</span>. For each step of simulation, update action-values by Sarsa: <span class="math display">\[\triangle Q(S,A)=\alpha (R+\gamma Q(S&#39;,A&#39;)-Q(S,A))\]</span> Select actions based on action-value <span class="math inline">\(Q(s,a)\)</span>, e.g. <span class="math inline">\(\epsilon\)</span>-greedy. May also use function approximation for <span class="math inline">\(Q\)</span>.</p><p><strong>Dyna-2</strong></p><p>In Dyna-2, the agent stores two sets of feature weights:</p><ul><li><strong>Long-term</strong> memory</li><li><strong>Short-term</strong> (working) memory</li></ul><p>Long-term memory is updated from <strong>real experience</strong> using TD learning</p><ul><li>General domain knowledge that applies to any episode</li></ul><p>Short-term memory is updated from <strong>simulated experience</strong> using TD search</p><ul><li>Specific local knowledge about the current situation</li></ul><p>Over value function is sum of long and short-term memories.</p><p>End.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In last lecture, we learn &lt;strong&gt;policy&lt;/strong&gt; directly from experience. In previous lectures, we learn &lt;strong&gt;value function&lt;/strong&gt; directly from experience. In this lecture, we will learn &lt;strong&gt;model&lt;/strong&gt; directly from experience and use &lt;strong&gt;planning&lt;/strong&gt; to construct a value function or policy. Integrate learning and planning into a single architecture.&lt;/p&gt;
&lt;p&gt;Model-Based RL&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn a model from experience&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt; value function (and/or policy) from model&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="å­¦ä¹ ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="AlphaGo" scheme="http://www.52coding.com.cn/tags/AlphaGo/"/>
    
      <category term="å¢å¼ºå­¦ä¹ " scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="MCTS" scheme="http://www.52coding.com.cn/tags/MCTS/"/>
    
      <category term="TD Search" scheme="http://www.52coding.com.cn/tags/TD-Search/"/>
    
      <category term="Dyna" scheme="http://www.52coding.com.cn/tags/Dyna/"/>
    
  </entry>
  
  <entry>
    <title>RL - Policy Gradient</title>
    <link href="http://www.52coding.com.cn/2018/01/06/RL%20-%20Policy%20Gradient/"/>
    <id>http://www.52coding.com.cn/2018/01/06/RL - Policy Gradient/</id>
    <published>2018-01-06T05:42:09.000Z</published>
    <updated>2018-11-06T04:48:57.673Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="introduction">Introduction</span></h2><p>This lecture talks about methods that optimise policy directly. Instead of working with value function as we consider so far, we seek experience and use the experience to update our policy in the direction that makes it better.</p><p>In the last lecture, we approximated the value or action-value function using parameters <span class="math inline">\(\theta\)</span>, <span class="math display">\[V_\theta(s)\approx V^\pi(s)\\Q_\theta(s, a)\approx Q^\pi(s, a)\]</span> A policy was generated directly from the value function using <span class="math inline">\(\epsilon\)</span>-greedy.</p><p>In this lecture we will directly parametrise the policy <span class="math display">\[\pi_\theta(s, a)=\mathbb{P}[a|s, \theta]\]</span> We will focus again on <span class="math inline">\(\color{red}{\mbox{model-free}}\)</span> reinforcement learning.</p><a id="more"></a><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#finite-difference-policy-gradient">Finite Difference Policy Gradient</a></li><li><a href="#monte-carlo-policy-gradient">Monte-Carlo Policy Gradient</a></li><li><a href="#actor-critic-policy-gradient">Actor-Critic Policy Gradient</a></li><li><a href="#summary-of-policy-gradient-algorithms">Summary of Policy Gradient Algorithms</a></li></ul><!-- tocstop --><p><strong>Value-Based and Policy-Based RL</strong></p><ul><li>Value Based<ul><li>Learnt Value Function</li><li>Implicit policy (e.g. <span class="math inline">\(\epsilon\)</span>-greedy)</li></ul></li><li>Policy Based<ul><li>No Value Function</li><li>Learnt Policy</li></ul></li><li>Actor-Critic<ul><li>Learnt Value Function</li><li>Learnt Policy</li></ul></li></ul><p><img src="/images/vfp.png"></p><p><strong>Advantages of Policy-Based RL</strong></p><p>Advantages:</p><ul><li>Better convergence properties</li><li>Effective in high-dimensional or contimuous action spaces (<em>without computing max</em>)</li><li>Can learn stochastic policies</li></ul><p>Disadvantages:</p><ul><li>Typically converge to a local rather than global optimum</li><li>Evaluating a policy is typically inefficient and high variance</li></ul><p>Deterministic policy or taking max is not also the best. Take the rock-paper-scissors game for example.</p><p><img src="/images/rps.png"></p><p>Consider policies <em>iterated</em> rock-paper-scissors</p><ul><li>A deterministic policy is easily exploited</li><li>A uniform random policy is optimal (according to Nash equilibrium)</li></ul><p><strong>Aliased Gridworld Example</strong></p><p><img src="/images/agw.png"></p><p>The agent cannot differentiate the grey states.</p><p>Consider features of the following form (for all N, E, S, W) <span class="math display">\[\phi(s, a)=1(\mbox{wall to N, a = move E})\]</span> Compare value-based RL, using an approximate value function <span class="math display">\[Q_\theta(s, a)=f(\phi(s, a), \theta)\]</span> To policy-based RL, using a parametrised policy <span class="math display">\[\pi_\theta(s, a)=g(\phi(s, a), \theta)\]</span> Since the agent cannot differentiate the grey states given the feature, if you take a <strong>deterministic</strong> policy, you must pick the same action at two grey states.</p><p><img src="/images/deagw.png"></p><p>Under aliasing, an optimal <span class="math inline">\(\color{red}{\mbox{deterministic}}\)</span> policy will either</p><ul><li>move W in both grey states (as shown by red arrows)</li><li>move E in both grey states</li></ul><p>Either way, it can get stuck and never reach the money.</p><p>Value-based RL learns a near-deterministic policy, so it will traverse the corridor for a long time.</p><p><img src="/images/ranagw.png"></p><p>An optimal <span class="math inline">\(\color{red}{\mbox{stochastic}}\)</span> policy will randomly move E or W in grey states: <span class="math display">\[\pi_\theta(\mbox{wall to N and S, move E}) = 0.5\\\pi_\theta(\mbox{wall to N and S, move W}) = 0.5\\\]</span> It will reach the goal state in a few steps with high probability. Policy-based RL can learn the optimal stochastic policy.</p><p>These examples show that a stochastic policy can be better than the deterministic policy, especially in the case that the MDP is <strong>partialy observed</strong> or cannot fully represent the state.</p><p><strong>Policy Objective Functions</strong></p><p>Goal: given policy <span class="math inline">\(\pi_\theta(s, a)\)</span> with parameters <span class="math inline">\(\theta\)</span>, find best <span class="math inline">\(\theta\)</span>. But how do we measure the quality of a policy <span class="math inline">\(\pi_\theta\)</span>?</p><ul><li><p>In episodic environments we can use the <strong>start value</strong> <span class="math display">\[J_1(\theta)=V^{\pi_\theta}(s_1)=\mathbb{E}_{\pi_\theta}[v_1]\]</span></p></li><li><p>In continuing environments we can use the <strong>average value</strong> <span class="math display">\[J_{av}v(\theta)=\sum_s d^{\pi_\theta}(s)V^{\pi_\theta}(s)\]</span></p></li><li><p>Or the <strong>average reward per time-step</strong></p><p>â€‹ <span class="math display">\[J_{av}R(\theta)=\sum_s d^{\pi_\theta}(s)\sum_a\pi_\theta(s, a)\mathcal{R}^a_s\]</span></p></li><li><p>where <span class="math inline">\(d^{\pi_\theta}(s)\)</span> is <strong>stationary distribution</strong> of Markov chain for <span class="math inline">\(\pi_\theta\)</span>.</p></li></ul><p><strong>Policy Optimisation</strong></p><p>Policy based reinforcement learning is an <strong>optimisation</strong> problem. Find <span class="math inline">\(\theta\)</span> that maximises <span class="math inline">\(J(\theta)\)</span>.</p><p>Some approaches do not use gradient</p><ul><li>Hill climbing</li><li>Simplex / amoeba / Nelder Mead</li><li>Genetic algorithms</li></ul><p>However, greater efficiency often possible using gradient</p><ul><li>Gradient descent</li><li>Conjugate gradient</li><li>Quasi-newton</li></ul><p>We focus on gradient descent, many extensions possible. And on methods that exploit sequential structure.</p><h2><span id="finite-difference-policy-gradient">Finite Difference Policy Gradient</span></h2><p><strong>Policy Gradient</strong></p><p>Let <span class="math inline">\(J(\theta)\)</span> be any policy objective function. Policy gradient algorithms search for a local maximum in <span class="math inline">\(J(\theta)\)</span> by ascending the gradient of the policy, w.r.t. parameters <span class="math inline">\(\theta\)</span> <span class="math display">\[\triangle\theta = \alpha\nabla_\theta J(\theta)\]</span> Where <span class="math inline">\(\bigtriangledown_\theta J(\theta)\)</span> is the <span class="math inline">\(\color{red}{\mbox{policy gradient}}\)</span>, <span class="math display">\[\nabla_\theta J(\theta)=\begin{pmatrix}\frac{\partial J(\theta)}{\partial \theta_1}  \\\vdots\\\frac{\partial J(\theta)}{\partial \theta_n}\end{pmatrix}\]</span> and <span class="math inline">\(\alpha\)</span> is a step-size parameter.</p><p><strong>Computing Gradients By Finite Differences (Numerical)</strong></p><p>To evaluate policy gradient of <span class="math inline">\(\pi_\theta(s, a)\)</span>.</p><ul><li>For each dimension <span class="math inline">\(k\in[1, n]\)</span>:<ul><li><p>Estimate <span class="math inline">\(k\)</span>th partial derivative of objective function w.r.t. <span class="math inline">\(\theta\)</span></p></li><li><p>By perturbing <span class="math inline">\(\theta\)</span> by small amount <span class="math inline">\(\epsilon\)</span> in <span class="math inline">\(k\)</span>th dimension <span class="math display">\[\frac{\partial J(\theta)}{\partial \theta_k}\approx \frac{J(\theta+\epsilon u_k)-J(\theta)}{\epsilon}\]</span> where <span class="math inline">\(u_k\)</span> is unit vector with 1 in <span class="math inline">\(k\)</span>th component, 0 elsewhere</p></li></ul></li><li>Uses <span class="math inline">\(n\)</span> evaluations to compute policy gradient in <span class="math inline">\(n\)</span> dimensions</li></ul><p>This is a simple, noisy, inefficient, but sometimes effective method. It works for <strong>arbitrary</strong> policies, even if policy is <strong>not</strong> differentiable.</p><p>The algorithm is efficient when the dimension of <span class="math inline">\(\theta\)</span> is low.</p><h2><span id="monte-carlo-policy-gradient">Monte-Carlo Policy Gradient</span></h2><p><strong>Score Function</strong></p><p>We now compute the policy gradient <em>analytically</em>.</p><p>Assume policy <span class="math inline">\(\pi_\theta\)</span> is differentiable whenever it is non-zero and we know the gradient <span class="math inline">\(\nabla_\theta\pi_\theta(s, a)\)</span>.</p><p><span class="math inline">\(\color{red}{\mbox{Likelihood ratios}}\)</span> exploit the following identity <span class="math display">\[\begin{align}\nabla_\theta\pi_\theta(s, a) &amp; =\pi_\theta(s, a) \frac{\nabla_\theta\pi_\theta(s, a) }{\pi_\theta(s, a) } \\&amp; = \pi_\theta(s, a) \nabla_\theta\log \pi_\theta(s, a)  \\\end{align}\]</span> The <span class="math inline">\(\color{red}{\mbox{score function}}\)</span> is $<em></em>(s, a) $. Let's take two examples to see what the score function looks like.</p><p><em>Softmax Policy</em></p><p>We will use a softmax policy as a running example. Weight actions using linear combination of features <span class="math inline">\(\phi(s, a)^T\theta\)</span>. Probability of action is proportional to exponentiated weight: <span class="math display">\[\pi_\theta(s, a)\varpropto e^{\phi(s, a)^T\theta}\]</span> The score function is <span class="math display">\[\nabla_\theta\log\pi_\theta(s, a)=\phi(s, a)-\mathbb{E}_{\pi_\theta}[\phi(s, \cdot)]\]</span> (Intuition: log gradient = the feature for the action that we actually took minus the average feature for all actions.)</p><p><em>Gaussian Policy</em></p><p>In continuous action spaces, a Gaussian policy is natural.</p><ul><li>Mean is a linear combination of state features <span class="math inline">\(\mu(s) = \phi(s)^T\theta\)</span>.</li><li>Variance may be fixed <span class="math inline">\(\sigma^2\)</span>, or can also parametrised</li></ul><p>Policy is Gaussian, <span class="math inline">\(a\sim \mathcal{N}(\mu(s), \sigma^2)\)</span>. The score function is <span class="math display">\[\nabla_\theta\log\pi_\theta(s, a)=\frac{(a-\mu(s))\phi(s)}{\sigma^2}\]</span> So far we just have a sense of what does the score function look like. Now we step into policy gradient theorem.</p><p><strong>One-Step MDPs</strong></p><p>Consider a simple class of one-step MDPs:</p><ul><li>Starting in state <span class="math inline">\(s\sim d(s)\)</span></li><li>Terminating after one time-step with reward <span class="math inline">\(r=\mathcal{R}_{s,a}\)</span></li></ul><p>Use likelihood ratios to compute the policy gradient <span class="math display">\[\begin{align}J(\theta) &amp;=\mathbb{E}_{\pi_\theta}[r]\\&amp;=\sum_{s\in\mathcal{S}}d(s)\sum_{a\in\mathcal{A}}\pi_\theta(s, a)\mathcal{R}_{s,a}\end{align}\]</span></p><p><span class="math display">\[\begin{align}\nabla_\theta J(\theta) &amp;=\sum_{s\in\mathcal{S}}d(s)\sum_{a\in\mathcal{A}}\pi_\theta(s, a)\nabla_\theta\log\pi_\theta(s, a)\mathcal{R}_{s,a}\\&amp;=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s, a)r]\end{align}\]</span></p><p>The policy gradient theorem generalises the likelihood ratio approach to multi-step MDPs.</p><ul><li>Replaces instantaneous reward <span class="math inline">\(r\)</span> with long-term value <span class="math inline">\(Q^\pi(s, a)\)</span></li></ul><p>Policy gradient theorem applies to start state objective, average reward, and average value objective.</p><blockquote><p>Theorem</p><p>For any differentiable policy <span class="math inline">\(\pi_\theta(s,a)\)</span>, for any of the policy objective functions mentioned earlier, the policy gradient is <span class="math display">\[\nabla_\theta J(\theta)=\color{red}{\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s, a)Q^{\pi_\theta}(s, a)]}\]</span></p></blockquote><p><strong>Monte-Carlo Policy Gradient (REINFORCE)</strong></p><p>Update parameters by stochastic gradient ascent using policy gradient theorem. And using return <span class="math inline">\(v_t\)</span> as an <strong>unbiased sample</strong> of <span class="math inline">\(Q^{\pi_\theta}(s_t,a_t)\)</span>: <span class="math display">\[\triangle\theta_t=\alpha\nabla_\theta\log\pi_\theta(s_t, a_t)v_t\]</span> <img src="/images/mcpseudo.png"></p><p>(Note: MCPG is slow.)</p><h2><span id="actor-critic-policy-gradient">Actor-Critic Policy Gradient</span></h2><p><strong>Reducing Variance Using a Critic</strong></p><p>Monte-Carlo policy gradient still has high variance, we use a <span class="math inline">\(\color{red}{critic}\)</span> to estimate the action-value function: <span class="math display">\[Q_w(s, a)\approx Q^{\pi_\theta}(s, a)\]</span> Actor-critic algorithms maintain two sets of parameters:</p><ul><li>Critic: Updates action-value function parameters <span class="math inline">\(w\)</span></li><li>Actor: Updates policy parameters <span class="math inline">\(\theta\)</span>, in direction suggested by critic</li></ul><p>Actor-critic algorithms follow an <em>approximate</em> policy gradient: <span class="math display">\[\nabla_\theta J(\theta)\approx \mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s, a)Q_w(s, a)]\\\triangle\theta= \alpha\nabla_\theta\log\pi_\theta(s, a)Q_w(s, a)\]</span> The critic is solving a familiar problem: policy evaluation. This problem was explored in previous lectures:</p><ul><li>Monte-Carlo policy evaluation</li><li>Temporal-Difference learning</li><li>TD(<span class="math inline">\(\lambda\)</span>)</li><li>Least Squares policy evaluation</li></ul><p>Simple actor-critic algorithm based on action-value critic using linear value function approximation. <span class="math inline">\(Q_w(s, a)=\phi(s,a)^Tw\)</span></p><ul><li>Critic: Updates <span class="math inline">\(w\)</span> by linear TD(0)</li><li>Actor: Updates <span class="math inline">\(\theta\)</span> by policy gradient</li></ul><p><img src="/images/qacpseudo.png"></p><p><strong>Bias in Actor-Critic Algorithms</strong></p><p>Approximating the policy gradient introduces bias. A biased policy gradient may not find the right solution. Luckily, if we choose value function approximation carefully, then we can avoid introducing any bias. That is we can still follow the exact policy gradient.</p><blockquote><p><strong>Compatible Function Approximation Theorem</strong></p><p>If the following two conditions are satisdied:</p><ol type="1"><li><p>Value function approximator is <strong>compatible</strong> to the policy <span class="math display">\[\nabla_w Q_w(s, a)=\nabla_\theta \log\pi_\theta(s, a)\]</span></p></li><li><p>Value function parameters <span class="math inline">\(w\)</span> minimise the mean-squared error <span class="math display">\[\epsilon=\mathbb{E}_{\pi_\theta}[(Q^{\pi_\theta}(s, a)-Q_w(s, a))^2]\]</span></p></li></ol><p>Then the policy gradient is exact, <span class="math display">\[\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)Q_w(s,a)]\]</span></p></blockquote><p><strong>Trick: Reducing Variance Using a Baseline</strong></p><p>We substract a baseline function <span class="math inline">\(B(s)\)</span> from the policy gradient. This can <strong>reduce variance, without changing expectation</strong>: <span class="math display">\[\begin{align}\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)B(s)]&amp;=\sum_{s\in\mathcal{S}}d^{\pi_\theta}(s)\sum_a\nabla_\theta\pi_\theta(s,a)B(s)\\&amp;= \sum_{s\in\mathcal{S}}d^{\pi_\theta}B(s)\nabla_\theta\sum_{a\in\mathcal{A}}\pi_\theta(s,a)\\&amp;=  \sum_{s\in\mathcal{S}}d^{\pi_\theta}B(s)\nabla_\theta 1 \\&amp;=0\end{align}\]</span> A good baseline is the state value function <span class="math inline">\(B(s)=V^{\pi_\theta}(s)\)</span>. So we can rewrite the policy gradient using the <span class="math inline">\(\color{red}{\mbox{advantage function}}\ A^{\pi_\theta}(s,a)\)</span>. <span class="math display">\[A^{\pi_\theta}(s,a)=Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s)\\\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)\color{red}{A^{\pi_\theta}(s,a)}]\]</span> where <span class="math inline">\(V^{\pi_\theta}(s)â€‹\)</span> is the state value function of <span class="math inline">\(sâ€‹\)</span>.</p><p><strong>Intuition</strong>: The advantage function <span class="math inline">\(A^{\pi_\theta}(s,a)\)</span> tells us how much better than usual is it to take action <span class="math inline">\(a\)</span>.</p><p><strong>Estimating the Advantage Function</strong></p><p>How do we know the state value function <span class="math inline">\(V\)</span>?</p><p>One way to do that is to estimate both <span class="math inline">\(V^{\pi_\theta}(s)\)</span> and <span class="math inline">\(Q^{\pi_\theta}(s,a)\)</span>. Using two function approximators and two parameter vectors, <span class="math display">\[V_v(s)\approx V^{\pi_\theta}(s)\\Q_w(s,a)\approx Q^{\pi_\theta}(s,a)\\A(s,a)=Q_w(s,a)-V_v(s)\]</span> And updating both value functions by e.g. TD learning.</p><p>Another way is to use the TD error to compute the policy gradient. For the true value function <span class="math inline">\(V^{\pi_\theta}(s)\)</span>, the TD error <span class="math inline">\(\delta^{\pi_\theta}\)</span> <span class="math display">\[\delta^{\pi_\theta}=r+\gamma V^{\pi_\theta}(s&#39;)-V^{\pi_\theta}(s)\]</span> is an unbiased estimate of the advantage function: <span class="math display">\[\begin{align}\mathbb{E}_{\pi_\theta}[\delta^{\pi_\theta}|s, a] &amp;= \mathbb{E}_{\pi_\theta}[r+\gamma V^{\pi_\theta}(s&#39;)|s, a]-V^{\pi_\theta}(s)\\&amp;= Q^{\pi_\theta}(s, a) - V^{\pi_\theta}(s)\\&amp;= \color{red}{A^{\pi_\theta}(s,a)}\end{align}\]</span> So we can use the TD error to compute the policy gradient <span class="math display">\[\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)\color{red}{\delta^{\pi_\theta}}]\]</span> In practice we can use an approximate TD error: <span class="math display">\[\delta_v=r+\gamma V_v(s&#39;)-V_v(s)\]</span> This approach only requires one set of critic parameters <span class="math inline">\(v\)</span>.</p><p><strong>Critics and Actors at Different Time-Scales</strong></p><p>Critic can estimate value function <span class="math inline">\(V_\theta(s)\)</span> from many targets at different time-scales</p><ul><li><p>For MC, the target is return <span class="math inline">\(v_t\)</span> <span class="math display">\[\triangle \theta=\alpha(\color{red}{v_t}-V_\theta(s))\phi(s)\]</span></p></li><li><p>For TD(0), the target is the TD target <span class="math inline">\(r+\gamma V(s&#39;)\)</span> <span class="math display">\[\triangle \theta=\alpha(\color{red}{r+\gamma V(s&#39;)}-V_\theta(s))\phi(s)\]</span></p></li><li><p>For forward-view TD(<span class="math inline">\(\lambda\)</span>), the target is the return <span class="math inline">\(_vt^\lambda\)</span> <span class="math display">\[\triangle \theta=\alpha(\color{red}{v_t^\lambda}-V_\theta(s))\phi(s)\]</span></p></li><li><p>For backward-view TD(<span class="math inline">\(\lambda\)</span>), we use eligibility traces <span class="math display">\[\begin{align}\delta_t &amp;= r_{t+1}+\gamma V(s_{t+1})-V(s_t) \\e_t&amp; = \gamma\lambda e_{t-1} +\phi(s_t) \\\triangle\theta&amp;=\alpha\delta_te_t\end{align}\]</span></p></li></ul><p>The policy gradient can also be estimated at many time-scales <span class="math display">\[\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)\color{red}{A^{\pi_\theta}(s,a)}]\]</span></p><ul><li><p>MC policy gradient uses error from complete return <span class="math display">\[\triangle\theta=\alpha(\color{red}{v_t}-V_v(s_t))\nabla_\theta\log\pi_\theta(s_t,a_t)\]</span></p></li><li><p>Actor-critic policy gradient uses the one-step TD error <span class="math display">\[\triangle\theta=\alpha(\color{red}{r+\gamma V_v(s_{t+1})}-V_v(s_t))\nabla_\theta\log\pi_\theta(s_t,a_t)\]</span></p></li><li><p>Just like forward-view TD(<span class="math inline">\(\lambda\)</span>), we can mix over time-scale <span class="math display">\[\triangle \theta=\alpha(\color{red}{v_t^\lambda}-V_v(s_t))\nabla_\theta\log\pi_\theta(s_t,a_t)\]</span> where <span class="math inline">\(v_t^\lambda-V_v(s_t)\)</span> is a biased estimate of advantage function.</p></li><li><p>Like backward-view TD(<span class="math inline">\(\lambda\)</span>), we can also use eligibility traces by substituting <span class="math inline">\(\phi(s)=\nabla_\theta\log\pi_\theta(s,a)\)</span> <span class="math display">\[\begin{align}\delta_t &amp;= r_{t+1}+\gamma V_v(s_{t+1})-V_v(s_t) \\e_{t+1}&amp; = \gamma\lambda e_{t} +\nabla_\theta\log\pi_\theta(s,a) \\\triangle\theta&amp;=\alpha\delta_te_t\end{align}\]</span></p></li></ul><h2><span id="summary-of-policy-gradient-algorithms">Summary of Policy Gradient Algorithms</span></h2><p>The policy gradient has many equivalent forms</p><p><img src="/images/sumpg.png"></p><p>Each leads a stochastic gradient ascent algorithm. Critic uses policy evaluation to estimate <span class="math inline">\(Q^\pi(s, a)\)</span>, <span class="math inline">\(A^\pi(s, a)\)</span> or <span class="math inline">\(V^\pi(s)\)</span>.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This lecture talks about methods that optimise policy directly. Instead of working with value function as we consider so far, we seek experience and use the experience to update our policy in the direction that makes it better.&lt;/p&gt;
&lt;p&gt;In the last lecture, we approximated the value or action-value function using parameters &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;, &lt;span class=&quot;math display&quot;&gt;\[
V_\theta(s)\approx V^\pi(s)\\
Q_\theta(s, a)\approx Q^\pi(s, a)
\]&lt;/span&gt; A policy was generated directly from the value function using &lt;span class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;-greedy.&lt;/p&gt;
&lt;p&gt;In this lecture we will directly parametrise the policy &lt;span class=&quot;math display&quot;&gt;\[
\pi_\theta(s, a)=\mathbb{P}[a|s, \theta]
\]&lt;/span&gt; We will focus again on &lt;span class=&quot;math inline&quot;&gt;\(\color{red}{\mbox{model-free}}\)&lt;/span&gt; reinforcement learning.&lt;/p&gt;
    
    </summary>
    
      <category term="å­¦ä¹ ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="å¢å¼ºå­¦ä¹ " scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy Gradient" scheme="http://www.52coding.com.cn/tags/Policy-Gradient/"/>
    
      <category term="REINFORCE" scheme="http://www.52coding.com.cn/tags/REINFORCE/"/>
    
      <category term="Actor-Critic" scheme="http://www.52coding.com.cn/tags/Actor-Critic/"/>
    
  </entry>
  
  <entry>
    <title>RL - Value Function Approximation</title>
    <link href="http://www.52coding.com.cn/2018/01/03/RL%20-%20Value%20Function%20Approximation/"/>
    <id>http://www.52coding.com.cn/2018/01/03/RL - Value Function Approximation/</id>
    <published>2018-01-03T10:09:09.000Z</published>
    <updated>2018-11-06T03:47:58.984Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="introduction">Introduction</span></h2><p>This lecture will introduce how to scale up our algorithm to real practical RL problems by value function approximation.</p><p>Reinforcement learning can be used to solve <em>large</em> problems, e.g.</p><ul><li>Backgammon: <span class="math inline">\(10^{20}\)</span> states</li><li>Computer Go: <span class="math inline">\(10^{170}\)</span> states</li><li>Helicopter: continuous state space</li></ul><a id="more"></a><p>How can we scale up the model-free methods for prediction and control from the last two lectures?</p><p>So far we have represented value function by a <strong>lookup</strong> table:</p><ul><li>Every state <span class="math inline">\(s\)</span> has an entry <span class="math inline">\(V(s)\)</span></li><li>Or every state-action pair <span class="math inline">\(s, a\)</span> has an entry <span class="math inline">\(Q(s, a)\)</span></li></ul><p>Problems with large MDPs:</p><ul><li>There are too many states and/or actions to store in memory</li><li>It is too slow to learn the value of each state individually</li></ul><p>Solution for large MDPs: Estimate value function with <em>function approximation</em> <span class="math display">\[\hat{v}(s, \mathbb{w})\approx v_\pi(s)\\\mbox{or }\hat{q}(s, a, \mathbb{w})\approx q_\pi(s, a)\]</span> where <span class="math inline">\(\hat{v}\)</span> or <span class="math inline">\(\hat{q}\)</span> are function approximations of real <span class="math inline">\(v_\pi\)</span> or <span class="math inline">\(q_\pi\)</span>, and <span class="math inline">\(\mathbb{w}\)</span> are the parameters. This apporach has a major advantage:</p><ul><li><strong>Generalise</strong> from seen state to unseen states</li></ul><p>We can fit the <span class="math inline">\(\hat{v}\)</span> or <span class="math inline">\(\hat{q}\)</span> to <span class="math inline">\(v_\pi\)</span> or <span class="math inline">\(q_\pi\)</span> by MC or TD learning.</p><p><strong>Types of Value Function Approximation</strong></p><p><img src="/images/vftypes.png"></p><p>We consider <span class="math inline">\(\color{red}{\mbox{differentiable}}\)</span> function approximators, e.g.</p><ul><li>Linear combinations of features</li><li>Neural network</li></ul><p>Futhermore, we require a training method that is suitable for <span class="math inline">\(\color{red}{\mbox{non-stationary}}\)</span>, <span class="math inline">\(\color{red}{\mbox{non-idd}}\)</span> (idd = independent and identical distributed) data.</p><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#incremental-methods">Incremental Methods</a><ul><li><a href="#value-function-approximation">Value Function Approximation</a></li><li><a href="#action-value-function-approximation">Action-Value Function Approximation</a></li></ul></li><li><a href="#batch-methods">Batch Methods</a><ul><li><a href="#least-square-prediction">Least Square Prediction</a></li><li><a href="#least-squares-control">Least Squares Control</a></li></ul></li></ul><!-- tocstop --><h2><span id="incremental-methods">Incremental Methods</span></h2><h3><span id="value-function-approximation">Value Function Approximation</span></h3><p><strong>Gradient Descent</strong></p><p>Let <span class="math inline">\(J(\mathbb{w})\)</span> be a differentiable function of parameter vector <span class="math inline">\(\mathbb{w}\)</span>.</p><p>Define the gradient of <span class="math inline">\(J(\mathbb{w})\)</span> to be <span class="math display">\[\bigtriangledown_wJ(\mathbb{w})=\begin{pmatrix}\frac{\partial J(\mathbb{w})}{\partial \mathbb{w}_1} \\\vdots\\\frac{\partial J(\mathbb{w})}{\partial \mathbb{w}_n} \end{pmatrix}\]</span> To find a local minimum of <span class="math inline">\(J(\mathbb{w})\)</span>, adjust <span class="math inline">\(\mathbb{w}\)</span> in direction of -ve gradient <span class="math display">\[\triangle \mathbb{w}=-\frac{1}{2}\alpha \bigtriangledown_\mathbb{w}J(\mathbb{w})\]</span> where <span class="math inline">\(\alpha\)</span> is a step-size parameter.</p><p>So let's apply the <em>stochastic gradient descent</em> to <strong>value fucntion approximation</strong>.</p><p>Goal: find parameter vector <span class="math inline">\(\mathbb{w}\)</span> minimising mean-squared error between approximate value function <span class="math inline">\(\hat{v}(s, \mathbb{w})\)</span> and true value function <span class="math inline">\(v_\pi(s)\)</span>. <span class="math display">\[J(\mathbb{w})=\mathbb{E}_\pi[(v_\pi(S)-\hat{v}(S, \mathbb{w}))^2]\]</span> Gradient descent finds a local minimum <span class="math display">\[\begin{align}\triangle\mathbb{w}&amp;=-\frac{1}{2}\alpha \bigtriangledown_\mathbb{w}J(\mathbb{w})\\&amp; = \alpha\mathbb{E}_\pi[(v_\pi(S)-\hat{v}(S, \mathbb{w}))\bigtriangledown_\mathbb{w}\hat{v}(S, \mathbb{w})] \\\end{align}\]</span> Stochastic gradient descent <em>samples</em> the gradient <span class="math display">\[\triangle\mathbb{w}=\alpha(v_\pi(S)-\hat{v}(S, \mathbb{w}))\bigtriangledown_\mathbb{w}\hat{v}(S, \mathbb{w})\]</span> Expected update is equal to full gradient update.</p><p><strong>Feature Vectors</strong></p><p>Let's make this idea more concrete.</p><p>Represent state by a <em>feature vector</em>: <span class="math display">\[x(S) =\begin{pmatrix}x_1(S) \\\vdots\\x_n(S)\end{pmatrix}\]</span> For example:</p><ul><li>Distance of robot from landmarks</li><li>Trend in the stock market</li><li>Piece and pawn configurations in chess</li></ul><p><strong>Linear Value Function Approximation</strong></p><p>Represent value function by a linear combination of features <span class="math display">\[\hat{v}(S, \mathbb{w})=x(S)^T\mathbb{w}=\sum^n_{j=1}x_j(S)\mathbb{w}_j\]</span> Objective function is quadratic in parameters <span class="math inline">\(\mathbb{w}\)</span> <span class="math display">\[J(\mathbb{w})=\mathbb{E}_\pi[(v_\pi(S)-x(S)^T\mathbb{w})^2]\]</span> Stochastic gradient descent converges on global optimum.</p><p>Update rule is particularly simple <span class="math display">\[\bigtriangledown_\mathbb{w}\hat{v}(S, \mathbb{w})=x(S)\\\triangle \mathbb{w}=\alpha(v_\pi(S)-\hat{v}(S, \mathbb{w}))x(S)\]</span> Update = step-size <span class="math inline">\(\times\)</span> prediction error <span class="math inline">\(\times\)</span> feature value.</p><p>So far we have assumed true value function <span class="math inline">\(v_\pi(s)\)</span> given by supervisor. But in RL there is <strong>no supervisor, only rewards</strong>.</p><p>In practice, we substitute a <em>target</em> for <span class="math inline">\(v_\pi(s)\)</span>:</p><ul><li><p>For MC, the target is return <span class="math inline">\(G_tâ€‹\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{G_t}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w})\]</span></p></li><li><p>For TD(0), the target is the TD target <span class="math inline">\(R_{t+1}+\gamma\hat{v}(S_{t+1}, \mathbb{w})\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{R_{t+1}+\gamma\hat{v}(S_{t+1}, \mathbb{w})}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w})\]</span></p></li><li><p>For TD(<span class="math inline">\(\lambda\)</span>), the target is the return <span class="math inline">\(G_t^\lambda\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{G_t^\lambda}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w})\]</span></p></li></ul><p><strong>Monte-Carlo with Value Function Approximation</strong></p><p>Return <span class="math inline">\(G_t\)</span> is unbiased, noisy sample of true value <span class="math inline">\(v_\pi(S_t)\)</span>. We can build our &quot;training data&quot; to apply supervised learning: <span class="math display">\[&lt;S_1, G_1&gt;, &lt;S_2, G_2&gt;, ..., &lt;S_T, G_T&gt;\]</span> For example, using <em>linear Monte-Carlo policy evaluation</em> <span class="math display">\[\begin{align}\triangle \mathbb{w}&amp;=\alpha(\color{red}{G_t}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w}) \\&amp; = \alpha(G_t-\hat{v}(S_t, \mathbb{w}))x(S_t)\\\end{align}\]</span> Monte-Carlo evaluation converges to a local optimum even when using non-linear value function approximation.</p><p><strong>TD Learning with Value Function Approximation</strong></p><p>The TD-target <span class="math inline">\(R_{t+1}+\gamma \hat{v}(S_{t+1}, \mathbb{w})\)</span> is a biased sample of true value <span class="math inline">\(v_\pi(S_t)\)</span>. We can still apply supervised learning to &quot;traning data&quot;: <span class="math display">\[&lt;S_1, R_2 +\gamma\hat{v}(S_2, \mathbb{w})&gt;,&lt;S_2, R_3 +\gamma\hat{v}(S_3, \mathbb{w})&gt;,...,&lt;S_{T-1}, R_T&gt;\]</span> For example, using <em>linear TD(0)</em> <span class="math display">\[\begin{align}\triangle \mathbb{w}&amp;=\alpha(\color{red}{R+\gamma\hat{v}(S&#39;, \mathbb{w})}-\hat{v}(S, \mathbb{w}))\bigtriangledown_w \hat{v}(S, \mathbb{w}) \\&amp; = \alpha\delta x(S)\\\end{align}\]</span> Linear TD(0) converges (close) to global optimum.</p><p><strong>TD(<span class="math inline">\(\lambda\)</span>) with Value Function Approximation</strong></p><p>The <span class="math inline">\(\lambda\)</span>-return <span class="math inline">\(G_t^\lambda\)</span> is also a biased sample of true value <span class="math inline">\(v_\pi(s)\)</span>. We can also apply supervised learning to &quot;training data&quot;: <span class="math display">\[&lt;S_1, G_1^\lambda&gt;, &lt;S_2, G_2^\lambda&gt;, ..., &lt;S_{T-1}, G_{T-1}^\lambda&gt;\]</span> Can use either forward view linear TD(<span class="math inline">\(\lambda\)</span>): <span class="math display">\[\begin{align}\triangle \mathbb{w}&amp;=\alpha(\color{red}{G_t^\lambda}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w}) \\&amp; = \alpha(G_t-\hat{v}(S_t, \mathbb{w}))x(S_t)\\\end{align}\]</span> or backward view linear TD(<span class="math inline">\(\lambda\)</span>): <span class="math display">\[\begin{align}\delta_t &amp;= R_{t+1}+\gamma \hat{v}(S_{t+1}, \mathbb{w})-\hat{v}(S_t, \mathbb{w}) \\E_t&amp; = \gamma\lambda E_{t-1} +x(S_t) \\\triangle\mathbb{w}&amp;=\alpha\delta_tE_t\end{align}\]</span></p><h3><span id="action-value-function-approximation">Action-Value Function Approximation</span></h3><p><img src="/images/avfa.png"></p><p>Approximate the action-value function: <span class="math display">\[\hat{q}(S, A, \mathbb{w}) \approx q_\pi(S, A)\]</span> Minimise mean-squared error between approximate action-value function <span class="math inline">\(\hat{q}(S, A, \mathbb{w})\)</span> and true action-value function <span class="math inline">\(q_\pi(S, A)\)</span>: <span class="math display">\[J(\mathbb{w})=\mathbb{E}_\pi[(q_\pi(S, A)-\hat{q}(S, A, \mathbb{w}))^2]\]</span> Use stochastic gradient descent to find a local minimum: <span class="math display">\[-\frac{1}{2}\bigtriangledown_w J(\mathbb{w})=(q_\pi(S, A)-\hat{q}(S, A, \mathbb{w}))\bigtriangledown_w\hat{q}(S, A, \mathbb{w})\\\triangle\mathbb{w}=\alpha (q_\pi(S, A)-\hat{q}(S, A, \mathbb{w}))\bigtriangledown_w\hat{q}(S, A, \mathbb{w})\]</span> Represent state and action by a feature vector: <span class="math display">\[\mathbb{x}(S, A)=\begin{pmatrix}x_1(S, A) \\\vdots\\x_n(S, A)\end{pmatrix}\]</span> Represent action-value function by linear combination of features: <span class="math display">\[\hat{q}(S, A, \mathbb{w})=\mathbb{x}(S, A)^T\mathbb{w}=\sum^n_{j=1}x_j (S, A)\mathbb{w}_j\]</span> Stochastic gradient descent update: <span class="math display">\[\bigtriangledown_w\hat{q}(S, A, \mathbb{w})=\mathbb{x}(S, A)\\\triangle \mathbb{w}=\alpha(q_\pi(S, A)-\hat{q}(S,  A, \mathbb{w}))\mathbb{x}(S, A)\]</span> Like prediction, we must subsitute a target for <span class="math inline">\(q_\pi(S, A)\)</span>:</p><ul><li><p>For MC, the target is the return <span class="math inline">\(G_t\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{G_t}-\hat{q}(S_t,  A_t, \mathbb{w}))\bigtriangledown_w\hat{q}(S_t, A_t, \mathbb{w})\]</span></p></li><li><p>For TD(0), the target is the TD target <span class="math inline">\(R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})â€‹\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{R_{t+1}+\gamma \hat{q}(S_{t+1},  A_{t+1}, \mathbb{w})}-\hat{q}(S_t,  A_t, \mathbb{w}))\bigtriangledown_w\hat{q}(S_t, A_t, \mathbb{w})\]</span></p></li><li><p>For forward-view TD(<span class="math inline">\(\lambda\)</span>), target is the action-value <span class="math inline">\(\lambda\)</span>-return <span class="math display">\[\triangle\mathbb{w}=\alpha(\color{red}{q_t^\lambda}-\hat{q}(S_t, A_t,\mathbb{w}))\bigtriangledown\hat{q}(S_t, A_t, \mathbb{w})\]</span></p></li><li><p>For backward-view TD(<span class="math inline">\(\lambda\)</span>), equivalent update is <span class="math display">\[\begin{align}\delta_t&amp; =R_{t+1}+\gamma\hat{q}(S_{t+1}, A_{t+1}, \mathbb{w})-\hat{q}(S_t, A_t, \mathbb{w}) \\E_t&amp; = \gamma\lambda E_{t-1}+\bigtriangledown_w\hat{q}(S_t, A_t, \mathbb{w}) \\\triangle\mathbb{w}&amp;= \alpha\delta_t E_t\end{align}\]</span></p></li></ul><p><strong>Linear Sarsa with Coarse Coding in Mountain Car</strong></p><p><img src="/images/linsarsa.png"></p><p>The goal is to control our car to reach the top of the mountain. We represent state by the car's position and velocity. The height of the diagram shows the value of each state. Finally, the value function is like:</p><p><img src="/images/linsarfin.png"></p><p><strong>Study of <span class="math inline">\(\lambda\)</span>: Should We Bootstrap?</strong></p><p><img src="/images/lambdastudy.png"></p><p>The answer is <strong>yes</strong>. We can see from above picture, choose some approprite <span class="math inline">\(\lambda\)</span> can certainly reduce the training steps as well as the cost.</p><p>However, temporal-difference learning in many cases doesn't guarantee to converge. It may also diverge.</p><p><strong>Convergence of Prediction Algorithms</strong></p><p><img src="/images/converge.png"></p><p>TD dose not follow the gradient of <em>any</em> objective function. This is why TD can diverge when off-policy or using non-linear function approximation. <strong>Gradient TD</strong> follows true gradient of projected Bellman error.</p><p><img src="/images/gtd.png"></p><p><strong>Convergence of Control Algorithms</strong></p><p><img src="/images/convca.png"></p><h2><span id="batch-methods">Batch Methods</span></h2><p>Gradient descent is simple and appealing. But it is not <strong>sample efficient</strong>. Batch methods seek to find the best fitting value function given the agent's experience.</p><h3><span id="least-square-prediction">Least Square Prediction</span></h3><p>Give value function approximation <span class="math inline">\(\hat{v}(s, \mathbb{w})\approx v_\pi(s)\)</span> and experience <span class="math inline">\(\mathcal{D}\)</span> consisting of <em>&lt;state, value&gt;</em> pairs: <span class="math display">\[\mathcal{D} = \{&lt;s_1, v_1^\pi&gt;, &lt;s_2, v_2^\pi&gt;, ..., &lt;s_T, v_T^\pi&gt; \}\]</span> Which parameters <span class="math inline">\(\mathbb{w}\)</span> give the best fitting value function <span class="math inline">\(\hat{v}(s, \mathbb{w})\)</span> ?</p><p><span class="math inline">\(\color{red}{\mbox{Least squares}}\)</span> algorithms find parameter vector <span class="math inline">\(\mathbb{w}\)</span> minimising sum-squared error between <span class="math inline">\(\hat{v}(s_t, \mathbb{w})\)</span> and target values <span class="math inline">\(v_t^\pi\)</span>, <span class="math display">\[\begin{align}LS(\mathbb{w}) &amp; = \sum^T_{t=1}(v_t^\pi-\hat{v}(s_t, \mathbb{w}))^2 \\&amp; = \mathbb{E}_\mathcal{D}[(v^\pi-\hat{v}(s, \mathbb{w}))^2] \\\end{align}\]</span> Given experience consisting of <em>&lt;state, value&gt;</em> pairs <span class="math display">\[\mathcal{D}=\{&lt;s_1, v_1^\pi&gt;, &lt;s_2, v_2^\pi&gt;, ..., &lt;s_T, v_T^\pi&gt;\}\]</span> Repeat:</p><ol type="1"><li><p>Sample state, value from experience <span class="math display">\[&lt;s, v^\pi&gt; \sim \mathcal{D}\]</span></p></li><li><p>Apply stochastic gradient descent update <span class="math display">\[\triangle \mathbb{w}=\alpha(v^\pi-\hat{v}(s, \mathbb{w}))\bigtriangledown_w\hat{v}(s, w)\]</span></p></li></ol><p>Converges to least squares solution <span class="math display">\[\mathbb{w}^\pi=\arg\min_w LS(w)\]</span> <strong>Deep Q-Networks (DQN)</strong></p><p>DQN uses <span class="math inline">\(\color{red}{\mbox{experience replay}}\)</span> and <span class="math inline">\(\color{red}{\mbox{fixed Q-targets}}\)</span>:</p><ul><li><p>Take action <span class="math inline">\(a_t\)</span> according to <span class="math inline">\(\epsilon\)</span>-greedy policy</p></li><li><p>Store transition <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> in replay memory <span class="math inline">\(\mathcal{D}\)</span></p></li><li><p>Sample random mini-batch of transitions <span class="math inline">\((s, a, r, s&#39;)\)</span> from <span class="math inline">\(\mathcal{D}\)</span></p></li><li><p>Compute Q-learning targets w.r.t. old, fixed parameters <span class="math inline">\(w^-\)</span></p></li><li><p>Optimise MSE between Q-network and Q-learning target <span class="math display">\[\mathcal{L}(w_i)=\mathbb{E}_{s,a,r,s&#39;\sim\mathcal{D}_i}[(r+\gamma\max_{a&#39;}Q(s&#39;,a&#39;;w^-_i)-Q(s, a;w_i))^2]\]</span></p></li><li><p>Using variant of stochastic gradient descent</p></li></ul><p>Note: <span class="math inline">\(\color{red}{\mbox{fixed Q-targets}}\)</span> means we use two Q-networks. One of it using fixed old parameters to generate the Q-target to update the fresh Q-network, which can keep the update <strong>stable</strong>. Otherwise, when you update the Q-network, you also update the Q-target, which can cause diverge.</p><p>DQN in Atari</p><ul><li>End-to-end learning of values <span class="math inline">\(Q(s, a)\)</span> from pixels <span class="math inline">\(s\)</span></li><li>Input state <span class="math inline">\(s\)</span> is stack of raw pixels from last 4 frames</li><li>Output is <span class="math inline">\(Q(s, a)\)</span> for 18 joystick/button positions</li><li>Rewards is change in score for that step</li></ul><p><img src="/images/ataridqn.png"></p><p>Results</p><p><img src="/images/dqnres.png"></p><p>How much does DQN help?</p><p><img src="/images/dqnhelp.png"></p><p><strong>Linear Least Squares Prediction - Normal Equation</strong></p><p>Experience replay finds least squares solution but it may take many iterations. Using <em>linear value function approximation</em> <span class="math inline">\(\hat{v}(s, w) = x(s)^Tw\)</span>, we can solve squares soluton directly.</p><p>At minimum of <span class="math inline">\(LS(w)\)</span>, the expected update must be zero:</p><p><img src="/images/llsp.png"></p><p>For <span class="math inline">\(N\)</span> features, direct solution time is <span class="math inline">\(O(N^3)\)</span>. Incremental solution time is <span class="math inline">\(O(N^2)\)</span> using Shermann-Morrison.</p><p>We do not know true values <span class="math inline">\(v_t^\pi\)</span>. In practice, our &quot;training data&quot; must be noisy or biased samples of <span class="math inline">\(v_t^\pi\)</span>:</p><p><img src="/images/llspa.png"></p><p>In each case solve directly for fixed point of MC / TD / TD(<span class="math inline">\(\lambda\)</span>).</p><p><strong>Convergence of Linear Least Squares Prediction Algorithms</strong></p><p><img src="/images/cllspa.png"></p><h3><span id="least-squares-control">Least Squares Control</span></h3><p><strong>Least Squares Policy Iteration</strong></p><p><img src="/images/lspi.png"></p><p><strong>Least Squares Action-Value Function Approximation</strong></p><p>Approximate action-value function <span class="math inline">\(q_\pi(s, a)\)</span> using linear combination of features <span class="math inline">\(\mathbb{x}(s, a)\)</span>: <span class="math display">\[\hat{q}(s, a, \mathbb{w})=\mathbb{x}(s, a)^T\mathbb{w}\approx q_\pi(s, a)\]</span> Minimise least squares error between <span class="math inline">\(\hat{q}(s, a, \mathbb{w})\)</span> and <span class="math inline">\(q_\pi(s, a)\)</span> from experience generated using policy <span class="math inline">\(\pi\)</span> consisting of <em>&lt;(state, action), value&gt;</em> pairs: <span class="math display">\[\mathcal{D}=\{&lt;(s_1,a_1),v_1^\pi&gt;,&lt;(s_2,a_2),v_2^\pi&gt;,...,&lt;(s_T,a_T),v_T^\pi&gt;\}\]</span> <strong>Least Squares Control</strong></p><p>For policy evaluation, we want to efficiently use all experience. For control, we also want to improve the policy. This experience is generated from many policies. So to evaluate <span class="math inline">\(q_\pi(S, A)\)</span> we must learn <span class="math inline">\(\color{red}{\mbox{off-policy}}\)</span>.</p><p>We use the same idea as Q-learning:</p><ul><li>Use experience generated by old policy <span class="math inline">\(S_t, A_t, R_{t+1}, S_{t+1} \sim \pi_{old}\)</span></li><li>Consider alternative successor action <span class="math inline">\(A&#39;=\pi_{new}(S_{t+1})\)</span></li><li>Update <span class="math inline">\(\hat{q}(S_t, A_t,\mathbb{w})\)</span> towards value of alternative action <span class="math inline">\(R_{t+1}+\gamma \hat{q}(S_{t+1}, A&#39;, \mathbb{w})\)</span></li></ul><p>Consider the following linear Q-learning update <span class="math display">\[\delta=R_{t+1}+\gamma \hat{q}(S_{t+1}, \color{red}{\pi(S_{t+1})}, \mathbb{w})-\hat{q}(S_t, A_t, \mathbb{w})\\\triangle \mathbb{w}=\alpha\delta\mathbb{x}(S_t, A_t)\]</span> LSTDQ algorithm: solve for total update = zero:</p><p><img src="/images/lstdq.png"></p><p>The following pseudocode uses LSTDQ for policy evaluation. It repeatedly re-evaluates experience <span class="math inline">\(\mathcal{D}\)</span> with different policies.</p><p><img src="/images/lspipseudo.png"></p><p><strong>Convergence of Control Algorithms</strong></p><p><img src="/images/ccal.png"></p><p>End.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This lecture will introduce how to scale up our algorithm to real practical RL problems by value function approximation.&lt;/p&gt;
&lt;p&gt;Reinforcement learning can be used to solve &lt;em&gt;large&lt;/em&gt; problems, e.g.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Backgammon: &lt;span class=&quot;math inline&quot;&gt;\(10^{20}\)&lt;/span&gt; states&lt;/li&gt;
&lt;li&gt;Computer Go: &lt;span class=&quot;math inline&quot;&gt;\(10^{170}\)&lt;/span&gt; states&lt;/li&gt;
&lt;li&gt;Helicopter: continuous state space&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="å­¦ä¹ ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="å¢å¼ºå­¦ä¹ " scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="DQN" scheme="http://www.52coding.com.cn/tags/DQN/"/>
    
      <category term="Neural Network" scheme="http://www.52coding.com.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>RL - Model-Free Control</title>
    <link href="http://www.52coding.com.cn/2017/12/21/RL%20-%20Model-Free%20Control/"/>
    <id>http://www.52coding.com.cn/2017/12/21/RL - Model-Free Control/</id>
    <published>2017-12-21T12:00:09.000Z</published>
    <updated>2018-11-06T03:47:38.964Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="introduction">Introduction</span></h2><p>Last lecture:</p><ul><li>Model-free prediction</li><li><em>Estimate</em> the value function of an <em>unknown</em> MDP</li></ul><p>This lecture:</p><ul><li>Model-free control</li><li><strong>Optimise</strong> the value function of an unknown MDP</li></ul><a id="more"></a><p>Why we care about model-free control? So, let's see some example problems that can be modelled as MDPs:</p><ul><li>Helicopter, Robocup Soccer, Quake</li><li>Portfolio management, Game of Go...</li></ul><p>For most of these problems, either:</p><ul><li>MDP model is <strong>unknown</strong>, but experience can be sampled</li><li>MDP model is known, but is <strong>too big to use</strong>, except by samples</li></ul><p><span class="math inline">\(\color{red}{\mbox{Model-free control}}\)</span> can sovlve these problems.</p><p>There are two branches of model-free control:</p><ul><li><span class="math inline">\(\color{red}{\mbox{On-policy}}\)</span> learning<ul><li>&quot;Learn on the job&quot;</li><li>Learn about policy <span class="math inline">\(\pi\)</span> from experience sampled from <span class="math inline">\(\pi\)</span></li></ul></li><li><span class="math inline">\(\color{red}{\mbox{Off-policy}}\)</span> learning<ul><li>&quot;Look over someone's shoulder&quot;</li><li>Learn about policy <span class="math inline">\(\pi\)</span> from experience sampled from <span class="math inline">\(\mu\)</span></li></ul></li></ul><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#on-policy-monte-carlo-control">On-Policy Monte-Carlo Control</a></li><li><a href="#on-policy-temporal-difference-learning">On-Policy Temporal-Difference Learning</a><ul><li><a href="#sarsalambda">Sarsa(<span class="math inline">\(\lambda\)</span>)</a></li></ul></li><li><a href="#off-policy-learning">Off-Policy Learning</a><ul><li><a href="#q-learning">Q-Learning</a></li></ul></li><li><a href="#summary">Summary</a></li></ul><!-- tocstop --><h2><span id="on-policy-monte-carlo-control">On-Policy Monte-Carlo Control</span></h2><p>In previous lectures, we have seen that using policy iteration to find the best policy. Today, we also use this central idea plugging in MC or TD algorithm.</p><p><img src="/images/pi.png"></p><p><strong>Generalised Policy Iteration With Monte-Carlo Evaluation</strong></p><p>A simple idea is</p><ul><li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(V = v_\pi\)</span>?</li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: Greedy policy improvement?</li></ul><p>Well, this idea has two major problems:</p><ul><li><p>Greedy policy improvement over <span class="math inline">\(V(s)\)</span> requires <strong>model of MDP</strong> <span class="math display">\[\pi^\prime(s) = \arg\max_{a\in\mathcal{A}}\mathcal{R}^a_s+\mathcal{P}^a_{ss&#39;}V(s&#39;)\]</span> since, we do not know the state transition probability matrix <span class="math inline">\(\mathcal{P}\)</span>.</p></li><li><p>Exploration issue: cannot guarantee to explore all states</p></li></ul><p>So, the alternative is to use action-value function <span class="math inline">\(Q\)</span>:</p><ul><li>Greedy policy improvement over <span class="math inline">\(Q(s, a)\)</span> is model-free <span class="math display">\[\pi^\prime=\arg\max_{a\in\mathcal{A}}Q(s,a)\]</span></li></ul><p>Let's replace it in the algorithm:</p><p><img src="/images/avf.png"></p><ul><li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(\color{red}{Q = q_\pi}\)</span></li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: Greedy policy improvement?</li></ul><p>We still have one problems about the algorithm, which is exploration issue. Here is a example of greedy action selection:</p><p><img src="/images/gaseg.png"></p><p>The reward of the two doors are stochastic. However, because of the greedy action selection, we always choose the right door without exploring the value of the left one.</p><p>One simple algorithm to ensure keeping exploration is <strong><span class="math inline">\(\epsilon\)</span>-greedy exploration</strong>.</p><p><strong><span class="math inline">\(\epsilon\)</span>-Greedy Exploration</strong></p><p>All <span class="math inline">\(m\)</span> actions are tried with non-zero probalility,</p><ul><li>With probability <span class="math inline">\(1-\epsilon\)</span> choose the greedy action</li><li>With probability <span class="math inline">\(\epsilon\)</span> choose an action at <strong>random</strong></li></ul><p><span class="math display">\[\pi(a|s)=\begin{cases} \epsilon/m+1-\epsilon,  &amp; \mbox{if } a^* = \arg\max_{a\in\mathcal{A}}Q(s,a) \\\epsilon/m, &amp; \mbox{otherwise }\end{cases}\]</span></p><blockquote><p>Theorem</p><p>For any <span class="math inline">\(\epsilon\)</span>-greedy policy <span class="math inline">\(\pi\)</span>, the <span class="math inline">\(\epsilon\)</span>-greedy policy <span class="math inline">\(\pi^\prime\)</span> with respect to <span class="math inline">\(q_\pi\)</span> is an improvement, <span class="math inline">\(v_{\pi^\prime}â‰¥v_\pi(s)\)</span>.</p></blockquote><p><img src="/images/egpi.png"></p><p>Therefore from policy improvement theorem, <span class="math inline">\(v_{\pi^\prime}(s) â‰¥ v_\pi(s)\)</span>.</p><p><strong>Monte-Carlo Policy Iteration</strong></p><p><img src="/images/mcpi.png"></p><ul><li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(Q = q_\pi\)</span></li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: <span class="math inline">\(\color{red}{\epsilon}\)</span>-greedy policy improvement</li></ul><p><strong>Monte-Carlo Control</strong></p><p><img src="/images/mcc.png"></p><p><span class="math inline">\(\color{red}{\mbox{Every episode}}\)</span>:</p><ul><li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(\color{red}{Q \approx q_\pi}\)</span></li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: <span class="math inline">\(\epsilon\)</span>-greedy policy improvement</li></ul><p>The method is once evaluate over an episode, immediately improve the policy. The idea is since we already have a better evaluation, why waiting to update the policy after numerous episodes. That is improving the policy right after evaluating one episode.</p><p><strong>GLIE</strong></p><blockquote><p>Definition</p><p><strong>Greedy in the Limit with Infinite Exploration</strong> (GLIE)</p><ul><li><p>All state-action pairs are explored infinitely many times, <span class="math display">\[\lim_{k\rightarrow\infty}N_k(s,a)=\infty\]</span></p></li><li><p>The policy converges on a greedy policy, <span class="math display">\[\lim_{k\rightarrow\infty}\pi_k(a|s)=1(a=\arg\max_{a^\prime \in\mathcal{A}}Q_k(s, a^\prime))\]</span></p></li></ul></blockquote><p>For example, <span class="math inline">\(\epsilon\)</span>-greedy is GLIE if <span class="math inline">\(\epsilon\)</span> reduces to zero at <span class="math inline">\(\epsilon_k=\frac{1}{k}\)</span>.</p><p><strong>GLIE Monte-Carlo Control</strong></p><p>Sample <span class="math inline">\(k\)</span>th episode using <span class="math inline">\(\pi\)</span>: <span class="math inline">\(\{S_1, A_1, R_2, â€¦, S_T\} \sim \pi\)</span></p><ul><li><p><span class="math inline">\(\color{red}{\mbox{Evaluation}}\)</span></p><ul><li>For each state <span class="math inline">\(S_t\)</span> and action <span class="math inline">\(A_t\)</span> in the episode, <span class="math display">\[\begin{array}{lcl}N(S_t, A_t) \leftarrow N(S_t, A_t)+1 \\Q(S_t, A_t) \leftarrow Q(S_t, A_t)+\frac{1}{N(S_t, A_t)}(G_t-Q(S_t, A_t))\end{array}\]</span></li></ul></li><li><p><span class="math inline">\(\color{red}{\mbox{Improvement}}\)</span></p><ul><li>Improve policy based on new action-value function <span class="math display">\[\begin{array}{lcl}\epsilon\leftarrow \frac{1}{k} \\\pi \leftarrow \epsilon\mbox{-greedy}(Q)\end{array}\]</span></li></ul></li></ul><p>GLIE Monte-Carlo control converges to the optimal action-value function, <span class="math inline">\(Q(s,a) \rightarrow q_*(s,a)\)</span>.</p><p><strong>Blackjack Example</strong></p><p><img src="/images/mccb.png"></p><p>Using Monte-Carlo control, we can get the optimal policy above.</p><h2><span id="on-policy-temporal-difference-learning">On-Policy Temporal-Difference Learning</span></h2><p>Temporal-difference (TD) learning has several advantages over Monte-Carlo (MC):</p><ul><li>low variance</li><li>Online</li><li>Incomplete sequences</li></ul><p>A natural idea is using TD instead of MC in our control loop:</p><ul><li>Apply TD to <span class="math inline">\(Q(S, A)\)</span></li><li>Use <span class="math inline">\(\epsilon\)</span>-greedy policy improvement</li><li>Update every <em>time-step</em></li></ul><p><strong>Sarsa Update</strong></p><p><img src="/images/sarsa.png"></p><p>Updating action-value functions with Sarsa: <span class="math display">\[Q(S,A) \leftarrow Q(S, A) + \alpha(R+\gamma Q(S^\prime, A^\prime)-Q(S, A))\]</span> <img src="/images/mcc.png"></p><p>So, the full algorithm is:</p><ul><li>Every <span class="math inline">\(\color{red}{\mbox{time-step}}\)</span>:<ul><li><span class="math inline">\(\color{blue}{\mbox{Policy evaluation}}\)</span> <span class="math inline">\(\color{red}{\mbox{Sarsa}}\)</span>, <span class="math inline">\(Q\approx q_\pi\)</span></li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span> <span class="math inline">\(\epsilon\)</span>-greedy policy improvement</li></ul></li></ul><p><img src="/images/pesedotd.png"></p><p><strong>Windy Gridworld Example</strong></p><p><img src="/images/wweg.png"></p><p>The 'S' represents start location and 'G' marks the goal. There is a number at the bottom of each column which represents the wind will blow the agent up how many grids if the agent stays at that column.</p><p>The result of apply Sarsa to the problem is</p><p><img src="/images/wwegres.png"></p><h3><span id="sarsalambda">Sarsa(<span class="math inline">\(\lambda\)</span>)</span></h3><p><strong>n-Step Sarsa</strong></p><p>Consider the following <span class="math inline">\(n\)</span>-step returns for <span class="math inline">\(n=1,2,..\infty\)</span>:</p><p><img src="/images/nsarsa.png"></p><p>Define the <span class="math inline">\(n\)</span>-step <span class="math inline">\(Q\)</span>-return <span class="math display">\[q_t^{(n)}=R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^n Q(S_{t+n})\]</span> <span class="math inline">\(n\)</span>-step Sarsa updates <span class="math inline">\(Q(s, a)\)</span> towards the <span class="math inline">\(n\)</span>-step <span class="math inline">\(Q\)</span>-return <span class="math display">\[Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha(q_t^{(n)}-Q(S_t,A_t))\]</span> <strong>Forward View Sarsa(<span class="math inline">\(\lambda\)</span>)</strong></p><p><img src="/images/sarsalam.png"></p><p>The <span class="math inline">\(q^\lambda\)</span> return combines all <span class="math inline">\(n\)</span>-step Q-returns <span class="math inline">\(q_t^{(n)}\)</span> using weight <span class="math inline">\((1-\lambda)\lambda^{n-1}\)</span>: <span class="math display">\[q_t^\lambda = (1-\lambda)\sum^\infty_{n=1}\lambda^{n-1}q_t^{(n)}\]</span> Forward-view Sarsa(<span class="math inline">\(\lambda\)</span>): <span class="math display">\[Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha(q_t^\lambda-Q(S_t, A_t))\]</span> <strong>Backward View Sarsa(<span class="math inline">\(\lambda\)</span>)</strong></p><p>Just like TD(<span class="math inline">\(\lambda\)</span>), we use <span class="math inline">\(\color{red}{\mbox{eligibility traces}}\)</span> in an online algorithm, but Sarsa(<span class="math inline">\(\lambda\)</span>) has one eligibility trace for each state-action pair: <span class="math display">\[E_0(s, a) = 0\]</span></p><p><span class="math display">\[E_t(s, a) = \gamma\lambda E_{t-1}(s,a)+1(S_t=s, A_t=a)\]</span></p><p><span class="math inline">\(Q(s, a)\)</span> is updated for every state <span class="math inline">\(s\)</span> and action <span class="math inline">\(a\)</span> in proportion to TD-error <span class="math inline">\(\delta_t\)</span> and eligibility trace <span class="math inline">\(E_t(s, a)\)</span>: <span class="math display">\[\delta_t=R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})-Q(S_t, A_t)\]</span></p><p><span class="math display">\[Q(s, a) \leftarrow Q(s, a) +\alpha \delta_t E_t(s, a)\]</span></p><p><img src="/images/sarcode.png"></p><p>The difference between Sarsa and Sarsa(<span class="math inline">\(\lambda\)</span>):</p><p><img src="/images/sarsadiff.png"></p><p>If we initial all <span class="math inline">\(Q(s, a) = 0\)</span>, then we first do a random walk and reach the goal. Using Sarsa, we can only update the Q-value of the previous state before reaching the goal since all other <span class="math inline">\(Q\)</span> are zero. So the reward can only propagate one state. On the contrary, if we using Sarsa(<span class="math inline">\(\lambda\)</span>), the reward can propagate from the last state to the first state with a exponential decay.</p><h2><span id="off-policy-learning">Off-Policy Learning</span></h2><p>Evaluate target policy <span class="math inline">\(\pi(a|s)\)</span> to compute <span class="math inline">\(v_\pi(s)\)</span> or <span class="math inline">\(q_\pi(s, a)\)</span> while following behaviour policy <span class="math inline">\(\mu(a|s)\)</span> <span class="math display">\[\{S_1, A_1, R_2, ..., S_T\}\sim \mu\]</span> So, why is this important? There are several reasons:</p><ul><li>Learn from observing hunman or other agents</li><li>Re-use experience generated from old policies <span class="math inline">\(\pi_1, \pi_2, â€¦, \pi_{t-1}\)</span></li><li>Learn about <strong>optimal</strong> policy while following <span class="math inline">\(\color{red}{\mbox{exploratory policy}}\)</span></li><li>Learn about <strong>multiple</strong> policies while following one policy</li></ul><p><strong>Importance Sampling</strong></p><p>Estimate the expectation of a different distribution <span class="math display">\[\mathbb{E}_{X\sim P}[f(X)] = \sum P(X)f(X)=\sum Q(X)\frac{P(X)}{Q(X)}f(X)=\mathbb{E}_{X\sim Q}[\frac{P(X)}{Q(X)}f(X)]\]</span> <strong>Off-Policy Monte-Carlo</strong></p><p>Use returns generated from <span class="math inline">\(\mu\)</span> to evaluate <span class="math inline">\(\pi\)</span>. Weight return <span class="math inline">\(G_t\)</span> according to <strong>similarity</strong> between policies. Multiply importance sampling corrections along whole episode: <span class="math display">\[G_t^{\pi/\mu}=\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}\frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})}...\frac{\pi(A_T|S_T)}{\mu(A_T|S_T)}G_t\]</span> Update value towards <em>corrected</em> return: <span class="math display">\[V(S_t)\leftarrow V(S_t)+\alpha (\color{red}{G_t^{\pi/\mu}}-V(S_t))\]</span> But it has two major problems:</p><ul><li>Cannot use if <span class="math inline">\(\mu\)</span> is zero when <span class="math inline">\(\pi\)</span> is non-zero</li><li>Importance sampling can dramatically increase variance, so it is useless in practice</li></ul><p><strong>Off-Policy TD</strong></p><p>Use TD targets generated from <span class="math inline">\(\mu\)</span> to evaluate <span class="math inline">\(\pi\)</span>. Weight TD target <span class="math inline">\(R+\gamma V(S&#39;)\)</span> by importance sampling. Only need a single importance sampling correction: <span class="math display">\[V(S_t)\leftarrow V(S_t)+\alpha \left(\color{red}{\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1}+\gamma V(S_{t+1}))}-V(S_t)\right)\]</span> This algorithm has much lower variance than Monte-Carlo importance sampling because policies only need to be similar over a single step.</p><h3><span id="q-learning">Q-Learning</span></h3><p>We now consider off-policy learning of action-values <span class="math inline">\(Q(s, a)\)</span>. The benefit of it is no importance sampling is required.</p><p>The next action is chosen using <strong>behaviour</strong> policy <span class="math inline">\(A_{t+1}\sim\mu(\cdot|S_t)\)</span>. But we consider <strong>alternative</strong> successor action <span class="math inline">\(A&#39;\sim \pi(\cdot|S_t)\)</span>. And update <span class="math inline">\(Q(S_t, A_t)\)</span> towards value of alternative action <span class="math display">\[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1}+\gamma Q(S_{t+1}, \color{red}{A&#39;})-Q(S_t, A_t))\]</span> We now allow both behaviour and target policies to <strong>improve</strong>.</p><p>The <strong>target</strong> policy <span class="math inline">\(\pi\)</span> is <span class="math inline">\(\color{red}{\mbox{greedy}}\)</span> w.r.t <span class="math inline">\(Q(s, a)\)</span>: <span class="math display">\[\pi(S_{t+1})=\arg\max_{a&#39;}Q(S_{t+1}, a&#39;)\]</span> The <strong>behaviour</strong> policy <span class="math inline">\(\mu\)</span> is e.g. <span class="math inline">\(\color{red}{\epsilon \mbox{-greedy}}\)</span> w.r.t. <span class="math inline">\(Q(s,a)\)</span>.</p><p>The <strong>Q-learning</strong> target then simplifies: <span class="math display">\[\begin{align}\mbox{Q-learning Target} &amp;= R_{t+1}+\gamma Q(S_{t+1}, A&#39;) \\&amp; = R_{t+1}+\gamma Q(S_{t+1}, \arg\max_{a&#39;}Q(S_{t+1}, a&#39;)) \\&amp;= R_{t+1}+\max_{a&#39;}\gamma Q(S_{t+1}, a&#39;)\end{align}\]</span> So the Q-learning control algorithm is</p><p><img src="/images/qlalg.png"></p><p>Of course, the Q-learning control still converges to the optimal action-value function, <span class="math inline">\(Q(s, a)\rightarrow q_*(s,a)\)</span>.</p><p><img src="/images/qlcode.png"></p><h2><span id="summary">Summary</span></h2><p><strong>Relationship Between DP and TD</strong></p><p><img src="/images/rbtddp.png"></p><p><img src="/images/rbtddp2.png"></p><p>In a word, TD backup can be seen as the sample of corresponding DP backup. This lecture introduces model-free control which is optimise the value function of an unknown MDP with on-policy and off-policy methods. Next lecture will introduce function approximation which is easy to scale up and can be applied into big MDPs.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Last lecture:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model-free prediction&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Estimate&lt;/em&gt; the value function of an &lt;em&gt;unknown&lt;/em&gt; MDP&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This lecture:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model-free control&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimise&lt;/strong&gt; the value function of an unknown MDP&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="å­¦ä¹ ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="å¢å¼ºå­¦ä¹ " scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Monte-Carlo Control" scheme="http://www.52coding.com.cn/tags/Monte-Carlo-Control/"/>
    
      <category term="Sarsa" scheme="http://www.52coding.com.cn/tags/Sarsa/"/>
    
      <category term="Q-learning" scheme="http://www.52coding.com.cn/tags/Q-learning/"/>
    
  </entry>
  
  <entry>
    <title>RL - Model-Free Prediction</title>
    <link href="http://www.52coding.com.cn/2017/12/16/RL%20-%20Model-Free%20Prediction/"/>
    <id>http://www.52coding.com.cn/2017/12/16/RL - Model-Free Prediction/</id>
    <published>2017-12-16T07:00:09.000Z</published>
    <updated>2018-11-06T03:47:43.363Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="introduction">Introduction</span></h2><p>Last lecture, David taught us how to solve a <em>known</em> MDP, which is <em>planning by dynamic programming</em>. In this lecture, we will learn how to estimate the value function of an <strong>unknown</strong> MDP, which is <em>model-free prediction</em>. And in the next lecture, we will <em>optimise</em> the value function of an unknown MDP.</p><a id="more"></a><p>In summary:</p><ul><li>Planning by dynamic programming<ul><li>Solve a <em>known MDP</em></li></ul></li><li><strong>Model-Free prediction</strong><ul><li>Estimate the value function of an <em>unknown</em> MDP</li></ul></li><li>Model-Free control<ul><li>Optimise the value function of an <em>unknown</em> MDP</li></ul></li></ul><p>We have two major methods to estimate the value function of an unknown MDP:</p><ul><li>Monte-Carlo Learning</li><li>Temporal-Difference Learning</li></ul><p>We will introduce the two methods and combine them to a general method.</p><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#monte-carlo-learning">Monte-Carlo Learning</a></li><li><a href="#temporal-difference-learning">Temporal-Difference Learning</a><ul><li><a href="#unified-view">Unified View</a></li></ul></li><li><a href="#tdlambda">TD(<span class="math inline">\(\lambda\)</span>)</a><ul><li><a href="#forward-view-tdlambda">Forward View TD(<span class="math inline">\(\lambda\)</span>)</a></li><li><a href="#backward-view-tdlambda">Backward View TD(<span class="math inline">\(\lambda\)</span>)</a></li><li><a href="#relationship-between-forward-and-backward-td">Relationship Between Forward and Backward TD</a></li></ul></li></ul><!-- tocstop --><h2><span id="monte-carlo-learning">Monte-Carlo Learning</span></h2><p>MC (Monte-Carlo) methods learn directly from <strong>episodes of experience</strong>, which means:</p><ul><li>MC is <em>model-free</em>: no knowledge of MDP transitions / rewards</li><li>MC learns from complete episodes: no bootstrapping</li><li>MC uses the simplest possible idea: value = mean return</li><li>Can only apply MC to <em>episodic</em> MDPs: all episodes must terminate</li></ul><p><strong>Goal</strong>: learn <span class="math inline">\(v_\pi\)</span> from episodes of experience under policy <span class="math inline">\(\pi\)</span> <span class="math display">\[S_1, A_1, R_2, ..., S_k \sim \pi\]</span> Recall that the <strong>return</strong> is the total discounted reward: <span class="math display">\[G_t = R_{t+1}+\gamma R_{t+2} + ... + \gamma^{T-1}R_T\]</span> Recall that the <strong>value function</strong> is the expected return: <span class="math display">\[v_\pi(s) = \mathbb{E}_\pi[G_t|S_t=s]\]</span> Monte-Carlo policy evaluation uses <em>empirical mean</em> return instead of <em>expected</em> return.</p><p><strong>First-Visit Monte-Carlo Policy Evaluation</strong></p><p>To evaluate state <span class="math inline">\(s\)</span>, the <strong>first</strong> time-step <span class="math inline">\(t\)</span> that state <span class="math inline">\(s\)</span> is visited in <strong>an episode</strong>:</p><ul><li>Increment counter <span class="math inline">\(N(s) \leftarrow N(s) + 1\)</span></li><li>Increment total return <span class="math inline">\(S(s) \leftarrow S(s) + G_t\)</span></li></ul><p>Value is estimated by mean return <span class="math inline">\(V(s) = S(s) / N(s)\)</span>, by <em>law of large numbers</em>, <span class="math inline">\(V(s) \rightarrow v_\pi(s)\)</span> as <span class="math inline">\(N(s) \rightarrow \infty\)</span>.</p><p><strong>Every-Visit Monte-Carlo Policy Evaluation</strong></p><p>To evaluate state <span class="math inline">\(s\)</span>, <strong>every</strong> time-step <span class="math inline">\(t\)</span> that state <span class="math inline">\(s\)</span> is visited in <strong>an episode</strong>:</p><ul><li>Increment counter <span class="math inline">\(N(s) \leftarrow N(s) + 1\)</span></li><li>Increment total return <span class="math inline">\(S(s) \leftarrow S(s) + G_t\)</span></li></ul><p>Value is estimated by mean return <span class="math inline">\(V(s) = S(s) / N(s)\)</span>. Again, by <em>law of large numbers</em>, <span class="math inline">\(V(s) \rightarrow v_\pi(s)\)</span> as <span class="math inline">\(N(s) \rightarrow \infty\)</span>.</p><p><strong>Blackjack Example</strong></p><p>Please refer to https://www.wikiwand.com/en/Blackjack to learn the rule of <em>Blackjack</em>.</p><p>If we build an RL agent to play blackjack, the <strong>states</strong> would have 3-dimension:</p><ul><li>Current sum (12 - 21)<ul><li>We just consider this range because if the current sum is lower than 12, we will always take another card.</li></ul></li><li>Dealer's showing card (ace - 10)</li><li>Do I have a &quot;useable&quot; ace? (yes - no)</li></ul><p>So there would be 200 different states.</p><p>The actions are:</p><ul><li><strong>Stick</strong>: stop receiving cards and terminate</li><li><strong>Twist</strong>: take another card (no replacement)</li></ul><p>And <em>reward</em> for action</p><ul><li><strong>Stick</strong><ul><li><span class="math inline">\(+1\)</span> if sum of cards <span class="math inline">\(&gt;\)</span> sum of dealer cards</li><li><span class="math inline">\(0\)</span> if sum of cards <span class="math inline">\(=\)</span> sum of dealer cards</li><li><span class="math inline">\(-1\)</span> if sum of cards <span class="math inline">\(&lt;\)</span> sum of dealer cards</li></ul></li><li><strong>Twist</strong><ul><li><span class="math inline">\(-1\)</span> if sum of cards <span class="math inline">\(&gt; 21\)</span> and terminate</li><li><span class="math inline">\(0\)</span> otherwise</li></ul></li></ul><p>Transitions: automatically <em>twist</em> if sum of cards &lt; 12.</p><p>Policy: <strong>stick</strong> if sum of cards <span class="math inline">\(â‰¥ 20\)</span>, otherwise <strong>twist</strong>.</p><p><img src="/images/backjack.png"></p><p>In the above diagrams, the height represents the value function of that point. Since it's a simple policy, the value funtion achieves high value only if player sum is higher than 20.</p><p><strong>Incremental Mean</strong></p><p>The mean <span class="math inline">\(\mu_1, \mu_2, â€¦\)</span> of a sequence <span class="math inline">\(x_1, x_2, â€¦\)</span> can be computed incrementally, <span class="math display">\[\begin{align}\mu_k &amp; = \frac{1}{k}\sum^k_{j=1}x_j \\&amp; = \frac{1}{k}(x_k+\sum^{k-1}_{j=1}x_j) \\&amp;= \frac{1}{k}(x_k+(k-1)\mu_{k-1}) \\&amp;= \mu_{k-1}+\frac{1}{k}(x_k-\mu_{k-1}) \\\end{align}\]</span> which means the current mean equals to previous mean plus some error. The error is <span class="math inline">\(x_k - \mu_{k-1}\)</span> and the step-size is <span class="math inline">\(\frac{1}{k}\)</span>, which is dynamic.</p><p><strong>Incremental Monte-Carlo Updates</strong></p><p>Update <span class="math inline">\(V(s)\)</span> incrementally after episode <span class="math inline">\(S_1, A_1, R_2, â€¦, S_T\)</span>, for each state <span class="math inline">\(S_t\)</span> with return <span class="math inline">\(G_t\)</span>, <span class="math display">\[N(S_t) \leftarrow N(S_t) + 1\]</span></p><p><span class="math display">\[V(S_t)\leftarrow V(S_t)+\frac{1}{N(S_t)}(G_t-V(S_t))\]</span></p><p>In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes, <span class="math display">\[V(S_t)\leftarrow V(S_t) + \alpha(G_t-V(S_t))\]</span> So, that's the part for Monte-Carlo learning. It's a very simple idea: you run out an episode, look the complete return and update the mean value of the sample return for each state you have visited.</p><h2><span id="temporal-difference-learning">Temporal-Difference Learning</span></h2><p>Temporal-Difference (TD) methods learn directly from episodes of experiences, which means</p><ul><li>TD is <em>model-free</em>: no knowledge of MDP transitions / rewards</li><li>TD learns from <strong>incomplete</strong> episodes, by <em>bootstrapping</em>. (A major difference from MC method)</li><li>TD updates a guess towards a guess.</li></ul><p>Goal: learn <span class="math inline">\(v_\pi\)</span> online from experience under policy <span class="math inline">\(\pi\)</span>.</p><p><em>Incremental every-visit Monte-Carlo</em></p><ul><li>Update value <span class="math inline">\(V(S_t)\)</span> toward actual return <span class="math inline">\(\color{Red}{G_t}\)</span> <span class="math display">\[V(S_t)\leftarrow V(S_t)+\alpha (\color{Red}{G_t}-V(S_t))\]</span></li></ul><p>Simplest temporal-difference learning algorithm: <strong>TD(0)</strong></p><ul><li><p>Update value <span class="math inline">\(V(S_t)\)</span> towards <em>estimated</em> return <span class="math inline">\({\color{Red}{R_{t+1}+\gamma V(S_{t+1})}}\)</span> <span class="math display">\[V(S_t)\leftarrow V(S_t)+ \alpha ({\color{Red}{R_{t+1}+\gamma V(S_{t+1})}}-V(S_t))\]</span></p></li><li><p><span class="math inline">\(R_{t+1}+\gamma V(S_{t+1})â€‹\)</span> is called the <em>TD target</em>;</p></li><li><p><span class="math inline">\(\delta = R_{t+1}+\gamma V(S_{t+1})-V(S_t)\)</span> is called the <em>TD error</em>.</p></li></ul><p>Let's see a concret <strong>driving home example</strong>.</p><p><img src="/images/dheg.png"></p><p>The <em>Elapsed Time</em> shows the actual time that has spent, the <em>Predicted Time to Go</em> represents the predicted time to arrive home from current state, and the <em>Predicted Total Time</em> means the predicted time to arrive home from leaving office.</p><p><strong>Advantages and Disadvantages of MC vs. TD</strong></p><p>TD can learn <em>before</em> knowing the final outcome</p><ul><li>TD can learn online after every step</li><li>MC must wait until end of episode before return is known</li></ul><p>TD can learn <em>without</em> the final outcome</p><ul><li>TD can learn from incomplete sequences</li><li>MC can only learn from complete sequences</li><li>TD works in continuing (non-terminating) environments</li><li>MC only works for episodic (terminating) environments</li></ul><p>MC has high variance, zero bias</p><ul><li>Good convergence properties (even with function approximation)</li><li>Not very sensitive to initial value</li><li>Very simple to understand and use</li></ul><p>TD has low variance, some bias</p><ul><li>Usually more efficient than MC</li><li>TD(0) converges to <span class="math inline">\(v_\pi(s)\)</span> (but not always with function approximation)</li><li>More sensitive to initial value</li></ul><p><strong>Bias/Variance Trade-Off</strong></p><p>Return <span class="math inline">\(G_t = R_{t+1} + \gamma R_{t+2}+â€¦+\gamma^{T-1}R_T\)</span> is <strong>unbiased</strong> estimate of <span class="math inline">\(v_\pi(S_t)\)</span>.</p><p>True TD target <span class="math inline">\(R_{t+1}+\gamma v_\pi(S_{t+1})\)</span> is <strong>unbiased</strong> estimate of <span class="math inline">\(v_\pi(S_t)\)</span></p><blockquote><p>Explanation of bias and variance:</p><ul><li>The <a href="https://www.wikiwand.com/en/Bias_of_an_estimator" target="_blank" rel="noopener">bias of an estimator</a> is the difference between an estimator's expected value and the true value of the parameter being estimated.</li><li>A <strong>variance</strong> value of zero indicates that all values within a set of numbers are identical; all variances that are non-zero will be positive numbers. A large variance indicates that numbers in the set are far from the mean and each other, while a small variance indicates the opposite. Read more: <a href="https://www.investopedia.com/terms/v/variance.asp#ixzz51J8RTueh" target="_blank" rel="noopener">Variance</a></li></ul></blockquote><p>While TD target <span class="math inline">\(R_{t+1}+\gamma V(S_{t+1})\)</span> is <strong>biased</strong> estimate of <span class="math inline">\(v_\pi(S_t)\)</span>.</p><p>However, TD target is much lower <em>variance</em> than the return, since</p><ul><li>Return depends on <em>many</em> random actions, transitions, rewards</li><li>TD target depends on <em>one</em> random actions, transition, reward</li></ul><p><strong>Random Walk Example</strong></p><p><img src="/images/rweg.png"></p><p>There are several states on a street, the black rectangles are terminate states. Each transition has 0.5 probability and the reward is marked on the line. The question is what is the value function of each state?</p><p>Using <em>TD</em> to solve the problem:</p><p><img src="/images/rwtd.png"></p><p>The x-axis represents each state, y-axis represent the estimated value. Each line represents the result of TD algorithm that run different episodes. We can see, at the begining, all states have initial value <span class="math inline">\(0.5\)</span>. After 100 episodes, the line converges to diagonal, which is the true values.</p><p>Using <em>MC</em> to solve the problem:</p><p><img src="/images/rwmc.png"></p><p>The x-axis represents the number of episodes that algorithm takes. The y-axis shows the error of the algorithm. The black lines shows using MC methods with different step-size, while the grey lines below represents using TD methods with different step-size. We can see TD methods are more efficient than MC methods.</p><p><strong>Batch MC and TD</strong></p><p>We know that MC and TD converge: <span class="math inline">\(V(s) \rightarrow v_\pi(s)\)</span> as experience <span class="math inline">\(\rightarrow \infty\)</span>. But what about batch solution for finite experience? If we <strong>repeatly</strong> train some <em>finite</em> sample episodes with MC and TD respectively, do the two algorithms give <strong>same</strong> result?</p><p><em>AB Example</em></p><p>To get more intuition, let's see the <em>AB</em> example.</p><p>There are two states in a MDP, <span class="math inline">\(A, B\)</span> with no discounting. And we have 8 episodes of experience:</p><ul><li>A, 0, B, 0</li><li>B, 1</li><li>B, 1</li><li>B, 1</li><li>B, 1</li><li>B, 1</li><li>B, 1</li><li>B, 0</li></ul><p>For example, the first episode means we in state <span class="math inline">\(A\)</span> and get <span class="math inline">\(0\)</span> reward, then transit to state <span class="math inline">\(B\)</span> getting <span class="math inline">\(0\)</span> reward, and then terminate.</p><p>So, What is <span class="math inline">\(V(A), V(B)\)</span> ?</p><p>First, let's consider <span class="math inline">\(V(B)\)</span>. <span class="math inline">\(B\)</span> state shows 8 times and 6 of them get reward <span class="math inline">\(1\)</span>, 2 of them get reward <span class="math inline">\(0\)</span>. So <span class="math inline">\(V(B) = \frac{6}{8} = 0.75\)</span> according to TD and MC.</p><p>However, if we consider <span class="math inline">\(V(A)\)</span>, MC method will give <span class="math inline">\(V(A) = 0\)</span>, since <span class="math inline">\(A\)</span> just shows in one episode and the reward of that episode is <span class="math inline">\(0\)</span>. TD method will give <span class="math inline">\(V(A) = 0 + V(B) = 0.75\)</span>.</p><p>The MDP of these experiences can be illustrated as</p><p><img src="/images/abmdp.png"></p><p><strong>Certainty Equivalence</strong></p><p>As we show above,</p><ul><li><p><strong>MC</strong> converges to solution with <strong>minimum mean-squared error</strong></p><ul><li><p>Best fit to the <strong>observed returns</strong> <span class="math display">\[\sum^K_{k=1}\sum^{T_k}_{t=1}(G^k_t-V(s^k_t))^2\]</span></p></li><li><p>In the AB example, <span class="math inline">\(V(A) = 0\)</span></p></li></ul></li><li><p><strong>TD(0)</strong> converges to solution of <strong>max likelihood Markov model</strong></p><ul><li><p>Solution to the <strong>MDP <span class="math inline">\(&lt;\mathcal{S, A, P, R, }\gamma&gt;\)</span> that best fits the data</strong></p><p><img src="/images/cemath.png"></p><p>(First, count the transitions. Then compute rewards.)</p></li><li><p>In the AB example, <span class="math inline">\(V(A) = 0.75\)</span></p></li></ul></li></ul><p><strong>Advantages and Disadvantages of MC vs. TD (2)</strong></p><ul><li>TD exploits <strong>Markov property</strong><ul><li>Usually more efficient in Markov environments</li></ul></li><li>MC does <strong>not</strong> exploit Markov property<ul><li>Usually more effective in non-Markov environments</li></ul></li></ul><h3><span id="unified-view">Unified View</span></h3><p><strong>Monte-Carlo Backup</strong></p><p><img src="/images/mcbackup.png"></p><p>We start from <span class="math inline">\(S_t\)</span> to look-ahead and build a look-ahead tree. What Monte-Carlo do is to sample a episode until it terminates and use the episode to update the value of state <span class="math inline">\(S_t\)</span>.</p><p><strong>Temporal-Difference Backup</strong></p><p><img src="/images/tdbackup.png"></p><p>On the contrary, TD backup just sample one-step ahead and use the value of <span class="math inline">\(S_{t+1}\)</span> to update <span class="math inline">\(S_t\)</span>.</p><p><strong>Dynamic Programming Backup</strong></p><p><img src="/images/dpbackup.png"></p><p>In dynamic programming backup, we do not sample. Since we know the environment, we look all possible one-step ahead and weighted them to update the value of <span class="math inline">\(S_t\)</span>.</p><p><strong>Bootstrapping and Sampling</strong></p><ul><li><strong>Bootstrapping</strong>: update involves an estimate<ul><li>MC does not bootstrap</li><li>DP bootstraps</li><li>TD bootstraps</li></ul></li><li><strong>Sampling</strong>: update samples an expectation<ul><li>MC samples</li><li>DP does not sample</li><li>TD samples</li></ul></li></ul><p><strong>Unified View of Reinforcement Learning</strong></p><p><img src="/images/uvrl.png"></p><h2><span id="tdlambda">TD(<span class="math inline">\(\lambda\)</span>)</span></h2><p>Let TD target look <span class="math inline">\(n\)</span> steps into the future,</p><figure><img src="/images/tdlam.png" alt="ds"><figcaption>ds</figcaption></figure><p>Consider the following <span class="math inline">\(n\)</span>-step returns for <span class="math inline">\(n = 1, 2, â€¦, \infty\)</span>:</p><p><img src="/images/tdlamret.png"></p><p>Define the <span class="math inline">\(n\)</span>-step return <span class="math display">\[G_t^{(n)} = R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^n V(S_{t+n})\]</span> <span class="math inline">\(n\)</span>-step temporal-difference learning: <span class="math display">\[V(S_t)\leftarrow V(S_t)+\alpha (G_t^{(n)}-V(S_t))\]</span> We know that <span class="math inline">\(n \in [1, \infty)\)</span>, but which <span class="math inline">\(n\)</span> is the best?</p><p>There are some experiments about that:</p><p><img src="/images/rmn.png"></p><p>So, you can see that the optimal <span class="math inline">\(n\)</span> changes with on-line learning and off-line leanring. If the MDP changes, the best <span class="math inline">\(n\)</span> also changes. Is there a robust algorithm to fit any different situation?</p><h3><span id="forward-view-tdlambda">Forward View TD(<span class="math inline">\(\lambda\)</span>)</span></h3><p><strong>Averaging n-step Returns</strong></p><p>We can average n-step returns over different <span class="math inline">\(n\)</span>, e.g. average the 2-step and 4-step returns: <span class="math display">\[\frac{1}{2}G^{(2)}+\frac{1}{2}G^{(4)}\]</span> But can we efficiently combine information from all time-steps?</p><p>The answer is yes.</p><p><strong><span class="math inline">\(\lambda\)</span>-return</strong></p><p><img src="/images/tdlambda.png"></p><p>The <span class="math inline">\(\lambda\)</span>-return <span class="math inline">\(G_t^{\lambda}\)</span> combines all n-step returns <span class="math inline">\(G_t^{(n)}\)</span> using weight <span class="math inline">\((1-\lambda)\lambda^{n-1}\)</span>: <span class="math display">\[G_t^\lambda = (1-\lambda)\sum^\infty_{n=1}\lambda^{n-1}G_t^{(n)}\]</span> <strong>Forward-view</strong> <span class="math inline">\(TD(\lambda)\)</span>, <span class="math display">\[V(S_t) \leftarrow V(S_t) + \alpha (G_t^\lambda-V(S_t))\]</span> <img src="/images/tdgeo.png"></p><p>We can see the weight decay geometrically and the weights sum to 1.</p><p>The reason we use geometrical decay rather than other weight because it's efficient to compute, we can compute TD(<span class="math inline">\(\lambda\)</span>) as efficient as TD(0).</p><p><img src="/images/forwardtd.png"></p><p><strong>Forward-view</strong> <span class="math inline">\(TD(\lambda)\)</span></p><ul><li>Updates value function towards the <span class="math inline">\(\lambda\)</span>-return</li><li>Looks into the future to compute <span class="math inline">\(G_t^\lambda\)</span></li><li>Like MC, can only be computed from <strong>complete episodes</strong></li></ul><p><img src="/images/fortdlam.png"></p><h3><span id="backward-view-tdlambda">Backward View TD(<span class="math inline">\(\lambda\)</span>)</span></h3><p><strong>Eligibility Traces</strong></p><p><img src="/images/bellexe.png"></p><p>Recall the <a href="https://www.52coding.com.cn/index.php?/Articles/single/69#header-n50">rat example</a> in lecture 1, credit assignment problem: did bell or light cause shock?</p><ul><li><strong>Frequency heuristic</strong>: assign credit to most frequent states</li><li><strong>Recency heuristic</strong>: assign credit to most recent states</li></ul><p><em>Eligibility traces</em> combine both heuristics.</p><p><img src="/images/egt.png"></p><p>If visit state <span class="math inline">\(s\)</span>, <span class="math inline">\(E_t(s)\)</span> plus <span class="math inline">\(1\)</span>; otherwise <span class="math inline">\(E_t(s)\)</span> decay exponentially.</p><p><strong>Backward View TD(<span class="math inline">\(\lambda\)</span>)</strong></p><ul><li>Keep an eligibility trace for every state <span class="math inline">\(s\)</span></li><li>Update value <span class="math inline">\(V(s)\)</span> for every state <span class="math inline">\(s\)</span> in proportion to TD-error <span class="math inline">\(\delta_t\)</span> and eligibility trace <span class="math inline">\(E_t(s)\)</span></li></ul><p><span class="math display">\[\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)\]</span></p><p><span class="math display">\[V(s)\leftarrow V(s)+\alpha \delta_tE_t(s)\]</span></p><p><img src="/images/bvtdlam.png"></p><p>When <span class="math inline">\(\lambda = 0\)</span>, only current state is updated, which is exactly equivalent to TD(0) update: <span class="math display">\[E_t(s) = 1(S_t = s)\]</span></p><p><span class="math display">\[V(s)\leftarrow V(s)+\alpha\delta_tE_t(s) = V(S_t)+\alpha\delta_t\]</span></p><p>When <span class="math inline">\(\lambda = 1\)</span>, credit is deferred until end of episode, total update for TD(1) is the same as total update for MC.</p><h3><span id="relationship-between-forward-and-backward-td">Relationship Between Forward and Backward TD</span></h3><blockquote><p><strong>Theorem</strong></p><p>The sum of offline updates is identical for forward-view and backward-view TD(<span class="math inline">\(\lambda\)</span>) <span class="math display">\[\sum^T_{t=1}\alpha\delta_tE_t(s)=\sum^T_{t=1}\alpha(G_t^\lambda-V(S_t))1(S_t=s)\]</span></p></blockquote><p><strong>MC and TD(1)</strong></p><p>Consider an episode where <span class="math inline">\(s\)</span> is visited once at time-step <span class="math inline">\(k\)</span>, TD(1) eligiblity trace discounts time since visit, <span class="math display">\[E_t(s) = \gamma E_{t-1}(s)+1(S_t = s) = \begin{cases} 0,  &amp; \mbox{if }t&lt;k \\\gamma^{t-k}, &amp; \mbox{if }tâ‰¥k\end{cases}\]</span> TD(1) updates accumulate error <em>online</em> <span class="math display">\[\sum^{T-1}_{t=1}\alpha\delta_tE_t(s)=\alpha\sum^{T-1}_{t=k}\gamma^{t-k}\delta_t\]</span> By end of episode it accumulates total error <span class="math display">\[\begin{align}\mbox{TD(1) Error}&amp;= \delta_k+\gamma\delta_{k+1}+\gamma^2\delta_{k+2}+...+\gamma^{T-1-k}\delta_{T-1} \\&amp; = R_{t+1}+\gamma V(S_{t+1}) -V(S_t) \\&amp;+ \gamma R_{t+2}+\gamma^2V(S_{t+2}) - \gamma V(S_{t+1})\\&amp;+ \gamma^2 R_{t+3}+\gamma^3V(S_{t+3}) - \gamma^2 V(S_{t+2})\\&amp;+\ ... \\&amp;+ \gamma^{T-1-t}R_T+\gamma^{T-t}V(S_T)-\gamma^{T-1-t}V(S_{T-1})\\&amp;= R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3} ... + \gamma^{T-1-t}R_T-V(S_t)\\&amp;= G_t-V(S_t)\\&amp;= \mbox{MC Error}\end{align}\]</span> TD(1) is roughly equivalent to every-visit Monte-Carlo, error is accumulated online, step-by-step.</p><p>If value function is only updated offline at end of episode, then total update is exactly the same as MC.</p><p><strong>Forward and Backward Equivalence</strong></p><p>For general <span class="math inline">\(\lambda\)</span>, TD errors also telescope to <span class="math inline">\(\lambda\)</span>-error, <span class="math inline">\(G_t^\lambda-V(S_t)\)</span></p><p><img src="/images/teltdlam.png"></p><p>Consider an episode where <span class="math inline">\(s\)</span> is visited once at time-step <span class="math inline">\(k\)</span>, TD(<span class="math inline">\(\lambda\)</span>) eligibility trace discounts time since visit, <span class="math display">\[E_t(s) = \gamma\lambda E_{t-1}(s)+1(S_t = s) = \begin{cases} 0,  &amp; \mbox{if }t&lt;k \\(\gamma\lambda)^{t-k}, &amp; \mbox{if }tâ‰¥k\end{cases}\]</span> Backward TD(<span class="math inline">\(\lambda\)</span>) updates accumulate error <em>online</em> <span class="math display">\[\sum^{T-1}_{t=1}\alpha\delta_tE_t(s)=\alpha\sum^{T-1}_{t=k}(\gamma\lambda)^{t-k}\delta_t = \alpha(G_k^\lambda-V(S_k))\]</span> By end of episode it accumulates total error for <span class="math inline">\(\lambda\)</span>-return.</p><p>For multiple visits to <span class="math inline">\(s\)</span>, <span class="math inline">\(E_t(s)\)</span> accumulates many errors.</p><p><strong>Offline</strong> Updates</p><ul><li>Updates are accumulated within episode but applied in batch at the end of episode</li></ul><p><strong>Online</strong> Updates</p><ul><li>TD(<span class="math inline">\(\lambda\)</span>) updates are applied online at each step within episode, forward and backward view TD(<span class="math inline">\(\lambda\)</span>) are slightly different.</li></ul><p>In summary,</p><p><img src="/images/tdsum.png"></p><ul><li>Forward view provides <strong>theory</strong></li><li>Backward view provids <strong>mechanism</strong><ul><li>update online, every step, from incomplete sequences</li></ul></li></ul><p>This lecture just talks about how to evaluate a policy given an unknown MDP. Next lecture will introduce Model-free Control.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Last lecture, David taught us how to solve a &lt;em&gt;known&lt;/em&gt; MDP, which is &lt;em&gt;planning by dynamic programming&lt;/em&gt;. In this lecture, we will learn how to estimate the value function of an &lt;strong&gt;unknown&lt;/strong&gt; MDP, which is &lt;em&gt;model-free prediction&lt;/em&gt;. And in the next lecture, we will &lt;em&gt;optimise&lt;/em&gt; the value function of an unknown MDP.&lt;/p&gt;
    
    </summary>
    
      <category term="å­¦ä¹ ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="å¢å¼ºå­¦ä¹ " scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Model-Free" scheme="http://www.52coding.com.cn/tags/Model-Free/"/>
    
      <category term="Monte-Carlo Learning" scheme="http://www.52coding.com.cn/tags/Monte-Carlo-Learning/"/>
    
      <category term="TD" scheme="http://www.52coding.com.cn/tags/TD/"/>
    
  </entry>
  
  <entry>
    <title>RL - Planning by Dynamic Programming</title>
    <link href="http://www.52coding.com.cn/2017/12/07/RL%20-%20Planning%20by%20Dynamic%20Programming/"/>
    <id>http://www.52coding.com.cn/2017/12/07/RL - Planning by Dynamic Programming/</id>
    <published>2017-12-07T07:48:19.000Z</published>
    <updated>2018-11-06T03:47:48.747Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#introduction">Introduction</a></li><li><a href="#policy-evaluation">Policy Evaluation</a></li><li><a href="#policy-iteration">Policy Iteration</a></li><li><a href="#value-iteration">Value Iteration</a></li><li><a href="#extentions-to-dynamic-programming">Extentions to Dynamic Programming</a></li><li><a href="#contraction-mapping">Contraction Mapping</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="introduction">Introduction</span></h2><p><strong>What is Dynamic Programming?</strong></p><p><strong>Dynamic</strong>: sequential or temporal component to the problem <strong>Programming</strong>: optimising a &quot;program&quot;, i.e. a policy</p><ul><li>c.f. linear programming</li></ul><p>So, Dynamic Programming is a method for solving complex problems by breaking them down into <strong>subproblems</strong>.</p><ul><li>Solve the subproblems</li><li>Combine solutions to subproblems</li></ul><p>Dynamic Programming is a very general solution method for problems which have two properties:</p><ul><li><strong>Optimal substructure</strong><ul><li><em>Principle of optimality applies</em></li><li>Optimal solution can be decomposed into subproblems</li></ul></li><li><strong>Overlapping subproblems</strong><ul><li>Subproblems recur many times</li><li>Solution can be cached and reused</li></ul></li></ul><p><strong>Markov decision processes</strong> satisfy both properties:</p><ul><li><strong>Bellman euqtion</strong> gives recursive decomposition</li><li><strong>Value function</strong> stores and reuses solutions</li></ul><p><strong>Planning by Dynamic Programming</strong></p><p><strong>Planning</strong> means dynamic programming assumes full knowledge of the MDP.</p><ul><li>For prediction (Policy Evaluation):<ul><li>Input: MDP<span class="math inline">\(&lt;S,A,P,R,\gamma&gt;\)</span> and policy <span class="math inline">\(\pi\)</span></li><li>Output: value function <span class="math inline">\(v_{\pi}\)</span></li></ul></li><li>For <strong>control</strong>:<ul><li>Input: MDP<span class="math inline">\(&lt;S,A,P,R,\gamma&gt;\)</span></li><li>Output: optimal value function <span class="math inline">\(v_*\)</span> and optimal policy <span class="math inline">\(\pi_*\)</span></li></ul></li></ul><p>We first learn how to evaluate a policy and then put it into a loop to find the optimal policy.</p><h2><span id="policy-evaluation">Policy Evaluation</span></h2><ul><li><p>Problem: evaluate a given policy <span class="math inline">\(\pi\)</span></p></li><li><p>Solution: iterative application of <strong>Bellman expectation equation</strong></p></li><li><p><span class="math inline">\(v_1\)</span> -&gt; <span class="math inline">\(v_2\)</span> -&gt; <span class="math inline">\(v_3\)</span> -&gt; â€¦ -&gt; <span class="math inline">\(v_\pi\)</span></p></li><li><p><em>Synchronous</em> backups</p><ul><li><p>At each iteration <span class="math inline">\(k+1\)</span></p></li><li><p>For all states <span class="math inline">\(s \in S\)</span></p></li><li><p>Update <span class="math inline">\(v_{k+1}(s)\)</span> from <span class="math inline">\(v_k(s&#39;)\)</span>, where <span class="math inline">\(s&#39;\)</span> is a successor state of <span class="math inline">\(s\)</span></p><p><img src="/images/vpi2.png"> <span class="math display">\[v_{k+1}(s)=\sum_{a\in\mathcal{A}}\pi(a|s)(\mathcal{R}^a_s+\gamma\sum_{s&#39;\in\mathcal{S}}P^a_{ss&#39;}v_k(s&#39;))\]</span></p><p><span class="math display">\[v^{k+1}=\mathcal{R}^\pi+\gamma \mathcal{P}^\pi v^k\]</span></p></li></ul></li></ul><p><em>Example</em>: Evaluating a Random Policy in the Small Gridworld</p><p><img src="/images/grid.png"></p><ul><li><p>Actions are move North/East/South/West for one grid.</p></li><li><p>Undiscounted episodic MDP (<span class="math inline">\(\gamma = 1\)</span>)</p></li><li><p>Nontermial states <span class="math inline">\(1, â€¦, 14\)</span></p></li><li><p>One terminal State (shown twice as shaded squares)</p></li><li><p>Reward is <span class="math inline">\(-1\)</span> until the terminal state is reahed</p></li><li><p>Agent follows uniform random policy <span class="math display">\[\pi(n|\cdot)=\pi(e|\cdot)=\pi(s|\cdot)=\pi(w|\cdot) = 0.25\]</span></p></li></ul><p>Let's use dynamic programming to solve the MDP.</p><p><img src="/images/ipe1.png"></p><p><img src="/images/ipe2.png"></p><p>The grids on the left show the value function of each state, the update rule shown by the illustration. Finally, it converges to the true value function of the policy. It basically tell us <em>if we take the random walk under the policy, how much reward on average we will get when we reach the terminal state</em>.</p><p>The right-hand column shows how to find better policy with respect to the value funtions.</p><h2><span id="policy-iteration">Policy Iteration</span></h2><p>How to improve a Policy</p><ul><li><p>Given a policy <span class="math inline">\(\pi\)</span></p><ul><li><p><strong>Evaluate</strong> the policy <span class="math inline">\(\pi\)</span> <span class="math display">\[v_\pi(s) = E[R_{t+1}+\gamma R_{t+2} + ... | S_t = s]\]</span></p></li><li><p><strong>Improve</strong> the policy by acting <em>greedily</em> with respect to <span class="math inline">\(v_\pi\)</span> <span class="math display">\[\pi&#39;=greddy(v_\pi)\]</span></p></li></ul></li><li><p>In general, need more iterations of improvement / evaluation</p></li><li><p>But this process of <em>policy iteration</em> always converges to <span class="math inline">\(\pi_*\)</span></p></li></ul><p><img src="/images/pi.png"></p><p><strong>Demonstration</strong></p><p>Consider a deterministic policy <span class="math inline">\(a = \pi(s)\)</span>, we can improve the policy by acting greedily <span class="math display">\[\pi&#39;(s) = \arg\max_{a\in\mathcal{A}}q_\pi(s, a)\]</span> (Note: <span class="math inline">\(q_\pi\)</span> is the action value function following policy <span class="math inline">\(\pi\)</span>)</p><p>This improves the value from any state <span class="math inline">\(s\)</span> over one step, <span class="math display">\[q_\pi(s, \pi&#39;(s)) = \max_{a\in\mathcal{A}}q_\pi(s,a)â‰¥q_\pi(s, \pi(s))=v_\pi(s)\]</span> (Note: <span class="math inline">\(q_\pi(s, \pi&#39;(s))\)</span> means the action value of taking one step following policy <span class="math inline">\(\pi&#39;\)</span> then following policy <span class="math inline">\(\pi\)</span> forever.)</p><p>If therefore improves the value function, <span class="math inline">\(v_{\pi&#39;}(s) â‰¥ v_\pi (s)\)</span> <span class="math display">\[\begin{align}v_\pi(s) &amp; â‰¤ q_\pi(s, \pi&#39;(s))=E_{\pi&#39;}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t = s]  \\&amp; â‰¤ E_{\pi&#39;}[R_{t+1} + \gamma q_\pi(S_{t+1}, \pi&#39;(S_{t+1}))|S_t=s] \\&amp;â‰¤ E_{\pi&#39;}[R_{t+1} + \gamma R_{t+2} + \gamma^2q_\pi(S_{t+2}, \pi&#39;(S_{t+2}))|S_t = s]\\&amp;â‰¤ E_{\pi&#39;}[R_{t+1} + \gamma R_{t+2} + ..... | S_t = s] = v_{\pi&#39;}(s)\end{align}\]</span> (Unroll the equation to the second, third â€¦ step by taking the Bellman euqation into it.)</p><p>If improvements stop, <span class="math display">\[q_\pi(s, \pi&#39;(s)) = \max_{a\in\mathcal{A}}q_\pi(s,a) = q_\pi(s, \pi(s)) = v_\pi(s)\]</span> Then the <strong>Bellman optimality</strong> equation has been satisfied <span class="math display">\[v_\pi(s) = \max_{a\in\mathcal{A}}q_\pi(s, a)\]</span> Therefore <span class="math inline">\(v_\pi(s) = v_*(s)\)</span> for all <span class="math inline">\(s \in \mathcal{S}\)</span>, so <span class="math inline">\(\pi\)</span> is an optimal policy.</p><p><strong>Early Stopping</strong></p><p>Question: Does policy evaluation need to converge to <span class="math inline">\(v_\pi\)</span> ?</p><ul><li>e.g. in the small gridworld <span class="math inline">\(k = 3\)</span> was sufficient to acheive optimal policy</li></ul><p>Or shoule we introduce a stopping condition</p><ul><li>e.g. <span class="math inline">\(\epsilon\)</span>-convergence of value function</li></ul><p>Or simply stop after <span class="math inline">\(k\)</span> iterations of iterative policy evaluation?</p><h2><span id="value-iteration">Value Iteration</span></h2><p><strong>Principle of Optimality</strong></p><p>Any optimal policy can be subdivided into two components:</p><ul><li>An optimal first action <span class="math inline">\(A_*\)</span></li><li>Followed by an optimal policy from successor state <span class="math inline">\(S&#39;\)</span></li></ul><blockquote><p>Theorem: Principle of Optimality</p><p>A policy <span class="math inline">\(\pi(a|s)\)</span> achieves the optimal value from state <span class="math inline">\(s\)</span>, <span class="math inline">\(v_\pi(s) = v_*(s)\)</span> if and only if</p><ul><li>For any state <span class="math inline">\(s&#39;\)</span> reachable from <span class="math inline">\(s\)</span>, <span class="math inline">\(\pi\)</span> achieves the optimal value from state <span class="math inline">\(s&#39;\)</span></li></ul></blockquote><p>If we know the solution to subproblems <span class="math inline">\(v_\ast(s&#39;)\)</span>, then solution <span class="math inline">\(v_\ast(s)\)</span> can be found by one-step look ahead: <span class="math display">\[v_\ast(s) \leftarrow \max_{a\in\mathcal{A}}\mathcal{R}^a_s+\gamma \sum_{s&#39;\in \mathcal{S}}P^a_{ss&#39;}v_\ast(s&#39;)\]</span> The idea of value iteration is to apply these updates iteratively.</p><ul><li>Intuition: start with final rewards and work backwards</li><li>Still works with loopy, stochatis MDPs</li></ul><p><strong>Example: Shortest Path</strong></p><p><img src="/images/grid2.png"></p><ul><li>The goal state is on the left-up corner</li><li>Each step get -1 reward</li><li>The number showed in each grid is the value of that state</li><li>At each iteration, update all states</li></ul><p><strong>Value Iteration</strong></p><ul><li>Problem: find optimal policy <span class="math inline">\(\pi\)</span></li><li>Solution: iterative application of Bellman optimality backup</li><li><span class="math inline">\(v_1 \rightarrow v_2 \rightarrow â€¦ \rightarrow v_*\)</span></li><li><em>Synchronous</em> backups<ul><li>At each iteration <span class="math inline">\(k+1\)</span></li><li>For all states <span class="math inline">\(s\in \mathcal{S}\)</span></li><li>Update <span class="math inline">\(v_{k+1}(s)\)</span> from <span class="math inline">\(v_k(s&#39;)\)</span></li></ul></li><li>Convergence to <span class="math inline">\(v_*\)</span> will be proven later</li><li>Unlike policy iteration, there is no explicit policy</li><li>Intermediate value functions may not correspond to any policy</li></ul><p><strong>Synchronous Dynamic Programming Algorithms</strong></p><table><colgroup><col style="width: 12%"><col style="width: 51%"><col style="width: 35%"></colgroup><thead><tr class="header"><th>problem</th><th>bellman equation</th><th>algorithm</th></tr></thead><tbody><tr class="odd"><td>Prediction</td><td>Bellman Expectation Equation</td><td>Iterative Policy Evaluation</td></tr><tr class="even"><td>Control</td><td>Bellman Expectation Equation + Greedy Policy Improvement</td><td>Policy Iteration</td></tr><tr class="odd"><td>Control</td><td>Bellman Optimatility Equation</td><td>Value Iteration</td></tr></tbody></table><p>Algorithms are based on state-value function <span class="math inline">\(v_\pi(s)\)</span> or <span class="math inline">\(v_*(s)\)</span></p><ul><li><span class="math inline">\(O(mn^2)\)</span> per iteration, for <span class="math inline">\(m\)</span> actions and <span class="math inline">\(n\)</span> states</li></ul><p>Could also apply to action-value function <span class="math inline">\(q_\pi(s, a)\)</span> or <span class="math inline">\(q_*(s, a)\)</span></p><ul><li><span class="math inline">\(O(m^2n^2)\)</span> per iteration</li></ul><h2><span id="extentions-to-dynamic-programming">Extentions to Dynamic Programming</span></h2><p><strong>Asynchronous Dynamic Programming</strong></p><p><em>Asynchronous DP</em> backs up states individually, in any order. For each selected state, apply the appropriate backup, which can significantly reduce computation. It also guaranteed to converge if all states continue to be selected.</p><p>Three simple ideas for asynchronous dynamic programming:</p><ul><li><p><em>In-place</em> dynamic programming</p><p><img src="/images/in-place.png"></p></li><li><p><em>Prioritised sweeping</em></p><p><img src="/images/ps.png"></p></li><li><p><em>Real-time</em> dynamic programming</p><p><img src="/images/real-time.png"></p></li></ul><p><strong>Full-Width Backups</strong></p><p>DP uses <em>full-width</em> backups</p><ul><li><em>full-width</em> means when we look aheah, we consider all branches(actions) that could happen</li><li>For each backup (sync or async)<ul><li>Every successor state and action is considered</li><li>Using knowledge of the MDP transitions and reward function</li></ul></li></ul><p><img src="/images/fw.png"></p><h2><span id="contraction-mapping">Contraction Mapping</span></h2><p>Information about <em>contraction mapping theorem</em>, please refer to http://www.math.uconn.edu/~kconrad/blurbs/analysis/contraction.pdf</p><p>Consider the vector space <span class="math inline">\(\mathcal{V}\)</span> over value functions. There are <span class="math inline">\(|\mathcal{S}|\)</span> dimensions.</p><ul><li>Each point in this space fully specifies a value function <span class="math inline">\(v(s)\)</span></li></ul><p>We will measure distance between state-value functions <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> by the <span class="math inline">\(\infty\)</span>-norm. <span class="math display">\[||u-v||_\infty = \max_{s\in\mathcal{S}}|u(s)-v(s)|\]</span> <em>Bellman Expectation Backup is a Contraction</em></p><p>Define the <em>Bellman expectation backup operator</em> <span class="math inline">\(T^\pi\)</span>, <span class="math display">\[T^\pi(v) = \mathcal{R}^\pi + \gamma\mathcal{P}^\pi v\]</span> This operator is a <span class="math inline">\(\gamma\)</span>-contraction, it makes value functions closer bt at least <span class="math inline">\(\gamma\)</span>, <span class="math display">\[\begin{align}||T^\pi(u)-T^\pi(v)||_\infty &amp;= ||(\mathcal{R}^\pi + \gamma\mathcal{P}^\pi v) - (\mathcal{R}^\pi + \gamma\mathcal{P}^\pi u)||_\infty \\&amp;= ||\gamma P^\pi(u-v)||_\infty\\&amp;â‰¤||\gamma P^\pi||u-v||_\infty||_\infty\\&amp;â‰¤\gamma||u-v||_\infty\end{align}\]</span></p><blockquote><p>Theorem: <strong>Contraction Mapping Theorem</strong></p><p>For any metric space <span class="math inline">\(\mathcal{V}\)</span> that is complete (closed) under an operator <span class="math inline">\(T(v)\)</span>, where <span class="math inline">\(T\)</span> is a <span class="math inline">\(\gamma\)</span>-contraction,</p><ul><li><span class="math inline">\(T\)</span> converges to a unique fixed point</li><li>At a linear convergence rate of <span class="math inline">\(\gamma\)</span></li></ul></blockquote><p><strong>Convergence of Iterative Policy Evaluation and Policy Iteration</strong></p><p>The Bellman expectation operator <span class="math inline">\(T^\pi\)</span> has a unique fixed point <span class="math inline">\(v_\pi\)</span>.</p><p>By contraction mapping theorem,</p><ul><li>Iterative policy evaluation converges on <span class="math inline">\(v_\pi\)</span>;</li><li>Policy iteration converges on <span class="math inline">\(v_*\)</span>.</li></ul><p><em>Bellman Optimality Backup is a Contraction</em></p><p>Define the <em>Bellman Optimality backup operator</em> <span class="math inline">\(T^\ast\)</span>, <span class="math display">\[T^\ast(v) = \max_{a\in\mathcal{A}}\mathcal{R}^a+\gamma \mathcal{P}^av\]</span> This operator is a <span class="math inline">\(\gamma\)</span>-contraction, it makes value functions closer by at least <span class="math inline">\(\gamma\)</span>, <span class="math display">\[||T^\ast(u)-T\ast(v)||_\inftyâ‰¤\gamma ||u-v||_\infty\]</span> <strong>Convergence of Value Iteration</strong></p><p>The Bellman optimality operator <span class="math inline">\(T^âˆ—\)</span> has a unique fixed point <span class="math inline">\(v_*\)</span>.</p><p>By contraction mapping theorem, value iteration converges on <span class="math inline">\(v_*\)</span>.</p><p>In summary, what does a Bellman backup do to points in value function space is to bring value functions <em>closer</em> to a unique fixed point. And therefore the backups must converge on a unique solution.</p><p>This lecture (note) introduces how to use dynamic programming to solve <em>planning</em> problems. Next lecture will introduce model-free prediction, which is a really RL problem.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#policy-evaluation&quot;&gt;Policy Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#policy-iteration&quot;&gt;Policy Iteration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#value-iteration&quot;&gt;Value Iteration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#extentions-to-dynamic-programming&quot;&gt;Extentions to Dynamic Programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#contraction-mapping&quot;&gt;Contraction Mapping&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="å­¦ä¹ ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="å¢å¼ºå­¦ä¹ " scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="MDP" scheme="http://www.52coding.com.cn/tags/MDP/"/>
    
      <category term="Dynamic Programming" scheme="http://www.52coding.com.cn/tags/Dynamic-Programming/"/>
    
      <category term="Policy Iteration" scheme="http://www.52coding.com.cn/tags/Policy-Iteration/"/>
    
      <category term="Value Iteration" scheme="http://www.52coding.com.cn/tags/Value-Iteration/"/>
    
  </entry>
  
  <entry>
    <title>RL - Markov Decision Processes</title>
    <link href="http://www.52coding.com.cn/2017/08/18/RL%20-%20Markov%20Decision%20Processes/"/>
    <id>http://www.52coding.com.cn/2017/08/18/RL - Markov Decision Processes/</id>
    <published>2017-08-18T07:02:19.000Z</published>
    <updated>2018-11-06T03:47:34.671Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#markov-processes">Markov Processes</a></li><li><a href="#markov-reward-process">Markov Reward Process</a></li><li><a href="#markov-decision-process">Markov Decision Process</a></li></ul><!-- tocstop --><h2><span id="markov-processes">Markov Processes</span></h2><p>Basically, <strong>Markov decision processes</strong> formally describe an environment for reinforcement learning, where the environment is <strong>fully observable</strong>, which means the current state completely characterises the process.</p><a id="more"></a><p>Almost all RL problems can be formalised as MDPs, e.g.</p><ul><li>Optimal control primarily deals with continuous MDPs</li><li>Partially observable problems can be converted into MDPs</li><li>Bandits are MDPs with one state</li></ul><p>So, if we solve MDP, we can solve all above RL problems.</p><p><strong>Markov Property</strong></p><p>Markov Property is &quot;The future is independent of the past given the present&quot;, like <a href="https://www.52coding.com.cn/index.php?/Articles/single/69">last note</a> said. The formal definition is: <span class="math display">\[P[S_{t+1}|S_t]=P[S_{t+1}|S_1, ..., S_t]\]</span> where <span class="math inline">\(S\)</span> represents a state.</p><p>The formula means the current can capture all relevant information from the history. Once the state is known, the history may be thrown away, i.e. the state is a sufficient statistic of the future.</p><p><strong>State Transition Matrix</strong></p><p>We know that given the current state, we can use its information to reach the next state, but how? â€” It is characterized by the <em>state transition probability</em>.</p><p>For a Markov state <span class="math inline">\(s\)</span> and successor state <span class="math inline">\(s&#39;\)</span> , the state transition probability is deï¬ned by <span class="math display">\[P_{ss&#39;}=P[S_{t+1}=s&#39;|S_t=s]\]</span> We can put all of the probabities into a matrix called transition matrix, denoted by <span class="math inline">\(P\)</span> : <span class="math display">\[P = \begin{bmatrix}P_{11}     &amp; \cdots &amp; P_{1n}      \\\vdots &amp; \ddots &amp; \vdots \\P_{n1}     &amp; \cdots &amp; P_{nn}\end{bmatrix}\]</span> where each row of the matrix sums to 1.</p><p><strong>Markov Process</strong></p><p>Formally, a Markov process is a <strong>memoryless</strong> random process, i.e. a sequence of random states <span class="math inline">\(S_1, S_2, â€¦\)</span> with the <strong>Markov property</strong>.</p><blockquote><p>Definition</p><p><strong>A Markov Process (or Markov Chain) is tuple</strong> <span class="math inline">\(&lt;S, P&gt;\)</span></p><ul><li><strong><span class="math inline">\(S\)</span> is a (finite) set of states</strong></li><li><strong><span class="math inline">\(P\)</span> is a state transition probability matrix, <span class="math inline">\(P_{ss&#39;} = P[S_{t+1}=s&#39;|S_t=s]\)</span></strong></li></ul></blockquote><p><em>Example</em></p><p><img src="/images/markov.png"></p><p>The above figure show a markov chains of a student's life. Process starts from <em>Class 1</em>, taking class 1 may be boring, so he have either 50% probability to look <em>Facebook</em> or to move to <em>Class 2</em>. â€¦. And finally, he reach the final state <em>Sleep</em>. It's a final state just because it is a self-loop with probability 1 which is nothing special.</p><p>We can sample sequences from such process. Sample <strong>episodes</strong> for Student Markov Chain starting from <span class="math inline">\(S_1 = C_1\)</span>: <span class="math display">\[S_1, S_2, ..., S_T\]</span></p><ul><li>C1 C2 C3 Pass Sleep</li><li>C1 FB FB C1 C2 Sleep</li><li>C1 C2 C3 Pub C2 C3 Pass Sleep</li><li>C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep</li></ul><p>Also, we can make the transition matrix from such markov chain:</p><p><img src="/images/trans_mat.png"></p><p>If we have this matrix, we can fully describe the Markov process.</p><h2><span id="markov-reward-process">Markov Reward Process</span></h2><p>So far, we have never talked about Reinforcement Learning, there is no reward at all. So, let's talk about the <em>Markov Reward Process</em>.</p><p>The most important is adding reward to Markov process, so a Markov reward process is a Markov chain with values.</p><blockquote><p>Definition</p><p>A Markov Reward Process is tuple <span class="math inline">\(&lt;S, P, R, \gamma&gt;\)</span></p><ul><li><span class="math inline">\(S\)</span> is a (finite) set of states</li><li><span class="math inline">\(P\)</span> is a state transition probability matrix, <span class="math inline">\(P_{ss&#39;} = P[S_{t+1}=s&#39;|S_t=s]\)</span></li><li><strong><span class="math inline">\(R\)</span> is a reward function, <span class="math inline">\(R_s=E[R_{t+1}|S_t=s]\)</span></strong></li><li><strong><span class="math inline">\(\gamma\)</span> is a discount factor, <span class="math inline">\(\gamma \in [0, 1]\)</span></strong></li></ul></blockquote><p>Note that <span class="math inline">\(R\)</span> is the <strong>immediate reward</strong>, it characterize the reward you will get if you currently stay on state <span class="math inline">\(s\)</span>.</p><p><em>Example</em></p><p>Let's back to the student example:</p><p><img src="/images/mrp.png"></p><p>At each state, we have corresponding reward represents the goodness/badness of that state.</p><p><strong>Return</strong></p><p>We don't actually care about the immediate reward, we care about the whole random sequence's total reward. So we define the term <em>return</em>:</p><blockquote><p>Definition</p><p><strong>The return <span class="math inline">\(G_t\)</span> is the total dicounted reward from time-step <span class="math inline">\(t\)</span>.</strong> <span class="math display">\[G_t=R_{t+1}+\gamma R_{t+2}+...=\sum_{k=0}^\infty \gamma^kR_{t+k+1}\]</span></p></blockquote><p>The discount <span class="math inline">\(\gamma \in [0,1]\)</span> is the present value of future rewards. So the value of receiving reward <span class="math inline">\(R\)</span> after <span class="math inline">\(k+1\)</span> time-steps is <span class="math inline">\(\gamma^k R\)</span>.</p><p><strong>Note</strong>: <span class="math inline">\(R_{t+1}\)</span> is the immediate reward of state <span class="math inline">\(S_t\)</span>.</p><p>This values <strong>immediate reward</strong> above <strong>delayed reward</strong>:</p><ul><li><span class="math inline">\(\gamma\)</span> closes to <span class="math inline">\(0\)</span> leads to &quot;myopic&quot; evaluation</li><li><span class="math inline">\(\gamma\)</span> closes to <span class="math inline">\(1\)</span> leads to &quot;far-sighted&quot; evaluation</li></ul><p>Most Markov reward and decision processes are discounted. <strong>Why?</strong></p><ul><li><strong>Mathematically convenient</strong> to discount rewards</li><li><strong>Avoids inï¬nite returns</strong> in cyclic Markov processes</li><li><strong>Uncertainty</strong> about the future may not be fully represented</li><li>If the reward is ï¬nancial, immediate rewards may earn more interest than delayed rewards</li><li><strong>Animal/human behaviour</strong> shows preference for immediate reward</li><li>It is sometimes possible to use undiscounted Markov reward processes (i.e. Î³ = 1), e.g. if all sequences terminate.</li></ul><p><strong>Value Function</strong></p><p>The value function <span class="math inline">\(v(s)\)</span> gives the long-term value of state <span class="math inline">\(s\)</span>.</p><blockquote><p>Definition</p><p><strong>The state value funtion <span class="math inline">\(v(s)\)</span> of an MRP is the expected return starting from state <span class="math inline">\(s\)</span></strong> <span class="math display">\[v(s) = E[G_t|S_t=s]\]</span></p></blockquote><p>We use expectation because it is a random process, we want to figure out the expected value of a state, not such a sequence sampled starts it.</p><p><em>Example</em></p><p>Sample <strong>returns</strong> from Student MRP, starting from <span class="math inline">\(S_1 = C1\)</span> with <span class="math inline">\(\gamma = \frac{1}{2}\)</span>: <span class="math display">\[G_1=R_2+\gamma R_3+...+\gamma^{T-2}R_T\]</span> <img src="/images/samret.png"></p><p>The <em>return</em> is random, but the <em>value function</em> is not random, rather, it is expectation of all samples' return.</p><p>Let's see the example of state <em>value function</em>:</p><p><img src="/images/svf.png"></p><p>When <span class="math inline">\(\gamma = 0\)</span>, the value function just consider the reward of current state no matter how it changes future.</p><p><img src="/images/gamma0.9.png"></p><p><img src="/images/gamma1.png"></p><p><strong>Bellman Equation for MRPs</strong></p><p>The value function can be decomposed into two parts:</p><ul><li>immediate reward <span class="math inline">\(R_{t+1}\)</span></li><li>discounted value of successor state <span class="math inline">\(\gamma v(S_{t+1})\)</span></li></ul><p>So as to we can apply dynamic programming to solve the value function.</p><p>Here is the demonstration: <span class="math display">\[\begin{align}v(s) &amp; = \mathbb{E}[G_t|S_t=s] \\&amp; = \mathbb{E}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...|S_t=s] \\&amp;= \mathbb{E}[R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+...)|S_t=s]\\&amp;= \mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t=s]\\&amp;= \mathbb{E}[R_{t+1}+\gamma v(S_{t+1})|S_t=s]\end{align}\]</span> Here we get: <span class="math display">\[v(s) =\mathbb{E}[R_{t+1}+\gamma v(S_{t+1})|S_t=s]=R_{t+1}+\gamma\mathbb{E}[ v(S_{t+1})|S_t=s]\]</span> We look ahead one-step, and averaging all value function of next possible state:</p><p><img src="/images/bf2.png"> <span class="math display">\[v(s) = R_s+\gamma\sum_{s&#39;\in S}P_{ss&#39;}v(s&#39;)\]</span> We can use the Bellman equation to vertify a MRP:</p><p><img src="/images/verMRP.png"></p><p><em>Bellman Equation in Matrix Form</em></p><p>The Bellman equation can be expressed concisely using matrices, <span class="math display">\[v = R + \gamma Pv\]</span> where <span class="math inline">\(v\)</span> is a column vector with one entry per state: <span class="math display">\[\begin{bmatrix}v(1)         \\\vdots  \\v(n)    \end{bmatrix}=\begin{bmatrix}R_1         \\\vdots  \\R_n   \end{bmatrix}+\gamma \begin{bmatrix}P_{11}     &amp; \cdots &amp; P_{1n}      \\\vdots &amp; \ddots &amp; \vdots \\P_{n1}     &amp; \cdots &amp; P_{nn}\end{bmatrix}\begin{bmatrix}v(1)         \\\vdots  \\v(n)    \end{bmatrix}\]</span> Because the Bellman equation is a linear equation, it can be solved directly: <span class="math display">\[\begin{align}v &amp; = R+\gamma Pv \\(I-\gamma P)v&amp; =R \\v &amp;= (I-\gamma P)^{-1}R\end{align}\]</span> However, the Computational complexity is <span class="math inline">\(O(n^3)\)</span> for <span class="math inline">\(n\)</span> states, because of the inverse operation. This method can be applied to solve small MRPs.</p><p>There are many iterative methods for large MRPs, e.g.</p><ul><li>Dynamic programming</li><li>Monte-Carlo evaluation</li><li>Temporal-Diï¬€erence learning</li></ul><p>So far with MRP, all we want to do is to make decisions, so let's move on <em>Markov Decision Process</em>, which we actually using in RL.</p><h2><span id="markov-decision-process">Markov Decision Process</span></h2><p>A <strong>Markov decision process (MDP)</strong> is a Markov reward process with decisions. It is an <em>environment</em> in which all states are Markov.</p><blockquote><p>Definition</p><p><strong>A Markov Decision Process is a tuple</strong> <span class="math inline">\(&lt;S, A,P,R,\gamma&gt;\)</span></p><ul><li><span class="math inline">\(S\)</span> is a finite set of states</li><li><span class="math inline">\(A\)</span> <strong>is a finite set of actions</strong></li><li><span class="math inline">\(P\)</span> is a state transition probability matrix, <span class="math inline">\(P^a_{ss&#39;}=\mathbb{P}[S_{t+1}=s&#39;|S_t=s, A_t=a]\)</span></li><li><span class="math inline">\(R\)</span> is a reward function, <span class="math inline">\(R^a_s=\mathbb{E}[R_{t+1}|S_t=s,A_t=t]\)</span></li><li><span class="math inline">\(\gamma\)</span> is a discount factor <span class="math inline">\(\gamma \in[0,1]\)</span></li></ul></blockquote><p><em>Example</em></p><p><img src="/images/mdpstu.png"></p><p>Red marks represents the actions or decisions, what we want to do is to find the best path that maximize the value function.</p><p><strong>Policy</strong></p><p>Formally, the decision can be defined as <em>policy</em>:</p><blockquote><p>Definition</p><p><strong>A policy Ï€ is a distribution over actions given states,</strong> <span class="math display">\[\pi(a|s)=\mathbb{P}[A_t=a|S_t=s]\]</span></p></blockquote><p>A policy fully deï¬nes the behaviour of an agent.</p><p>Note that MDP policies depend on the current state (not the history), i.e. policies are stationary (time-independent), <span class="math inline">\(A_t~\pi(\cdot|S_t), \forall t&gt;0\)</span>.</p><p>An MDP can transform into a Markov process or an MRP:</p><ul><li><p>Given an MDP <span class="math inline">\(\mathcal{M}=&lt;\mathcal{S,A,P,R}, \gamma&gt;\)</span> and a policy <span class="math inline">\(\pi\)</span></p></li><li><p>The state sequence <span class="math inline">\(&lt;S_1 , S_2 , ... &gt;\)</span> is a Markov process <span class="math inline">\(&lt;\mathcal{S, P}^Ï€&gt;\)</span></p></li><li><p>The state and reward sequence <span class="math inline">\(&lt;S_1 , R_2 , S_2 , â€¦&gt;\)</span> is a Markov reward process <span class="math inline">\(&lt;\mathcal{S,P}^\pi,\mathcal{R}^\pi,\gamma&gt;\)</span> where, <span class="math display">\[\mathcal{P}^\pi_{s,s&#39;}=\sum_{a\in\mathcal{A}}\pi(a|s)P^a_{ss&#39;}\]</span></p><p><span class="math display">\[\mathcal{R}^\pi_s=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{R}^a_s\]</span></p></li></ul><p><strong>Value Function</strong></p><p>There are two value functions: the first one is called <em>state-value function</em> which represents the expected return following policy <span class="math inline">\(\pi\)</span>, the other is called <em>action-value function</em> which measures the goodness/badness of an action following policy <span class="math inline">\(\pi\)</span>.</p><blockquote><p>Definition</p><p>The <strong>state-value function</strong> <span class="math inline">\(v_Ï€ (s)\)</span> of an MDP is the expected return starting from state <span class="math inline">\(s\)</span>, and then following policy Ï€ <span class="math display">\[v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]\]</span></p></blockquote><blockquote><p>Definition</p><p>The <strong>action-value function</strong> <span class="math inline">\(q_Ï€ (s, a)\)</span> is the expected return starting from state <span class="math inline">\(s\)</span>, taking action <span class="math inline">\(a\)</span>, and then following policy <span class="math inline">\(Ï€\)</span> <span class="math display">\[q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]\]</span></p></blockquote><p><em>Example</em></p><p><img src="/images/svformdp.png"></p><p><strong>Bellman Expectation Equation</strong></p><p>The <strong>state-value function</strong> can again be decomposed into immediate reward plus discounted value of successor state, <span class="math display">\[v_\pi(s)=\mathbb{E}_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\]</span> The <strong>action-value function</strong> can similarly be decomposed, <span class="math display">\[q_\pi(s,a)=\mathbb{E}_\pi[R_{t+1}+\gamma q_\pi (S_{t+1},A_{t+1})|S_t=s,A_t=a]\]</span> <em>Bellman Expectation Equation for <span class="math inline">\(V_Ï€\)</span></em></p><p><img src="/images/vpi.png"> <span class="math display">\[v_\pi(s)=\sum_{a\in\mathcal{A}}\pi(a|s)q_\pi(s,a)\]</span> The look-ahead approach is taking an action and computing its reward, all we need to do is to averaging all possible actions' rewards, which is equal to current state-value.</p><p><em>Bellman Expectation Equation for <span class="math inline">\(Q_Ï€\)</span></em></p><p><img src="/images/qpi.png"> <span class="math display">\[q_\pi(s,a)=\mathcal{R}_s^a + \gamma \sum_{s&#39;\in\mathcal{S}}P^a_{ss&#39;}v_\pi(s&#39;)\]</span> It is identical to the immediate reward of taking action <span class="math inline">\(a\)</span> plus the average of the reward/value of all possible states which the action could lead to.</p><p><em>Bellman Expectation Equation for <span class="math inline">\(V_Ï€\)</span></em></p><p><img src="/images/vpi2.png"> <span class="math display">\[v_\pi(s)=\sum_{a\in\mathcal{A}}\pi(a|s)(\mathcal{R}^a_s+\gamma\sum_{s&#39;\in\mathcal{S}}P^a_{ss&#39;}v_\pi(s&#39;))\]</span> This is a two-step look-ahead approach, just combining the last two equations.</p><p><img src="/images/qpi2.png"> <span class="math display">\[q_\pi(s,a)=\mathcal{R}^a_s+\gamma\sum_{s&#39;\in\mathcal{S}}P^a_{ss&#39;}\sum_{a&#39;\in\mathcal{A}}\pi(a&#39;|s&#39;)q_\pi(s&#39;,a&#39;)\]</span> <em>Example</em>: State-value function</p><p><img src="/images/qgbee.png"></p><p><em>Bellman Expectation Equation (Matrix Form)</em></p><p>The Bellman expectation equation can be expressed concisely using the induced MRP, <span class="math display">\[v_\pi=R^\pi+\gamma P^\pi v_\pi\]</span> with direct solution <span class="math display">\[v_\pi=(I-\gamma P^{\pi-1})^{-1}R^\pi\]</span> <strong>Optimal Value Function</strong></p><blockquote><p>Definition</p><p><strong>The optimal state-value function <span class="math inline">\(v_âˆ—(s)\)</span> is the maximum value function over all policies</strong> <span class="math display">\[v_\ast(s)=\max_{\pi}v_\pi(s)\]</span> <strong>The optimal action-value function <span class="math inline">\(q_âˆ— (s, a)\)</span> is the maximum action-value function over all policies</strong> <span class="math display">\[q_\ast(s,a)=\max_\pi q_\pi(s,a)\]</span></p></blockquote><p>The optimal value function speciï¬es the best possible performance in the MDP.</p><p>If we know the <span class="math inline">\(q_*(s,a)\)</span>, we &quot;solve&quot; MDP because we know what actions should take at each state to maximize the reward.</p><p><em>Example</em></p><p><img src="/images/egvalue.png"></p><p><img src="/images/egstar.png"></p><p><strong>Optimal Policy</strong></p><p>Deï¬ne a partial ordering over policies <span class="math display">\[\piâ‰¥\pi&#39;\ if\ v_\pi(s)â‰¥v_{\pi&#39;}, \forall s\]</span></p><blockquote><p>Theorem</p><ul><li><strong>There exists an optimal policy <span class="math inline">\(Ï€_âˆ—\)</span> that is better than or equal to all other policies,</strong>, <span class="math inline">\(\pi_* â‰¥ \pi, \forall \pi\)</span></li><li><strong>All optimal policies achieve the optimal value function,</strong> <span class="math inline">\(v_{pi_*}=v_*(s)\)</span></li><li><strong>All optimal policies achieve the optimal action-value function,</strong> <span class="math inline">\(q_{\pi_*}(s,a)=q_*(s,a)\)</span></li></ul></blockquote><p><em>Finding an Optimal Policy</em></p><p>An optimal policy can be found by maximising over <span class="math inline">\(q_âˆ— (s, a)\)</span>,</p><p><span class="math display">\[\pi_\ast(a|s)=\begin{cases}1\ if\ a=\arg\max_{a\in\mathcal{A}}q_\ast(s,a) \\0 \ otherwise\end{cases}\]</span></p><p>There is always a deterministic optimal policy for any MDP. So if we know <span class="math inline">\(q_âˆ— (s, a)\)</span>, we immediately have the optimal policy.</p><p><img src="/images/optpol.png"></p><p>The optimal policy is highlight in red.</p><p><strong>Bellman Optimality Equation</strong></p><p>The optimal value functions are recursively related by the Bellman optimality equations:</p><p><img src="/images/boev.png"></p><p><span class="math display">\[v_\ast(s)=\max_aq_\ast(s,a)\]</span></p><p><img src="/images/boeq.png"></p><p><span class="math display">\[q_\ast(s,a)=\mathcal{R}^a_s+\gamma\sum_{s&#39;\in\mathcal{S}}\mathcal{P}^a_{ss&#39;}v_\ast(s&#39;)\]</span></p><p><img src="/images/boev2.png"></p><p><span class="math display">\[v_\ast(s)=\max_a\mathcal{R}^a_s+\gamma\sum_{s&#39;\in\mathcal{S}}\mathcal{P}^a_{ss&#39;}v_\ast(s&#39;)\]</span></p><p><img src="/images/boeq2.png"></p><p><span class="math display">\[q_\ast(s,a)=\mathcal{R}^a_s+\gamma\sum_{s&#39;\in\mathcal{S}}P^a_{ss&#39;}\max_{a&#39;}q_\ast(s&#39;,s)\]</span></p><p><em>Example</em></p><p><img src="/images/boe_in_stu_mdp.png"></p><p><em>Solving the Bellman Optimality Equation</em></p><p>Bellman Optimality Equation is non-linear, so it is not able to be sovle as solving linear equation. And there is no closed from solution (in general).</p><p>Many <strong>iterative</strong> solution methods</p><ul><li>Value Iteration</li><li>Policy Iteration</li><li>Q-learning</li><li>Sarsa</li></ul><p>End. Next note will introduce how to solve the Bellman Optimality Equation by dynamic programming.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#markov-processes&quot;&gt;Markov Processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#markov-reward-process&quot;&gt;Markov Reward Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#markov-decision-process&quot;&gt;Markov Decision Process&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
&lt;h2 id=&quot;markov-processes&quot;&gt;Markov Processes&lt;/h2&gt;
&lt;p&gt;Basically, &lt;strong&gt;Markov decision processes&lt;/strong&gt; formally describe an environment for reinforcement learning, where the environment is &lt;strong&gt;fully observable&lt;/strong&gt;, which means the current state completely characterises the process.&lt;/p&gt;
    
    </summary>
    
      <category term="å­¦ä¹ ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="å¢å¼ºå­¦ä¹ " scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="MDP" scheme="http://www.52coding.com.cn/tags/MDP/"/>
    
  </entry>
  
  <entry>
    <title>RL - Introduction to Reinforcement Learning</title>
    <link href="http://www.52coding.com.cn/2017/08/15/RL%20-%20Introduction%20to%20Reinforcement%20Learning/"/>
    <id>http://www.52coding.com.cn/2017/08/15/RL - Introduction to Reinforcement Learning/</id>
    <published>2017-08-15T07:55:19.000Z</published>
    <updated>2018-11-06T03:47:27.932Z</updated>
    
    <content type="html"><![CDATA[<p>RL, especially DRL (Deep Reinforcement Learning) has been an fervent research area during these years. One of the most famous RL work would be AlphaGo, who has beat <a href="https://www.wikiwand.com/en/Lee_Sedol" target="_blank" rel="noopener">Lee Sedol</a>, one of the best players at Go, last year. And in this year (2017), AlphaGo won three games with Ke Jie, the world No.1 ranked player. Not only in Go, AI has defeated best human play in many games, which illustrates the powerful of the combination of Deep Learning and Reinfocement Learning. However, despite AI plays better games than human, AI takes more time, data and energy to train which cannot be said to be very intelligent. Still, there are numerous unexplored and unsolved problems in RL research, that's also why we want to learn RL.</p><p>This is the first note of David Silver's <a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">RL course</a>.</p><a id="more"></a><!-- toc --><ul><li><a href="#about-reinforcement-learning">About Reinforcement Learning</a></li><li><a href="#the-reinforcement-learning-problem">The Reinforcement Learning Problem</a></li><li><a href="#inside-an-rl-agent">Inside An RL Agent</a></li><li><a href="#problems-within-reinforcement-learning">Problems within Reinforcement Learning</a></li></ul><!-- tocstop --><h2><span id="about-reinforcement-learning">About Reinforcement Learning</span></h2><p>Reinforcement Learning is one of the three major branches of Machine Learning, and is also the intersect of many different disciplines, as the following two figure illustrated.</p><p><img src="/images/branches.png"></p><p><img src="/images/faces.png"></p><p>There are several reasons that makes reinforcement learning different from other machine learning paradigms:</p><ul><li>There is no supervisor, only a <em>reward</em> signal</li><li>Feedback is delayed, not instantaneous</li><li>Time really matters (sequential, non i.i.d data)</li><li>Agent's actions affect the subsequent data it receives</li></ul><p>There are some examples of Reinforcement Learning:</p><ul><li>Fly stunt manoeuvres in a helicopter</li><li>Defeat the world champion at Backgammon</li><li>Manage an investment portfolio</li><li>Control a power station</li><li>Make a humanoid robot walk</li><li>Play many diï¬€erent Atari games better than humans</li></ul><h2><span id="the-reinforcement-learning-problem">The Reinforcement Learning Problem</span></h2><p><strong>Rewards</strong></p><p>We say RL do not have a supervisor, just a <em>reward</em> signal. Then what is <em>reward</em> ?</p><ul><li>A <strong>reward</strong> <span class="math inline">\(R_t\)</span> is a scalar feedback signal</li><li>Indicates how well agent is doing at step <span class="math inline">\(t\)</span></li><li>The agent's job is to maximise cumulative reward</li></ul><p>Reinforcement Learning is based on the <strong>reward hypothesis</strong>, which is</p><blockquote><p>Definition of reward hypothesis</p><p><strong>All goals can be described by the maximisation of expected cumulative reward.</strong></p></blockquote><p>There are some examples of <em>rewards</em> :</p><ul><li>Fly stunt manoeuvres in a helicopter<ul><li>+ve reward for following desired trajectory</li><li>âˆ’ve reward for crashing</li></ul></li><li>Defeat the world champion at Backgammon<ul><li>+/âˆ’ve reward for winning/losing a game</li></ul></li><li>Manage an investment portfolio<ul><li>+ve reward for each $ in bank</li></ul></li><li>Control a power station<ul><li>+ve reward for producing power</li><li>âˆ’ve reward for exceeding safety thresholds</li></ul></li><li>Make a humanoid robot walk<ul><li>+ve reward for forward motion</li><li>âˆ’ve reward for falling over</li></ul></li><li>Play many diï¬€erent Atari games better than humans<ul><li>+/âˆ’ve reward for increasing/decreasing score</li></ul></li></ul><p><strong>Sequential Decision Making</strong></p><p>So, according to the <em>reward hypothesis</em>, our goal is to <strong>select actions to maximise total future reward</strong>. Actions may have long term consequences as well as reward may be delayed. It may be better to sacrifice immediate reward to gain more long-term reward. For instance, a ï¬nancial investment may take months to mature and a helicopter might prevent a crash in several hours.</p><p><strong>Agent and Environment</strong></p><p><img src="/images/aae.png"></p><p>Agents and envrionments are big concepts in RL. They have following relationships:</p><ul><li>At each step <span class="math inline">\(t\)</span> the agent:<ul><li>Excutes action <span class="math inline">\(A_t\)</span></li><li>Receives observation <span class="math inline">\(O_t\)</span></li><li>Receives scalar reward <span class="math inline">\(R_t\)</span></li></ul></li><li>The environment:<ul><li>Receives action <span class="math inline">\(A_t\)</span></li><li>Emits observation <span class="math inline">\(O_{t+1}\)</span></li><li>Emits scalar reward <span class="math inline">\(R_{t+1}\)</span></li></ul></li><li><span class="math inline">\(t\)</span> increments at env. step</li></ul><p><strong>State</strong></p><p><em>History and State</em></p><p>The <strong>history</strong> is the sequence of observations, actions, rewards: <span class="math display">\[H_t = O_1, R_1, A_1, ..., A_{t-1}, O_t, R_t\]</span> which means all observable variables up to time <span class="math inline">\(t\)</span>, i.e. the sensorimotor stream of a robot or embodied agent.</p><p>What happens next depends on the history:</p><ul><li>The agent selects actions</li><li>The environments select observations/rewards</li></ul><p><strong>State</strong> is the information used to determine what happens next. Formally, state is a function of the history: <span class="math display">\[S_t = f(H_t)\]</span> <em>Environment State</em></p><p>The <strong>environment state</strong> <span class="math inline">\(S^e_t\)</span> is the environment's private representation, i.e. whatever data the environment uses to pick the next observation /reward. The environment state is not usually visible to the agent. Even if <span class="math inline">\(S^e_t\)</span> is visible, it may contain irrelevant information.</p><p><em>Agent State</em></p><p>The <strong>agent state</strong> <span class="math inline">\(S^a_t\)</span> is the agent's internal representation, i.e. whatever information the agent uses to pick the next action. It is the information used by reinforcement learning algotithms. It can be any function of history: <span class="math display">\[S^a_t=f(H_t)\]</span> <em>Information State</em></p><p>An <strong>information state</strong> (a.k.a. <strong>Markov state</strong>) contains all useful information from the history.</p><blockquote><p>Definition</p><p><strong>A state <span class="math inline">\(S_t\)</span> is Markov if and only if</strong> <span class="math display">\[P[S_{t+1}|S_t]=P[S_{t+1}|S_1,...,S_t]\]</span></p></blockquote><p>The above formular means:</p><ul><li><p>&quot;The future is independent of the past given the present&quot; <span class="math display">\[H_{1:t}\rightarrow S_t\rightarrow H_{t+1:\infty}\]</span></p></li><li><p>Once the state is known, the history may be thrown away.</p></li><li><p>The state is a sufficient statistic of the future</p></li><li><p>The environment state <span class="math inline">\(S^e_t\)</span> is Markov</p></li><li><p>The history <span class="math inline">\(H_t\)</span> is Markov</p></li></ul><p><em>Rat Example</em></p><p>Here is an example to explain what is state, imagine you are a rat and your master would decide whether to excuted you or give you a pice of cheese. The master makes decisions according to a sequence of signals, the first two sequence and the result are shown in the figure below:</p><p><img src="/images/firsttwo.png"></p><p>The question is, what would you get if the sequence is like below:</p><p><img src="/images/que.png"></p><p>Well, the answer you may give is decided by what is your agent stateï¼š</p><ul><li>If agent state = last 3 items in sequence, then the answer would be being excuted.</li><li>If agent state = counters for lights, bells and levers, then the answer would be given a piece of cheese.</li><li>What if agent state = complete sequence?</li></ul><p><em>Fully Observable Environments</em></p><p><strong>Full observability</strong>: agent <strong>directly</strong> observes environment state: <span class="math display">\[O_t = S^a_t=S^e_t\]</span></p><ul><li>Agent state = environment state = information state</li><li>Formally, this is a <strong>Markov decision process</strong> (MDP)</li></ul><p><em>Partially Observable Environments</em></p><p><strong>Partial observability</strong>: agent <strong>indirectly</strong> observes environment:</p><ul><li>A robot with camera vision isn't told its absolute location</li><li>A trading agent only observes current prices</li><li>A poker playing agent only observes public cards</li></ul><p>With partial observability, agent state â‰  environment state, formally this is a <strong>partially observable Markov decision process</strong> (POMDP).</p><p>In this situation, agent must construct its own state representation <span class="math inline">\(S^a_t\)</span>, e.g.</p><ul><li>Complete history: <span class="math inline">\(S^a_t = H_t\)</span></li><li><strong>Beliefs</strong> of environment state: <span class="math inline">\(S_t^a = (P[S^e_t=s^1], â€¦, P[S^e_t=s^n])\)</span></li><li>Recurrent neural network: <span class="math inline">\(S^a_t=\sigma(S^a_{t-1}W_s+O_tW_o)\)</span></li></ul><h2><span id="inside-an-rl-agent">Inside An RL Agent</span></h2><p>There are three major components of an RL agent, actually, an agent may include one or more of these:</p><ul><li>Policy: agent's behaviour function</li><li>Value functionï¼šhow good is each state and/or action</li><li>Modelï¼šagent's representation of the environment</li></ul><p><strong>Policy</strong></p><p>A <strong>Policy</strong> is the agent's behaviour, it is a map from state to action, e.g.</p><ul><li>Deterministic policyï¼š<span class="math inline">\(a = \pi(s)\)</span></li><li>Stochastic policy: <span class="math inline">\(\pi(a|s)=P[A_t=a|S_t=s]\)</span></li></ul><p><strong>Value Function</strong></p><p>Value function is a prediction of future reward, it is used to evaluate the goodness/badness of states. And therefore to select between actions: <span class="math display">\[v_\pi(s)=E_\pi[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+...|S_t=s]\]</span> <strong>Model</strong></p><p>A <strong>model</strong> predicts what the environment will do next, denoted by <span class="math inline">\(P\)</span> which predicts the next state and by <span class="math inline">\(R\)</span> predicts the next (immediate) reward: <span class="math display">\[P^a_{ss&#39;}=P[S_{t+1}=s&#39;|S_t=s,A_t=a]\]</span></p><p><span class="math display">\[R^a_s=E[R_{t+1}|S_t=s, A_t=a]\]</span></p><p><em>Maze Example</em></p><p><img src="/images/maze.png"></p><p>Let an RL agent to solve the maze, the parameters are:</p><ul><li>Rewards: -1 per time-step</li><li>Actions: N, E, S, W</li><li>States: Agent's location</li></ul><p>Then the policy map would be (arrows represent policy <span class="math inline">\(\pi(s)\)</span> for each state <span class="math inline">\(s\)</span>):</p><p><img src="/images/mpolicy.png"></p><p>And the value function at each state would be (numbers represent value <span class="math inline">\(v_\pi(s)\)</span> of each state <span class="math inline">\(s\)</span>):</p><p><img src="/images/mvf.png"></p><p>Model could be visualize as following:</p><ul><li>Grid layout represents transition model <span class="math inline">\(P^a_{ss&#39;}\)</span></li><li>Numbers represent immediate reward <span class="math inline">\(R^a_s\)</span> from each state <span class="math inline">\(s\)</span> (same for all <span class="math inline">\(a\)</span>)</li></ul><p><img src="/images/mm.png"></p><ul><li>Agent may have an internal model of the environment</li><li>Dynamics: how actions change the state</li><li>Rewards: how much reward from each state</li><li>The model may be imperfect</li></ul><p><strong>Categorizing RL agents</strong></p><p>RL agents could be categorized into several categories:</p><ul><li>Value Based<ul><li>No Policy (Implicit)</li><li>Value Function</li></ul></li><li>Policy Based<ul><li>Policy</li><li>No Value Function</li></ul></li><li>Actor Critic<ul><li>Policy</li><li>Value Function</li></ul></li><li>Model Free<ul><li>Policy and/or Value Function</li><li>No Model</li></ul></li><li>Model Based<ul><li>Policy and/or Value Function</li><li>Model</li></ul></li></ul><p><img src="/images/agcat.png"></p><h2><span id="problems-within-reinforcement-learning">Problems within Reinforcement Learning</span></h2><p>This section only proposes questions without providing the solutions.</p><p><strong>Learning and Planning</strong></p><p>Two fundamental problems in sequential decision making:</p><ul><li>Reinforcement Learning<ul><li>The environment is initially unknown</li><li>The agent interacts with the environment</li><li>The agent improves its policy</li></ul></li><li>Planning:<ul><li>A model of the environment is known</li><li>The agent performs computations with its model (without any external interaction)</li><li>The agent improves its policy a.k.a. deliberation, reasoning, introspection, pondering, thought, search</li></ul></li></ul><p><em>Atari Example: Reinforcement Learning</em></p><p><img src="/images/atari.png"></p><p>In atari games, rules of the game are unknown, the agent learns directly from interactive game-play by picking actions on joystick and seeing pixels and scores.</p><p><em>Atari Example: Planning</em></p><p><img src="/images/plan.png"></p><p>In such case, rules of the game are known, which means the agent could query the emulator as known as a perfect model inside agent's brain. Agents need plan ahead to ï¬nd optimal policy, e.g. tree search.</p><p><strong>Exploration and Exploitation</strong></p><ul><li>Reinforcement learning is like trial-and-error learning</li><li>The agent should discover a good policy</li><li>From its experiences of the environment</li><li>Without losing too much reward along the way</li><li><strong>Exploration</strong> ï¬nds more information about the environment</li><li><strong>Exploitation</strong> exploits known information to maximise reward</li><li>It is usually important to explore as well as exploit</li></ul><p><em>Examples</em></p><ul><li>Restaurant Selection<ul><li>Exploitation Go to your favourite restaurant</li><li>Exploration Try a new restaurant</li></ul></li><li>Online Banner Advertisements<ul><li>Exploitation Show the most successful advert</li><li>Exploration Show a diï¬€erent advert</li></ul></li><li>Game Playing<ul><li>Exploitation Play the move you believe is best</li><li>Exploration Play an experimental move</li></ul></li></ul><p><strong>Prediction and Control</strong></p><p><strong>Prediction</strong>: evaluate the future</p><ul><li>Given a policy</li></ul><p><strong>Control</strong>: optimise the future</p><ul><li>Find the best policy</li></ul><p>End. Next note will introduce the Markov Decision Processes.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RL, especially DRL (Deep Reinforcement Learning) has been an fervent research area during these years. One of the most famous RL work would be AlphaGo, who has beat &lt;a href=&quot;https://www.wikiwand.com/en/Lee_Sedol&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Lee Sedol&lt;/a&gt;, one of the best players at Go, last year. And in this year (2017), AlphaGo won three games with Ke Jie, the world No.1 ranked player. Not only in Go, AI has defeated best human play in many games, which illustrates the powerful of the combination of Deep Learning and Reinfocement Learning. However, despite AI plays better games than human, AI takes more time, data and energy to train which cannot be said to be very intelligent. Still, there are numerous unexplored and unsolved problems in RL research, that&#39;s also why we want to learn RL.&lt;/p&gt;
&lt;p&gt;This is the first note of David Silver&#39;s &lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;RL course&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="å­¦ä¹ ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="AlphaGo" scheme="http://www.52coding.com.cn/tags/AlphaGo/"/>
    
      <category term="å¢å¼ºå­¦ä¹ " scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="DRL" scheme="http://www.52coding.com.cn/tags/DRL/"/>
    
  </entry>
  
  <entry>
    <title>Paper Reading - Stacked Attention Networks for Image QA</title>
    <link href="http://www.52coding.com.cn/2017/08/15/SAN%20for%20Image%20QA/"/>
    <id>http://www.52coding.com.cn/2017/08/15/SAN for Image QA/</id>
    <published>2017-08-15T01:30:19.000Z</published>
    <updated>2018-11-06T03:48:03.977Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Zichao Yang, Xiaodong He, Jianfeng Gao , Li Deng , Alex Smola <a href="https://arxiv.org/abs/1511.02274" target="_blank" rel="noopener">Stacked Attention Networks for Image Question Answering</a></p></blockquote><p>è¿™ç¯‡æ–‡ç« å‘è¡¨åœ¨CVPR2016ï¼Œä½œè€…æŠŠ attention æœºåˆ¶åº”ç”¨åœ¨ Visual QAï¼Œä¸ä½†èƒ½ç†è§£ç¥ç»ç½‘ç»œç”Ÿæˆç­”æ¡ˆçš„ multiple resoningï¼Œè€Œä¸”è·å¾—äº†å½“æ—¶æœ€å¥½çš„æ•ˆæœã€‚</p><p>SANæ€»å…±ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š</p><ul><li>Image Modelï¼šç”¨æ¥ç¼–ç å›¾ç‰‡ä¿¡æ¯</li><li>Question Moelï¼šç”¨æ¥ç¼–ç é—®é¢˜ä¿¡æ¯</li><li>Stacked Attention Networksï¼šé€šè¿‡å¤šå±‚ attention layer ä¸æ–­ä¼˜åŒ–å¯¹é—®é¢˜çš„ç¼–ç </li></ul><a id="more"></a><p><img src="/images/san.png"></p><h2><span id="image-model">Image Model</span></h2><p>Image model ä½¿ç”¨ VGGNet å¤„ç†æºå›¾ç‰‡ï¼Œç”¨æœ€åä¸€ä¸ªæ± åŒ–å±‚ä½œä¸ºæå–çš„å›¾ç‰‡ç‰¹å¾: <span class="math display">\[f_I = CNN_{vgg}(I)\]</span> æŠŠè¾“å…¥å›¾ç‰‡è½¬åŒ–ä¸º <span class="math inline">\(448 \times 448\)</span> å¤§å°ï¼Œ è¾“å‡ºçš„ç‰¹å¾å³ä¸º <span class="math inline">\(512 \times 14 \times 14\)</span>ï¼Œå…¶ä¸­ <span class="math inline">\(512\)</span> ä¸ºç‰¹å¾å‘é‡ï¼ˆfeature vectorï¼‰ <span class="math inline">\(f_i\)</span> çš„ç»´åº¦ï¼Œ<span class="math inline">\(14 \times 14\)</span> æ˜¯åŒºåŸŸï¼ˆç‰¹å¾å‘é‡ï¼‰çš„ä¸ªæ•°ï¼Œæ¯ä¸ªç‰¹å¾å‘é‡ <span class="math inline">\(f_i\)</span> ä»£è¡¨æºå›¾ç‰‡ä¸­ <span class="math inline">\(32 \times 32\)</span> å¤§å°çš„åŒºåŸŸã€‚</p><p><img src="/images/vgg.png"></p><p>ä¸ºäº†åé¢æ–¹ä¾¿å¤„ç†ï¼Œé€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚æŠŠå›¾ç‰‡ç‰¹å¾è½¬åŒ–ä¸ºå’Œé—®é¢˜ç‰¹å¾ä¸€æ ·çš„ç»´åº¦ï¼š <span class="math display">\[v_I = \tanh(W_If_I + b_I)\]</span> å…¶ä¸­ï¼Œ<span class="math inline">\(v_I\)</span> æ˜¯ä¸ªçŸ©é˜µï¼Œå®ƒçš„ç¬¬ <span class="math inline">\(i\)</span> åˆ— <span class="math inline">\(v_i\)</span> æ˜¯åŒºåŸŸ <span class="math inline">\(i\)</span> çš„ç‰¹å¾å‘é‡ï¼ˆfeature vectorï¼‰ã€‚</p><h2><span id="question-model">Question Model</span></h2><p>ä½œè€…é‡‡ç”¨äº†ä¸¤ç§æ¨¡å‹å¯¹é—®é¢˜è¿›è¡Œç¼–ç ï¼Œåˆ†åˆ«åŸºäº LSTM å’Œ CNNã€‚</p><h3><span id="lstm-based-question-model">LSTM based question model</span></h3><p>åŸºäº LSTM çš„æ¨¡å‹å¾ˆç®€å•ï¼Œå°±æ˜¯ç”¨ä¸€ä¸ªæ™®é€šçš„ LSTM å¯¹é—®é¢˜è¿›è¡Œç¼–ç ï¼ˆæ²¡å‡†æ‰©å±•æˆbi-LSTMæ•ˆæœä¼šæ›´å¥½ä¸€äº›ï¼‰ï¼Œæ¯ä¸ªæ—¶åˆ»å¤„ç†ä¸€ä¸ªè¯ï¼ŒæŠŠæœ€åä¸€ä¸ªè¯å¯¹åº”çš„ hidden state ä½œä¸ºç¼–ç ç»“æœ: <span class="math display">\[\begin{alignat}{3}x_t &amp;= W_eq_t, \ t\in\{1, 2, ... T\} \\h_t &amp;= LSTM(x_t), \ t\in\{1, 2, ... T\}\\v_Q &amp;= h_T\end{alignat}\]</span> å…¶ä¸­ï¼Œ <span class="math inline">\(q_t\)</span> ä¸ºè¯çš„ one-hot encodingï¼Œ<span class="math inline">\(W_e\)</span> ä¸º embedding çŸ©é˜µï¼Œ<span class="math inline">\(x_t\)</span> å°±ä¸ºè¯çš„ word embeddingï¼ˆæ€»è§‰å¾—è¿™æ ·çš„è¯ç¼–ç å¤ªç®€å•äº†ï¼‰ï¼Œ<span class="math inline">\(v_Q\)</span> ä¸ºå¯¹é—®é¢˜çš„ç¼–ç ã€‚</p><p><img src="/images/lstmq.png"></p><h3><span id="cnn-based-question-model">CNN based question model</span></h3><p><img src="/images/cnn_basedq.png"></p><p>è¿™åº”è¯¥ç®—æ˜¯CNNçš„ä¸€ä¸ªå˜ç§ï¼Œå®ƒçš„ filter æœ‰ä¸‰ç§ï¼Œåˆ†åˆ«ä¸º unigram, bigram, trigramï¼Œåˆ†åˆ«å¯¹åº”çª—å£å¤§å° <span class="math inline">\(c = 1, 2, 3\)</span>ã€‚å®šä¹‰ç¬¦å· <span class="math inline">\(x_{i:j}\)</span> ä¸º <span class="math inline">\(x_i, x_{i+1}, â€¦, x_j\)</span> çš„è¿æ¥ï¼Œæ‰€ä»¥é—®é¢˜å‘é‡å¯è¡¨ç¤ºä¸º: <span class="math display">\[x_{1:T} = [x1, x2, ..., x_T]\]</span></p><p>ç„¶åå¯¹æ¯ä¸€ä¸ª filter åˆ†åˆ«åœ¨ <span class="math inline">\(x_{1:T}\)</span> ä¸Šè¿›è¡Œå·ç§¯æ“ä½œï¼Œç¬¬ <span class="math inline">\(t\)</span> æ¬¡å·ç§¯æ“ä½œçš„è¾“å‡ºä¸ºï¼š <span class="math display">\[h_{c,t} = \tanh(W_cx_{t:t+c-1}+b_c)\]</span> çª—å£å¤§å°ä¸º <span class="math inline">\(c\)</span> çš„ feature map ä¸ºï¼š <span class="math display">\[h_c = [h_{c,1}, h_{c,2}, ..., h_{c,T-c+1}]\]</span> ç„¶åå¯¹æ¯ä¸ª feature map è¿›è¡Œ max poolingï¼Œå¾—åˆ°æœ€ç»ˆçš„é—®é¢˜ç‰¹å¾ï¼š <span class="math display">\[\hat{h}_c = \max_t(h_{c,1}, h_{c, 2} ..., h_{c,T-c+1})\]</span></p><p><span class="math display">\[v_Q = h = [\hat{h}_1,\hat{h}_2,\hat{h}_3]\]</span></p><h2><span id="stacked-attention-networks">Stacked Attention Networks</span></h2><p><strong>ç¬¬ä¸€å±‚ attention network</strong></p><p><img src="/images/san1st.png"></p><p>é¦–å…ˆæ ¹æ®å›¾åƒç‰¹å¾çŸ©é˜µ <span class="math inline">\(v_I\)</span> å’Œé—®é¢˜ç‰¹å¾å‘é‡ <span class="math inline">\(v_Q\)</span> è®¡ç®— attention mapï¼š <span class="math display">\[\begin{alignat}{3}h_A &amp;= \tanh(W_{I,A}v_I\oplus (W_{Q,A}v_Q+b_A))\\p_I &amp;= softmax(W_Ph_A+b_P)\end{alignat}\]</span> å…¶ä¸­ï¼Œ<span class="math inline">\(v_I\in R^{d\times m}\)</span>, <span class="math inline">\(d\)</span> æ˜¯å›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œ<span class="math inline">\(m\)</span> æ˜¯å›¾åƒåŒºåŸŸä¸ªæ•°ï¼›<span class="math inline">\(v_Q \in R^d\)</span>ï¼›<span class="math inline">\(W_{I, A}, W_{Q,A} \in R^{k \times d}\)</span>ï¼Œ<span class="math inline">\(b_A \in R^{k}\)</span>ï¼›å®šä¹‰ <span class="math inline">\(\oplus\)</span> ä¸ºçŸ©é˜µå’Œå‘é‡çš„åŠ æ³•ï¼Œå…¶è¿ç®—è§„åˆ™ä¸ºçŸ©é˜µçš„æ¯ä¸€åˆ—åˆ†åˆ«å’Œè¯¥å‘é‡ç›¸åŠ ï¼Œæ‰€ä»¥ <span class="math inline">\(h_A \in R^{k\times m}\)</span>ã€‚<span class="math inline">\(W_P \in R^{1\times k}, b_P\in R^{1\times m}\)</span>ï¼Œ<span class="math inline">\(p_I \in R^{1\times m}\)</span> ä¸º attention vectorï¼Œå®ƒæ¯ä¸€é¡¹éƒ½æ˜¯ä¸€ä¸ªæ¦‚ç‡ï¼Œè¡¨ç¤ºè¯¥é—®é¢˜çš„ç­”æ¡ˆæ‰€åœ¨æŸä¸ªåŒºåŸŸçš„æ¦‚ç‡ï¼Œæˆ–è€…è¯´é—®äº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæ³¨æ„åŠ›åº”è¯¥é›†ä¸­åœ¨å“ªé‡Œã€‚</p><p>ä¹‹åç”¨ attention vector è®¡ç®— å›¾åƒç‰¹å¾çš„åŠ æƒå’Œï¼Œç„¶åä¸é—®é¢˜ç‰¹å¾ç›¸åŠ ï¼Œå¾—åˆ°<strong>ä¼˜åŒ–çš„é—®é¢˜ç‰¹å¾</strong>ï¼š <span class="math display">\[\begin{alignat}{3}\widehat{v}_I &amp;= \sum_ip_iv_i \\u &amp;= \hat{v}_I+v_Q\end{alignat}\]</span> åé¢çš„æ¯å±‚ attention network ç»“æ„éƒ½æ˜¯ä¸€æ ·çš„ï¼ŒåŒºåˆ«åœ¨äºä¸å†ä½¿ç”¨åŸå§‹çš„é—®é¢˜ç‰¹å¾ <span class="math inline">\(v_Q\)</span>ï¼Œè€Œæ˜¯ç”¨ä¼˜åŒ–åçš„ <span class="math inline">\(u\)</span>:</p><p><img src="/images/san2nd.png"></p><p><strong>ç¬¬ k å±‚ attention network</strong> <span class="math display">\[\begin{alignat}{3}h_A^k &amp;= \tanh(W_{I,A}^kv_I\oplus (W_{Q,A}^ku_{k-1}+b_A^k))\\p_I^k &amp;= softmax(W_P^kh_A^k+b_P^k)\\\widehat{v}_I^k &amp;= \sum_ip_i^kv_i \\u^k &amp;= \hat{v}_I^k+u^{k-1}\end{alignat}\]</span> ä½œè€…é€šè¿‡å®éªŒå‘ç°ï¼Œç¬¬ä¸€å±‚ attention å¯ä»¥è¯†åˆ«é—®é¢˜ä¸­å‡ºç°çš„å®ä½“ï¼Œç¬¬äºŒå±‚åˆ™å¯ä»¥æ¶ˆé™¤æ— å…³çš„ï¼Œåªå…³å¿ƒä¸ç­”æ¡ˆç›¸å…³çš„å®ä½“ï¼Œå¤šåŠ å‡ å±‚å¯¹è¯†åˆ«æ•ˆæœæ²¡æœ‰æ˜æ˜¾æå‡ã€‚</p><p><strong>è¾“å‡ºå±‚</strong></p><p><img src="/images/vqa_out.png"></p><p>ç”±äºè¾“å‡ºåªæ˜¯ä¸€ä¸ªè¯ï¼Œæ‰€ä»¥å¯ä»¥è½¬åŒ–ä¸ºåˆ†ç±»é—®é¢˜ï¼Œåœ¨æ‰€æœ‰å€™é€‰ç­”æ¡ˆé‡ŒæŒ‘ä¸€ä¸ªè¯å‡ºæ¥ï¼š <span class="math display">\[p_{ans} = softmax(W_uu^K+b_u)\]</span> å…¶ä¸­ <span class="math inline">\(K\)</span> ä¸º attention çš„å±‚æ•°ã€‚</p><h2><span id="å¯è§†åŒ–-attention-layer">å¯è§†åŒ– Attention Layer</span></h2><ul><li>æ­£ç¡®ç»“æœ</li></ul><p><img src="/images/true_vqa.png"></p><ul><li>é”™è¯¯ç»“æœ</li></ul><p><img src="/images/false_vqa.png"></p><h2><span id="æ€»ç»“">æ€»ç»“</span></h2><p>è¿™ç¯‡æ–‡ç« çš„ä¸»è¦å·¥ä½œåœ¨äºæŠŠ attention æœºåˆ¶åº”ç”¨åœ¨ Visual QA é—®é¢˜ä¸­ï¼Œæ•ˆæœå“ç¾¤ï¼Œå¯è§£é‡Šæ€§å¼ºã€‚ä½†ä¹Ÿæœ‰å¯æ”¹è¿›çš„åœ°æ–¹ï¼Œå¦‚å›¾ç‰‡ç¼–ç é€‰æ‹© ResNet è€Œä¸æ˜¯ VGGNetï¼›é—®é¢˜çš„ word embedding é‡‡ç”¨ word2vecï¼›å¯¹é—®é¢˜çš„ç¼–ç é‡‡ç”¨ bi-LSTM ç­‰ï¼Œä¹Ÿè®¸ä¼šè¿›ä¸€æ­¥æé«˜æ•´ä½“çš„è¡¨ç°ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Zichao Yang, Xiaodong He, Jianfeng Gao , Li Deng , Alex Smola &lt;a href=&quot;https://arxiv.org/abs/1511.02274&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Stacked Attention Networks for Image Question Answering&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;è¿™ç¯‡æ–‡ç« å‘è¡¨åœ¨CVPR2016ï¼Œä½œè€…æŠŠ attention æœºåˆ¶åº”ç”¨åœ¨ Visual QAï¼Œä¸ä½†èƒ½ç†è§£ç¥ç»ç½‘ç»œç”Ÿæˆç­”æ¡ˆçš„ multiple resoningï¼Œè€Œä¸”è·å¾—äº†å½“æ—¶æœ€å¥½çš„æ•ˆæœã€‚&lt;/p&gt;
&lt;p&gt;SANæ€»å…±ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Image Modelï¼šç”¨æ¥ç¼–ç å›¾ç‰‡ä¿¡æ¯&lt;/li&gt;
&lt;li&gt;Question Moelï¼šç”¨æ¥ç¼–ç é—®é¢˜ä¿¡æ¯&lt;/li&gt;
&lt;li&gt;Stacked Attention Networksï¼šé€šè¿‡å¤šå±‚ attention layer ä¸æ–­ä¼˜åŒ–å¯¹é—®é¢˜çš„ç¼–ç &lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="å­¦ä¹ ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="Attention" scheme="http://www.52coding.com.cn/tags/Attention/"/>
    
      <category term="CV" scheme="http://www.52coding.com.cn/tags/CV/"/>
    
      <category term="CNN" scheme="http://www.52coding.com.cn/tags/CNN/"/>
    
      <category term="VQA" scheme="http://www.52coding.com.cn/tags/VQA/"/>
    
  </entry>
  
  <entry>
    <title>Paper Reading - Neural Machine Translation In Linear Time (ByteNet)</title>
    <link href="http://www.52coding.com.cn/2017/08/14/Neural%20Machine%20Translation%20In%20Linear%20Time%20(ByteNet)/"/>
    <id>http://www.52coding.com.cn/2017/08/14/Neural Machine Translation In Linear Time (ByteNet)/</id>
    <published>2017-08-14T01:30:19.000Z</published>
    <updated>2018-11-06T03:46:52.613Z</updated>
    
    <content type="html"><![CDATA[<p>ByteNet å¯ç”¨äº<strong>å­—ç¬¦çº§</strong>çš„æœºå™¨ç¿»è¯‘æ¨¡å‹å¹¶ä¸”æœ‰ç€å¾ˆå¥½çš„è¡¨ç°ï¼Œå®ƒçš„ç‰¹ç‚¹åœ¨äºå¯ä»¥åœ¨çº¿æ€§æ—¶é—´ (linear time) å®Œæˆç¿»è¯‘è€Œä¸”èƒ½å¤Ÿå¤„ç†é•¿è·ç¦»ä¾èµ–ã€‚å®ƒä¹Ÿé‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¹¶ä¸”ç¼–ç å™¨å’Œè§£ç å™¨éƒ½ç”±CNNç»„æˆã€‚</p><p>ByteNet ä¹‹æ‰€ä»¥æœ‰ä¸Šè¿°çš„è¿™äº›ç‰¹æ€§ï¼Œæ˜¯å› ä¸ºä½¿ç”¨äº†å¦‚ä¸‹ä¸€äº›æŠ€æœ¯ï¼š</p><ul><li>Dynamic Unfolding<ul><li>è§£å†³äº†ç”Ÿæˆä¸åŒé•¿åº¦ç¿»è¯‘çš„é—®é¢˜</li></ul></li><li>Dilated Convolution<ul><li>ç¼©çŸ­äº†ä¾èµ–ä¼ æ’­çš„è·ç¦»</li></ul></li><li>Masked 1D Convolution<ul><li>ä¿è¯è®­ç»ƒæ—¶åªç”¨è¿‡å»çš„ä¿¡æ¯ç”Ÿæˆå½“å‰å­—ç¬¦</li></ul></li><li>Residual Blocks<ul><li>è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜</li></ul></li></ul><a id="more"></a><p><img src="/images/byte.png"></p><h2><span id="dynamic-unfolding">Dynamic Unfolding</span></h2><p><img src="/images/dy_unfold.png"></p><p>Encoder çš„è¾“å‡ºçš„å¥å­ç¼–ç çš„é•¿åº¦å›ºå®šä¸º <span class="math inline">\(|\hat{t}|\)</span> (ä¸å¤Ÿä¼šè¡¥é›¶)ï¼Œæ˜¯ç›®æ ‡å¥å­é•¿åº¦ <span class="math inline">\(|t|\)</span> çš„ä¸Šç•Œï¼Œå¯ä»¥é€šè¿‡ä¸‹å¼å¾—åˆ°ï¼š <span class="math display">\[|\hat{t}| = a|s| + b\]</span> å…¶ä¸­ï¼Œ<span class="math inline">\(|s|\)</span> è¡¨ç¤ºæºå¥å­é•¿åº¦ï¼Œé€šè¿‡é€‰æ‹©é€‚å½“çš„å‚æ•° <span class="math inline">\(a\)</span> å’Œ <span class="math inline">\(b\)</span>ï¼Œä½¿å¾— <span class="math inline">\(|\hat{t}|\)</span> åŸºæœ¬éƒ½å¤§äºå®é™…é•¿åº¦ <span class="math inline">\(|t|\)</span>ï¼Œå¹¶ä¸”æ²¡æœ‰å¤ªå¤šå†—ä½™ã€‚</p><p>åœ¨æ¯ä¸€æ­¥ï¼Œdecoder æ ¹æ®å½“å‰è¾“å…¥å­—ç¬¦å’Œå¥å­ç‰¹å¾è¾“å‡ºä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œç›´åˆ°ç”ŸæˆEOSã€‚è‡³äº decoder æ€ä¹ˆæ¥æ”¶è¾“å…¥å¹¶ conditioned on ç¼–ç å™¨çš„è¾“å‡ºï¼Œè®ºæ–‡åœ¨å¹¶æ²¡æœ‰æåŠï¼Œä¸è¿‡ä»ä¸€ä¸ª<a href="https://github.com/paarthneekhara/byteNet-tensorflow/blob/master/ByteNet/model.py" target="_blank" rel="noopener">å¼€æºå®ç°</a>ä¸­çœ‹å‡ºæ˜¯ç›´æ¥æŠŠè¾“å…¥å’Œç¼–ç å™¨åœ¨å¯¹åº”ä½ç½®çš„è¾“å‡ºè¿æ¥èµ·æ¥ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºã€‚</p><h2><span id="dilated-convolution">Dilated Convolution</span></h2><p>ä¸€ç»´ç¦»æ•£å·ç§¯çš„å®šä¹‰ä¸ºï¼š <span class="math display">\[(f*g)[n] = \sum_{m=-\infty}^{\infty}f[m]g[n-m] = \sum_{m=-M}^Mf[n - m]g[m]\]</span> ä¾‹ï¼šå¦‚æœ <span class="math inline">\(f = [0, 1, 2, -1, 1, -3, 0]\)</span>, <span class="math inline">\(g = [1, 0, -1]\)</span>ï¼Œåˆ™æŒ‰ç…§ä¸Šå¼ï¼Œå·ç§¯è®¡ç®—å¦‚ä¸‹æ‰€ç¤ºï¼š <span class="math display">\[\begin{array}{lcl}(f*g)[2]        &amp; = &amp; f[2]g[0] + f[1]g[1] + f[0]g[2] = -2 \\(f*g)[3]  &amp; = &amp; f[3]g[0] + f[2]g[1] + f[1]g[2] = 2 \\...\\(f*g)[6]  &amp; = &amp; f[6]g[0] + f[5]g[1] + f[4]g[2] = 1 \\\end{array}\]</span> å’Œ stride = 1 çš„æ™®é€šå·ç§¯ç½‘ç»œè®¡ç®—ä¸€è‡´ã€‚</p><p><img src="/images/stride.jpeg"></p><p><strong>Dilated Convolution</strong>çš„å®šä¹‰ä¸ºï¼š <span class="math display">\[(f*_lg)[n] = \sum_{m=-\infty}^{\infty}f[m]g[n-lm] = \sum_{m=-M}^Mf[n - lm]g[m]\]</span> å…¶ä¸­ï¼Œ<span class="math inline">\(l\)</span> ä¸º dilation factorï¼Œæ§åˆ¶æ‰©å¼ å¤§å°ï¼Œè¿™æ · <span class="math inline">\(l = 2\)</span> æ—¶ä¸Šé¢ä¾‹å­ä¸­çš„å·ç§¯å°±å˜æˆäº†ï¼š <span class="math display">\[\begin{array}{lcl}(f*_2g)[4]        &amp; = &amp; f[4]g[0] + f[2]g[1] + f[0]g[2]  \\(f*_2g)[5]  &amp; = &amp; f[5]g[0] + f[3]g[1] + f[1]g[2] \\(f*_2g)[6]  &amp; = &amp; f[6]g[0] + f[4]g[1] + f[2]g[2]  \\\end{array}\]</span> å½“ <span class="math inline">\(l = 3\)</span> æ—¶ï¼Œç›¸åº”å·ç§¯å°±ä¸º: <span class="math display">\[(f*_3g)[6] = f[6]g[0] + f[3]g[1] + f[0]g[2]\]</span> è¿™æ ·è™½ç„¶å·ç§¯æ ¸éƒ½ä¸º3ï¼Œä½† receptive field çš„å¤§å°å´å¤§äº†å¾ˆå¤šï¼Œæ‰€ä»¥ä½¿ç”¨ dialted conv èƒ½ä½¿ <strong>receptive field</strong> çš„å¤§å°å‘ˆ<strong>æŒ‡æ•°å¢é•¿</strong>ï¼Œè€Œç›¸åº”<strong>å‚æ•°</strong>å´æ˜¯<strong>çº¿æ€§å¢é•¿</strong>çš„ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä½¿ç”¨ dilated conv å°±å¯ä»¥æœ‰æ•ˆåœ°ç¼©çŸ­ä¾èµ–ä¼ æ’­çš„è·ç¦»ã€‚</p><p><img src="/images/dilated.png"></p><p>å‚è€ƒï¼š<a href="https://arxiv.org/abs/1511.07122" target="_blank" rel="noopener">MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS</a></p><h2><span id="residual-block">Residual Block</span></h2><p><img src="/images/residual.png"></p><p>æ¯ä¸€å±‚ï¼ˆåŒ…æ‹¬ Encoder å’Œ Decoderï¼‰éƒ½å°è£…äº†ä¸€ä¸ª residual blockï¼ˆä¸Šå›¾ï¼‰ï¼Œå…¶ä¸­æ¯ä¸ªé»„è‰²çš„æ ¼å­ä»£è¡¨ä¸€ä¸ªå·ç§¯å±‚ï¼Œé‡Œé¢çš„æ•°å­—æ˜¯ç›¸åº”çš„ filter sizeã€‚ä¸­é—´çš„ Masked 1 x K æ˜¯è¿™å±‚çš„ä¸»åŠ›ï¼Œå…¶ä»–éƒ½æ˜¯ä¸ºäº†ä½¿ä»–å‘æŒ¥æ›´å¤§æ•ˆæœçš„é™ªè¡¬ã€‚</p><h2><span id="linear-time">Linear Time</span></h2><p><img src="/images/lt.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ByteNet å¯ç”¨äº&lt;strong&gt;å­—ç¬¦çº§&lt;/strong&gt;çš„æœºå™¨ç¿»è¯‘æ¨¡å‹å¹¶ä¸”æœ‰ç€å¾ˆå¥½çš„è¡¨ç°ï¼Œå®ƒçš„ç‰¹ç‚¹åœ¨äºå¯ä»¥åœ¨çº¿æ€§æ—¶é—´ (linear time) å®Œæˆç¿»è¯‘è€Œä¸”èƒ½å¤Ÿå¤„ç†é•¿è·ç¦»ä¾èµ–ã€‚å®ƒä¹Ÿé‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¹¶ä¸”ç¼–ç å™¨å’Œè§£ç å™¨éƒ½ç”±CNNç»„æˆã€‚&lt;/p&gt;
&lt;p&gt;ByteNet ä¹‹æ‰€ä»¥æœ‰ä¸Šè¿°çš„è¿™äº›ç‰¹æ€§ï¼Œæ˜¯å› ä¸ºä½¿ç”¨äº†å¦‚ä¸‹ä¸€äº›æŠ€æœ¯ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dynamic Unfolding
&lt;ul&gt;
&lt;li&gt;è§£å†³äº†ç”Ÿæˆä¸åŒé•¿åº¦ç¿»è¯‘çš„é—®é¢˜&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Dilated Convolution
&lt;ul&gt;
&lt;li&gt;ç¼©çŸ­äº†ä¾èµ–ä¼ æ’­çš„è·ç¦»&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Masked 1D Convolution
&lt;ul&gt;
&lt;li&gt;ä¿è¯è®­ç»ƒæ—¶åªç”¨è¿‡å»çš„ä¿¡æ¯ç”Ÿæˆå½“å‰å­—ç¬¦&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Residual Blocks
&lt;ul&gt;
&lt;li&gt;è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="å­¦ä¹ ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://www.52coding.com.cn/tags/NLP/"/>
    
      <category term="NMT" scheme="http://www.52coding.com.cn/tags/NMT/"/>
    
      <category term="Deep NLP" scheme="http://www.52coding.com.cn/tags/Deep-NLP/"/>
    
      <category term="CNN" scheme="http://www.52coding.com.cn/tags/CNN/"/>
    
      <category term="ByteBet" scheme="http://www.52coding.com.cn/tags/ByteBet/"/>
    
  </entry>
  
  <entry>
    <title>Paper Reading - Attention Is All You Need</title>
    <link href="http://www.52coding.com.cn/2017/08/13/Attention%20Is%20All%20You%20Need/"/>
    <id>http://www.52coding.com.cn/2017/08/13/Attention Is All You Need/</id>
    <published>2017-08-13T01:30:19.000Z</published>
    <updated>2018-11-06T03:44:27.227Z</updated>
    
    <content type="html"><![CDATA[<p>Googleçš„<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">è¿™ç¯‡è®ºæ–‡</a>æå‡ºäº†ä¸€ä¸ªåªä½¿ç”¨Attentionæœºåˆ¶çš„ç¥ç»ç¿»è¯‘æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¾æ—§é‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨ï¼ˆEncoder-Decoderï¼‰æ¶æ„ï¼Œä½†æœªä½¿ç”¨RNNå’ŒCNNã€‚æ–‡ç« çš„ä¸»è¦ç›®çš„æ˜¯åœ¨å‡å°‘è®¡ç®—é‡å’Œæé«˜å¹¶è¡Œæ•ˆç‡çš„åŒæ—¶ä¸æŸå®³æœ€ç»ˆçš„å®éªŒç»“æœï¼Œåˆ›æ–°ä¹‹å¤„åœ¨äºæå‡ºäº†ä¸¤ä¸ªæ–°çš„Attentionæœºåˆ¶ï¼Œåˆ†åˆ«å«åš Scaled Dot-Product Attention å’Œ Multi-Head Attention.</p><a id="more"></a><h2><span id="æ•´ä½“æ¡†æ¶">æ•´ä½“æ¡†æ¶</span></h2><figure><img src="/images/at_all.png" alt="æ•´ä½“æ¡†æ¶"><figcaption>æ•´ä½“æ¡†æ¶</figcaption></figure><ul><li>è¾“å…¥ï¼šä¸€ä¸ªå¥å­ <span class="math inline">\(z = (z_1, â€¦, z_n)\)</span>ï¼Œå®ƒæ˜¯åŸå§‹å¥å­ <span class="math inline">\(x = (x_1, â€¦, x_n)\)</span> çš„ Embeddingï¼Œå…¶ä¸­ <span class="math inline">\(n\)</span> æ˜¯å¥å­é•¿åº¦ã€‚</li><li>è¾“å‡ºï¼šç¿»è¯‘å¥½çš„å¥å­ <span class="math inline">\((y_1, â€¦, y_m)\)</span></li></ul><h3><span id="encoder">Encoder</span></h3><ul><li>è¾“å…¥ <span class="math inline">\(z \in R^{n \times d_{model}}\)</span></li><li>è¾“å‡ºå¤§å°ä¸å˜</li><li>Positional Encoding</li><li>6ä¸ªBlock<ul><li>Multi-Head Self-Attention</li><li>Position-wise Feed Forward</li><li>Residual connection<ul><li>LayerNorm(x + Sublayer(x))</li><li>å¼•å…¥äº†æ®‹å·®ï¼Œå°½å¯èƒ½ä¿ç•™åŸå§‹è¾“å…¥xçš„ä¿¡æ¯</li></ul></li><li><span class="math inline">\(d_{model} = 512\)</span></li></ul></li></ul><h3><span id="decoder">Decoder</span></h3><ul><li>Positional Encoding</li><li>6ä¸ªBlock<ul><li>Multi-Head Self Attention (with mask)<ul><li>é‡‡ç”¨ 0-1mask æ¶ˆé™¤å³ä¾§å•è¯å¯¹å½“å‰å•è¯ attention çš„å½±å“</li></ul></li><li>Multi-Head Self Attention (with encoder)<ul><li>ä½¿ç”¨Encoderçš„è¾“å‡ºä½œä¸ºä¸€éƒ¨åˆ†è¾“å…¥</li></ul></li><li>Position-wise Feed Forward</li><li>Residual connection</li></ul></li></ul><h3><span id="multi-head-self-attention">Multi-Head Self Attention</span></h3><p><img src="/images/at_attention.png"></p><p><strong>Multi-Head Attention</strong></p><p>è¾“å…¥ <span class="math inline">\(Q \in R^{n \times d_{model}}\)</span>ã€<span class="math inline">\(K \in R^{n \times d_{model}}\)</span>ã€<span class="math inline">\(V \in R^{n \times d_{model}}\)</span>ï¼Œåˆ†åˆ«ä»£è¡¨queryã€key-value pairã€‚è¿™é‡Œçš„ key, value, å’Œ query éœ€è¦è§£é‡Šä¸€ä¸‹ï¼Œè¿™é‡ŒæŠŠ attention æŠ½è±¡ä¸ºå¯¹ value (<span class="math inline">\(V\)</span>) çš„æ¯ä¸ª token è¿›è¡ŒåŠ æƒï¼Œè€ŒåŠ æƒçš„ weightå°±æ˜¯ attention weightï¼Œè€Œ attention weight å°±æ˜¯æ ¹æ® query å’Œ key è®¡ç®—å¾—åˆ°ï¼Œå…¶æ„ä¹‰ä¸ºï¼š<strong>ä¸ºäº†ç”¨ value æ±‚å‡º query çš„ç»“æœ, æ ¹æ® query å’Œ key æ¥å†³å®šæ³¨æ„åŠ›åº”è¯¥æ”¾åœ¨ value çš„å“ªéƒ¨åˆ†</strong>ã€‚ä»¥å‰çš„ attention æ˜¯ç”¨ LSTM åš encoderï¼Œä¹Ÿå°±æ˜¯ç”¨å®ƒæ¥ç”Ÿæˆ key å’Œ value ï¼Œç„¶åç”± decoder æ¥ç”Ÿæˆ queryã€‚å…·ä½“åˆ° Bahdanau çš„è®ºæ–‡ Neural machine translation by jointly learning to align and translateï¼Œkey å’Œ value æ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯æ–‡ä¸­çš„ <span class="math inline">\(h\)</span>ï¼Œè€Œ query æ˜¯æ–‡ä¸­çš„ <span class="math inline">\(s\)</span>ã€‚è€Œåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼š</p><ul><li>åœ¨encoderå—ä¸­ï¼Œkey, value, query åŒä¸º<code>encoder_input</code>ï¼ˆä¸Šä¸€å±‚çš„è¾“å‡ºï¼‰ï¼Œå› ä¸ºæ˜¯<a href="https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/self_attention_embedding.md" target="_blank" rel="noopener">self-attention</a>ï¼Œå¯ä»¥ç†è§£ä¸ºç”Ÿæˆå¯¹è¿™ä¸ªå¥å­çš„ç¼–ç ï¼Œå¯ä»¥å…¨é¢è·å–è¾“å…¥åºåˆ—ä¸­positionsä¹‹é—´ä¾èµ–å…³ç³»ã€‚</li><li>åœ¨decoderå—ä¸­ï¼Œç¬¬ä¸€å±‚Multi-Head Attentionçš„è¾“å…¥éƒ½ä¸º<code>decoder_input</code>ï¼›ç¬¬äºŒå±‚çš„ <span class="math inline">\(Q\)</span> ä¸ºå‰ä¸€å±‚çš„è¾“å‡ºï¼Œ<span class="math inline">\(K, V\)</span> ä¸ºencoderçš„è¾“å‡ºã€‚å¯ä»¥ç†è§£ä¸ºï¼Œæ¯”å¦‚åˆšç¿»è¯‘å®Œä¸»è¯­ï¼Œæ¥ä¸‹æ¥æƒ³è¦æ‰¾è°“è¯­ï¼Œã€æ‰¾è°“è¯­ã€‘è¿™ä¸ªä¿¡æ¯å°±æ˜¯ queryï¼Œç„¶å key æ˜¯æºå¥å­çš„ç¼–ç ï¼Œé€šè¿‡ query å’Œ key è®¡ç®—å‡º attention weight ï¼ˆåº”è¯¥å…³æ³¨çš„è°“è¯­çš„ä½ç½®ï¼‰ï¼Œæœ€åå’Œ value ï¼ˆæºå¥å­ç¼–ç ï¼‰è®¡ç®—åŠ æƒå’Œã€‚</li></ul><p>ç„¶åæŠŠ <span class="math inline">\(Q, K, V\)</span> çº¿æ€§æ˜ å°„ <span class="math inline">\(h\)</span> æ¬¡ï¼Œåˆ†åˆ«æ˜ å°„åˆ° <span class="math inline">\(d_k, d_k, d_v\)</span> ç»´åº¦ï¼Œæ€»çš„attentionä¸ºæ¯ä¸ªæ˜ å°„çš„attentionè¿æ¥èµ·æ¥ï¼ˆè¿™ <span class="math inline">\(h\)</span> ä¸ªattentionå¯ä»¥å¹¶è¡Œè®¡ç®—ï¼‰ï¼Œå³ï¼š</p><p><img src="/images/at_multi.png"></p><p>å…¶ä¸­ï¼ŒæŠ•å½±çš„å‚æ•°çŸ©é˜µ <span class="math inline">\(W^Q_i \in R^{d_{model} \times d_k}\)</span>, <span class="math inline">\(W^K_i \in R^{d_{model} \times d_k}\)</span>, <span class="math inline">\(W^V_i \in R^{d_{model} \times d_v}\)</span>. åœ¨è®ºæ–‡ä¸­ <span class="math inline">\(h = 8\)</span>ï¼Œ<span class="math inline">\(d_k = d_v = \frac{d_{model}}{h} = 64\)</span>ï¼Œæ‰€ä»¥è¿™å±‚çš„è¾“å‡ºå’Œè¾“å…¥å¤§å°ç›¸åŒã€‚</p><p>è¿™äº›çº¿æ€§æ˜ å°„ä½¿å¾—æ¨¡å‹å¯ä»¥ä»ä¸åŒçš„å­ç©ºé—´çš„ä¸åŒä½ç½®ä¸­å­¦ä¹ æ³¨æ„åŠ›ï¼</p><p><strong>Scaled Dot-Product Attention</strong></p><p>ä¸Šå¼ä¸­çš„attentionæ­£æ˜¯Scaled Dot-Product Attentionï¼Œå®ƒä¹Ÿæ¥æ”¶ <span class="math inline">\(Q, K, V\)</span> ä¸‰ä¸ªå‚æ•°ï¼Œè®¡ç®—æ–¹æ³•å¦‚ä¸‹ï¼š <span class="math display">\[Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span> Dot-ProductæŒ‡çš„æ˜¯ <span class="math inline">\(QK^T\)</span>ï¼Œscaledæ˜¯æŒ‡é™¤ä»¥äº† <span class="math inline">\(\sqrt{d_k}\)</span> ï¼ˆå› ä¸ºå‡è®¾ä¸¤ä¸ª <span class="math inline">\(d_k\)</span> ç»´å‘é‡æ¯ä¸ªåˆ†é‡éƒ½æ˜¯ä¸€ä¸ªç›¸äº’ç‹¬ç«‹çš„æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„éšæœºå˜é‡ï¼Œé‚£ä¹ˆä»–ä»¬çš„ç‚¹ä¹˜çš„æ–¹å·®å°±æ˜¯ <span class="math inline">\(d_k\)</span>ï¼Œæ¯ä¸€ä¸ªåˆ†é‡é™¤ä»¥ <span class="math inline">\(\sqrt{d_k}\)</span> å¯ä»¥è®©ç‚¹ä¹˜çš„æ–¹å·®å˜æˆ 1ï¼‰ã€‚</p><p>æ€»å…±æœ‰ä¸¤ç§æµè¡Œçš„attentionè®¡ç®—æ–¹æ³•ï¼š</p><ul><li>additive attention<ul><li>é€šè¿‡ä¸€ä¸ªå°å‹ç¥ç»ç½‘ç»œè®¡ç®—æ³¨æ„åŠ›</li></ul></li><li>dot-product attention<ul><li>ä¸Šé¢çš„æ–¹æ³•</li></ul></li></ul><p>å…¶ä¸­ç¬¬äºŒç§æ–¹æ³•æ¯”ç¬¬ä¸€ç§æ–¹æ³•å¿«å¾ˆå¤šï¼Œç¬¬ä¸€ç§æ–¹æ³•åœ¨ <span class="math inline">\(d_k\)</span> è¾ƒå¤§æ—¶æ¯”ç¬¬äºŒç§æ–¹æ³•è¡¨ç°å¥½ï¼Œè®ºæ–‡ä½œè€…è§‰å¾—å¯èƒ½æ˜¯å› ä¸ºdot-productä½¿æ¢¯åº¦å˜å¾—å¾ˆå¤§ä»¥è‡³äºå¤±å»ä½œç”¨ï¼Œæ‰€ä»¥è¿›è¡Œäº†scaleã€‚</p><p><strong>å¯è§†åŒ– self-attention</strong></p><p><img src="/images/attn_vis.png"></p><p><img src="/images/attn_vis2.png"></p><h3><span id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</span></h3><p><span class="math display">\[FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2\]</span></p><p>è¿™æ˜¯ä¸€ä¸ª MLP ï¼ˆå¤šå±‚ï¼‰ç½‘ç»œï¼Œä¸Šå±‚çš„è¾“å‡ºä¸­ï¼Œæ¯ä¸ª <span class="math inline">\(d_{model}\)</span> ç»´å‘é‡ <span class="math inline">\(x\)</span> åœ¨æ­¤å…ˆç”± <span class="math inline">\(xW_1+b_1\)</span> å˜ä¸º <span class="math inline">\(d_{ff}\)</span> ç»´çš„ <span class="math inline">\(x&#39;\)</span>ï¼Œå†ç»è¿‡ <span class="math inline">\(\max(0, x&#39;)W_2+b_2\)</span> å›å½’ <span class="math inline">\(d_{model}\)</span> ç»´ã€‚ä¹‹åå†æ˜¯ä¸€ä¸ª residual connectionã€‚è¾“å‡ºå¤§å°å’Œè¾“å…¥å¤§å°ä¸€æ ·ï¼Œéƒ½ <span class="math inline">\(\in R^{n \times d_{model}}\)</span>.</p><h3><span id="positional-encoding">Positional Encoding</span></h3><p>å› ä¸ºè¿™ä¸ªç½‘ç»œæ²¡æœ‰ recurrenceï¼ˆå› ä¸ºdecoderåœ¨è®­ç»ƒæ—¶ç»™ground truthåšä¸ºè¾“å…¥ï¼Œè¿™æ ·ç”Ÿæˆä¸åŒä½ç½®çš„è¯æ˜¯å¯ä»¥å¹¶è¡Œçš„ï¼‰å’Œ convolutionï¼Œä¸ºäº†è¡¨ç¤ºè¯åœ¨åºåˆ—ä¸­çš„ä½ç½®ä¿¡æ¯ï¼Œè¦ç”¨ä¸€ç§ç‰¹æ®Šçš„ä½ç½®ç¼–ç ã€‚</p><p>æœ¬ç¯‡è®ºæ–‡ä¸­ä½¿ç”¨ <span class="math inline">\(\sin\)</span> å’Œ <span class="math inline">\(\cos\)</span> æ¥ç¼–ç ï¼š <span class="math display">\[\begin{array}{lcl}PE_{(pos, 2i)}        &amp; = &amp; \sin(\frac{pos}{10000^{2i/d_{model}}})\\PE_{(pos, 2i+1)}        &amp; = &amp; \cos(\frac{pos}{10000^{2i/d_{model}}})\end{array}\]</span> å…¶ä¸­ï¼Œ<span class="math inline">\(pos\)</span> ä»£è¡¨ä½ç½®ï¼Œ<span class="math inline">\(i\)</span> ä»£è¡¨ç»´åº¦ï¼ˆ<span class="math inline">\([0, d_{model}-1]\)</span>ï¼‰ã€‚</p><p>è¿™æ ·åšçš„ç›®çš„æ˜¯å› ä¸ºæ­£å¼¦å’Œä½™å¼¦å‡½æ•°å…·æœ‰å‘¨æœŸæ€§ï¼Œå¯¹äºå›ºå®šé•¿åº¦åå·® <span class="math inline">\(k\)</span>ï¼ˆç±»ä¼¼äºå‘¨æœŸï¼‰ï¼Œ<span class="math inline">\(pos+k\)</span> ä½ç½®çš„ PE å¯ä»¥è¡¨ç¤ºæˆå…³äº <span class="math inline">\(pos\)</span> ä½ç½® PE çš„ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼ˆ<span class="math inline">\(\sin(pos + k) =\sin(pos)\cos(k)+\sin(k)\cos(pos)\)</span>ï¼‰ï¼Œè¿™æ ·å¯ä»¥æ–¹ä¾¿æ¨¡å‹å­¦ä¹ è¯ä¸è¯ä¹‹é—´çš„ä¸€ä¸ªç›¸å¯¹ä½ç½®å…³ç³»ã€‚</p><p>å¦ä¸€ç§è§£é‡Šï¼Œæ¥è‡ª <a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650728452&amp;idx=4&amp;sn=fcc845a7ff15e6ceb331161d71899402&amp;chksm=871b2c7ab06ca56c9d746a2c2977578ec391ed526ec05d7b1da1910d1987597b5801ac6f98d1&amp;mpshare=1&amp;scene=23&amp;srcid=0701ZtSHzIyPmVKwYJdpsdvM%23rd" target="_blank" rel="noopener">WarBean</a> ï¼š</p><blockquote><p>æ¯ä¸¤ä¸ªç»´åº¦æ„æˆä¸€ä¸ªäºŒç»´çš„å•ä½å‘é‡ï¼Œæ€»å…±æœ‰ <span class="math inline">\(d_{model} / 2\)</span> ç»„ã€‚æ¯ä¸€ç»„å•ä½å‘é‡ä¼šéšç€ <span class="math inline">\(pos\)</span> çš„å¢å¤§è€Œæ—‹è½¬ï¼Œä½†æ˜¯æ—‹è½¬å‘¨æœŸä¸åŒï¼ŒæŒ‰ç…§è®ºæ–‡é‡Œé¢çš„è®¾ç½®ï¼Œæœ€å°çš„æ—‹è½¬å‘¨æœŸæ˜¯ <span class="math inline">\(2\pi\)</span>ï¼Œæœ€å¤§çš„æ—‹è½¬å‘¨æœŸæ˜¯ <span class="math inline">\(10000 \times 2\pi\)</span>ã€‚è‡³äºä¸ºä»€ä¹ˆè¯´ç›¸é‚» <span class="math inline">\(k\)</span> æ­¥çš„ position embedding å¯ä»¥ç”¨ä¸€ä¸ªçº¿æ€§å˜æ¢å¯¹åº”ä¸Šï¼Œæ˜¯å› ä¸ºä¸Šè¿°æ¯ç»„å•ä½å‘é‡çš„æ—‹è½¬æ“ä½œå¯ä»¥ç”¨è¡¨ç¤ºä¸ºä¹˜ä»¥ä¸€ä¸ª 2 x 2 çš„æ—‹è½¬çŸ©é˜µã€‚</p></blockquote><h3><span id="ç‰¹ç‚¹">ç‰¹ç‚¹</span></h3><ol type="1"><li><p>è®­ç»ƒé˜¶æ®µå®Œå…¨å¯å¹¶è¡Œï¼ˆå› ä¸ºdecoderåœ¨è®­ç»ƒæ—¶ç»™ground truthåšä¸ºè¾“å…¥ï¼Œè¿™æ ·ç”Ÿæˆä¸åŒä½ç½®çš„è¯æ˜¯å¯ä»¥å¹¶è¡Œçš„ï¼Œè€Œencoderä¸€æ¬¡å¤„ç†ä¸€æ•´ä¸ªå¥å­ï¼‰</p></li><li><p>è§£å†³ long dependency çš„é—®é¢˜</p><blockquote><p>ä¼ ç»Ÿçš„ç”¨RNNå»ºæ¨¡è¯­è¨€çš„æ—¶åºç‰¹å¾ï¼Œå‰é¢çš„å•è¯ä¿¡æ¯éƒ½ä¾æ¬¡feedåˆ°åé¢ä¸€ä¸ªå•è¯ï¼Œè¿™ç§ä¿¡æ¯çš„å †å æ„Ÿè§‰æœ‰ç‚¹æµªè´¹ï¼Œè€Œä¸”åè€ŒæŠŠä¿¡æ¯ç³…æ‚åœ¨ä¸€èµ·ä¸å¥½åŒºåˆ†ï¼Œè™½ç„¶decoderé˜¶æ®µå¯¹æ¯ä¸ªå•è¯å¯¹åº”çš„encoderè¾“å‡ºä½ç½®åšattentionï¼Œä½†æ¯ä¸ªencoderè¾“å‡ºå·²ç»å¤¹æ‚äº†å‰é¢å•è¯çš„ä¿¡æ¯ã€‚åŒæ—¶å‰é¢å•è¯ä¿¡æ¯å¾€åä¼ ï¼Œèµ°çš„è·¯å¾„æ¯”è¾ƒé•¿ï¼Œä¹Ÿå°±æ˜¯long dependencyçš„é—®é¢˜ï¼Œè™½ç„¶LSTM/GRUè¿™ç§ç»“æ„èƒ½ä¸€å®šç¨‹åº¦ä¸Šè§£å†³ï¼Œä½†æ˜¯æ¯•ç«Ÿä¸èƒ½å®Œå…¨å»æ‰ long dependencyã€‚è€Œconvåœ¨å¤„ç†dependencyé—®é¢˜æ—¶ï¼Œåˆ©ç”¨å·ç§¯çš„æ„Ÿå—é‡receptive fieldï¼Œé€šè¿‡å †å å·ç§¯å±‚æ¥æ‰©å¤§æ¯ä¸ªencoderè¾“å‡ºä½ç½®æ‰€è¦†ç›–å•è¯çš„èŒƒå›´ï¼Œæ¯ä¸ªå•è¯èµ°çš„è·¯å¾„å¤§è‡´æ˜¯logk(n)æ­¥ï¼Œç¼©çŸ­äº†dependencyçš„é•¿åº¦ã€‚è€Œè¿™ç¯‡è®ºæ–‡çš„åšæ³•æ˜¯ç›´æ¥ç”¨encoderæˆ–è€…decoderçš„å±‚ä¸å±‚ä¹‹é—´ç›´æ¥ç”¨attentionï¼Œå¥å­ä¸­çš„å•è¯dependencyé•¿åº¦æœ€å¤šåªæœ‰1ï¼Œå‡å°‘äº†ä¿¡æ¯ä¼ è¾“è·¯å¾„ã€‚è€Œä¸”è¿™ç§attentionçš„æ–¹å¼ç›´æ¥å¯ä»¥æŒ–æ˜å¥å­å†…éƒ¨å•è¯ä¸å•è¯çš„è¯­ä¹‰ç»„åˆå…³ç³»ï¼Œå°†å®ƒä½œä¸ºä¸€ä¸ªè¯­ä¹‰æ•´ä½“ï¼Œä½¿å¾—ç¿»è¯‘æ—¶æ›´å¥½åœ°åˆ©ç”¨å•è¯ç»„åˆç”šè‡³æ˜¯çŸ­è¯­çš„ä¿¡æ¯ï¼Œæ›´å¥½åœ°decodeå‡ºè¯­ä¹‰åŒ¹é…çš„ç›®æ ‡è¯­è¨€å•è¯ï¼ˆè½¬è‡ª<a href="https://www.zhihu.com/question/61077555/answer/183884003" target="_blank" rel="noopener">è°­æ—­</a>ï¼‰</p></blockquote></li></ol><p><img src="/images/path_table.png"></p><h3><span id="å¼€æºä»£ç åˆ†æ">å¼€æºä»£ç åˆ†æ</span></h3><p>ä»£ç æ¥è‡ªï¼šhttps://github.com/jadore801120/attention-is-all-you-need-pytorch</p><p>ä½¿ç”¨ PyTorch æ¡†æ¶</p><p><strong>Encoder</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">''' A encoder model with self attention mechanism. '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_src_vocab, n_max_seq, n_layers=<span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 n_head=<span class="number">8</span>, d_k=<span class="number">64</span>, d_v=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">         d_word_vec=<span class="number">512</span>, d_model=<span class="number">512</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                 d_inner_hid=<span class="number">1024</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="comment"># .....</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src_seq, src_pos)</span>:</span></span><br><span class="line">        <span class="comment"># Word embedding look up</span></span><br><span class="line">        enc_input = self.src_word_emb(src_seq)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Position Encoding addition</span></span><br><span class="line">        enc_input += self.position_enc(src_pos)</span><br><span class="line">        enc_outputs, enc_slf_attns = [], []</span><br><span class="line"></span><br><span class="line">        enc_output = enc_input</span><br><span class="line">        enc_slf_attn_mask = </span><br><span class="line">        get_attn_padding_mask(src_seq, src_seq)</span><br><span class="line">        <span class="comment"># å¯¹æ¯ä¸€å±‚è®¡ç®— encoder output å’Œ self-attention</span></span><br><span class="line">        <span class="keyword">for</span> enc_layer <span class="keyword">in</span> self.layer_stack:</span><br><span class="line">            enc_output, enc_slf_attn = enc_layer(</span><br><span class="line">                enc_output, slf_attn_mask=enc_slf_attn_mask)</span><br><span class="line">            <span class="comment"># æŠŠæ¯å±‚çš„è¾“å‡ºæ·»åŠ åˆ°æ€»è¾“å‡ºåˆ—è¡¨</span></span><br><span class="line">            enc_outputs += [enc_output]</span><br><span class="line">            enc_slf_attns += [enc_slf_attn]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> enc_outputs, enc_slf_attns</span><br></pre></td></tr></table></figure><p><strong>Positional Encoding</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">position_encoding_init</span><span class="params">(n_position, d_pos_vec)</span>:</span></span><br><span class="line">    <span class="string">''' Init the sinusoid position encoding table '''</span></span><br><span class="line"><span class="comment"># å¯¹æ¯ä¸ªä½ç½®è¿›è¡Œç¼–ç ï¼Œä½ç½®0ä¸ºå…¨0ï¼Œå…¶ä»–ä½ç½®æŒ‰ç…§ç›¸åº”å…¬å¼è®¡ç®—</span></span><br><span class="line">    position_enc = np.array([</span><br><span class="line">        [pos / np.power(<span class="number">10000</span>, <span class="number">2</span>*i/d_pos_vec) </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(d_pos_vec)]</span><br><span class="line">        <span class="keyword">if</span> pos != <span class="number">0</span> <span class="keyword">else</span> np.zeros(d_pos_vec) </span><br><span class="line">        <span class="keyword">for</span> pos <span class="keyword">in</span> range(n_position)])</span><br><span class="line"><span class="comment"># dim 2i</span></span><br><span class="line">    position_enc[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(position_enc[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>]) </span><br><span class="line">    <span class="comment"># dim 2i+1</span></span><br><span class="line">    position_enc[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(position_enc[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>]) </span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(position_enc)</span><br></pre></td></tr></table></figure><p><strong>Scaled Dot-Product Attention</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScaledDotProductAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">''' Scaled Dot-Product Attention '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, attn_dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(ScaledDotProductAttention, self).__init__()</span><br><span class="line">        self.temper = np.power(d_model, <span class="number">0.5</span>)</span><br><span class="line">        self.dropout = nn.Dropout(attn_dropout)</span><br><span class="line">        self.softmax = BottleSoftmax()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, q, k, v, attn_mask=None)</span>:</span></span><br><span class="line">        attn = torch.bmm(q, k.transpose(<span class="number">1</span>, <span class="number">2</span>)) / self.temper</span><br><span class="line">        ...</span><br><span class="line">        attn = self.softmax(attn)</span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line">        output = torch.bmm(attn, v)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br></pre></td></tr></table></figure><p>ä»ä¸­å¯è§ï¼Œself-attention æŒ‡ softmax æ“ä½œä¹‹åçš„éƒ¨åˆ†ï¼Œå±‚è¾“å‡ºæ˜¯ <span class="math inline">\(attention \times V\)</span>ã€‚</p><p><strong>Encoder layer</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">''' Compose with two layers '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_inner_hid, n_head, d_k, d_v,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.slf_attn = MultiHeadAttention(</span><br><span class="line">            n_head, d_model, d_k, d_v, dropout=dropout)</span><br><span class="line">        self.pos_ffn = PositionwiseFeedForward(d_model, </span><br><span class="line">                                              d_inner_hid, </span><br><span class="line">                                              dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, enc_input, slf_attn_mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># Q, K, V éƒ½æ˜¯ enc_input</span></span><br><span class="line">        enc_output, enc_slf_attn = self.slf_attn(</span><br><span class="line">            enc_input, enc_input, enc_input,</span><br><span class="line">            attn_mask=slf_attn_mask)</span><br><span class="line">        enc_output = self.pos_ffn(enc_output)</span><br><span class="line">        <span class="keyword">return</span> enc_output, enc_slf_attn</span><br></pre></td></tr></table></figure><p>å¯è§ï¼Œä¼ ç»™ <code>MultiHeadAttention</code> çš„<span class="math inline">\(Q, K, V\)</span> éƒ½æ˜¯ <code>enc_input</code>ã€‚</p><p><strong>Decoder Layer</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">''' Compose with three layers '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="comment"># ....</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, dec_input, enc_output, </span></span></span><br><span class="line"><span class="function"><span class="params">                slf_attn_mask=None, dec_enc_attn_mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># ç¬¬ä¸€ä¸ªattentionå±‚æ¥æ”¶çš„ Q, K, V éƒ½æ˜¯ dec_input</span></span><br><span class="line">        dec_output, dec_slf_attn = self.slf_attn(</span><br><span class="line">            dec_input, dec_input, dec_input, </span><br><span class="line">            attn_mask=slf_attn_mask)</span><br><span class="line">        <span class="comment"># ç¬¬äºŒä¸ªattentionå±‚æ¥æ”¶çš„ Q æ˜¯ dec_outputï¼Œ</span></span><br><span class="line">        <span class="comment"># K å’Œ V æ˜¯ enc_output</span></span><br><span class="line">        dec_output, dec_enc_attn = self.enc_attn(</span><br><span class="line">            dec_output, enc_output, enc_output, </span><br><span class="line">            attn_mask=dec_enc_attn_mask)</span><br><span class="line">        dec_output = self.pos_ffn(dec_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dec_output, dec_slf_attn, dec_enc_attn</span><br></pre></td></tr></table></figure><p><strong>Transformer</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">''' å®Œæ•´çš„ transformer '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            self, n_src_vocab, n_tgt_vocab, n_max_seq,</span></span></span><br><span class="line"><span class="function"><span class="params">        n_layers=<span class="number">6</span>, n_head=<span class="number">8</span>, d_word_vec=<span class="number">512</span>, d_model=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        d_inner_hid=<span class="number">1024</span>, d_k=<span class="number">64</span>, d_v=<span class="number">64</span>, dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        proj_share_weight=True, embs_share_weight=True)</span>:</span></span><br><span class="line"></span><br><span class="line">        super(Transformer, self).__init__()</span><br><span class="line">        <span class="comment"># åˆå§‹åŒ– encoder</span></span><br><span class="line">        self.encoder = Encoder(</span><br><span class="line">            n_src_vocab, n_max_seq, n_layers=n_layers,</span><br><span class="line">            n_head=n_head, d_word_vec=d_word_vec, </span><br><span class="line">            d_model=d_model, d_inner_hid=d_inner_hid,</span><br><span class="line">            dropout=dropout)</span><br><span class="line">        <span class="comment"># åˆå§‹åŒ– decoder</span></span><br><span class="line">        self.decoder = Decoder(</span><br><span class="line">            n_tgt_vocab, n_max_seq, n_layers=n_layers,</span><br><span class="line">            n_head=n_head, d_word_vec=d_word_vec, </span><br><span class="line">            d_model=d_model, d_inner_hid=d_inner_hid,</span><br><span class="line">            dropout=dropout)</span><br><span class="line">      <span class="comment"># ....</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt)</span>:</span></span><br><span class="line">        src_seq, src_pos = src</span><br><span class="line">        tgt_seq, tgt_pos = tgt</span><br><span class="line"></span><br><span class="line">        tgt_seq = tgt_seq[:, :<span class="number">-1</span>]</span><br><span class="line">        tgt_pos = tgt_pos[:, :<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># ç¼–ç </span></span><br><span class="line">        enc_outputs, enc_slf_attns = self.encoder(src_seq,</span><br><span class="line">                                                  src_pos)</span><br><span class="line">        <span class="comment"># è§£ç </span></span><br><span class="line">        dec_outputs, dec_slf_attns, dec_enc_attns =</span><br><span class="line">        self.decoder( tgt_seq, tgt_pos, src_seq,</span><br><span class="line">                         enc_outputs)</span><br><span class="line">        dec_output = dec_outputs[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        seq_logit = self.tgt_word_proj(dec_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> seq_logit.view(<span class="number">-1</span>, seq_logit.size(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Googleçš„&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;è¿™ç¯‡è®ºæ–‡&lt;/a&gt;æå‡ºäº†ä¸€ä¸ªåªä½¿ç”¨Attentionæœºåˆ¶çš„ç¥ç»ç¿»è¯‘æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¾æ—§é‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨ï¼ˆEncoder-Decoderï¼‰æ¶æ„ï¼Œä½†æœªä½¿ç”¨RNNå’ŒCNNã€‚æ–‡ç« çš„ä¸»è¦ç›®çš„æ˜¯åœ¨å‡å°‘è®¡ç®—é‡å’Œæé«˜å¹¶è¡Œæ•ˆç‡çš„åŒæ—¶ä¸æŸå®³æœ€ç»ˆçš„å®éªŒç»“æœï¼Œåˆ›æ–°ä¹‹å¤„åœ¨äºæå‡ºäº†ä¸¤ä¸ªæ–°çš„Attentionæœºåˆ¶ï¼Œåˆ†åˆ«å«åš Scaled Dot-Product Attention å’Œ Multi-Head Attention.&lt;/p&gt;
    
    </summary>
    
      <category term="å­¦ä¹ ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://www.52coding.com.cn/tags/NLP/"/>
    
      <category term="NMT" scheme="http://www.52coding.com.cn/tags/NMT/"/>
    
      <category term="Attention" scheme="http://www.52coding.com.cn/tags/Attention/"/>
    
      <category term="Deep NLP" scheme="http://www.52coding.com.cn/tags/Deep-NLP/"/>
    
  </entry>
  
  <entry>
    <title>Deep NLP - Question Answering</title>
    <link href="http://www.52coding.com.cn/2017/08/12/Deep%20NLP%20-%20Question%20Answering/"/>
    <id>http://www.52coding.com.cn/2017/08/12/Deep NLP - Question Answering/</id>
    <published>2017-08-12T13:30:19.000Z</published>
    <updated>2018-11-06T03:45:15.601Z</updated>
    
    <content type="html"><![CDATA[<p>Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.</p><a id="more"></a><!-- toc --><ul><li><a href="#semantic-parsing">Semantic Parsing</a></li><li><a href="#reading-comprehension">Reading Comprehension</a></li><li><a href="#answer-sentence-selection">Answer Sentence Selection</a></li><li><a href="#visual-question-answering">Visual Question Answering</a></li><li><a href="#summary">Summary</a></li></ul><!-- tocstop --><p><strong>Questions</strong></p><table><colgroup><col style="width: 46%"><col style="width: 53%"></colgroup><thead><tr class="header"><th style="text-align: left;">Question</th><th style="text-align: left;">answer</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">When were the ï¬rst pyramids built?</td><td style="text-align: left;">2630 BC</td></tr><tr class="even"><td style="text-align: left;">Jean-Claude Juncker</td><td style="text-align: left;">Jean-Claude Juncker is a Luxembourgish politician. Since 2014, Juncker has been President of the European Commission.</td></tr><tr class="odd"><td style="text-align: left;">How old is Keir Starmer?</td><td style="text-align: left;">54 years</td></tr><tr class="even"><td style="text-align: left;">What is the current price for AAPL?</td><td style="text-align: left;">136.50 USD</td></tr><tr class="odd"><td style="text-align: left;">Whatâ€™s the weather like in London?</td><td style="text-align: left;">7 degrees Celsius. Clear with some clouds.</td></tr><tr class="even"><td style="text-align: left;">Whom did Juncker meet with?</td><td style="text-align: left;">The European Commission president was speaking after meeting with Irish Taoiseach Enda Kenny in Brussels.</td></tr><tr class="odd"><td style="text-align: left;">When did you get to this lecture?</td><td style="text-align: left;">Five minutes after it started.</td></tr><tr class="even"><td style="text-align: left;">Why do we yawn?</td><td style="text-align: left;">When weâ€™re bored or tired we donâ€™t breathe as deeply as we normally do. This causes a drop in our blood-oxygen levels and yawning helps us counter-balance that.</td></tr></tbody></table><p><strong>Why do we care about QA ?</strong></p><p>Because <strong>QA is awesome</strong></p><ol type="1"><li><p><strong>QA is an AI-complete problem.</strong></p><p>If we solve QA, we have solved every other problem, too.</p></li><li><p>Many immediate and obvious applications</p><p>Search, dialogue, information extraction, summarisation, ...</p></li><li><p>Some pretty nice results already</p><p>IBM Watson and Jeopardy!, Siri, Google Search ...</p></li><li><p>Lots left to do!</p><p>Plenty of interesting research and hard problems as well as low-hanging fruit.</p></li></ol><p><strong>Data</strong></p><table><colgroup><col style="width: 25%"><col style="width: 37%"><col style="width: 37%"></colgroup><thead><tr class="header"><th>question</th><th>context/source</th><th>answer</th></tr></thead><tbody><tr class="odd"><td>Factual questions</td><td>Sets of documents (corpus)</td><td>A single fact</td></tr><tr class="even"><td>Complex/narrative questions</td><td>A single document</td><td>An explanation</td></tr><tr class="odd"><td>Information Retrieval</td><td>Knowledge Base</td><td>A document</td></tr><tr class="even"><td>Library Reference</td><td>Non-linguistic types of data (GPS, images, sensors, ...)</td><td>A sentence or paragraph extracted from somewhere</td></tr><tr class="odd"><td></td><td></td><td>An image or other type of object</td></tr><tr class="even"><td></td><td></td><td>Another question</td></tr></tbody></table><p><strong>Question Taxonomy</strong></p><p>Many possible taxonomies for questions:</p><ul><li>Wh- words</li><li>Subject of question</li><li>The form of expected answers</li><li>Types of sources from which answers may be drawn</li></ul><p>For the purposes of building QA systems it is useful to start by considering the sources an answer may be drawn from. <strong>Focus on the answer</strong> rather than the question.</p><p><em>Three Questions for building a QA System</em></p><ul><li>What do the answers look like?</li><li>Where can I get the answers from?</li><li>What does my training data look like?</li></ul><p><strong>Areas in Question Answering</strong></p><ul><li>Reading Comprehension<ul><li>Answer based on a document</li><li>Context is a speciï¬c document</li></ul></li><li>Semantic Parsing<ul><li>Answer is a logical form, possible executed against a KB</li><li>Context is a Knowledge Base</li></ul></li><li>Visual QA<ul><li>Answer is simple and factual</li><li>Context is one/multiple image(s)</li></ul></li><li>Information Retrieval<ul><li>Answer is a document/paragraph/sentence</li><li>Context is a corpus of documents</li></ul></li><li>Library Reference<ul><li>Answer is another question</li><li>Context is the structured knowledge available in the library and the librarians view of it.</li></ul></li></ul><h2><span id="semantic-parsing">Semantic Parsing</span></h2><p>Semantic Parsing is the process of mapping natural language into a formal representation of its meaning. Depending on the chosen formalism this <strong>logical representation</strong> can be used to query a <strong>structured knowledge base</strong>.</p><p><img src="/images/NLP/se_par.png"></p><p><em>Semantic Parsing</em> is <strong>Questionâ†’Logical Form.</strong></p><p>We (often mistakenly) then assume that <strong>LFâ†’Answer</strong> is trivial.</p><p><strong>Knowledge Bases for QA with Semantic Parsing</strong></p><p>Knowledge bases typically represent their data as triplesï¼š</p><ul><li>Generally: <em>(relation, entity1, entity2)</em></li><li>(married-to, Michelle Obama, Barack Obama)</li><li>(member-of, United Kingdom, European Union)</li></ul><p>There are several (large) databases freely available to use, e.g.:</p><ul><li><strong>Freebase</strong>: 1.9 billion triples on general knowledge. Defunct as of 2016 and replaced by Google Knowledge Graph</li><li><strong>WikiData</strong>: Information on 25 million entities</li><li><strong>OpenStreetMap</strong>: 3 billion triples on geography</li><li><strong>GeoQuery</strong>: 700 facts about US geography. Tiny dataset, but frequently used in semantic parsing work.</li></ul><p><strong>Supervised Data is expensive!</strong></p><ul><li><strong>Free917</strong>: 917 freebase annotated questions</li><li><strong>GeoQuery</strong>: 880 questions on US geography</li><li><strong>NLMaps</strong>: 2,380 natural language queries on the OSM data</li></ul><p>These kinds of datasets are incredibly expensive to create as they require experts for the manual annotation process, who are trained in using a given database schema:</p><ul><li><p>â€œ<em>Where are kindergartens in Hamburg?</em>â€</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query(area(keyval(name,Hamburg)), nwr(keyval(amenity,kindergarten)), qtype(latlong))</span><br></pre></td></tr></table></figure></li></ul><p><strong>A Deep Learning Approach to Semantic Parsing</strong></p><p>Semantic parsing can be viewed as a sequence to sequence model, not unlike <strong>machine translation</strong>.</p><p><img src="/images/NLP/dl_sen.png"></p><p>Details</p><ul><li>âœ… Encode sentence with sequence models</li><li>âœ… Decode with standard mechanisms from MT</li><li>âŒ Supervised training data hard to come by</li><li>âŒ Depending on formalism used, highly complex target side</li><li>âŒ How to deal with proper nouns and numbers?</li></ul><p><strong>One Solution to Sparsity: Avoid Logical Forms</strong></p><p>Semantic parsing frequently reduce the reliance on supervised data (language-logical form) by exploiting other types of data such as <strong>question-answer pairs</strong> or corpora of <strong>questions only</strong>.</p><blockquote><p>Berant et al. (2013): Semantic Parsing on Freebase from QA Pairs Reddy et al. (2014): Large-scale Semantic Parsing without QA Pairs</p></blockquote><p><img src="/images/NLP/graph.png"></p><p><strong>Improved Neural Semantic Parsing</strong></p><p>We can apply the same idea to neural semantic parsing, and further take mechanisms from <strong>machine translation</strong> to improve performance and data eï¬ƒciency:</p><ul><li>Like in MT, using attention can be helpful<ul><li>Dong and Lapata (2016): Language to Logical Form with Neural Attention</li></ul></li><li>Exploit the highly rigid structure in the target side to constrain generation<ul><li>Liang et al. (2016): Neural Symbolic Machines</li><li>Ling et al. (2016): Latent predictor networks for code generation</li></ul></li><li>Make use of semi-supervised training to counter sparsity<ul><li>Kocisky et al. (2016): Semantic Parsing with Semi-Supervised Sequential Autoencoders</li></ul></li></ul><p><em>Generation with multiple sources</em></p><blockquote><p>Ling et al. (2016): Latent predictor networks for code generation</p></blockquote><p><img src="/images/NLP/multi_src.png"></p><p><strong>Semantic Parsing Summary</strong></p><ul><li>âœ… LF instead of answer makes system robust</li><li>âœ… Answer independent of question and parsing mechanism</li><li>âœ… Can deal with rapidly changing information</li><li>âŒ Constrained to queriable questions in DB schema</li><li>âŒ No database is large enough</li><li>âŒ Training data hard to ï¬nd</li></ul><p><em>Questions</em></p><ul><li>âœ… When were the pyramids built?</li><li>â“ Jean-Claude Juncker</li><li>âœ… How old is Keir Starmer?</li><li>âœ… What is the price for AAPL?</li><li>âœ… Whatâ€™s the weather in London?</li><li>âŒ Whom did Juncker meet with?</li><li>âŒ When did you get here?</li><li>âŒ Why do we yawn?</li></ul><p>Caveat: Each of these examples requires a <strong>diï¬€erent</strong> underlying KB!</p><h2><span id="reading-comprehension">Reading Comprehension</span></h2><p>Answer a question related to a given document.</p><p><strong>Corpora for Reading Comprehension</strong></p><ul><li><strong>CNN/DailyMail</strong>: Over 1 million cloze form QA pairs with articles from CNN and Mail online for context. Pick an anonymised entity.</li><li><strong>CBT</strong>: 700k QA pairs, childrenâ€™s books as context. Pick one of 10 candidates.</li><li><strong>SQuAD</strong>: 100k manual QA pairs with 500 Wikipedia articles for context. Answer is a span.</li></ul><p>Assumptions made in all of the above tasks</p><ul><li>Context is read on the ï¬‚y and unknown during training phase</li><li>Answer is contained in the context as a single word or span</li><li>This constraint does not hold for reading comprehension in general!</li></ul><p><em>CNN article Example</em></p><ul><li>Document<ul><li>The BBC producer allegedly struck by Jeremy Clarkson will not press charges against the â€œTop Gearâ€ host, his lawyer said Friday. Clarkson, who hosted one of the most-watched television shows in the world, was dropped by the BBC Wednesday after an internal investigation by the British broadcaster found he had subjected producer Oisin Tymon â€œto an unprovoked physical and verbal attack.â€ . . .</li></ul></li><li>Query<ul><li>Producer X will not press charges against Jeremy Clarkson, his lawyer says.</li></ul></li><li>Answer<ul><li>Oisin Tymon</li></ul></li></ul><p>We formulate <em>Cloze style</em> queries from the story paraphrases.</p><p>Out of vocabulary (OOV) and proper nouns are dealt with by <strong>replacing all entities with anonymised markers</strong>. This greatly reduces the vocabulary size.</p><ul><li><p>Document</p><ul><li><p>the ent381 producer allegedly struck by ent212 will not press</p><p>charges against the â€œ ent153 â€ host , his lawyer said friday . ent212 , who hosted one of the most - watched television shows in the world , was dropped by the ent381 wednesday after an internal investigation by the ent180 broadcaster found he had subjected producer ent193 â€œ to an unprovoked physical and verbal attack . â€ . . .</p></li></ul></li><li><p>Query</p><ul><li>Producer X will not press charges against ent212 , his lawyer says .</li></ul></li><li><p>Answer</p><ul><li>ent193</li></ul></li></ul><p><strong>A Generic Neural Model for Reading Comprehension</strong></p><p>Given context <span class="math inline">\(d\)</span> and question <span class="math inline">\(q\)</span>, the probability of an answer <span class="math inline">\(a\)</span> can be represented as: <span class="math display">\[p(a|q, d) \varpropto \exp(W(a)g(q, d)), \ \ s.t.a \in V\]</span> Details</p><ul><li>âœ… Encode question and context with sequence models</li><li>âœ… Combine <span class="math inline">\(q\)</span> and <span class="math inline">\(d\)</span> with an MLP or attention <span class="math inline">\(g\)</span></li><li>âœ… Select answer from attention map, by using a classiï¬er, or with generative setup</li><li>âŒ How to deal with out of vocabulary (OOV) terms?</li><li>âŒ How to deal with proper nouns and numbers?</li></ul><p><img src="/images/NLP/rc_attn.png"></p><ul><li>Read (encode) context document and question</li><li>Use question to attend to context</li><li>Use joint representation to generate answer<ul><li>Predict based on attention map</li><li>Generate conditioned on joint representation</li><li>Classify over set of candidate answers</li></ul></li></ul><p>Denote the outputs of a bidirectional LSTM as <span class="math inline">\(\overrightarrow{y(t)}\)</span> and <span class="math inline">\(\overleftarrow{y(t)}\)</span>. Form two encodings, one for the query and one for each token in the document, <span class="math display">\[u = \overrightarrow{y_q}(|q|)\ ||\ \overleftarrow{y_q}(1),\ \ y_d(t) = \overrightarrow{y_d}(t)\ ||\ \overleftarrow{y_d}(t)\]</span> The representation <span class="math inline">\(r\)</span> of the document <span class="math inline">\(d\)</span> is formed by a weighted sum of the token vectors. The weights are interpreted as the modelâ€™s attention, <span class="math display">\[\begin{alignat}{3}m(t) &amp;= \tanh(W_{ym}y_d(t)+W_{um}u) \\s(t) &amp;\varpropto \exp(w_{ms}^Tm(t)) \\r &amp;= y_ds \\\end{alignat}\]</span> Deï¬ne the joint document and query embedding via a non-linear combination: <span class="math display">\[g^{AR}(d, q) = \tanh(W_{rg}r+W_{ug}u)\]</span> Training</p><p><img src="/images/NLP/traing.png"></p><p>Models were trained using asynchronous minibatch stochastic gradient descent (RMSProp) on approximately 25 GPUs.</p><p><em>Attention Sum Reader</em></p><blockquote><p>Kadlec et al. (2016), Text Understanding with the Attention Sum Reader Network</p></blockquote><p>The model can be modiï¬ed to make use of the fact that <strong>the answer is a word from the context document</strong>. Now we calculate the probability of the answer being in position <span class="math inline">\(i\)</span> of the context: <span class="math display">\[p(i|q, d)  \varpropto \exp(f_i(d)\cdot g(q))\]</span> Positional probabilities can then be summed to form token-based probabilities: <span class="math display">\[P(w|q, d) \varpropto \sum_{i(w,d)}P(i|q,d)\]</span> The rest of the model is equivalent to the attentive reader model presented before.</p><p><strong>Reading Comprehension Summary</strong></p><ul><li>âœ… Ask questions in context</li><li>âœ… Easily used in discriminative and generative fashion</li><li>âœ… Large datasets available</li><li>âŒ Constraint on context often artiï¬cial</li><li>âŒ Many types of questions unanswerable</li></ul><p><em>Questions</em></p><ul><li>â“ When were the pyramids built?</li><li>â“ Jean-Claude Juncker</li><li>â“ How old is Keir Starmer?</li><li>â“ What is the price for AAPL?</li><li>â“ Whatâ€™s the weather in London?</li><li>âœ… Whom did Juncker meet with?</li><li>âŒ When did you get here?</li><li>âŒ Why do we yawn?</li></ul><p>Caveat: Need context for any of these, and incredibly up-to-date context for some of these.</p><h2><span id="answer-sentence-selection">Answer Sentence Selection</span></h2><p><img src="/images/NLP/trump.jpg"></p><p><strong>Answer Sentence Selection</strong> describes the task of picking a suitable sentence from a corpus that can be used to answer a question.</p><ul><li><strong>Questions</strong>: Factual questions, possibly with context</li><li><strong>Data Source</strong>: â€œThe Webâ€ or the output of some IR system</li><li><strong>Answer</strong>: One or several excerpts pertinent to the answer</li></ul><p>The answer is <strong>guaranteed to be extracted</strong>, while in reading comprehension it could be either generated or extracted.</p><p><strong>Data Corpora</strong></p><ul><li><strong>TREC QA track (8-13)</strong>: Several hundred manually-annotated question answer pairs with around 20 candidates per instance.</li><li><strong>MS MARCO</strong>: 100k question-answer pairs with 10 contextual passages each. Can also be used as a QA dataset for reading comprehension.</li></ul><p>Likewise, answer sentence selection plays a role in any information retrieval setup, and datasets from IR and other QA tasks can easily be converted into answer selection style datasets.</p><p><em>A Neural Model for Answer Sentence Selection</em></p><blockquote><p>Yu et al., 2014</p></blockquote><p>We need to compute the probability of an answer candidate <span class="math inline">\(a\)</span> and a question <span class="math inline">\(q\)</span> matching. Note that this is diï¬€erent from the previous task as we now calculate that score independently of all other candidates: <span class="math display">\[p(y=1|q, a) = \sigma(q^TMa + b)\]</span> <img src="/images/NLP/ass.png"></p><p><strong>Evaluation</strong></p><p>Unlike single entity style QA where we can use a simple accuracy measure, tasks such as answer sentence selection require more specialised metrics for evaluating model performance.</p><table><colgroup><col style="width: 20%"><col style="width: 40%"><col style="width: 40%"></colgroup><thead><tr class="header"><th>measure</th><th>description</th><th>formula</th></tr></thead><tbody><tr class="odd"><td>Accuracy</td><td>Binary measure</td><td>#true/#toal</td></tr><tr class="even"><td>Mean Reciprocal Rank</td><td>Measures position of ï¬rst relevant document in return set.</td><td><span class="math inline">\(\frac{1}{\|Q\|}\sum_{i=1}^{\|Q\|}\frac{1}{rank_i}\)</span></td></tr><tr class="odd"><td>BLEU Score</td><td>Machine Translation measure for translation accuracy</td><td>complicated</td></tr></tbody></table><p><strong>Answer Selection Summary</strong></p><ul><li>âœ… Designed to deal with large amounts of context</li><li>âœ… More robust than â€˜trueâ€™ QA systems as it turns provides context with its answers</li><li>âœ… Obvious pipeline step between IR and QA</li><li>âŒ Does not provide answers, provides context only</li><li>âŒ Real-world use depends on underlying IR pipeline</li></ul><p><em>Questions</em></p><ul><li>âœ… When were the pyramids built?</li><li>âœ… Jean-Claude Juncker</li><li>âœ… How old is Keir Starmer?</li><li>âŒ What is the price for AAPL?</li><li>âŒ Whatâ€™s the weather in London?</li><li>â“ Whom did Juncker meet with?</li><li>âŒ When did you get here?</li><li>âœ… Why do we yawn?</li></ul><p>Note: Things like age or stock price may produce answers, but with no guarantee of accuracy (any mention of any AAPL price might be a good ï¬t).</p><h2><span id="visual-question-answering">Visual Question Answering</span></h2><p>Sometimes questions require context outside of pure language.</p><p><img src="/images/NLP/visual.jpg"></p><p><strong>Task and Corpora</strong></p><p>In recent years a number of visual QA datasets have sprung up. Some of the more popular ones include:</p><ul><li><strong>VisualQA</strong>: Agrawal et al. (2015)</li><li><strong>VQA 2.0</strong> Goyal et al. (2016)</li><li><strong>COCO-QA</strong> Ren et al. (2015)</li></ul><p>Details between these datasets vary, but the basic organisation remains the same of images paired with simple questions and answers (either free form or from a list of options).</p><p>All of these are reasonably large (100ks of images, over 1M questions).</p><p><strong>Visual QA</strong></p><ul><li>Question is language â†’ some encoder</li><li>Context is a single picture â†’ convolutional network</li><li>Answer is a single word â†’ classiï¬er function</li></ul><p>We have covered all the components already:</p><p><img src="/images/NLP/visualQA.jpg"></p><p><strong>Blind Model</strong></p><blockquote><p>Goyal et al. (2016)</p></blockquote><p><em>Ignoring the images is a good baseline!</em></p><ul><li>What colour is the cat?</li><li>How many chairs are around the table?</li><li>What furniture is in the bedroom?</li><li>Where is the person sleeping?</li></ul><p>We can get reasonably good guesses in at many of these questions without seeing an image for context.</p><p><strong>Attention Methods for Visual QA</strong></p><blockquote><p>Yang et al. (2015): Stacked Attention Networks for Image Question Answering</p></blockquote><p>Viewing VQA from the perspective of our default QA paradigm, there is signiï¬cant overlap with reading comprehension style models. We use similar techniques to improve performance.</p><p>We can use attention on visual representations:</p><p><img src="/images/NLP/attn_vqa.png"></p><p><img src="/images/NLP/vqa_eg.jpg"></p><p><strong>VIisual Question Answering Summary</strong></p><ul><li>âœ… Extra modality â€˜for freeâ€™</li><li>âœ… Plenty of training data available as of recently</li><li>âŒ Currently quite gimmicky</li><li>âŒ Still a long way to go</li></ul><p><em>Questions</em></p><ul><li>âŒ When were the pyramids built?</li><li>âŒ Jean-Claude Juncker</li><li>âŒ How old is Keir Starmer?</li><li>âŒ What is the price for AAPL?</li><li>â“ Whatâ€™s the weather in London?</li><li>âŒ Whom did Juncker meet with?</li><li>âŒ When did you get here?</li><li>âŒ Why do we yawn?</li></ul><h2><span id="summary">Summary</span></h2><p><strong>How to build your own QA system ?</strong></p><p>Build a QA model in seven questions</p><ul><li>What is the task?</li><li>What do question, answer and context look like?</li><li>Where does the data come from?</li><li>Can you augment the data?</li><li>How to encode question and context?</li><li>How to combine question and context?</li><li>How to predict or generate an answer?</li></ul><p>There are plenty of open questions left in QA. Just remember to <strong>start with the data</strong>!</p><p><em>Sources and Further Reading</em></p><ul><li><strong>Question Answering Theory and Datasets</strong><ul><li>Pomerantz (2005), A Linguistic Analysis of Question Taxonomies</li><li>Nguyen et al. (2016), MS MARCO: A Human Generated Machine Reading Comprehension Dataset</li><li>Haas and Riezler (2016), A Corpus and Semantic Parser for Multilingual Natural Language Querying of OpenStreetMap</li></ul></li><li><strong>Semantic Parsing</strong><ul><li>Artzi et al. (2013), Semantic Parsing with CCG</li><li>Berant et al. (2013), Semantic Parsing on Freebase from Question-Answer Pairs</li><li>http://nlp.stanford.edu/software/sempre/</li></ul></li><li><strong>Reading Comprehension</strong><ul><li>Hermann et al. (2015), Teaching Machines to Read and Comprehend</li><li>Kadlec et al. (2016), Text Understanding with the Attention Sum Reader Network</li></ul></li><li><strong>Visual QA</strong><ul><li>Yang et al. (2015), Stacked Attention Networks for Image Question Answering</li><li>Ren et al. (2015), Exploring Models and Data for Image Question Answering</li><li>Goyal et al. (2016), Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering.</li><li>https://avisingh599.github.io/deeplearning/visual-qa/</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.&lt;/p&gt;
    
    </summary>
    
      <category term="å­¦ä¹ ç¬”è®°" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://www.52coding.com.cn/tags/NLP/"/>
    
      <category term="Attention" scheme="http://www.52coding.com.cn/tags/Attention/"/>
    
      <category term="Deep NLP" scheme="http://www.52coding.com.cn/tags/Deep-NLP/"/>
    
      <category term="QA" scheme="http://www.52coding.com.cn/tags/QA/"/>
    
  </entry>
  
</feed>
