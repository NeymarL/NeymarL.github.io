<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>NIUHE</title>
  
  <subtitle>日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.52coding.com.cn/"/>
  <updated>2018-11-23T02:07:33.538Z</updated>
  <id>http://www.52coding.com.cn/</id>
  
  <author>
    <name>NIUHE</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>RL - Trust Region Policy Optimization (TRPO)</title>
    <link href="http://www.52coding.com.cn/2018/11/22/RL%20-%20TRPO/"/>
    <id>http://www.52coding.com.cn/2018/11/22/RL - TRPO/</id>
    <published>2018-11-22T09:10:09.000Z</published>
    <updated>2018-11-23T02:07:33.538Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="noopener">Trust Region Policy Optimization (TRPO) </a>算法是由伯克利大学的Schulman等人于2015年提出的策略梯度（Policy Gradients）算法。TRPO通过最大化新策略相对于当前策略的优势来保证每次更新都是单调递增的（稳定），同时找到尽可能大的更新步幅。算法推导出的最终结果是在KL约束下最大化替代优势函数。</p><a id="more"></a><h2><span id="背景">背景</span></h2><p>考虑经典的 MDP$&lt;S, A, P, r, \rho_0, \gamma&gt;$，其中 $S$ 是所有状态（state）的集合，$A$ 是所有动作（action）的集合，$P: S\times A\times S \rightarrow \mathbb{R}$ 是转移概率分布，$r$ 是奖励（reward）函数，$\rho_0$ 是初始状态（$s_0$）分布，$\gamma$ 是折扣因子（ discount factor）。</p><p>定义 $\pi$ 为一个随机策略：$\pi: S\times A\rightarrow [0, 1]$，定义 $\eta(\pi)$ 来衡量策略 $\pi$ 的好坏：<br>$$<br>\eta(\pi)=\mathbb{E}_{s_0, a_0, …\sim\pi}[\sum_{t=0}^\infty\gamma^tr(s_t)]<br>$$<br>接着定义 state-action value function $Q_\pi$, value function $V_\pi$, 优势函数（advantage function）$A_\pi$:<br>$$<br>Q_\pi(s_t, a_t) = \mathbb{E}_{s_{t+1}, a_{t+1}, …\sim\pi}[\sum_{l=0}^\infty\gamma^lr_{s_{t+l}}]<br>$$</p><p>$$<br>V_\pi(s_t) =\mathbb{E}_{a_t, s_{t+1}, …\sim\pi}[\sum_{l=0}^\infty\gamma^lr_{s_{t+l}}]<br>$$</p><p>$$<br>A_\pi(s, a) = Q_\pi(s, a) - V_\pi(s)<br>$$</p><p>然后可以通过下式来衡量策略 $\tilde{\pi}$ 相对于策略 $\pi$ 的优势（证明详见论文）：<br>$$<br>\begin{align}<br>\eta(\tilde{\pi})&amp;=\eta(\pi)+\mathbb{E}_{s_0, a_0, …\sim\color{red}{\tilde{\pi}}}[\sum_{t=0}^\infty\gamma^tA_\pi(s_t,a_t)]\\<br>&amp;= \eta(\pi)+\sum_s\color{red}{\rho_\tilde{\pi}(s)}\sum_a\tilde{\pi}(a|s)A_\pi(s, a)<br>\end{align}<br>$$<br>其中 $\rho_\pi$ 为策略 $\pi$ 的折扣访问频率（discounted visitation frequency）：<br>$$<br>\rho_\pi(s) = P(s_0=s)+\gamma P(s_1=s) + \gamma^2 P(s_2=s)+…<br>$$<br>通过上式可知，只要每个状态 $s$ 的期望优势非负，即 $\sum_a\tilde{\pi}(a|s)A_\pi(s, a)&gt;0$，就可以保证更新是单调非递减的，这其实就是经典的<a href="https://www.52coding.com.cn/2017/12/07/RL%20-%20Planning%20by%20Dynamic%20Programming/">策略迭代（policy iteration）</a>的更新方式。然而，由于 $\rho_\tilde{\pi}(s)$ 的存在，导致直接优化上式很困难，所以引入一个<strong>替代优势</strong>（surrogate advantage）：<br>$$<br>\begin{align}<br>L_\pi(\tilde{\pi})&amp;=\eta(\pi)+\sum_s\color{red}{\rho_\pi(s)}\sum_a\tilde{\pi}(a|s)A_\pi(s, a)\\<br>\end{align}<br>$$<br>经过一系列推导，可以得到策略 $\tilde{\pi}$ 的优势下界：<br>$$<br>\eta(\tilde{\pi})≥L_\pi(\tilde{\pi})-C\cdot D_{KL}^\max(\pi, \tilde{\pi})<br>$$<br>其中，$C=\frac{4\epsilon\gamma}{(1-\gamma)^2}$，$D_{KL}^\max$ 是最大的KL散度。</p><p>基于上述公式，可以得到如下的类策略迭代的算法，即可保证每轮迭代都是单调的：</p><p><img src="/images/IMG_1925FD469BD9-1.jpeg" alt=""></p><h2><span id="trpo">TRPO</span></h2><p>由于Deep RL都是使用参数为 $\theta$ 的神经网络来拟合策略 $\pi_\theta$，为了使公式更简洁，把算法1中公式的 $\pi$ 替换成 $\theta$:<br>$$<br>\theta = \arg\max_{\theta}[L(\theta_{old}, \theta)-C\cdot D_{KL}^\max(\theta_{old}|| \theta)]<br>$$<br>其中，<br>$$<br>L(\theta_{old}, \theta) = \mathbb{E}_{s,a\sim\pi_{\theta_{old}}}[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)}A^{\pi_{\theta_{old}}}(s,a)]<br>$$<br>TRPO是算法1的近似，区别在于：TRPO没有使用惩罚项 $C$，而是使用 KL散度约束（i.e. trust region constraint）：<br>$$<br>\theta = \arg\max_\theta L(\theta_{old}, \theta)\\<br>\text{ s.t. }\bar{D}_{KL}(\theta||\theta_{old})≤\delta<br>$$<br>其中，$\bar{D}_{KL}$ 是平均KL散度：<br>$$<br>\bar{D}_{KL}(\theta||\theta_{old})=\mathbb{E}_{s\sim\pi_{\theta_{old}}}[D_{KL}(\pi_{\theta}(\cdot|s)||\pi_{\theta_{old}}(\cdot|s))]<br>$$<br>然而上面的带约束优化也并非容易，所以TRPO对上式进行了一些近似，对目标函数和约束进行一阶泰勒近似得：<br>$$<br>L(\theta_{old}, \theta) \approx g^T(\theta-\theta_{old})<br>$$</p><p>$$<br>\bar{D}_{KL}(\theta||\theta_{old})\approx \frac{1}{2}(\theta-\theta_{old})^TH(\theta-\theta_{old})<br>$$</p><p>其中，$g$ 是替代函数的梯度在 $\theta=\theta_{old}$ 处的值，凑巧的是，它和策略梯度的值正好相等：$g = \triangledown_\theta J(\pi_\theta)|_{\theta_{old}}$；$H$ 是对于 $\theta$ 的海森矩阵（Hessian matrix）。</p><p>于是得到如下的近似优化问题：<br>$$<br>\theta_{k+1}=\arg\max_\theta g^T(\theta-\theta_k)\\<br>\text{s.t. }\frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k)≤\delta<br>$$<br>通过拉格朗日法求解上述约束优化问题得：<br>$$<br>\theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g<br>$$<br>这个就是 <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf" target="_blank" rel="noopener">Natural Policy Gradient</a> 的更新公式。不过，由于泰勒近似引入了误差，上式的解可能不满足 KL 约束，所以 TRPO 增加了一个线性搜索（backtracking line search）：<br>$$<br>\theta_{k+1}=\theta_k+\alpha^j\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g<br>$$<br>其中，$\alpha\in(0,1)$ 是回溯系数（backtracking coefficient），$j$ 是最小的非负整数使得 $\pi_{\theta_{k+1}}$ 满足 KL 约束并且产生<strong>正</strong>的替代优势，这样就可以保证训练进步是单调的。</p><p>最后，计算和存储 $H^{-1}$ 的开销是很大的，尤其是神经网络的参数动不动就几M。TRPO 使用<a href="https://www.wikiwand.com/en/Conjugate_gradient_method" target="_blank" rel="noopener">共轭梯度法（conjugate gradient）</a>来解 $Hx = g$，这样就不用直接计算和存储 $H$。</p><p>最终的更新公式为：<br>$$<br>\theta_{k+1}=\theta_k+\alpha^j\sqrt{\frac{2\delta}{\hat{x}^TH\hat{x}}}\hat{x}<br>$$<br>其中，<br>$$<br>\begin{align}<br>\hat{x}&amp;\approx H^{-1}g &amp; \text{(using conjugate gradient)}<br>\end{align}<br>$$</p><p>$$<br>H\hat{x} = \triangledown_\theta((\triangledown_\theta\bar{D}_{KL}(\theta||\theta_k))^T\hat{x})<br>$$</p><p><strong>TRPO算法</strong></p><p><img src="/images/trpo.svg" alt=""></p><h2><span id="performance">Performance</span></h2><p><strong>TRPO的一些特点</strong></p><ul><li>保证每次更新在当前训练数据上都是进步的，训练过程更加稳定</li><li>通过满足KL约束来找尽可能大的步长</li><li>on-policy 算法</li><li>可用于离散和连续的动作空间</li><li>算法完全从理论推导，理解起来很烧脑</li></ul><p><strong>实验性能</strong></p><p>在模拟机器人走路、游泳等任务中取得了在当时不错的效果；在通过视频输入玩Atari游戏的任务中表现不如DQN等方法。</p><p><img src="/images/trpo_per.jpg" alt=""></p><p>下图是我在 OpenAI <a href="https://gym.openai.com/" target="_blank" rel="noopener">gym</a> 的 <code>Walker2d-v2</code> 和 <code>MsPacman-ram-v0</code> 中的结果。</p><p><img src="/images/walker.png" alt=""></p><p><img src="/images/pacman.png" alt=""></p><h2><span id="references">References</span></h2><p>[1] <a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="noopener">https://arxiv.org/abs/1502.05477</a></p><p>[2] <a href="https://spinningup.openai.com/en/latest/algorithms/trpo.html" target="_blank" rel="noopener">https://spinningup.openai.com/en/latest/algorithms/trpo.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1502.05477&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Trust Region Policy Optimization (TRPO) &lt;/a&gt;算法是由伯克利大学的Schulman等人于2015年提出的策略梯度（Policy Gradients）算法。TRPO通过最大化新策略相对于当前策略的优势来保证每次更新都是单调递增的（稳定），同时找到尽可能大的更新步幅。算法推导出的最终结果是在KL约束下最大化替代优势函数。&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy Gradient" scheme="http://www.52coding.com.cn/tags/Policy-Gradient/"/>
    
      <category term="TRPO" scheme="http://www.52coding.com.cn/tags/TRPO/"/>
    
  </entry>
  
  <entry>
    <title>RL - DQN &amp; A3C</title>
    <link href="http://www.52coding.com.cn/2018/11/16/RL%20-%20DQN%20and%20A3C/"/>
    <id>http://www.52coding.com.cn/2018/11/16/RL - DQN and A3C/</id>
    <published>2018-11-16T06:24:32.000Z</published>
    <updated>2018-11-16T07:26:41.505Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="deep-q-network">Deep Q-Network</span></h2><p>使用非线性函数拟合 Q-value 的RL算法不稳定主要因为：</p><ol type="1"><li>同一个观测序列中的数据相关性较大</li><li>当 Q-value 发生了很小的改变，可能导致整个策略（policy）发生较大变化，从而导致 Q-value 和目标 <span class="math inline">\(r + \gamma * \max_{a&#39;}Q(s&#39; ,a&#39;)\)</span> 的差距不稳定</li></ol><a id="more"></a><p>DQN使用了两个trick来解决上述问题：</p><ul><li>Experience replay<ul><li>使用经验池缓存数据，每次训练时从经验池里sample数据，从而降低训练数据之间的相关性</li></ul></li><li>Two Q networks<ul><li>一个网络用来生成 Q-target，另一个网络进行探索；每隔一定时间两个网络进行同步</li><li>这样使得 Q-target 相对稳定</li></ul></li></ul><p><strong>整体算法</strong></p><p><img src="/images/dqn.jpg"></p><hr><h2><span id="asynchronous-actor-critic">Asynchronous Actor Critic</span></h2><p>使用上述经验池有以下问题：</p><ol type="1"><li>使用更多的内存和计算资源</li><li>只能使用 <strong>off-policy</strong> 的RL算法（学习 old policy 产生的数据）</li></ol><p>为了使用 on-policy 算法，Deep Mind提出了使用异步学习代替经验池的方法，同时也能保持算法的稳定性，其中使用最广泛的是A3C算法，它具有以下特点：</p><ul><li>并行地使用多个 agent 在各自的环境里探索，每个 agent 在同一时刻探索的内容各不相同，从而降低了数据相关性</li><li>在CPU上训练更加高效</li></ul><p><strong>整体算法</strong></p><ol type="1"><li>同步线程专属网络（<span class="math inline">\(\theta&#39;, \theta_v&#39;\)</span>）和全局网络（<span class="math inline">\(\theta, theta_v\)</span>）</li><li>每个 agent 使用线程专属网络各自进行探索</li><li>根据线程专属网络计算梯度：<span class="math inline">\(d\theta, d\theta_v\)</span></li><li>使用 <span class="math inline">\(d\theta, d\theta_v\)</span> 更新全局网络（<span class="math inline">\(\theta, theta_v\)</span>）</li><li>回到第一步</li></ol><p><img src="/images/IMG_25EB14880DD1-1.jpeg"></p><p><strong>其他细节</strong></p><ul><li><strong>主线程向子线程传参数，子线程向主线程传梯度</strong></li><li>agent 和 critic 共用一个网络，输出分为两头</li><li>增加了熵正则化（鼓励探索）<ul><li><span class="math inline">\(\triangledown_{\theta&#39;}\log\pi(a_t|s_t;\theta&#39;)(R_t-V(s_t;\theta_v))+\beta\triangledown_{\theta&#39;}H(\pi(s_t; \theta&#39;))\)</span></li><li><span class="math inline">\(H(X) = E[-\log P(X)]\)</span></li></ul></li><li>代码参考：https://github.com/NeymarL/Pacman-RL/blob/master/src/a3c.py<ul><li><strong>注</strong>：计算 policy loss 中的 advantage 的时候不能保留其梯度，否则 policy 的梯度会流入 value network 中，产生bug</li></ul></li></ul><hr><h2><span id="与题无关">与题无关</span></h2><h3><span id="batch-normalization">Batch-Normalization</span></h3><p>解决网络层数变多梯度<strong>消失</strong>/爆炸问题</p><ul><li>梯度截断</li><li>初始化</li><li>RELU</li></ul><p>对每层神经元处理结果进行归一化，但又不能破坏上一层提取的特征（变换重构，引入了可学习参数<span class="math inline">\(\gamma, \beta\)</span>）</p><figure><img src="/images/bn.png" alt="bn"><figcaption>bn</figcaption></figure><p>Inference时 <span class="math inline">\(\mu_B\)</span> 和 <span class="math inline">\(\sigma^2_B\)</span> 固定。</p><p>为什么不用白化？</p><ul><li>在模型训练过程中进行白化操作会带来过高的计算代价和运算时间</li></ul><p>在BN中，是通过将activation规范为均值和方差一致的手段使得原本会减小的activation的scale变大。 <span class="math display">\[\frac{\partial h_l}{\partial h_{l-1}} = \frac{\partial BN(w_l h_{l-1})}{\partial h_{l-1}} = \frac{\partial \alpha w_l h_{l-1}}{\partial h_{l-1}}\]</span> 其中 <span class="math inline">\(\alpha\)</span> 指缩放。可以看到此时反向传播乘以的数不再和 <span class="math inline">\(w\)</span> 的尺度相关，也就是说尽管我们在更新过程中改变 <span class="math inline">\(w\)</span> 的值，但是反向传播的梯度却不受影响。</p><h3><span id="variance-reduction-techniques">Variance reduction Techniques</span></h3><p><strong>Advantage Estimation</strong></p><p>Use advantage function to reduce variance. <span class="math display">\[A(s, a) = Q(s, a) - V(s)\]</span> How much better is selecting action <span class="math inline">\(a\)</span> than usual(mean)</p><p><strong>Reward Estimator</strong></p><p>Train a reward estimator <span class="math inline">\(\hat{R}(s_t)\)</span>, use <span class="math inline">\(\hat{R}\)</span> instead of <span class="math inline">\(R\)</span> when training RL model, this estimation will reduce the variance propagated to the value function.</p><p>Optimize <span class="math inline">\(\hat{R}\)</span>: <span class="math inline">\((r_t - \hat{R}(s_t))^2\)</span>.</p><h3><span id="activation-layers">Activation Layers</span></h3><h4><span id="relu">ReLU</span></h4><figure><img src="/images/relu.png" alt="relu"><figcaption>relu</figcaption></figure><p>整流线性单元易于优化，因为它们和线性单元非常类似。线性单元和整流线性单元的唯一区别在于整流线性单元在其一半的定义域上输出为零。这使得只要整流线性单元处于激活状态，它的导数都能保持较大。它的梯度不仅大而且一致。整流操作的二阶导数几乎处处为 0，并且在整流线性单元处于激活状态时，它的一阶导数处处为 1。这意味着相比于引入二阶效应的激活函数来说，它的梯度方向对于学习来说更加有用。</p><p>ReLU 的过程更接近生物神经元的作用过程</p><p><strong>Leaky ReLU</strong></p><p>ReLU 及其扩展都是基于一个原则，那就是如果它们的行为更接近线性，那么模型更容易优化。 <span class="math display">\[g(z; \alpha) = \max(0, z) + \alpha \min(0, z)\]</span> <span class="math inline">\(\alpha\)</span> 为固定值或可学习参数。</p><h4><span id="sigmoid-amp-tanh">Sigmoid &amp; Tanh</span></h4><p><img src="/images/sigmoid.png" alt="sigmoid"> <span class="math display">\[g(z) = \frac{1}{1 + e^{-z}}\]</span></p><ul><li>sigmoid 常作为输出单元用来预测二值型变量取值为 1 的概率</li><li>sigmoid 函数在输入取绝对值非常大的正值或负值时会出现<strong>饱和</strong>（saturate）现象，在图像上表现为开始变得很平，此时函数会对输入的微小改变会变得不敏感。仅当输入接近 0 时才会变得敏感。从而使得学习变困难。</li><li>如果要使用 sigmoid 作为激活函数时（浅层网络），tanh 通常要比 sigmoid 函数表现更好。</li></ul><h3><span id="bagging">Bagging</span></h3><p>思想：多个模型平均效果好于单个模型</p><p><strong>Bagging（bootstrap aggregating）</strong>是通过结合几个模型降低泛化误差的技术 (Breiman, 1994)。</p><p>具体来说，Bagging 涉及构造 k 个<strong>不同的数据集</strong>。每个数据集从原始数据集中<strong>重复采样</strong>构成，和原始数据集具有<strong>相同数量</strong>的样例。这意味着，每个数据集以高概率缺少一些来自原始数据集的例子，还包含若干重复的例子（更具体的，如果采样所得的训练集与原始数据集大小相同，那所得数据集中大概有原始数据集 <strong>2/3</strong> 的实例）</p><figure><img src="/images/bagging.png" alt="bagging"><figcaption>bagging</figcaption></figure><p>第一个分类器学到上面的圆圈就会认为数字是8，第二个分类器检测到下面的圈就会认为数字是8，把两个结合起来就知道只有当上下都有圈（置信概率最大）的时候数字才是8。</p><h3><span id="dropout">Dropout</span></h3><p>简单来说，Dropout (Srivastava et al., 2014) 通过<strong>参数共享</strong>提供了一种廉价的 <strong>Bagging</strong> 集成近似，能够训练和评估<strong>指数级数量</strong>的神经网络。</p><figure><img src="/images/dropout.png" alt="dropout"><figcaption>dropout</figcaption></figure><p><strong>Dropout与Bagging的不同点</strong>：</p><ul><li>Bagging 为串行策略；Dropout 为并行策略</li><li>在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型<strong>共享参数</strong>，其中每个模型继承父神经网络参数的不同子集。</li><li>在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;deep-q-network&quot;&gt;Deep Q-Network&lt;/h2&gt;
&lt;p&gt;使用非线性函数拟合 Q-value 的RL算法不稳定主要因为：&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;同一个观测序列中的数据相关性较大&lt;/li&gt;
&lt;li&gt;当 Q-value 发生了很小的改变，可能导致整个策略（policy）发生较大变化，从而导致 Q-value 和目标 &lt;span class=&quot;math inline&quot;&gt;\(r + \gamma * \max_{a&amp;#39;}Q(s&amp;#39; ,a&amp;#39;)\)&lt;/span&gt; 的差距不稳定&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="Q-learning" scheme="http://www.52coding.com.cn/tags/Q-learning/"/>
    
      <category term="DQN" scheme="http://www.52coding.com.cn/tags/DQN/"/>
    
      <category term="A3C" scheme="http://www.52coding.com.cn/tags/A3C/"/>
    
  </entry>
  
  <entry>
    <title>中国象棋Zero技术详解</title>
    <link href="http://www.52coding.com.cn/2018/11/07/CCZero/"/>
    <id>http://www.52coding.com.cn/2018/11/07/CCZero/</id>
    <published>2018-11-07T09:27:47.000Z</published>
    <updated>2018-11-16T07:52:09.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://cczero.org/" target="_blank" rel="noopener">中国象棋Zero（CCZero）</a>是一个开源项目，把<a href="https://arxiv.org/abs/1712.01815" target="_blank" rel="noopener">AlphaZero</a>的算法应用到了中国象棋上，旨在借助广大象棋爱好者之力一起训练出一个可以打败旋风名手的“象棋之神”。因为种种原因吧，这个目标到目前（2018/11/07）为止未能实现，或者说还差得远，而跑谱的人也越来越少了，很可能坚持不了多久了。</p><p>虽然未能实现目标，但在技术上还是有一定意义的，<a href="https://github.com/NeymarL/ChineseChess-AlphaZero" target="_blank" rel="noopener">GitHub</a>上也时不时有人询问技术细节，在此总结一下，记录一些坑以后不要再踩。</p><a id="more"></a><h2><span id="模块">模块</span></h2><p>程序主要分为三大模块（每个模块对应一个目录）：</p><ul><li><code>agents</code>：核心模块，决定如何下棋<ul><li><code>model.py</code>：神经网络模型</li><li><code>player.py</code>：MCTS，输出走法</li><li><code>api.py</code>：供外界调用model</li></ul></li><li><code>envrionment</code>：象棋规则<ul><li>训练（跑谱）使用<code>static_env.py</code>，速度快一些</li><li>用自带GUI下棋时使用的是<code>env.py</code>, <code>chessboard.py</code>这些，可以输出PNG格式的棋谱</li></ul></li><li><code>worker</code>：把agent和envrionment串联起来的脚本<ul><li><code>self_play.py</code>：自我博弈</li><li><code>compute_elo.py</code>：评测并上传结果到官网</li><li><code>optimize.py</code>：训练棋谱</li><li><code>_windows</code>后缀表示是在Windows平台上运行的相应功能，之所以分开是因为两个多进程的启动方式不同，导致代码结构也要发生一些变化，详见<a href="#自我博弈">自我博弈</a>。</li></ul></li></ul><h3><span id="神经网络模型">神经网络模型</span></h3><p><strong>网络输入</strong>：<span class="math inline">\(14\times10\times9\)</span></p><ul><li><span class="math inline">\(10 \times 9\)</span> 是中国象棋棋盘的大小</li><li><span class="math inline">\(14\)</span> 是所有棋子种类（红/黑算不同种类）</li><li>整体的输入就是14个棋盘堆叠在一起，每个棋盘表示一种棋子的位置：棋子所在的位置为1，其余位置为0。</li></ul><p><strong>网络输出</strong></p><ul><li>策略头（policy head）输出：<span class="math inline">\(2086\)</span><ul><li><span class="math inline">\(2086\)</span> 是行动空间的大小。行动空间就是说根据中国象棋的规则，任意棋子在任意位置的走法集合。</li></ul></li><li>价值头（value head）输出：<span class="math inline">\(1\)</span><ul><li>价值头输出一个标量衡量当前局势 <span class="math inline">\(v\in[-1, 1]\)</span>：当 <span class="math inline">\(v\)</span> 接近1时，局势大好；接近0为均势；接近-1为败势。</li></ul></li></ul><p>附：棋子编号表</p><table><thead><tr class="header"><th>棋子</th><th>编号</th></tr></thead><tbody><tr class="odd"><td>兵/卒</td><td>0</td></tr><tr class="even"><td>炮</td><td>1</td></tr><tr class="odd"><td>车</td><td>2</td></tr><tr class="even"><td>马</td><td>3</td></tr><tr class="odd"><td>相/象</td><td>4</td></tr><tr class="even"><td>仕/士</td><td>5</td></tr><tr class="odd"><td>帅/将</td><td>6</td></tr></tbody></table><p><strong>网络结构</strong></p><p>网络主体是 ResNet，输出部分分出两个头，分别输出 policy 和 value。现在的架构是中间有10个残叉块（Residual Block），每个块里面有两个CNN：卷积核大小为 <span class="math inline">\(3 \times 3\)</span>，过滤器个数为192。</p><h3><span id="蒙特卡洛树搜索">蒙特卡洛树搜索</span></h3><p><img src="/images/mcts0.png"></p><p>搜索树中的每个节点都包含所有合法移动 a ∈ A(s) 的边(s，a)。 每条边存储一组统计数据， <span class="math display">\[\{N(s,a) ,W(s,a) ,Q(s,a) ,P(s,a)\}\]</span> 其中 <span class="math inline">\(N(s,a)\)</span> 是访问计数，<span class="math inline">\(W(s,a)\)</span> 是总动作价值，<span class="math inline">\(Q(s,a)\)</span> 是平均动作价值，<span class="math inline">\(P(s,a)\)</span> 是选择该边的先验概率。 多个模拟在单独的搜索线程上并行执行。</p><ol type="1"><li><p>选择</p><p>每个模拟的第一个 in-tree 阶段开始于搜索树的根节点 <span class="math inline">\(s_0\)</span>，并且在模拟时刻 L 处到达叶节点 <span class="math inline">\(s_L\)</span> 时结束。在每个这些时刻 <span class="math inline">\(t &lt; L\)</span> 处，根据搜索树中的统计量选择一个移动: <span class="math inline">\(a_t = \arg\max_a(Q(s_t,a) + U(s_t,a))\)</span>，其中 <span class="math inline">\(U(s_t,a)\)</span> 使用PUCT算法的变体得到 <span class="math display">\[U(s,a)=c_{puct}P(s,a)\frac{\sqrt{\sum_bN(s,b)}}{1+N(s,a)}\]</span> 其中 <span class="math inline">\(c_{puct}​\)</span> 是一个决定探索程度的常数; 这种搜索控制策略最初倾向于具有高先验概率和低访问次数的行为，但后期倾向于具有高动作价值的行为。</p></li><li><p>扩展和评估</p><p>叶子结点 <span class="math inline">\(s_L\)</span> 被加入到等待评估队列进行评估: <span class="math inline">\((d_i(p),v)=f_\theta(d_i(s_L))\)</span>，其中 <span class="math inline">\(d_i\)</span>是旋转或反射操作。神经网络一次评估队列里的 8 个结点;搜索进程直到评估完毕才能继续工作。每个叶子结点和每条边 <span class="math inline">\((s_L,a)\)</span> 的统计值被初始化为 <span class="math inline">\(\{N(s_L,a) = 0,W(s_L,a) = 0,Q(s_L,a) =0, P(s_L, a) = p_a\}\)</span>，然后价值 v 开始回溯。</p></li><li><p>回溯</p><p>每条边的统计值延路径反向更新：访问计数递增 <span class="math inline">\(N(s_t,𝑎_t) = N(s_t,𝑎_t) +1\)</span>，移动价值更新为平均值 <span class="math inline">\(W(s_t,a_t)=W(s_t,a_t)+v\)</span>, <span class="math inline">\(Q(s_t,a_t)=\frac{W(s_t,a_t)}{N(s_t,a_t)}\)</span>。</p></li><li><p>下棋</p><p>在搜索结束时，AlphaGo Zero 在根位置 <span class="math inline">\(s_0\)</span> 选择移动 a，与其指数访问计数成比例，<span class="math inline">\(\pi(a|s_0) = \frac{N(s_0,a)^{1/\tau}}{\sum_bN(s,b)^{1/\tau}}\)</span>，其中 <span class="math inline">\(τ\)</span> 是控制探索水平的参数。搜索树可以在后面的时刻重用：与所选择的移动对应的子节点成为新的根节点; 在这个节点下面的子树被保留以及它的所有统计数据，而树的其余部分被丢弃。</p></li></ol><h4><span id="实现细节">实现细节</span></h4><p><strong>在选择的过程中，发现当前state在history中出现过（形成循环）怎么办？</strong></p><ul><li>根据比赛规则：闲着循环3次判和；违规（长捉、长将等）判负；对方违规判胜。</li></ul><p><strong>Virtual Loss</strong></p><ul><li><p>多线程搜索时，当某一线程选择了某个action时，为了鼓励其他线程选择其他action，应该降低该action的价值（施加virtual loss）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.tree[state].sum_n += <span class="number">1</span></span><br><span class="line">action_state = self.tree[state].a[sel_action]</span><br><span class="line">action_state.n += virtual_loss</span><br><span class="line">action_state.w -= virtual_loss</span><br><span class="line">action_state.q = action_state.w / action_state.n</span><br></pre></td></tr></table></figure></li><li><p>在回溯时，更新value要考虑到virtual loss的影响</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">node = self.tree[state]</span><br><span class="line">action_state = node.a[action]</span><br><span class="line">action_state.n += <span class="number">1</span> - virtual_loss</span><br><span class="line">action_state.w += v + virtual_loss</span><br><span class="line">action_state.q = action_state.w / action_state.n</span><br></pre></td></tr></table></figure></li></ul><p><strong>state表示</strong></p><p>虽然对于神经网络来说state就是<span class="math inline">\(14\times10\times9\)</span>的tensor，但是对于搜索树来说，显然不能用它来表示每个局面。</p><p>在初始版本中，象棋环境（<code>environment/chessboard.py</code>）里是用数组来表示棋盘的，所以在搜索中也使用相应的数组表示state，这样做虽然没什么问题，但是在搜索的过程中需要大量的深拷贝操作（因为需要回溯），增加了许多开销。</p><p>后来版本进行了改进，使用<a href="https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation" target="_blank" rel="noopener">FEN string</a>作为state的表示，降低了拷贝操作的开销；同时也优化了象棋环境（<code>environment/static_env.py</code>），可以直接对FEN进行操作，无需记录复杂的数组。</p><blockquote><p><strong>Forsyth–Edwards Notation</strong> (<strong>FEN</strong>) is a standard <a href="https://www.wikiwand.com/en/Chess_notation" target="_blank" rel="noopener">notation</a> for describing a particular board position of a <a href="https://www.wikiwand.com/en/Chess" target="_blank" rel="noopener">chess</a> game. The purpose of FEN is to provide all the necessary information to restart a game from a particular position.</p></blockquote><h3><span id="自我博弈">自我博弈</span></h3><p>为了提高CPU/GPU利用率，这里使用了多进程，每个进程各自进行自我博弈。Python的多进程有三个实现方式：<code>fork</code>, <code>spawn</code>, <code>forkserver</code>。</p><blockquote><p>On Windows only <code>'spawn'</code> is available. On Unix <code>'fork'</code> and <code>'spawn'</code> are always supported, with <code>'fork'</code> being the default.</p></blockquote><p>由于我自己在macOS/Linux上开发和测试，所以首先实现的是基于<code>fork</code>的多进程，而当我在程序加了<code>mp.set_start_method('spawn')</code>的时候，程序就跑不了了，会报pickling error（貌似是因为传给子进程的参数里不能出现queue的数据结构），于是只能换种方式实现来绕过这个问题。</p><h2><span id="分布式">分布式</span></h2><p>起初我是没打算做成分布式的，实现完上面说述模块之后我用实验室的K80进行训练，练了几天之后发现进步并不明显，几乎还是随机下，很弱智，这是我才意识到即使把它训练到一个业余玩家的水平也需要巨大的算力。</p><p><img src="/images/issueouashd.png"></p><p>后来有一天有人在GitHub上提了一个issue说你可以把它做成分布式的，像LeelaZero那样，我们可以帮你一起训练。<a href="https://zero.sjeng.org/" target="_blank" rel="noopener">LeelaZero</a>是国外一个开发者复现AlphaGo论文搞的围棋AI，因为DeepMind并没有公开程序或代码，所以他想训练出一个公开的围棋之神，然后就邀请网友帮他一起训练，具体的方法就是：网友们在自己的机器上进行自我博弈，然后把博弈的棋谱上传到他的服务器上，然后他攒够一定棋谱之后进行训练神经网络，训练好之后分发新的权重。</p><p>在国内也有很多人帮他训练（跑谱），给我提issue的那个人也是帮LeelaZero训练中的一员。当时正好程序写完了没什么事做，每天就只能等训练结果，然后就决定尝试一下这个模式。因为之前有过Web开发的经验，所以服务器很快就搭好了，测试基本没问题之后就开始运行。</p><p><strong>架构</strong></p><p><img src="/images/architecture.png"></p><p>在维护这个项目正常运行的过程中遇到很多<strong>坑</strong>，程序也做了很多改进：</p><ol type="1"><li>首先是帮忙跑谱的大多都是象棋爱好者，并非开发者，所以我要把Python代码打包成exe文件分发给他们一键执行，最终使用<a href="https://www.pyinstaller.org/" target="_blank" rel="noopener">PyInstaller</a>打包成功，这其中遇到了很多坑：<ul><li>卸载cytoolz；pandas的版本必须为0.20.3</li><li>代码里加上<code>mp.freeze_support()</code>，否则多进程不会正常工作</li></ul></li><li>服务器带宽有限，客户端下载权重太慢，解决办法：把权重放到云存储服务中，如腾讯云/七牛云的对象存储服务。</li><li>中国象棋棋规的完善。并不是说基础的马走日象走田这种规则，而是像长将、长捉等这种比赛规则，这个算是坑最大的一个，直到现在规则还存在少许问题。</li><li>部分支持了UCI协议。这样就可以使用其他的象棋界面加载这个引擎，并且能和其他引擎对弈。</li><li>因为“同行竞争”，我的服务器在今年暑假期间我的服务器经常遭受DDos攻击，由于买不起腾讯云的高防服务，只能尝试其他办法，包括配置弹性IP、配置防火墙、Cloudfare CDN等，但都不好用。最终把服务转移到<a href="https://www.ovh.com/" target="_blank" rel="noopener">OVH</a>提供的VPS上才解决了问题（OVH提供免费的DDos防护）。</li></ol><hr><h2><span id="alphazero-and-exit">AlphaZero and ExIt</span></h2><p><a href="https://arxiv.org/abs/1705.08439" target="_blank" rel="noopener">Expert Iteration（ExIt）</a>是一种模仿学习（Imitation Learning, IL）算法，普通的 IL 算法中，徒弟模仿专家的策略只能提高自己的策略，专家是不会有任何提高的，而 ExIt 算法就是想让师傅教徒弟的时候自己也有提高。</p><p><strong>ExIt 算法</strong> 师傅根据徒弟的策略进行前向搜索（例如MCTS，alpha-beta，贪心搜索等），得出比徒弟更好的策略，然后徒弟再学习师傅的策略，如此循环，随着徒弟的增强，师傅也会越来越强。</p><p><img src="/images/exit.png"></p><p>可见，AlphaZero也属于 ExIt 算法，师傅为 MCTS，徒弟就是神经网络。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://cczero.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;中国象棋Zero（CCZero）&lt;/a&gt;是一个开源项目，把&lt;a href=&quot;https://arxiv.org/abs/1712.01815&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AlphaZero&lt;/a&gt;的算法应用到了中国象棋上，旨在借助广大象棋爱好者之力一起训练出一个可以打败旋风名手的“象棋之神”。因为种种原因吧，这个目标到目前（2018/11/07）为止未能实现，或者说还差得远，而跑谱的人也越来越少了，很可能坚持不了多久了。&lt;/p&gt;
&lt;p&gt;虽然未能实现目标，但在技术上还是有一定意义的，&lt;a href=&quot;https://github.com/NeymarL/ChineseChess-AlphaZero&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;上也时不时有人询问技术细节，在此总结一下，记录一些坑以后不要再踩。&lt;/p&gt;
    
    </summary>
    
      <category term="踩坑现场" scheme="http://www.52coding.com.cn/categories/%E8%B8%A9%E5%9D%91%E7%8E%B0%E5%9C%BA/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="AlphaZero" scheme="http://www.52coding.com.cn/tags/AlphaZero/"/>
    
      <category term="MCTS" scheme="http://www.52coding.com.cn/tags/MCTS/"/>
    
      <category term="CCZero" scheme="http://www.52coding.com.cn/tags/CCZero/"/>
    
      <category term="中国象棋" scheme="http://www.52coding.com.cn/tags/%E4%B8%AD%E5%9B%BD%E8%B1%A1%E6%A3%8B/"/>
    
  </entry>
  
  <entry>
    <title>博客迁移踩坑记录</title>
    <link href="http://www.52coding.com.cn/2018/11/06/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E8%AE%B0%E5%BD%95/"/>
    <id>http://www.52coding.com.cn/2018/11/06/博客迁移记录/</id>
    <published>2018-11-06T02:41:47.000Z</published>
    <updated>2018-11-06T11:17:46.893Z</updated>
    
    <content type="html"><![CDATA[<p>博客迁移这个事早就想做了，但到现在才有时间和精力来完成。为什么要迁移呢？主要有几个原因：</p><ol type="1"><li>原博客更新、维护较麻烦。原博客是自己用<a href="https://www.52coding.com.cn/2015/12/21/%E8%AE%B0%E5%BD%95%EF%BC%9A%E7%94%A8PHP%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95web%E6%A1%86%E6%9E%B6/">PHP搭的</a>，之前的写作方式是用Markdown写好导出HTML，再修改HTML代码使得静态资源（图片等）加载正确，这就使得修改博客很麻烦；更换主题也很麻烦，博客的主题和Markdown的主题通常会有冲突，所以想换个样式就要改半天CSS。</li><li>觉得UI有些难看，想要简洁一些；</li><li>安全问题。</li></ol><p>现在的解决方案是<a href="https://pages.github.com/" target="_blank" rel="noopener">Github Pages</a> + <a href="https://hexo.io/zh-cn/index.html" target="_blank" rel="noopener">Hexo</a>，主题选的是<a href="https://github.com/CodeDaraW/Hacker" target="_blank" rel="noopener">Hacker</a>，迁移了两天终于搞完了，在此简单记录一下遇到的坑。</p><a id="more"></a><h3><span id="数学公式渲染">数学公式渲染</span></h3><p>由于这款主题并不是原生支持数学公式的，所以要添加些代码来使其支持Mathjax，参考http://searene.me/2016/10/01/Let-hexo-support-mathjax/。</p><p>首先在主题的<code>layout</code>目录下新建<code>mathjax.ejs</code>，文件内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;% if (theme.mathjax.enable)&#123; %&gt;</span><br><span class="line">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;</span><br><span class="line">  MathJax.Hub.Config(&#123;</span><br><span class="line">      tex2jax: &#123;</span><br><span class="line">        inlineMath: [ [&apos;$&apos;,&apos;$&apos;], [&quot;\\(&quot;,&quot;\\)&quot;] ],</span><br><span class="line">        processEscapes: true</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;</span><br><span class="line">  MathJax.Hub.Config(&#123;</span><br><span class="line">        tex2jax: &#123;</span><br><span class="line">          skipTags: [&apos;script&apos;, &apos;noscript&apos;, &apos;style&apos;, &apos;textarea&apos;, &apos;pre&apos;, &apos;code&apos;]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;</span><br><span class="line">  MathJax.Hub.Queue(function() &#123;</span><br><span class="line">          var all = MathJax.Hub.getAllJax(), i;</span><br><span class="line">          for(i=0; i &lt; all.length; i += 1) &#123;</span><br><span class="line">              all[i].SourceElement().parentNode.className += &apos; has-jax&apos;;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;</span><br><span class="line">&lt;/script&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure><p><strong>坑1</strong>：之前在网上查到的代码给的MathJax.js的链接多是过期的，如<code>https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML</code>，然后就被坑了。</p><p>然后在<code>layout.ejs</code>中加上<code>&lt;%- partial('mathjax') %&gt;</code>，文件整体内容为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;!DOCTYPE HTML&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;%- partial(&apos;components/head&apos;) %&gt;</span><br><span class="line"></span><br><span class="line">&lt;body&gt;</span><br><span class="line">  &lt;div class=&quot;blog&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;content&quot;&gt;</span><br><span class="line"></span><br><span class="line">      &lt;%- partial(&apos;components/header&apos;) %&gt;</span><br><span class="line"></span><br><span class="line">      &lt;main class=&quot;site-main posts-loop&quot;&gt;</span><br><span class="line">        &lt;%- body %&gt;</span><br><span class="line">      &lt;/main&gt;</span><br><span class="line"></span><br><span class="line">      &lt;%- partial(&apos;components/footer&apos;) %&gt;</span><br><span class="line">      &lt;%- partial(&apos;components/googleanalytics&apos;) %&gt;</span><br><span class="line">      &lt;!-- 新加的 --&gt;</span><br><span class="line">      &lt;%- partial(&apos;mathjax&apos;) %&gt; </span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line"></span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><p>最后在主题的<code>_config.yml</code>中加上： <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mathjax:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><p>如果没有安装MathJax插件的话需要安装一下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-math --save</span><br></pre></td></tr></table></figure><p>重新生成一下应该就可以渲染数学公式了。</p><p>不过还有些问题，就是你写在数学公式里的下划线(<code>_</code>)、反斜杠(<code>\</code>)、和星号(<code>*</code>)会被当作普通Markdown来处理，比如把下划线(<code>_</code>)和星号(<code>*</code>)替换成<code>&lt;em&gt;</code>标签等导致公式渲染错误。</p><p>解决方案来自https://zhuanlan.zhihu.com/p/33857596，打开<code>nodes_modules/marked/lib/marked.js</code>:</p><ol type="1"><li><p>找到下面的代码:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([\\`*&#123;&#125;\[\]()# +\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure><p>改为：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*&#123;&#125;\[\]()# +\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure></li><li><p>找到em的符号:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em: <span class="regexp">/^\b((?:[^]|_)+?)\b|^*((?:**|[\s\S])+?)*(?!*)/</span>,</span><br></pre></td></tr></table></figure><p>改为：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em:<span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure></li></ol><p>这样就去掉了<code>_</code>的斜体含义，在公式里使用<code>_</code>就没有问题了，不过要使用<code>*</code>的话要用<code>\ast</code>替代。</p><h3><span id="评论">评论</span></h3><p>Hacker这款主题支持两种评论方式，分别是<a href="https://github.com/imsun/gitment" target="_blank" rel="noopener">Gitment</a>和<a href="https://disqus.com/" target="_blank" rel="noopener">Disqus</a>。一开始试了试Gitment，配置好之后发现不能用，其原因是有一个服务过期了而作者也弃坑了没人管，我也懒得折腾就转向了Disqus，注册了之后就可以直接使用，十分方便（不过好像要翻墙才能访问= =）。</p><h3><span id="其他">其他</span></h3><p><strong>分割线</strong></p><p>Hexo中写作不能使用<code>___</code>来实现分割线，用了的话会generate失败，而且提示的错误很迷，曾经困扰了我很久。</p><p><strong>修改网站Icon</strong></p><p>在主题中找到<code>head.ejs</code>文件，其中有一行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;link href=&quot;&lt;%- config.root %&gt;favicon.ico&quot; rel=&quot;icon&quot;&gt;;</span><br></pre></td></tr></table></figure><p>按理来说只要往根目录（<code>source</code>）下放一个<code>favicon.ico</code>的文件即可。</p><p>可是我的就不行，不知道什么原因，把文件名换了就可以了，所以我改成了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;link href=&quot;&lt;%- config.root %&gt;icon.png&quot; type=&quot;image/png&quot; rel=&quot;icon&quot;&gt;</span><br></pre></td></tr></table></figure><p>然后往根目录下放一个<code>icon.png</code>，解决。</p><p><strong>分享</strong></p><p>分享接口使用<a href="https://github.com/overtrue/share.js" target="_blank" rel="noopener">Share.js</a>，只需引入相应的css和js文件，照文档使用即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;博客迁移这个事早就想做了，但到现在才有时间和精力来完成。为什么要迁移呢？主要有几个原因：&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;原博客更新、维护较麻烦。原博客是自己用&lt;a href=&quot;https://www.52coding.com.cn/2015/12/21/%E8%AE%B0%E5%BD%95%EF%BC%9A%E7%94%A8PHP%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95web%E6%A1%86%E6%9E%B6/&quot;&gt;PHP搭的&lt;/a&gt;，之前的写作方式是用Markdown写好导出HTML，再修改HTML代码使得静态资源（图片等）加载正确，这就使得修改博客很麻烦；更换主题也很麻烦，博客的主题和Markdown的主题通常会有冲突，所以想换个样式就要改半天CSS。&lt;/li&gt;
&lt;li&gt;觉得UI有些难看，想要简洁一些；&lt;/li&gt;
&lt;li&gt;安全问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;现在的解决方案是&lt;a href=&quot;https://pages.github.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github Pages&lt;/a&gt; + &lt;a href=&quot;https://hexo.io/zh-cn/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;，主题选的是&lt;a href=&quot;https://github.com/CodeDaraW/Hacker&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hacker&lt;/a&gt;，迁移了两天终于搞完了，在此简单记录一下遇到的坑。&lt;/p&gt;
    
    </summary>
    
      <category term="踩坑现场" scheme="http://www.52coding.com.cn/categories/%E8%B8%A9%E5%9D%91%E7%8E%B0%E5%9C%BA/"/>
    
    
      <category term="Mathjax" scheme="http://www.52coding.com.cn/tags/Mathjax/"/>
    
      <category term="Hexo" scheme="http://www.52coding.com.cn/tags/Hexo/"/>
    
      <category term="Disqus" scheme="http://www.52coding.com.cn/tags/Disqus/"/>
    
      <category term="Gitment" scheme="http://www.52coding.com.cn/tags/Gitment/"/>
    
      <category term="Github Pages" scheme="http://www.52coding.com.cn/tags/Github-Pages/"/>
    
  </entry>
  
  <entry>
    <title>Interdependence and the Gains from Trade</title>
    <link href="http://www.52coding.com.cn/2018/11/03/Interdependence%20and%20the%20Gains%20from%20Trade/"/>
    <id>http://www.52coding.com.cn/2018/11/03/Interdependence and the Gains from Trade/</id>
    <published>2018-11-03T12:10:47.000Z</published>
    <updated>2018-11-06T07:10:39.499Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#a-parable-for-the-modern-economy">A Parable for the Modern Economy</a><ul><li><a href="#production-possibilities">Production Possibilities</a></li><li><a href="#specialization-and-trade">Specialization and Trade</a></li></ul></li><li><a href="#comparative-advance-the-driving-force-of-specialization">Comparative Advance: The Driving Force of Specialization</a><ul><li><a href="#absolute-advantage">Absolute Advantage</a></li><li><a href="#opportunity-cost-and-comparative-advantage">Opportunity Cost and Comparative Advantage</a></li><li><a href="#comparative-advantage-and-trade">Comparative Advantage and Trade</a></li><li><a href="#the-price-and-the-trade">The Price and The Trade</a></li></ul></li><li><a href="#applications-of-comparative-advantage">Applications of Comparative Advantage</a><ul><li><a href="#should-tiger-woods-mow-his-own-lawn">Should Tiger Woods Mow His Own Lawn?</a></li><li><a href="#should-the-united-states-trade-with-other-countries">Should the United States Trade With Other Countries?</a></li></ul></li><li><a href="#summary">Summary</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="a-parable-for-the-modern-economy">A Parable for the Modern Economy</span></h2><h3><span id="production-possibilities">Production Possibilities</span></h3><p>The graph shows the various mixes of output that an economy can produce and illustrate that people face trade-offs.</p><p><img src="/images/IMG_CBD807794C2B-1.png"> If the farmer and rancher do not trade, the production possibilities frontier is also the consumption possibilities frontier. However, the frontier shows trade-offs but do not show what they will choose to do. So let’s suppose the choose the combinations identified in the graph by points A and B.</p><h3><span id="specialization-and-trade">Specialization and Trade</span></h3><p><img src="/images/IMG_FA9EF67DD902-1.png"> The farmer and rancher can both benefit because trade allows each of them to specialize in doing what they do best. The farmer will spend more time growing potatoes and less time raising cattle. The rancher will spend more time raising cattle and less time growing potatoes. As a result of specialization and trade, each of them can consume more meat and more potatoes without working any more hours.</p><h2><span id="comparative-advance-the-driving-force-of-specialization">Comparative Advance: The Driving Force of Specialization</span></h2><p>The puzzle is why the rancher does better in both fields, he still gain from trade? To solve this puzzle, we should answer first who has a lower cost to produce potatoes?</p><h3><span id="absolute-advantage">Absolute Advantage</span></h3><p>We can measure the cost through <em>absolute advantage</em> which is the ability to produce a good using fewer inputs than another producer. In our example, the only input is time. Because the rancher need fewer time to produce both items, he has the absolute advantage in producing both goods. Base on this the rancher has a lower cost to produce potatoes.</p><h3><span id="opportunity-cost-and-comparative-advantage">Opportunity Cost and Comparative Advantage</span></h3><p>Recall the <em>opportunity cost</em> is whatever must be given up to obtain some item. In our example, the opportunity cost of the rancher to produce 1 ounce potatoes is 1/2 ounce meat and the opportunity cost of him to produce 1 ounce meat is 2 ounce potatoes. Similarly, we can compute the opportunity cost for farmer which summarize in table 1. <img src="/images/IMG_857CB49E31D3-1.png"></p><p>We can also measure the cost through <em>comparative advantage</em>, which is the ability to produce a good at a lower opportunity cost than another producer. Through table 1 we can find out that farmer has comparative advantage in producing potatoes and rancher has comparative advantage in producing meat. That’s why the rancher can gain from trade.</p><p>Although it is possible for one person to have an absolute advantage in both goods, it is impossible for one to have a comparative advantage in both goods. Because the opportunity cost of one good is inverse of the opportunity cost of the other.</p><h3><span id="comparative-advantage-and-trade">Comparative Advantage and Trade</span></h3><p>The gains from specialization and trade based on comparative advantage. By specialization in what he has a comparative advantage, the total production of the society raises which means increase the size of economic pie.</p><p>Also, the price of the goods should lower than their opportunity cost of producing it. For example, the farmer exchange 15 ounce potatoes for 5 ounce meat. The price of meat for the farmer is 3 ounce potatoes which is lower than his opportunity cost of producing meat (4 ounce potatoes). Similarly, for the rancher, the price of 1 ounce potatoes is 1/3 ounce meat which is also lower than his opportunity cost of producing potatoes (1/2 ounce meat).</p><p>Conclude: <strong>Trade can benefit everyone in society because it allows people to specialize in activities in which they have a comparative advantage</strong>.</p><h3><span id="the-price-and-the-trade">The Price and The Trade</span></h3><p><strong>For both parties to gain from trade, the price at which they trade must lie between the two opportunity costs</strong>.</p><h2><span id="applications-of-comparative-advantage">Applications of Comparative Advantage</span></h2><h3><span id="should-tiger-woods-mow-his-own-lawn">Should Tiger Woods Mow His Own Lawn?</span></h3><p>Say Tiger Woods can mow his own lawn in 2 hours while, Forrest, the boy next door, can mow the lawn in 4 hours. Should Tiger Woods mow his own lawn?</p><p>Clearly, Tiger Woods has an absolute advantage in mowing the lawn but he has a higher opportunity cost in doing it because he could spend 2 hours filming a commercial advertisement earning $10000 while Forrest can only earn $20 in 4 hours. So Tiger should hire Forrest to mow the lawn and both of them will better off as long as the payment is between $20 and $10000.</p><h3><span id="should-the-united-states-trade-with-other-countries">Should the United States Trade With Other Countries?</span></h3><p>International trade can make some individuals worse off, even as it makes the country as a whole better off. When the US exports food and imports cars, the impact on an American farmer is not the same as the impact on an American autoworker. Yet, international trade is not like war, in which some countries win and others lose. <em>Trade allows all countries to achieve greater prosperity</em>.</p><h2><span id="summary">Summary</span></h2><ul><li>Each person consumes goods and services produced by many other people both in the United States and around the world. Interdependence and trade are desirable because they allow everyone to enjoy a greater <strong>quantity and variety</strong> of goods and services.</li><li>There are two ways to compare the ability of two people in producing a good. The person who can produce the good with the smaller quantity of inputs is said to have an <em>absolute advantage</em> in producing the good. The person who has the smaller <em>opportunity cost</em> of producing the good is said to have a <em>comparative advantage</em>. The gains from trade are based on comparative advantage, not absolute advantage.</li><li>Trade makes everyone better off because it allows people to specialize in those activities in which they have a comparative advantage.</li><li>The principle of comparative advantage applies to countries as well as to people. Economists use the principle of comparative advantage to advocate free trade among countries.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#a-parable-for-the-modern-economy&quot;&gt;A Parable for the Modern Economy&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#production-possibilities&quot;&gt;Production Possibilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#specialization-and-trade&quot;&gt;Specialization and Trade&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#comparative-advance-the-driving-force-of-specialization&quot;&gt;Comparative Advance: The Driving Force of Specialization&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#absolute-advantage&quot;&gt;Absolute Advantage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#opportunity-cost-and-comparative-advantage&quot;&gt;Opportunity Cost and Comparative Advantage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#comparative-advantage-and-trade&quot;&gt;Comparative Advantage and Trade&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#the-price-and-the-trade&quot;&gt;The Price and The Trade&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#applications-of-comparative-advantage&quot;&gt;Applications of Comparative Advantage&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#should-tiger-woods-mow-his-own-lawn&quot;&gt;Should Tiger Woods Mow His Own Lawn?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#should-the-united-states-trade-with-other-countries&quot;&gt;Should the United States Trade With Other Countries?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://www.52coding.com.cn/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="微观经济型原理" scheme="http://www.52coding.com.cn/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%9E%8B%E5%8E%9F%E7%90%86/"/>
    
      <category term="trade" scheme="http://www.52coding.com.cn/tags/trade/"/>
    
      <category term="comparative advantage" scheme="http://www.52coding.com.cn/tags/comparative-advantage/"/>
    
  </entry>
  
  <entry>
    <title>Unity学习笔记</title>
    <link href="http://www.52coding.com.cn/2018/11/01/Unity%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://www.52coding.com.cn/2018/11/01/Unity学习笔记/</id>
    <published>2018-11-01T02:41:47.000Z</published>
    <updated>2018-11-06T04:01:05.679Z</updated>
    
    <content type="html"><![CDATA[<p><strong>记录一些小功能的实现</strong></p><!-- toc --><ul><li><a href="#实现相机跟随">实现相机跟随</a></li><li><a href="#拖动图标在场景生成物体">拖动图标在场景生成物体</a></li><li><a href="#技能冷却效果">技能冷却效果</a></li><li><a href="#鼠标点击选中场景中的物体">鼠标点击选中场景中的物体</a></li><li><a href="#2d人物朝左朝右">2D人物朝左朝右</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="实现相机跟随">实现相机跟随</span></h2><ul><li>方法一<ul><li>把相机设置为目标的Child</li></ul></li><li>方法二<ul><li>设置好距目标的距离和角度，根据数学关系计算出相机位置</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">float distance = 15;// 距离</span><br><span class="line">float rot = 0;// 横向角度</span><br><span class="line">GameObject target;// 目标物体</span><br><span class="line">float roll = 30f * Mathf.PI * 2 / 360; // 纵向角度</span><br><span class="line"></span><br><span class="line">void LateUpdate () &#123;</span><br><span class="line">Vector3 targetPos = target.transform.position;</span><br><span class="line">Vector3 cameraPos;</span><br><span class="line">float d = distance * Mathf.Cos (roll);</span><br><span class="line">float height = distance * Mathf.Sin (roll);</span><br><span class="line">cameraPos.x = targetPos.x + d * Mathf.Cos (rot);</span><br><span class="line">cameraPos.z = targetPos.z + d * Mathf.Sin (rot);</span><br><span class="line">cameraPos.y = targetPos.y + height;</span><br><span class="line">Camera.main.transform.position = cameraPos;</span><br><span class="line">Camera.main.transform.LookAt (target.transform);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>相机随鼠标旋转</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">void Rotate()</span><br><span class="line">&#123;</span><br><span class="line"> float w = Input.GetAxis (&quot;Mouse X&quot;) * rotSpeed;</span><br><span class="line">rot -= w;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void Roll()</span><br><span class="line">&#123;</span><br><span class="line">float w = Input.GetAxis (&quot;Mouse Y&quot;) * rollSpeed * 0.5f;</span><br><span class="line">roll -= w;</span><br><span class="line">if (roll &gt; maxRoll) &#123;</span><br><span class="line">roll = maxRoll;</span><br><span class="line">&#125;</span><br><span class="line">if (roll &lt; minRoll) &#123;</span><br><span class="line">roll = minRoll;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void LateUpdate () &#123;</span><br><span class="line">Rotate();</span><br><span class="line">  Roll();</span><br><span class="line">....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="拖动图标在场景生成物体">拖动图标在场景生成物体</span></h2><p><strong>拖动UI</strong></p><p>新建<code>Drag</code>类，继承<code>IBeginDragHandler, IDragHandler, IEndDragHandler</code>，实现拖动UI功能有三个接口：</p><ul><li><code>public void OnBeginDrag (PointerEventData eventData)</code></li><li><code>public void OnDrag (PointerEventData eventData)</code></li><li><code>public void OnEndDrag (PointerEventData eventData)</code></li></ul><p>在<code>Drag</code>类里实现这三个接口即可实现想要的拖动效果，最后不用忘了把<code>Drag</code>脚本添加到想要被拖动的UI物体上。</p><p><strong>在场景中生成物体</strong></p><p>要实现这个功能:</p><ul><li>首先在<code>OnBeginDrag</code>中生成新的<code>GameObject</code>；</li><li>然后在<code>OnDrag</code>中，根据鼠标在场景里的位置调整<code>GameObject</code>的位置，再检测<code>GameObject</code>的collider有无和其他物体碰撞；</li><li>最后在<code>OnEndDrag</code>中，如果<code>GameObject</code>的最终位置合法，则不再移动；否则销毁物体，生成失败。</li></ul><h2><span id="技能冷却效果">技能冷却效果</span></h2><p><strong>定时器</strong></p><p>实现冷却效果计时器必不可少，实现方法也很简单，只需两个变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bool timerStarted = false;</span><br><span class="line">float remain = 10f;</span><br></pre></td></tr></table></figure><p>然后在<code>Update</code>中作如下更新：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">void Update ()</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">    if (timerStarted) &#123;</span><br><span class="line">remain -= Time.deltaTime;</span><br><span class="line">        if (remain &lt;= 0) &#123;</span><br><span class="line">            CloseTimer();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>UI效果</strong></p><p>Button的层次如下：</p><ul><li><p><code>Button</code></p><ul><li><p><code>Image</code>：按钮显示的图标</p></li><li><p><code>Mask</code>：可以用按钮的默认背景；调整颜色和透明度；ImageType为filled；通过调整Fill Amount来实现转动效果</p><p><img src="/images/Screen%20Shot%202018-10-30%20at%204.09.00%20PM.png"></p></li><li><p><code>CD Text</code>：显示剩余冷却时间</p></li></ul></li></ul><p><strong>Note</strong>：在开始冷却的同时，应把设置<code>btn.interactable = false;</code>，否则按钮可以在冷却过程中再次被点击。</p><p>这里Button的<code>OnClick</code>绑定了两个函数，分别给<code>CharacterController</code>实现技能效果，和给<code>UIManager</code>实现UI动效：</p><p><img src="/images/btnclick.png"></p><h2><span id="鼠标点击选中场景中的物体">鼠标点击选中场景中的物体</span></h2><p>思路：从点击位置向场景发射射线，检测是否击中物体</p><p>实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">void MousePick () &#123;</span><br><span class="line">    if (Input.GetMouseButtonUp (0)) &#123;</span><br><span class="line">        // 发射射线</span><br><span class="line">        Ray myRay = Camera.main.ScreenPointToRay (Input.mousePosition);</span><br><span class="line">        // 选择想被选中的layer</span><br><span class="line">        int layerMask = LayerMask.GetMask (&quot;Building&quot;);</span><br><span class="line">        // 检测碰撞</span><br><span class="line">        RaycastHit2D hit = Physics2D.Raycast (new Vector2 (myRay.origin.x, myRay.origin.y),</span><br><span class="line">            Vector2.down, Mathf.Infinity, layerMask);</span><br><span class="line">        if (hit.collider) &#123;</span><br><span class="line">            // 检测到碰撞，选中该物体</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="2d人物朝左朝右">2D人物朝左朝右</span></h2><p>思路：如果原sprite朝右，那么只要把transform的<code>scale.x</code>变成<code>-1</code>就是朝左了。</p><p><img src="/images/facingside.png"></p><p>实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">void LateUpdate () &#123;</span><br><span class="line">    Vector3 localScale = _transform.localScale;</span><br><span class="line"></span><br><span class="line">    if (_vx &gt; 0) &#123; // moving right so face right</span><br><span class="line">        _facingRight = true;</span><br><span class="line">    &#125; else if (_vx &lt; 0) &#123; // moving left so face left</span><br><span class="line">        _facingRight = false;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // check to see if scale x is right for the player</span><br><span class="line">    // if not, multiple by -1 which is an easy way to flip a sprite</span><br><span class="line">    if ((_facingRight) &amp;&amp; (localScale.x &lt; 0) || </span><br><span class="line">        ((localScale.x &gt; 0)) &#123;</span><br><span class="line">        localScale.x *= -1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // update the scale</span><br><span class="line">    _transform.localScale = localScale;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>未完待续</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;记录一些小功能的实现&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#实现相机跟随&quot;&gt;实现相机跟随&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#拖动图标在场景生成物体&quot;&gt;拖动图标在场景生成物体&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#技能冷却效果&quot;&gt;技能冷却效果&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#鼠标点击选中场景中的物体&quot;&gt;鼠标点击选中场景中的物体&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#2d人物朝左朝右&quot;&gt;2D人物朝左朝右&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="踩坑现场" scheme="http://www.52coding.com.cn/categories/%E8%B8%A9%E5%9D%91%E7%8E%B0%E5%9C%BA/"/>
    
    
      <category term="Unity" scheme="http://www.52coding.com.cn/tags/Unity/"/>
    
      <category term="C#" scheme="http://www.52coding.com.cn/tags/C/"/>
    
      <category term="Game Dev" scheme="http://www.52coding.com.cn/tags/Game-Dev/"/>
    
  </entry>
  
  <entry>
    <title>Thinking Like an Economist</title>
    <link href="http://www.52coding.com.cn/2018/10/03/Thinking%20Like%20an%20Economist/"/>
    <id>http://www.52coding.com.cn/2018/10/03/Thinking Like an Economist/</id>
    <published>2018-10-03T12:10:47.000Z</published>
    <updated>2018-11-06T06:39:17.839Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#the-economist-as-scientist">The Economist as Scientist</a><ul><li><a href="#the-scientific-method-observation-theory-and-more-observation">The Scientific Method: Observation, Theory, and More Observation</a></li><li><a href="#the-role-of-assumptions">The Role of Assumptions</a></li><li><a href="#economic-models">Economic Models</a></li><li><a href="#our-first-model-the-circular-flow-diagram">Our First Model: The Circular-Flow Diagram</a></li><li><a href="#our-second-model-the-production-possibilities-frontier">Our Second Model: The Production Possibilities Frontier</a></li><li><a href="#microeconomics-and-macroeconomics">Microeconomics and Macroeconomics</a></li></ul></li><li><a href="#the-economist-as-policy-adviser">The Economist as Policy Adviser</a><ul><li><a href="#positive-versus-normative-analysis">Positive versus Normative Analysis</a></li><li><a href="#economists-in-washington">Economists in Washington</a></li><li><a href="#why-economists-advice-is-not-always-followed">Why Economists’ Advice Is Not Always Followed</a></li></ul></li><li><a href="#why-economists-disagree">Why Economists Disagree</a><ul><li><a href="#differences-in-scientific-judgments">Differences in Scientific Judgments</a></li><li><a href="#difference-in-values">Difference in Values</a></li><li><a href="#perception-versus-reality">Perception versus Reality</a></li></ul></li></ul><!-- tocstop --><a id="more"></a><h2><span id="the-economist-as-scientist">The Economist as Scientist</span></h2><h3><span id="the-scientific-method-observation-theory-and-more-observation">The Scientific Method: Observation, Theory, and More Observation</span></h3><p>Invention an economic theory is just like in other scientific fields, which is <em>observation, summary to a theory and then collect data to test it</em>. However, it is often <em>difficult or impossible</em> for economists to <em>collect data</em> because you cannot change policies just for experiments. <strong>Therefore, economists often pay attention to the natural experiments offered by history</strong> which can not only give insight into the economy of the past, but also allow to illustrate and evaluate economic theories of the present.</p><h3><span id="the-role-of-assumptions">The Role of Assumptions</span></h3><p><strong>Make assumptions can simplify the question.</strong> e.g. once we understood the international trade in the simplified imaginary world, we are in a better position to understand international trader in the more complex world.</p><p>Also, the art in scientific thinking is <strong>deciding which assumptions to make</strong>. e.g. study short-run effect or long-run effect make different assumptions.</p><h3><span id="economic-models">Economic Models</span></h3><p>Economic models composed with graphs and equations which omit a lot of details to emphases the essence of economy. Each economic models make assumptions to simplify reality so as to improve our understanding of it.</p><h3><span id="our-first-model-the-circular-flow-diagram">Our First Model: The Circular-Flow Diagram</span></h3><p><img src="/images/IMG_9A38B6F35EC5-1.jpeg.jpg"></p><p>e.g. money in your wallet -&gt; buy coffee in markets for goods and services (local Starbucks) -&gt; revenue of the company -&gt; pay rental or wage -&gt; someone’s wallet</p><p>Because of its simplicity, this circular-flow diagram is useful to keep in mind when thinking about <strong>how the pieces of the economy fit together</strong>.</p><h3><span id="our-second-model-the-production-possibilities-frontier">Our Second Model: The Production Possibilities Frontier</span></h3><p><img src="/images/IMG_2122EF5B828A-1.jpeg.jpg"> The production possibilities frontier shows the <em>efficiency</em> of the society. Because resources are <em>scarce</em>, not every conceivable outcome is feasible. Points <strong>on</strong> the production possibilities frontier represent <em>efficient levels</em> of production.</p><p>It also reveals <em>trade-off</em> and <em>opportunity costs</em>: if produce more computers, means have to produce less cars. The <em>opportunity cost</em> is measured by the <strong>slope</strong> of the production possibilities frontier, which means point F’s opportunity cost of a car is lower and point E’ opportunity cost of producing a car is higher. That’s because when at point E, the society has let all of car engineers to produce cars. Producing one more car means moving some of the best computer technicians out of the computer industry and making them autoworkers.</p><p>The production possibilities frontier also change with time, which shows <em>economic growth</em>. <img src="/images/IMG_0EA219852402-1.jpeg.jpg"></p><h3><span id="microeconomics-and-macroeconomics">Microeconomics and Macroeconomics</span></h3><p>Economics is studied on various levels, which is traditionally divided into two broad subfields:</p><ul><li><strong>Microeconomics</strong> is the study of how households and firms make decisions and how they interact in specific markets.</li><li><strong>Macroeconomics</strong> is the study of economy-wide phenomena, including inflation, unemployment, and economic growth.</li></ul><h2><span id="the-economist-as-policy-adviser">The Economist as Policy Adviser</span></h2><h3><span id="positive-versus-normative-analysis">Positive versus Normative Analysis</span></h3><p><strong>positive statements</strong>: claims that attempt to describe the world as it is <strong>normative statements</strong>: claims that attempt to prescribe how the world should be</p><p>Normative statements comes from positive statements as well as value judgements. Deciding what is good or bad policy is not just a matter of science. It also involves our views on ethics, religion, and political philosophy.</p><h3><span id="economists-in-washington">Economists in Washington</span></h3><p>Economists in Whitehouse also face trade-offs. The influence of economists on policy goes beyond their role as advisers: Their research and writings often affect policy indirectly.</p><h3><span id="why-economists-advice-is-not-always-followed">Why Economists’ Advice Is Not Always Followed</span></h3><p>Economists offer crucial input into the policy process, but their advice is only one ingredient of a complex recipe.</p><h2><span id="why-economists-disagree">Why Economists Disagree</span></h2><h3><span id="differences-in-scientific-judgments">Differences in Scientific Judgments</span></h3><p><strong>Economic is a young science and there is still much to be learned.</strong> They disagree because they have different hunches about the validity of alternative theories or about the size of important parameters that measure how economic variables are related.</p><h3><span id="difference-in-values">Difference in Values</span></h3><p>Economists give conflicting advice sometimes because they have different values.</p><h3><span id="perception-versus-reality">Perception versus Reality</span></h3><p>Why do policies such as rent control persist if the experts are united in their opposition? It may be that the realities of the <strong>political process</strong> stand as immovable obstacles. But it also may be that economists have <strong>not yet convinced</strong> enough of the public that these policies are undesirable.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#the-economist-as-scientist&quot;&gt;The Economist as Scientist&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#the-scientific-method-observation-theory-and-more-observation&quot;&gt;The Scientific Method: Observation, Theory, and More Observation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#the-role-of-assumptions&quot;&gt;The Role of Assumptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#economic-models&quot;&gt;Economic Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#our-first-model-the-circular-flow-diagram&quot;&gt;Our First Model: The Circular-Flow Diagram&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#our-second-model-the-production-possibilities-frontier&quot;&gt;Our Second Model: The Production Possibilities Frontier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#microeconomics-and-macroeconomics&quot;&gt;Microeconomics and Macroeconomics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#the-economist-as-policy-adviser&quot;&gt;The Economist as Policy Adviser&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#positive-versus-normative-analysis&quot;&gt;Positive versus Normative Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#economists-in-washington&quot;&gt;Economists in Washington&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#why-economists-advice-is-not-always-followed&quot;&gt;Why Economists’ Advice Is Not Always Followed&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#why-economists-disagree&quot;&gt;Why Economists Disagree&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#differences-in-scientific-judgments&quot;&gt;Differences in Scientific Judgments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#difference-in-values&quot;&gt;Difference in Values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#perception-versus-reality&quot;&gt;Perception versus Reality&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://www.52coding.com.cn/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="微观经济型原理" scheme="http://www.52coding.com.cn/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%9E%8B%E5%8E%9F%E7%90%86/"/>
    
      <category term="Production Possibilities Frontier" scheme="http://www.52coding.com.cn/tags/Production-Possibilities-Frontier/"/>
    
  </entry>
  
  <entry>
    <title>Ten Principles of Economics</title>
    <link href="http://www.52coding.com.cn/2018/09/16/Chapter%201-%20Ten%20Principles%20of%20Economics/"/>
    <id>http://www.52coding.com.cn/2018/09/16/Chapter 1- Ten Principles of Economics/</id>
    <published>2018-09-16T02:41:47.000Z</published>
    <updated>2018-11-06T06:23:18.037Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#how-people-make-decisions">How People Make Decisions</a><ul><li><a href="#principle-1-people-face-trade-offs">Principle 1: People Face Trade-offs</a></li><li><a href="#principle-2-the-cost-of-something-is-what-you-give-up-to-get-it">Principle 2: The Cost of Something Is What You Give Up to Get It</a></li><li><a href="#principle-3-rational-people-think-at-the-margin">Principle 3: Rational People Think at the Margin</a></li><li><a href="#principle-4-people-respond-to-incentives">Principle 4: People Respond to Incentives</a></li></ul></li><li><a href="#how-people-interact">How People Interact</a><ul><li><a href="#principle-5-trade-can-make-everyone-better-off">Principle 5: Trade Can Make Everyone Better Off</a></li><li><a href="#principle-6-markets-are-usually-a-good-way-to-organize-economic-activity">Principle 6: Markets Are Usually a Good Way to Organize Economic Activity</a></li><li><a href="#principle-7-governments-can-sometimes-improve-market-outcomes">Principle 7: Governments Can Sometimes Improve Market Outcomes</a></li></ul></li><li><a href="#how-the-economy-as-a-whole-works">How the Economy as a Whole Works</a><ul><li><a href="#principle-8-a-countrys-standard-of-living-depends-on-its-ability-to-produce-goods-and-services">Principle 8: A Country’s Standard of Living Depends on Its Ability to Produce Goods and Services</a></li><li><a href="#principle-9-prices-rise-when-the-government-prints-too-much-money">Principle 9: Prices Rise When the Government Prints Too Much Money</a></li><li><a href="#principle-10-society-faces-a-short-run-trade-off-between-inflation-and-unemployment">Principle 10: Society Faces a Short-Run Trade-off between Inflation and Unemployment</a></li></ul></li><li><a href="#summary">Summary</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="how-people-make-decisions">How People Make Decisions</span></h2><p><strong>scarcity</strong>: the limited nature of society’s resources <strong>economics</strong>: the study of how society manages its <em>scarce</em> resources</p><h3><span id="principle-1-people-face-trade-offs">Principle 1: People Face Trade-offs</span></h3><p>To get one thing we like, we usually have to give up another thing that we like. <strong>Making decisions</strong> requires trading-off one goal against another.</p><ul><li>student cannot learn two or more things at the same time</li><li>how to spend family income</li><li>guns (defense) and butter (living conditions)</li></ul><p><strong>Efficiency</strong> and <strong> Equality</strong></p><ul><li><em>Efficiency</em> means the property of society getting the most it can from its scarce resources.</li><li><em>Equality</em> means the property of distributing economic prosperity uniformly among the members of society.</li><li>In other words, <em>efficiency</em> refers to the size of the economic pie, and <em>equality</em> refers to how the pie is divided into individual slices.</li><li>When government tries to cut the economic pie into more equal slices, the pie get smaller.</li></ul><p>Nonetheless, people are likely to make good decisions only if they understand the options they have available. Our study of economics, therefore, starts by acknowledging life’s trade-offs.</p><h3><span id="principle-2-the-cost-of-something-is-what-you-give-up-to-get-it">Principle 2: The Cost of Something Is What You Give Up to Get It</span></h3><p><strong>Opportunity cost</strong>: whatever must be given up to obtain some item. When making any decision, decision makers should be aware of the opportunity costs that accompany each possible action.</p><h3><span id="principle-3-rational-people-think-at-the-margin">Principle 3: Rational People Think at the Margin</span></h3><p><strong>Rational people</strong> systematically and purposefully do the best they can to achieve their objectives, given the available opportunities. <strong> Marginal change</strong>: a small incremental adjustment to a plan of action. e.g. when exam around, study one more hour instead of playing games.</p><p>Rational people often make decisions by comparing <em>marginal benefits</em> and <em>marginal costs</em>.</p><ul><li>airline ticket</li><li>why is water so cheap, while diamonds are so expensive?<ul><li>water is plentiful -&gt; margin benefit is small</li><li>diamonds are so rare -&gt; margin benefit is large A rational decision maker takes an action if and only if the <em>marginal benefit</em> of the action <strong>exceeds</strong> the <em>marginal cost</em>.</li></ul></li></ul><h3><span id="principle-4-people-respond-to-incentives">Principle 4: People Respond to Incentives</span></h3><p>An <strong>incentive</strong> is something that induces a person to act, such as the prospect of a punishment or a reward. <em>People respond to incentives, the rest is commentary.</em></p><p>Auto safety</p><ul><li>1950s, no seat belt, accident is costly -&gt; seat belt law -&gt; accident is not that costly -&gt; people drive faster (cost less time) -&gt; few deaths per accident but more accidents -&gt; little change in driver deaths and an increase in the number of pedestrian deaths.</li></ul><p>When analyzing any policy, we must consider not only the direct effects but also the less obvious indirect effects that work through incentives. If the policy changes incentives, it will cause people to alter their behavior.</p><p><em>Incentive Pay</em> Chicago buses do not take the shortcut when around congestion, because they have no incentive to do so. If they are paid by passengers like taxi rather than by bus company, they will choose the shortcuts to get more passengers like other cars do. It will increase the bus driver’s productivity but also increase the risk of having accidents.</p><h2><span id="how-people-interact">How People Interact</span></h2><h3><span id="principle-5-trade-can-make-everyone-better-off">Principle 5: Trade Can Make Everyone Better Off</span></h3><p><strong>Trade</strong> between two countries is not like a sports contest in which one side wins and the other side loses. In fact, the opposite is true: <em>Trade between two countries can make each country better off</em>.</p><p>Trade allows countries to specialize in what they do best and to enjoy a greater variety of goods and services.</p><h3><span id="principle-6-markets-are-usually-a-good-way-to-organize-economic-activity">Principle 6: Markets Are Usually a Good Way to Organize Economic Activity</span></h3><p><em>Communist</em> countries worked on the premise that government officials were in the best position to allocate the economy’s scarce resources. The theory behind <em>central planning</em> was that only the government could organize economic activity in a way that promoted <em>economic well-being for the country as a whole</em>. <em>Central planners</em> failed because they tried to run the economy with one hand tied behind their backs — the invisible hand of the marketplace.</p><p>In a <strong>market economy</strong>, the decisions of a central planner are replaced by the decisions of millions of firms and households.</p><blockquote><p>Households and firms interacting in markets act as if they are guided by an “invisible hand” that leads them to desirable market outcomes. — Adam Smith</p></blockquote><p>In any market, buyers look at the price when determining how much to demand, and sellers look at the price when deciding how much to supply. As a result of the decisions that buyers and sellers make, <em>market prices</em> reflect both <em>the value of a good to society</em> and <em>the cost to society of making the good</em>. Smith’s great insight was that <strong>prices</strong> adjust to <strong>guide</strong> these individual buyers and sellers to reach outcomes that, in many cases, <em>maximize the well-being of society as a whole</em>.</p><h3><span id="principle-7-governments-can-sometimes-improve-market-outcomes">Principle 7: Governments Can Sometimes Improve Market Outcomes</span></h3><p><strong>property right</strong>: the ability of an individual to own and exercise control over scarce resources. <strong>market failure</strong>: a situation in which a market left on its own fails to allocate resources efficiently. <strong>externality</strong>: the impact of one person’s actions on the well-being of a bystander. <strong>market power</strong>: the ability of a single economic actor (or a small group of actors) to have a substantial influence on market prices.</p><p><em>The invisible hand is powerful, but it is not omnipotent.</em> The economy needs the government to</p><ul><li>enforce the rules and maintain the institutions that are key to a market economy</li><li>enforce <strong>property right</strong><ul><li>We all rely on government-provided police and courts to enforce our rights over the things we produce — and the <em>invisible hand</em> counts on our ability to enforce our rights.</li></ul></li><li>promote <strong>efficiency</strong><ul><li><em>market failure</em> because <strong>externality</strong> (e.g. pollution) and <strong>market power</strong> (e.g. monopoly)</li></ul></li><li>promote <strong>equality</strong></li></ul><h2><span id="how-the-economy-as-a-whole-works">How the Economy as a Whole Works</span></h2><h3><span id="principle-8-a-countrys-standard-of-living-depends-on-its-ability-to-produce-goods-and-services">Principle 8: A Country’s Standard of Living Depends on Its Ability to Produce Goods and Services</span></h3><p>Why the differences in living standards among countries and over time are so large? Almost all variation in living standards is attributable to differences in countries’ <strong>productivity</strong> — that is, the amount of goods and services produced from each unit of labor input. When thinking about how any policy will affect our living standards, the key question is <em>how it will affect our ability to produce goods and services</em>.</p><h3><span id="principle-9-prices-rise-when-the-government-prints-too-much-money">Principle 9: Prices Rise When the Government Prints Too Much Money</span></h3><p><strong>inflation</strong>: an increase in the overall level of prices in the economy</p><p>What cause inflation? In almost all cases of large or persistent inflation, the culprit is <em>growth in the quantity of money</em>.</p><blockquote><p>The broken window fallacy Some teenagers, being the little beasts that they are, toss a brick through a bakery window. A crown gathers and laments, “What a shame”. But before you know it, someone suggests a silver lining to the situation: Now the baker will have to spend money to have the window repaired. This will add to the income of the repairman, who will spend his additional income, which will add to another seller’s income, and so on. The chain of spending will multiply and generate higher income and employment. If the broken window is large enough, it might produce an economic boom! But if the baker hadn’t spent his money on window repair, he would have spent it on the new suit he was saving to buy. Then the tailor would have the new income to spend, and so on. <em>The broken window didn’t create new spending; it just diverted spending from somewhere else.</em></p></blockquote><h3><span id="principle-10-society-faces-a-short-run-trade-off-between-inflation-and-unemployment">Principle 10: Society Faces a Short-Run Trade-off between Inflation and Unemployment</span></h3><p>Short-run effects of monetary injections as follows:</p><ul><li>Increasing the amount of money in the economy stimulates the overall level of spending and thus the demand for goods and services</li><li>Higher demand many over time cause firms to raise their prices, but in the meantime, it also encourage them to hire more workers and produce a larger quantity of goods and services.</li><li>More hiring means lower unemployment.</li></ul><p><strong>business cycle</strong>: fluctuations in economic activity, such as employment and production.</p><p>Case: 2008 deep economic downturn -&gt; Barack Obama: <em>stimulus package of reduced taxes and increased government spending</em> -&gt; Federal Reserve: <em>increased the supply of money</em> -&gt; <strong>reduce unemployment</strong> -&gt; might over time lead to an <strong>excessive level of inflation</strong>.</p><h2><span id="summary">Summary</span></h2><ol type="1"><li>The fundamental lessons about individual decision making are that people face trade-offs among alternative goals, that the cost of any action is measured in terms of forgone opportunities, that rational people make decisions by comparing marginal costs and marginal benefits, and that people change their behavior in response to the incentives they face.</li><li>The fundamental lessons about interactions among people are that trade and interdependence can be mutually beneficial, that markets are usually a good way of coordinating economic activity among people, and that the government can potentially improve market outcomes by remedying a market failure or by promoting greater economic equality.</li><li>The fundamental lessons about the economy as a whole are that productivity is the ultimate source of living standards, that growth in the quantity of money is the ultimate source of inflation, and that society faces a short-run trade-off between inflation and unemployment.</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#how-people-make-decisions&quot;&gt;How People Make Decisions&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#principle-1-people-face-trade-offs&quot;&gt;Principle 1: People Face Trade-offs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-2-the-cost-of-something-is-what-you-give-up-to-get-it&quot;&gt;Principle 2: The Cost of Something Is What You Give Up to Get It&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-3-rational-people-think-at-the-margin&quot;&gt;Principle 3: Rational People Think at the Margin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-4-people-respond-to-incentives&quot;&gt;Principle 4: People Respond to Incentives&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#how-people-interact&quot;&gt;How People Interact&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#principle-5-trade-can-make-everyone-better-off&quot;&gt;Principle 5: Trade Can Make Everyone Better Off&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-6-markets-are-usually-a-good-way-to-organize-economic-activity&quot;&gt;Principle 6: Markets Are Usually a Good Way to Organize Economic Activity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-7-governments-can-sometimes-improve-market-outcomes&quot;&gt;Principle 7: Governments Can Sometimes Improve Market Outcomes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#how-the-economy-as-a-whole-works&quot;&gt;How the Economy as a Whole Works&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#principle-8-a-countrys-standard-of-living-depends-on-its-ability-to-produce-goods-and-services&quot;&gt;Principle 8: A Country’s Standard of Living Depends on Its Ability to Produce Goods and Services&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-9-prices-rise-when-the-government-prints-too-much-money&quot;&gt;Principle 9: Prices Rise When the Government Prints Too Much Money&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-10-society-faces-a-short-run-trade-off-between-inflation-and-unemployment&quot;&gt;Principle 10: Society Faces a Short-Run Trade-off between Inflation and Unemployment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://www.52coding.com.cn/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="微观经济型原理" scheme="http://www.52coding.com.cn/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%9E%8B%E5%8E%9F%E7%90%86/"/>
    
      <category term="inflation" scheme="http://www.52coding.com.cn/tags/inflation/"/>
    
      <category term="marginal benefit" scheme="http://www.52coding.com.cn/tags/marginal-benefit/"/>
    
  </entry>
  
  <entry>
    <title>AlphaGo, AlphaGo Zero and AlphaZero</title>
    <link href="http://www.52coding.com.cn/2018/05/15/AlphaGo%20and%20AlphaGo%20Zero/"/>
    <id>http://www.52coding.com.cn/2018/05/15/AlphaGo and AlphaGo Zero/</id>
    <published>2018-05-15T07:55:19.000Z</published>
    <updated>2018-11-16T07:54:21.150Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="go">Go</span></h2><p>围棋起源于古代中国，是世界上最古老的棋类运动之一。在宋代的《梦溪笔谈》中探讨了围棋的局数变化数目，作者沈括称“大约连书万字四十三个，即是局之大数”，意思是说变化数目要写43个万字。根据围棋规则，没有气的子不能存活，扣除这些状态后的合法状态约有 <span class="math inline">\(2.08×10^{170}\)</span> 种。Robertson 与 Munro 在1978年证得围棋是一种 PSPACE-hard 的问题，其必胜法之记忆计算量在<span class="math inline">\(10^{600}\)</span> 以上，这远远超过可观测宇宙的原子总数 <span class="math inline">\(10^{75}\)</span>，可见围棋对传统的搜索方法是非常有挑战的。 <a id="more"></a></p><p><img src="/images/go1.png"></p><h2><span id="alphago">AlphaGo</span></h2><p><img src="/images/alphago_ori.png"></p><p>AlphaGo是第一个打败人类冠军的电脑程序。</p><p><strong>网络结构</strong></p><p>它由两个卷积神经网络组成，分别是策略网络和价值网络。</p><p><img src="/images/policynet.png"></p><p>策略网络 P 推荐下一步怎么走；它的输入就是棋盘的矩阵：白棋和黑棋的位置。这个网络由许多卷积层组成，逐渐学习围棋知识，最终输出行动（action）的概率分布，来推荐下一步怎么走。</p><p><img src="/images/valuenet.png"></p><p>价值网络也由卷积神经网络组成，它是用来预测这盘棋的胜者。它的输入也是棋盘矩阵，输出是一个属于 <span class="math inline">\([-1, +1]\)</span> 的标量，-1代表AlphaGo一定会输，+1代表一定会赢。</p><p><strong>训练流程</strong></p><p><img src="/images/alphago_train.png"></p><p>首先是监督学习，让策略网络学习人类专家的数据集：每一个棋面都有一个标签，对应人类专家的下法，让AlphaGo首先学习专家的走法。然后使用策略网络进行自我博弈，由于每局都会产生胜者，用这些数据来训练价值网络。</p><p><strong>搜索算法</strong></p><p><img src="/images/rebredth.png"></p><p>使用策略网络减少搜索宽度，只考虑网络推荐的下法。</p><p><img src="/images/red_val.png"></p><p>还可以使用价值网络来降低搜索树的深度，可以把搜索子树替换为一个值来表明这个局面赢的概率。</p><p><img src="/images/mcts_go.png"></p><p>不过实际上还是用的蒙特卡洛搜索树。它分为三步：</p><ol type="1"><li><p>选择</p><p>首先从树根向下遍历，每次选择置信度最高的走法，直到叶节点。置信度是由每个节点中存储的 Q-value 和策略网络给的先验概率 P 组成。</p></li><li><p>扩展和评估</p><p>到了叶节点之后就要扩展这颗树，用策略网络和价值网络分别评估当前局面，把概率最大的节点加入搜索树。</p></li><li><p>回溯</p><p>把新加入节点的价值 v 回溯到路径上的每一个节点的 Q-value 上。</p></li></ol><p>这就是初始版本的AlphaGo，这个版本赢了世界冠军李世石。</p><p><img src="/images/leesd.png"></p><h2><span id="alphago-zero">AlphaGo Zero</span></h2><p>AlphaGo Zero 除了围棋规则本身以外完全移除了人类的围棋知识，它与AlphaGo的主要区别如下：</p><ul><li>无人类数据<ul><li>完全从自我博弈中学习</li></ul></li><li>无手动编码的特征<ul><li>输入只是棋盘本身</li></ul></li><li>单一的神经网络<ul><li>策略网络和价值网络合二为一，并且结构改进为ResNet</li><li>输出部分分为两头，分别输出 policy 和 value</li></ul></li><li>更简单的搜索<ul><li>更简单的MCTS，无随机的快速走子，只用神经网络进行评估</li></ul></li></ul><p><strong>增强学习算法</strong></p><p><img src="/images/rl_zero.png"></p><p>目标：使用高质量（really really high quality）数据来训练神经网络，而最好的数据来源就是AlphaGo自我博弈。</p><p>所以流程就是这样的：</p><ol type="1"><li>输入当前的棋局，使用当前的神经网络来指导进行蒙特卡洛搜索，然后下搜索出的那步棋，接着输入后面的棋局、搜索….直到一盘棋结束。</li></ol><p><img src="/images/train_zero.png"></p><ol start="2" type="1"><li>下一步就是训练神经网络，使用之前自我对局的数据，训练策略的数据的特征就是任一棋局，标签就是蒙特卡洛搜索的结果，即策略更贴近于AlphaGo实际下的策略（MCTS的搜索结果）</li></ol><p><img src="/images/train_zero_val.png"></p><ol start="3" type="1"><li>与此同时，使用每盘对局的胜者训练价值网络部分。</li></ol><p><img src="/images/zero_iterate.png"></p><ol start="4" type="1"><li>最后，经过训练的神经网络又可以继续进行自我博弈，产生更高质量的数据，然后用这个数据继续训练…. 循环往复，循环的关键在于，经过每个循环，我们都会得到更强的棋手（神经网络），所以继续会得到更高质量的数据。最后就产生了非常强的棋手。</li></ol><p><img src="/images/rl_policy_ite.png"></p><p>这个算法可以被看作是增强学习里的策略迭代（Policy Iteration）算法：</p><ul><li>Search-Based Policy Improvement （策略增强）<ul><li>用当前的网络进行MCTS</li><li>MCTS搜索出来的结果 &gt; 神经网络直接选择的结果（因为搜索的结果结合了前瞻）</li></ul></li><li>Search-Based Policy Evaluation （策略评估）<ul><li>使用搜索算法和神经网络进行自我博弈</li><li>评估改进后的策略</li></ul></li></ul><p><strong>学习曲线</strong></p><p><img src="/images/gozero_curve.png"></p><p><strong>实力</strong></p><p><img src="/images/gozero_rating.png"></p><h2><span id="alphazero">AlphaZero</span></h2><p><img src="/images/alphazero.png"></p><p>AlphaZero使用同一种算法学习三种不同的棋类，并都取得了超人的水平。</p><p>棋类AI研究情况总结</p><ul><li>在AI的历史上很早就开始研究棋类，如图灵、香农、冯诺伊曼等</li><li>专一系统曾在国际象棋上成功过<ul><li>深蓝在1997年击败卡氏</li><li>现在的象棋程序人类已无法击败</li></ul></li><li>将棋（日本象棋）比国际象棋更难<ul><li>更大的棋盘和行动空间</li><li>只有最近的程序才达到了龙王的水平</li></ul></li><li>最前沿的引擎都是根据 alpha-beta 搜索<ul><li>人类大师手工优化的评估函数</li><li>搜索域针对不同棋类疯狂优化</li></ul></li></ul><p><img src="/images/gochess.png"></p><p>由上图可见围棋与将棋和象棋还是有很大不同的，但是AlphaZero的主要算法和AlphaGo Zero一样，都是自我博弈的增强学习，只是把一些只针对围棋的细节去掉了（比如通过旋转进行数据增强，因为围棋是对称的）和输入输出维度进行了改变。</p><p>它的学习曲线如下，均达到了顶尖水平：</p><p><img src="/images/zero_curve2.png"></p><h2><span id="alphazero-and-exit">AlphaZero and ExIt</span></h2><p><a href="https://arxiv.org/abs/1705.08439" target="_blank" rel="noopener">Expert Iteration（ExIt）</a>是一种模仿学习（Imitation Learning, IL）算法，普通的 IL 算法中，徒弟模仿专家的策略只能提高自己的策略，专家是不会有任何提高的，而 ExIt 算法就是想让师傅教徒弟的时候自己也有提高。</p><p><strong>ExIt 算法</strong> 师傅根据徒弟的策略进行前向搜索（例如MCTS，alpha-beta，贪心搜索等），得出比徒弟更好的策略，然后徒弟再学习师傅的策略，如此循环，随着徒弟的增强，师傅也会越来越强。</p><p><img src="/images/exit.png"></p><p>可见，AlphaZero也属于 ExIt 算法，师傅为 MCTS，徒弟就是神经网络。</p><h2><span id="summary">Summary</span></h2><p>现在棋类人工智能算法的发展趋势是越来越泛化，趋向于多功能。从 AlphaGo 的学习人类专家的棋谱到 AlphaGo Zero 的从零开始无需人类知识的自我博弈学习再到 AlphaZero 的同一算法适应不同棋类并且都取得超人水平。可见人工智能越来越向通用智能发展，虽然长路漫漫，现在的算法远不够泛化，但是很多东西，比如神经网络结构都是可以用到不同领域的。AlphaGo 系列的作者之一 David Silver 曾说:“每次你专门化一些东西都会伤害你的泛化能力” (Every time you specialize something you hurt your generalization ability.)。事实也的确如此，AlphaGo 系列架构越来越简单，而其性能和泛化能力却越来越强大。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;go&quot;&gt;Go&lt;/h2&gt;
&lt;p&gt;围棋起源于古代中国，是世界上最古老的棋类运动之一。在宋代的《梦溪笔谈》中探讨了围棋的局数变化数目，作者沈括称“大约连书万字四十三个，即是局之大数”，意思是说变化数目要写43个万字。根据围棋规则，没有气的子不能存活，扣除这些状态后的合法状态约有 &lt;span class=&quot;math inline&quot;&gt;\(2.08×10^{170}\)&lt;/span&gt; 种。Robertson 与 Munro 在1978年证得围棋是一种 PSPACE-hard 的问题，其必胜法之记忆计算量在&lt;span class=&quot;math inline&quot;&gt;\(10^{600}\)&lt;/span&gt; 以上，这远远超过可观测宇宙的原子总数 &lt;span class=&quot;math inline&quot;&gt;\(10^{75}\)&lt;/span&gt;，可见围棋对传统的搜索方法是非常有挑战的。
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="AlphaGo" scheme="http://www.52coding.com.cn/tags/AlphaGo/"/>
    
      <category term="AlphaZero" scheme="http://www.52coding.com.cn/tags/AlphaZero/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>论文翻译：在没有人类知识的情况下掌握围棋</title>
    <link href="http://www.52coding.com.cn/2018/03/10/%E5%9C%A8%E6%B2%A1%E6%9C%89%E4%BA%BA%E7%B1%BB%E7%9F%A5%E8%AF%86%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E6%8E%8C%E6%8F%A1%E5%9B%B4%E6%A3%8B/"/>
    <id>http://www.52coding.com.cn/2018/03/10/在没有人类知识的情况下掌握围棋/</id>
    <published>2018-03-10T06:01:09.000Z</published>
    <updated>2018-11-06T03:48:59.304Z</updated>
    
    <content type="html"><![CDATA[<h4><span id="1-前言">1. 前言</span></h4><p>​ 人工智能的一个长期目标是在一些有挑战的领域中从零开始学习出超人熟练程度的算法。最近，AlphaGo成为第一个在围棋比赛中击败世界冠军的程序。 AlphaGo中的树搜索使用深度神经网络评估位置和选定的移动。这些神经网络是通过监督学习来自人类专家的走法以及通过强化自我学习来进行训练的。这里我们只介绍一种基于强化学习的算法，没有超出游戏规则的人类数据，指导或领域知识。AlphaGo成为自己的老师：一个神经网络训练预测AlphaGo的移动选择和游戏的胜者。这个神经网络提高了树搜索的强度，在下一次迭代中拥有更高质量的移动选择和更强的自我学习。我们的新程序AlphaGo Zero从零开始学习，实现了超人的表现，与之前发布的夺冠冠军AlphaGo相比以100-0取胜。</p><a id="more"></a><h4><span id="2-概述">2. 概述</span></h4><p>​ 围棋程序在人工智能方面已经取得了很大的进展，使用经过训练的监督学习系统来复制人类专家的决定。但是，专家数据集通常很昂贵，不可靠或根本无法使用。即使有可靠的数据集，它们也可能会对以这种方式培训的系统的性能施加上限。相比之下，强化学习系统是根据他们自己的经验进行学习的，原则上允许他们超越人类能力，并在缺乏人力专业知识的领域运作。最近，通过强化学习训练的深度神经网络，朝着这个目标快速发展。这些系统在计算机游戏中胜过人类，如Atari游戏和3D虚拟环境。然而，在人类智力方面最具挑战性的领域 - 比如被广泛认为是人工智能的巨大挑战的围棋游戏 - 在广阔的搜索空间中需要精确和复杂的搜索。以前的方法没有在这些领域实现达到人类水平的表现。</p><p>​ AlphaGo 是第一个在围棋中实现超人表现的程序。之前发布的版本，我们称之为AlphaGo Fan，于2015年10月击败了欧洲冠军范辉。AlphaGo Fan 使用了两个深度神经网络：输出移动概率的策略网络和输出位置评估的价值网络。策略网络最初是通过监督学习来准确地预测人类专家的行为，随后通过策略升级强化学习进行了改进。价值网络经过训练，可以预测游戏的胜者。一旦开始训练，这些网络就会与蒙特卡洛树搜索（MCTS）结合使用，从而提供先行搜索，使用策略网络将搜索范围缩小为高概率移动，并使用价值网络来评估树中的位置。我们称之为 AlphaGo Lee 的后续版本使用了类似的方法，并于2016年3月击败了获得18个国际冠军的Lee Sedol。</p><p>​ AlphaGo Zero 与 AlphaGo Fan 和 AlphaGo Lee 在几个重要方面不同。首先，它只是通过自我增强强化学习进行训练，从随机比赛开始，没有任何监督或使用人类数据。其次，它只使用黑白棋位置作为输入。第三，它使用单一的神经网络，而不是单独的策略和价值网络。最后，它使用更简单的搜索树，该搜索依赖于这个单一的神经网络来评估位置和移动，而无需执行任何 Monte Carlo 回溯。为了实现这些结果，我们引入了一种新的强化学习算法，该算法在训练环内部结合了前瞻搜索，从而实现了快速改进和精确而稳定的学习。在方法一栏中中描述了搜索算法，训练过程和网络体系结构中的其他技术差异。</p><h4><span id="3-alphago-zero-中的增强学习">3. AlphaGo Zero 中的增强学习</span></h4><p>​ 我们的新方法使用参数为 <span class="math inline">\(\theta\)</span> 的深度神经网络 <span class="math inline">\(f(\theta)\)</span>。该神经网络将位置及其历史的原始平面表示 s 作为输入，并输出移动概率和价值 <span class="math inline">\((p,v)=f_\theta(s)\)</span>。 移动概率 p 的向量表示选择每个移动 a 的概率，<span class="math inline">\(P_a=Pr(a|s)\)</span> 。价值 v 是一个标量评估，用于估计当前玩家从位置 s 获胜的概率。这个神经网络将策略网络和价值网络结合到一个网络中。神经网络由卷积层，许多残差块组成，批量归一化和非线性整流器（参见方法）组成。</p><p>​ AlphaGo Zero 中的神经网络是通过一种新型的强化学习算法从自我博弈的游戏中训练出来的。在每个位置 <span class="math inline">\(s\)</span>，执行 MCTS 搜索，由神经网络 <span class="math inline">\(f(\theta)\)</span> 指导。MCTS 搜索输出每次移动的概率 <span class="math inline">\(π\)</span>。这些搜索概率通常选择比神经网络的原始移动概率 <span class="math inline">\(p\)</span> 更加强大;因此，MCTS 可被视为策略改进的操作。使用搜索进行自我博弈 - 使用改进的基于 MCTS 的策略来选择每个动作，然后使用游戏获胜者 <span class="math inline">\(z\)</span> 作为价值的样本 - 可以被视为一个强大的策略评估操作。我们的强化学习算法的主要思想是在策略迭代过程中重复使用这些操作：更新神经网络的参数以使移动概率和值 <span class="math inline">\((p,v)=f_\theta(s)\)</span> 更紧密匹配改进的搜索概率和获胜者 <span class="math inline">\((\pi,z)\)</span>；这些新参数将用于下一次自我博弈，以使搜索更加强大。图 1说明了自我博弈训练流程。</p><p><img src="/images/selfplay.png"></p><p>​ <em>图1 AlphaGo Zero中的自我博弈与增强学习训练流程</em></p><p>​ MCTS 使用神经网络 <span class="math inline">\(f(\theta)\)</span> 来指导其模拟（见图 2）。搜索树中的每个边 <span class="math inline">\((s,a)\)</span> 存储先验概率 <span class="math inline">\(P(s,a)\)</span>，访问计数 <span class="math inline">\(N(s,a)\)</span> 和动作价值 <span class="math inline">\(Q(s,a)\)</span> 。每个模拟从根状态开始，并且迭代地选择使置信上限 <span class="math inline">\(Q(s,a)+U(s,a)\)</span> 最大化的移动，其中<span class="math inline">\(U\propto \frac{P(s,a)}{1+N(s,a)}\)</span>，直到遇到叶节点 <span class="math inline">\(s&#39;\)</span>。该叶子位置被网络扩展和评估一次，以产生先验概率和评估，<span class="math inline">\((P(s&#39;, \cdot), v(s&#39;))=f_\theta(s&#39;)\)</span>。在模拟中遍历的每个边 <span class="math inline">\((s,a)\)</span> 被更新以增加其访问计数 <span class="math inline">\(N(s,a)\)</span>，并且将其动作价值更新为在这些模拟上的平均评估 <span class="math inline">\(Q(s,a) = \frac{1}{N(s,a)}\sum_{s&#39;|s,a\rightarrow s&#39;}V(s&#39;)\)</span>， 其中 <span class="math inline">\(s,a\rightarrow s&#39;\)</span> 表示在从位置 <span class="math inline">\(s\)</span> 执行行动 <span class="math inline">\(a\)</span> 后模拟最终达到位置 <span class="math inline">\(s&#39;\)</span>。</p><p><img src="/images/mcts0.png"></p><p>​ <em>图2 AlphaGo Zero中的蒙特卡洛搜索树</em></p><p>​ MCTS 可以被看作是一种自我博弈算法，在给定神经网络参数 <span class="math inline">\(θ\)</span> 和根位置 <span class="math inline">\(s\)</span> 的情况下，计算推荐移动的搜索概率矢量，<span class="math inline">\(\pi = a_\theta(s)\)</span>，与每次移动的访问计数的指数成比例，<span class="math inline">\(\pi_a\propto N(s,a)^{1/\tau}\)</span>，其中 <span class="math inline">\(τ\)</span> 是温度参数。</p><p>​ 神经网络通过使用 MCTS 选择每个动作的自我博弈增强化学习算法进行训练。首先，神经网络被初始化为随机权重 <span class="math inline">\(\theta_0\)</span>。在随后的每次迭代 <span class="math inline">\(i≥1\)</span> 时，产生自我博弈的数据（图 1）。在每个时刻 <span class="math inline">\(t\)</span>，使用先前的神经网络迭代 <span class="math inline">\(f_{\theta_{i-1}}\)</span> 执行 MCTS 搜索，并且通过对搜索概率 <span class="math inline">\(\pi_t\)</span> 进行采样来执行移动。当两个玩家都无路可走时或者当搜索值下降到低于阈值或当游戏超过最大长度时，游戏在时刻 <span class="math inline">\(T\)</span> 终止;然后对游戏进行评分以给出 <span class="math inline">\(r_T\in\{-1,+1\}\)</span> 的最终奖励（详见方法）。每个时刻 <span class="math inline">\(t\)</span> 的数据存储为 <span class="math inline">\((s_t,\pi_t,z_t)\)</span>，其中 <span class="math inline">\(z_t = \pm r_T\)</span> 是时刻 <span class="math inline">\(t\)</span> 从当前玩家角度出发的游戏获胜者。同时（如图 1），新的网络参数 <span class="math inline">\(\theta_i\)</span> 从最后一次自我博弈的所有时间中统一采样的数据 <span class="math inline">\((s,\pi,t)\)</span> 进行训练。调整神经网络 <span class="math inline">\((p,v)=f_{\theta_i}(s)\)</span> 以最小化预测值 <span class="math inline">\(v\)</span> 与实际赢得者 <span class="math inline">\(z\)</span>之间的误差，并使神经网络移动概率 <span class="math inline">\(p\)</span> 与搜索概率 <span class="math inline">\(π\)</span> 的相似性最大化。具体而言，参数 <span class="math inline">\(θ\)</span> 通过梯度下降在损失函数 <span class="math inline">\(l\)</span> 上进行调整，所述损失函数 <span class="math inline">\(l\)</span> 分别对均方误差和交叉熵误差进行求和： <span class="math display">\[(p,v)=f_\theta(s) \mbox{ and }l=(z-v)^2-\pi^T\log p+c||\theta||^2\]</span> 其中 <span class="math inline">\(c\)</span> 是控制 L2 正则化程度的超参数（为了防止过拟合）。</p><h4><span id="4-alphago-zero-的实验分析">4. AlphaGo Zero 的实验分析</span></h4><p>​ 我们使用上述强化学习流程来训练 AlphaGo Zero。训练从完全随机的行为开始，持续约三天且无人为干预。在训练过程中，每个 MCTS 使用 1,600 次模拟，每次移动的思考时间大约为 0.4s，从而产生了 490 万局自我博弈。 参数从 700,000 个包含 2048 个状态的批量中更新。神经网络包含 20 个残余块。</p><p>​ 图 3 显示了 AlphaGo Zero 在自我博弈过程中的表现，横坐标为训练时间，纵坐标为 Elo 量。整个训练过程进展顺利，并且没有遭受先前文献中提出的振荡或灾难性遗忘。令人惊讶的是，AlphaGo Zero 仅仅 36 小时就赢了AlphaGo Lee。相比之下，AlphaGo Lee 训练了几个月。在 72 小时后，我们根据在首尔人机比赛中使用的相同的 2 小时时间控制和匹配条件，对 AlphaGo Zero 与 AlphaGo Lee 的确切版本进行了评估，该版本击败了 Lee Sedol。AlphaGo Zero 使用带有4个张量处理单元（TPU）的单台机器，而 AlphaGo Lee 分布在多台机器上并使用 48 个TPU。AlphaGo Zero 将 AlphaGo Lee 以 100 比 0 击败。</p><p>​ 为了评估自我强化学习的优点，与从人类数据中学习相比，我们训练了第二个神经网络（使用相同的体系结构）来预测 KGS 服务器数据集中的专家动作; 与之前的工作相比，这实现了预测的准确性。 监督式学习的初始表现更好，并且更好地预测人类职业动作（图 3）。 值得注意的是，尽管监督学习获得了更高的移动预测准确度，但自学者的整体表现更好，在训练的前 24 小时内击败了训练有素的选手。这表明 AlphaGo Zero 可能正在学习一种与人类下棋不同的策略。</p><p><img src="/images/ag0em.png"></p><p>​ <em>图3 AlphaGo Zero的实验评估</em></p><p>​ 为了分离架构和算法的贡献，我们将 AlphaGo Zero 中的神经网络架构的性能与 AlphaGo Lee 中使用的以前的神经网络架构进行了比较（见图 4）。 新训练的AlphaGo Zero 有四个版本的神经网络，分别是：使用 AlphaGo Lee 的卷积网络架构；AlphaGo Zero 的剩余网络架构；使用 AlphaGo Zero 的卷积网络架构；使用AlphaGo Lee 的剩余网络架构。每个网络都经过训练，以最小化相同的损失函数，使用由 AlphaGo Zero 在自我训练 72 小时后产生的固定数据集。使用剩余网络更准确，实现了更低的误差，AlphaGo 的性能提高了 600 多 Elo。将策略和价值组合在一起成为一个网络，略微降低了移动预测的准确性，但是将降低了 AlphaGo 的价值误差和提高了博弈性能约 600 个 Elo。部分原因在于提高了计算效率，但更重要的是，双重目标将网络正则化为支持多种用例的表示。</p><p><img src="/images/ag02.png"></p><p>​ <em>图4 AlphaGo Zero和AlphaGo Lee的神经网络结构比较</em></p><h4><span id="5-alphago-zero-学习到的围棋知识">5. AlphaGo Zero 学习到的围棋知识</span></h4><p>​ AlphaGoZero在其自我博弈训练过程中发现了非凡的围棋知识水平。这不仅包括人类围棋知识的基本要素，还包括超出传统围棋知识范围的非标准策略。</p><p>​ 图 5显示了一个时间线，表明何时发现了专业 joseki（角点序列）;最终AlphaGo Zero 更喜欢先前未知的新的 joseki 变体（图5b ）。图5c 显示了几种在不同训练阶段进行的快速自我博弈。在整个训练中定期进行的锦标赛长度比赛在补充信息中显示。 AlphaGo Zero 从完全随机的移动过渡到对围棋概念的复杂理解，包括fuseki（开场），tesuji（战术），生与死，ko（重复棋局），yose（终局），捕捉比赛，sente（倡议），形状，影响力和领土，都是从最初的原则发现的。令人惊讶的是，Shocho - 人类学习的围棋知识的第一要素之一 - 只有在 AlphaGo Zero 的训练中才能被理解。</p><p><img src="/images/ag05.png"></p><p>​ <em>图5 AlphaGo Zero学到的围棋知识</em></p><h4><span id="6-alphago-zero-的最终水平">6. AlphaGo Zero 的最终水平</span></h4><p>​ 随后我们使用更大的神经网络和更长的持续时间将我们的强化学习管道应用于AlphaGo Zero的第二个实例。再次训练从完全随机行为开始并持续大约40天。</p><p>​ 在训练过程中，产生了 2900 万次自我博弈。参数从每个 2,048 个位置的 310 万个小型批量中更新。神经网络包含 40 个残余块。学习曲线如图 6a 所示。在整个训练过程中定期进行的比赛显示在补充信息中。</p><p><img src="/images/ag06.png"></p><p>​ <em>图6 AlphaGo Zero的评估</em></p><p>​ 我们使用 AlphaGo Fan，AlphaGo Lee 和之前的几个围棋程序的内部比赛评估了训练有素的 AlphaGo Zero。我们还与最强大的现有程序 AlphaGo Master 进行了游戏，该程序基于本文提供的算法和体系结构，但使用了人类数据和特征（请参阅方法） - 它在 60-0 在线游戏中击败了最强的人类职业玩家。在我们的评估中，所有程序都允许每个动作有5秒的思考时间; AlphaGo Zero 和 AlphaGo Master 每台在带有 4 个 TPU 的单台机器上博弈; AlphaGo Fan 和 AlphaGo Lee 分别分布有 176 个GPU和 48 个TPU。我们还包括一个完全基于 AlphaGo Zero 原始神经网络的选手; 该选手只是以最大的概率选择移动（不进行 MCTS 搜索）。</p><p>​ 图 6b显示了每个程序在Elo规模上的表现。未使用任何预测的原始神经网络实现了3,055的Elo评级。 AlphaGo Zero 获得了5,185的评分，而 AlphaGo Master 的4,858，AlphaGo Lee 的 3,739和 AlphaGo Fan 的3,144。</p><p>​ 最后，我们评估了 AlphaGo Zero 对阵 AlphaGo Master，在每场2小时的时间限定内进行了100场比赛，AlphaGo Zero 赢得了其中的89场。</p><h4><span id="7-结论">7. 结论</span></h4><p>​ 我们的研究结果全面证明，即使在最具挑战性的领域中，纯粹的强化学习方法也是完全可行的：在不超出基本规则的情况下，没有关于领域的知识，就可以训练到超人的水平，没有人类的例子或指导。此外，与用人类专家数据训练的程序相比，纯粹的强化学习方法只需要几个小时的训练时间就能达到更好的性能。使用这种方法，AlphaGo Zero 大幅度击败了使用人工数据训练的 AlphaGo 最强大的先前版本。</p><p>​ 人类已经积累了几千年来的围棋知识，发展成固定的模式，总结成谚语和书籍。在几天的时间里，AlphaGo Zero 从零学起，就能够重新发现许多的围棋知识，并能为这个古老的游戏提供新见解、新策略。</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;1. 前言&lt;/h4&gt;
&lt;p&gt;​ 人工智能的一个长期目标是在一些有挑战的领域中从零开始学习出超人熟练程度的算法。最近，AlphaGo成为第一个在围棋比赛中击败世界冠军的程序。 AlphaGo中的树搜索使用深度神经网络评估位置和选定的移动。这些神经网络是通过监督学习来自人类专家的走法以及通过强化自我学习来进行训练的。这里我们只介绍一种基于强化学习的算法，没有超出游戏规则的人类数据，指导或领域知识。AlphaGo成为自己的老师：一个神经网络训练预测AlphaGo的移动选择和游戏的胜者。这个神经网络提高了树搜索的强度，在下一次迭代中拥有更高质量的移动选择和更强的自我学习。我们的新程序AlphaGo Zero从零开始学习，实现了超人的表现，与之前发布的夺冠冠军AlphaGo相比以100-0取胜。&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="AlphaGo" scheme="http://www.52coding.com.cn/tags/AlphaGo/"/>
    
      <category term="AlphaZero" scheme="http://www.52coding.com.cn/tags/AlphaZero/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>RL - Integrating Learning and Planning</title>
    <link href="http://www.52coding.com.cn/2018/01/09/RL%20-%20Integrating%20Learning%20and%20Planning/"/>
    <id>http://www.52coding.com.cn/2018/01/09/RL - Integrating Learning and Planning/</id>
    <published>2018-01-09T13:11:09.000Z</published>
    <updated>2018-11-06T03:47:24.502Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="introduction">Introduction</span></h2><p>In last lecture, we learn <strong>policy</strong> directly from experience. In previous lectures, we learn <strong>value function</strong> directly from experience. In this lecture, we will learn <strong>model</strong> directly from experience and use <strong>planning</strong> to construct a value function or policy. Integrate learning and planning into a single architecture.</p><p>Model-Based RL</p><ul><li>Learn a model from experience</li><li><strong>Plan</strong> value function (and/or policy) from model</li></ul><a id="more"></a><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#model-based-reinforcement-learning">Model-Based Reinforcement Learning</a></li><li><a href="#integrated-architectures">Integrated Architectures</a></li><li><a href="#simulation-based-search">Simulation-Based Search</a></li></ul><!-- tocstop --><h2><span id="model-based-reinforcement-learning">Model-Based Reinforcement Learning</span></h2><p><img src="/images/mbrl.png"></p><p>Advantages of Model-Based RL</p><ul><li>Can efficiently learn model by supervised learning methods</li><li>Can reason about model uncertainty</li></ul><p>Disadvantages</p><ul><li>First learn a model, then construct a value function -&gt; two source of approximation error</li></ul><p><strong>What is a Model?</strong></p><p>A model <span class="math inline">\(\mathcal{M}\)</span> is a representation of an MDP <span class="math inline">\(&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}&gt;\)</span> parametrized by <span class="math inline">\(\eta\)</span>.</p><p>We will assume state space <span class="math inline">\(\mathcal{S}\)</span> and action space <span class="math inline">\(\mathcal{A}\)</span> are known. So a model <span class="math inline">\(\mathcal{M}=&lt;\mathcal{P}_, \eta\mathcal{R}_\eta&gt;\)</span> represents state transitions <span class="math inline">\(\mathcal{P}_\eta \approx \mathcal{P}\)</span> and rewards <span class="math inline">\(\mathcal{R}_\eta\approx \mathcal{R}\)</span>. <span class="math display">\[S_{t+1}\sim\mathcal{P}_\eta(S_{t+1}|S_t, A_t)\\R_{t+1}=\mathcal{R}_\eta(R_{t+1}|S_t, A_t)\]</span> Typically assume conditional independence between state transitions and rewards.</p><p>Goal: estimate model <span class="math inline">\(\mathcal{M}_\eta\)</span> from experience <span class="math inline">\(\{S_1, A_1, R_2, …, S_T\}\)</span>.</p><p>This is a supervised learning problem: <span class="math display">\[S_1, A_1 \rightarrow R_2, S_2 \\S_2, A_2 \rightarrow R_3, S_3 \\...\\S_{T-1}, A_{T-1} \rightarrow R_T, S_T \\\]</span> Learning <span class="math inline">\(s, a\rightarrow r\)</span> is a <em>regression</em> problem; learning <span class="math inline">\(s, a\rightarrow s&#39;\)</span> is a <em>density</em> estimation problem. Pick loss function, e.g. mean-squared error, KL divergence, … Find parameters <span class="math inline">\(\eta\)</span> that minimise empirical loss.</p><p>Examples of Models</p><ul><li>Table Lookup Model</li><li>Linear Expectation Model</li><li>Linear Gaussian Model</li><li>Gaussian Process Model</li><li>Deep Belief Network Model</li></ul><p><strong>Table Lookup Model</strong></p><p>Model is an explicit MDP. Count visits <span class="math inline">\(N(s, a)\)</span> to each state action pair: <span class="math display">\[\hat{\mathcal{P}}^a_{s,s&#39;}=\frac{1}{N(s,a)}\sum^T_{t=1}1(S_t,A_t,S_{t+1}=s, a, s&#39;)\\\hat{\mathcal{R}}^a_{s,s&#39;}=\frac{1}{N(s,a)}\sum^T_{t=1}1(S_t,A_t=s, a)R_t\]</span> Alternatively, at each time-step <span class="math inline">\(t\)</span>, record experience tuple <span class="math inline">\(&lt;S_t, A_t, R_{t+1}, S_{t+1}&gt;\)</span>. To sample model, randomly pick tuple matching <span class="math inline">\(&lt;s, a, \cdot, \cdot&gt;\)</span>.</p><p><strong>AB Example</strong></p><p><img src="/images/ab2.png"></p><p>We have contrusted a <strong>table lookup model</strong> from the experience. Next step, we will planning with a model.</p><p><strong>Planning with a model</strong></p><p>Given a model <span class="math inline">\(\mathcal{M}_\eta=&lt;\mathcal{P}_\eta, \mathcal{R}_\eta&gt;\)</span>, solve the MDP <span class="math inline">\(&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}_\eta, \mathcal{R}_\eta&gt;\)</span> using favorite planning algorithms</p><ul><li>Value iteration</li><li>Policy iteration</li><li>Tree search</li><li>….</li></ul><p><strong>Sample-Based Planning</strong></p><p>A simple but powerful approach to planning is to use the model <strong>only</strong> to generate samples.</p><p><strong>Sample</strong> experience from model <span class="math display">\[S_{t+1}\sim\mathcal{P}_\eta(S_{t+1}|S_t,A_t)\\R_{t+1}=\mathcal{R}_\eta(R_{t+1}|S_t,A_t)\]</span> Apply <strong>model-free</strong> RL to samples, e.g.:</p><ul><li>Monte-Carlo control</li><li>Sarsa</li><li>Q-learning</li></ul><p>Sample-based planning methods are often more efficient.</p><p><strong>Back to AB Example</strong></p><p><img src="/images/ab3.png"></p><p>We can use our model to sample more experience and apply model-free RL to them.</p><p><strong>Planning with an Inaccurate Model</strong></p><p>Given an imperfect model <span class="math inline">\(&lt;\mathcal{P}_\eta, \mathcal{R}_\eta&gt; ≠ &lt;\mathcal{P}, \mathcal{R}&gt;\)</span>. Performance of model-based RL is limited to optimal policy for approximate MDP <span class="math inline">\(&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}_\eta, \mathcal{R}_\eta&gt;\)</span> i.e. Model-based RL is only as good as the estimated model.</p><p>When the model is inaccurate, planning process will compute a suboptimal policy.</p><ul><li>Solution1: when model is wrong, use model-free RL</li><li>Solution2: reason explicitly about model uncertainty</li></ul><h2><span id="integrated-architectures">Integrated Architectures</span></h2><p>We consider two sources of experience:</p><ul><li><p>Real experience: Sampled from environment (true MDP) <span class="math display">\[S&#39;\sim \mathcal{P}^a_{s,s&#39;}\\R=\mathcal{R}^a_s\]</span></p></li><li><p>Simulated experience: Sampled from model (approximate MDP) <span class="math display">\[S&#39;\sim \mathcal{P}_\eta(S&#39;|S, A)\\R=\mathcal{R}_\eta(R|S, A)\]</span></p></li></ul><p><strong>Integrating Learning and Planning</strong></p><p>Dyna Architecture</p><ul><li>Learn a model from real experience</li><li>Learn and plan value function (and/or policy) from real and simulated experience</li></ul><p><img src="/images/dyna.png"></p><p>The simplest dyna algorithm is <em>Dyna-Q Algorithm</em>:</p><p><img src="/images/dynaq.png"></p><p><img src="/images/dynaqres.png"></p><p>From the experiments, we can see that using planning is more efficient than direct RL only.</p><p><strong>Dyna-Q with an Inaccurate Model</strong></p><p>The changed envrionment is <strong>harder</strong>:</p><p><img src="/images/dynaqhard.png"></p><p>There is a <strong>easier</strong> change:</p><p><img src="/images/dynaqeasy.png"></p><h2><span id="simulation-based-search">Simulation-Based Search</span></h2><p>Let's back to planning problems. Simulation-based search is another approach to solve MDP.</p><p><strong>Forward Search</strong></p><p>Forward search algorithms select the best action by <strong>lookahead</strong>. They build a <strong>search tree</strong> with the current state <span class="math inline">\(s_t\)</span> at the root using a model of the MDP to look ahead.</p><p><img src="/images/ftree.png"></p><p>We don't need to solve the whole MDP, just sub-MDP starting from <strong>now</strong>.</p><p><strong>Simulation-Based Search</strong></p><p>Simulation-based search is forward search paradigm using sample-based planning. Simulate episodes of experience from <strong>now</strong> with the model. Apply <strong>model-free</strong> RL to simulated episodes.</p><p><img src="/images/sbsearch.png"></p><p>Simulate episodes of experience from <strong>now</strong> with the model: <span class="math display">\[\{s_t^k, A^k_t,R^k_{t+1}, ..., S^k_T\}^K_{k=1}\sim\mathcal{M}_v\]</span> Apply <strong>model-free</strong> RL to simulated episodes</p><ul><li>Monte-Carlo control <span class="math inline">\(\rightarrow\)</span> Monte-Carlo search</li><li>Sarsa <span class="math inline">\(\rightarrow\)</span> TD search</li></ul><p><strong>Simple Monte-Carlo Search</strong></p><p>Given a model <span class="math inline">\(\mathcal{M}_v\)</span> and a simulation policy <span class="math inline">\(\pi\)</span>.</p><p>For each action <span class="math inline">\(a\in\mathcal{A}\)</span></p><ul><li><p>Simulate <span class="math inline">\(K\)</span> episodes from current (real) state <span class="math inline">\(s_t\)</span> <span class="math display">\[\{s_t, a, R^k_{t+1},S^k_{t+1},A^k_{t+1}, ..., S^k_T \}^K_{k=1}\sim \mathcal{M}_v, \pi\]</span></p></li><li><p>Evaluate actions by mean return (<strong>Monte-Carlo evaluation</strong>) <span class="math display">\[Q(s_t, a)=\frac{1}{K}\sum^k_{k=1}G_t\rightarrow q_\pi(s_t, a)\]</span></p></li></ul><p>Select current (real) action with maximum value <span class="math display">\[a_t=\arg\max_{a\in\mathcal{A}}Q(s_t, a)\]</span> <strong>Monte-Carlo Tree Search</strong></p><p>Given a model <span class="math inline">\(\mathcal{M}_v\)</span>. Simulate <span class="math inline">\(K\)</span> episodes from current state <span class="math inline">\(s_t\)</span> using current simulation policy <span class="math inline">\(\pi\)</span>. <span class="math display">\[\{s_t, A_t^k, R^k_{t+1},S^k_{t+1},A^k_{t+1}, ..., S^k_T \}^K_{k=1}\sim \mathcal{M}_v, \pi\]</span> Build a search tree containing visited states and actions. <strong>Evaluate</strong> states <span class="math inline">\(Q(s, a)\)</span> by mean return of episodes from <span class="math inline">\(s, a\)</span>: <span class="math display">\[Q(s, a)=\frac{1}{N(s,a)}\sum^K_{k=1}\sum^T_{u=t}1(S_u, A_u=s,a)G_u \to q_\pi(s,a)\]</span> After search is finished, select current (real) action with maximum value in search tree: <span class="math display">\[a_t=\arg\max_{a\in\mathcal{A}}Q(s_t, a)\]</span> In MCTS, the simulation policy <span class="math inline">\(\pi\)</span> <strong>improves</strong>.</p><p>Each simulation consists of two phases (in-tree, out-of-tree)</p><ul><li><strong>Tree policy</strong> (improves): pick actions to maximise <span class="math inline">\(Q(S,A)\)</span></li><li><strong>Default policy</strong> (fixed): pick actions randomly</li></ul><p>Repeat (each simulation)</p><ul><li><span class="math inline">\(\color{red}{\mbox{Evaluate}}\)</span> states <span class="math inline">\(Q(S,A)\)</span> by Monte-Carlo evaluation</li><li><span class="math inline">\(\color{red}{\mbox{Improve}}\)</span> tree policy, e.g. by <span class="math inline">\(\epsilon\)</span>-greedy(Q)</li></ul><p>MCTS is <strong>Monte-Carlo control</strong> applied to <strong>simulated experience</strong>.</p><p>Converges on the optimal search tree, <span class="math inline">\(Q(S, A) \to q_*(S, A)\)</span>.</p><p><strong>Case Study: the Game of Go</strong></p><p><img src="/images/go_2.png"></p><p><em>Rules of Go</em></p><ul><li>Usually played on 19$<span class="math inline">\(19, also 13\)</span><span class="math inline">\(13 or 9\)</span>$9 board</li><li>Black and white place down stones alternately</li><li>Surrounded stones are captured and removed</li><li>The player with more territory wins the game</li></ul><p><img src="/images/ruleofgo.png"></p><p><em>Position Evaluation in Go</em></p><p>The key problem is how good is a position <span class="math inline">\(s\)</span>?</p><p>So the reward function is if Black wins, the reward of the final position is 1, otherwise 0: <span class="math display">\[R_t = 0 \mbox{ for all non-terminal steps } t&lt;T\\R_T=\begin{cases} 1,  &amp; \mbox{if }\mbox{ Black wins} \\0, &amp; \mbox{if }\mbox{ White wins}\end{cases}\]</span> Policy <span class="math inline">\(\pi=&lt;\pi_B,\pi_W&gt;\)</span> selects moves for both players.</p><p>Value function (how good is position <span class="math inline">\(s\)</span>): <span class="math display">\[v_\pi(s)=\mathbb{E}_\pi[R_T|S=s]=\mathbb{P}[Black wins|S=s]\\v_*(s)=\max_{\pi_B}\min_{\pi_w}v_\pi(s)\]</span> <em>Monte Carlo Evaluation in Go</em></p><p><img src="/images/mcego.png"></p><p><img src="/images/amcts1.png"></p><p><img src="/images/amcts2.png"></p><p><img src="/images/amcts3.png"></p><p><img src="/images/amcts4.png"></p><p><img src="/images/amcts5.png"></p><p>So, MCTS will expand the tree towards the node that is most promising and ignore the useless parts.</p><p><strong>Advantages of MC Tree Search</strong></p><ul><li>Highly selective best-first search</li><li>Evaluates states <em>dynamically</em></li><li>Uses sampling to break curse of dimensionality</li><li>Works for &quot;black-box&quot; models (only requires samples)</li><li>Computationally efficient, anytime, parallelisable</li></ul><p><strong>Temporal-Difference Search</strong></p><ul><li>Simulation-based search</li><li>Using TD instead of MC (bootstrapping)</li><li>MC tree search applies MC control to sub-MDP from now</li><li>TD search applies Sarsa to sub-MDP from now</li></ul><p><strong>MC vs. TD search</strong></p><p>For model-free reinforcement learning, bootstrapping is helpful</p><ul><li>TD learning reduces variance but increase bias</li><li>TD learning is usually more efficient than MC</li><li>TD(<span class="math inline">\(\lambda\)</span>) can be much more efficient than MC</li></ul><p>For simulation-based search, bootstrapping is also helpful</p><ul><li>TD search reduces variance but increase bias</li><li>TD search is usually more efficient than MC search</li><li>TD(<span class="math inline">\(\lambda\)</span>) search can be much more efficient than MC search</li></ul><p><strong>TD Search</strong></p><p>Simulate episodes from the current (real) state <span class="math inline">\(s_t\)</span>. Estimate action-value function <span class="math inline">\(Q(s, a)\)</span>. For each step of simulation, update action-values by Sarsa: <span class="math display">\[\triangle Q(S,A)=\alpha (R+\gamma Q(S&#39;,A&#39;)-Q(S,A))\]</span> Select actions based on action-value <span class="math inline">\(Q(s,a)\)</span>, e.g. <span class="math inline">\(\epsilon\)</span>-greedy. May also use function approximation for <span class="math inline">\(Q\)</span>.</p><p><strong>Dyna-2</strong></p><p>In Dyna-2, the agent stores two sets of feature weights:</p><ul><li><strong>Long-term</strong> memory</li><li><strong>Short-term</strong> (working) memory</li></ul><p>Long-term memory is updated from <strong>real experience</strong> using TD learning</p><ul><li>General domain knowledge that applies to any episode</li></ul><p>Short-term memory is updated from <strong>simulated experience</strong> using TD search</p><ul><li>Specific local knowledge about the current situation</li></ul><p>Over value function is sum of long and short-term memories.</p><p>End.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In last lecture, we learn &lt;strong&gt;policy&lt;/strong&gt; directly from experience. In previous lectures, we learn &lt;strong&gt;value function&lt;/strong&gt; directly from experience. In this lecture, we will learn &lt;strong&gt;model&lt;/strong&gt; directly from experience and use &lt;strong&gt;planning&lt;/strong&gt; to construct a value function or policy. Integrate learning and planning into a single architecture.&lt;/p&gt;
&lt;p&gt;Model-Based RL&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn a model from experience&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt; value function (and/or policy) from model&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="AlphaGo" scheme="http://www.52coding.com.cn/tags/AlphaGo/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="MCTS" scheme="http://www.52coding.com.cn/tags/MCTS/"/>
    
      <category term="TD Search" scheme="http://www.52coding.com.cn/tags/TD-Search/"/>
    
      <category term="Dyna" scheme="http://www.52coding.com.cn/tags/Dyna/"/>
    
  </entry>
  
  <entry>
    <title>RL - Policy Gradient</title>
    <link href="http://www.52coding.com.cn/2018/01/06/RL%20-%20Policy%20Gradient/"/>
    <id>http://www.52coding.com.cn/2018/01/06/RL - Policy Gradient/</id>
    <published>2018-01-06T05:42:09.000Z</published>
    <updated>2018-11-19T04:24:04.939Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="introduction">Introduction</span></h2><p>This lecture talks about methods that optimise policy directly. Instead of working with value function as we consider so far, we seek experience and use the experience to update our policy in the direction that makes it better.</p><p>In the last lecture, we approximated the value or action-value function using parameters $\theta$,<br>$$<br>V_\theta(s)\approx V^\pi(s)\\<br>Q_\theta(s, a)\approx Q^\pi(s, a)<br>$$<br>A policy was generated directly from the value function using $\epsilon$-greedy.</p><p>In this lecture we will directly parametrise the policy<br>$$<br>\pi_\theta(s, a)=\mathbb{P}[a|s, \theta]<br>$$<br>We will focus again on $\color{red}{\mbox{model-free}}$ reinforcement learning.</p><a id="more"></a><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#finite-difference-policy-gradient">Finite Difference Policy Gradient</a></li><li><a href="#monte-carlo-policy-gradient">Monte-Carlo Policy Gradient</a></li><li><a href="#actor-critic-policy-gradient">Actor-Critic Policy Gradient</a></li><li><a href="#summary-of-policy-gradient-algorithms">Summary of Policy Gradient Algorithms</a></li></ul><!-- tocstop --><p><strong>Value-Based and Policy-Based RL</strong></p><ul><li>Value Based<ul><li>Learnt Value Function</li><li>Implicit policy (e.g. $\epsilon$-greedy)</li></ul></li><li>Policy Based<ul><li>No Value Function</li><li>Learnt Policy</li></ul></li><li>Actor-Critic<ul><li>Learnt Value Function</li><li>Learnt Policy</li></ul></li></ul><p><img src="/images/vfp.png" alt=""></p><p><strong>Advantages of Policy-Based RL</strong></p><p>Advantages:</p><ul><li>Better convergence properties</li><li>Effective in high-dimensional or contimuous action spaces (<em>without computing max</em>)</li><li>Can learn stochastic policies</li></ul><p>Disadvantages:</p><ul><li>Typically converge to a local rather than global optimum</li><li>Evaluating a policy is typically inefficient and high variance</li></ul><p>Deterministic policy or taking max is not also the best. Take the rock-paper-scissors game for example.</p><p><img src="/images/rps.png" alt=""></p><p>Consider policies <em>iterated</em> rock-paper-scissors</p><ul><li>A deterministic policy is easily exploited</li><li>A uniform random policy is optimal (according to Nash equilibrium)</li></ul><p><strong>Aliased Gridworld Example</strong></p><p><img src="/images/agw.png" alt=""></p><p>The agent cannot differentiate the grey states.</p><p>Consider features of the following form (for all N, E, S, W)<br>$$<br>\phi(s, a)=1(\mbox{wall to N, a = move E})<br>$$<br>Compare value-based RL, using an approximate value function<br>$$<br>Q_\theta(s, a)=f(\phi(s, a), \theta)<br>$$<br>To policy-based RL, using a parametrised policy<br>$$<br>\pi_\theta(s, a)=g(\phi(s, a), \theta)<br>$$<br>Since the agent cannot differentiate the grey states given the feature, if you take a <strong>deterministic</strong> policy, you must pick the same action at two grey states.</p><p><img src="/images/deagw.png" alt=""></p><p>Under aliasing, an optimal $\color{red}{\mbox{deterministic}}$ policy will either</p><ul><li>move W in both grey states (as shown by red arrows)</li><li>move E in both grey states</li></ul><p>Either way, it can get stuck and never reach the money.</p><p>Value-based RL learns a near-deterministic policy, so it will traverse the corridor for a long time.</p><p><img src="/images/ranagw.png" alt=""></p><p>An optimal $\color{red}{\mbox{stochastic}}$ policy will randomly move E or W in grey states:<br>$$<br>\pi_\theta(\mbox{wall to N and S, move E}) = 0.5\\<br>\pi_\theta(\mbox{wall to N and S, move W}) = 0.5\\<br>$$<br>It will reach the goal state in a few steps with high probability. Policy-based RL can learn the optimal stochastic policy.</p><p>These examples show that a stochastic policy can be better than the deterministic policy, especially in the case that the MDP is <strong>partialy observed</strong> or cannot fully represent the state.</p><p><strong>Policy Objective Functions</strong></p><p>Goal: given policy $\pi_\theta(s, a)$ with parameters $\theta$, find best $\theta$. But how do we measure the quality of a policy $\pi_\theta$?</p><ul><li><p>In episodic environments we can use the <strong>start value</strong><br>$$<br>J_1(\theta)=V^{\pi_\theta}(s_1)=\mathbb{E}_{\pi_\theta}[v_1]<br>$$</p></li><li><p>In continuing environments we can use the <strong>average value</strong><br>$$<br>J_{av}v(\theta)=\sum_s d^{\pi_\theta}(s)V^{\pi_\theta}(s)<br>$$</p></li><li><p>Or the <strong>average reward per time-step</strong></p><p>​<br>$$<br>J_{av}R(\theta)=\sum_s d^{\pi_\theta}(s)\sum_a\pi_\theta(s, a)\mathcal{R}^a_s<br>$$</p></li><li><p>where $d^{\pi_\theta}(s)$ is <strong>stationary distribution</strong> of Markov chain for $\pi_\theta$.</p></li></ul><p><strong>Policy Optimisation</strong></p><p>Policy based reinforcement learning is an <strong>optimisation</strong> problem. Find $\theta$ that maximises $J(\theta)$.</p><p>Some approaches do not use gradient</p><ul><li>Hill climbing</li><li>Simplex / amoeba / Nelder Mead</li><li>Genetic algorithms</li></ul><p>However, greater efficiency often possible using gradient</p><ul><li>Gradient descent</li><li>Conjugate gradient</li><li>Quasi-newton</li></ul><p>We focus on gradient descent, many extensions possible. And on methods that exploit sequential structure.</p><h2><span id="finite-difference-policy-gradient">Finite Difference Policy Gradient</span></h2><p><strong>Policy Gradient</strong></p><p>Let $J(\theta)$ be any policy objective function. Policy gradient algorithms search for a local maximum in $J(\theta)$ by ascending the gradient of the policy, w.r.t. parameters $\theta$<br>$$<br>\triangle\theta = \alpha\nabla_\theta J(\theta)<br>$$<br>Where $\bigtriangledown_\theta J(\theta)$ is the $\color{red}{\mbox{policy gradient}}$,<br>$$<br>\nabla_\theta J(\theta)=\begin{pmatrix}<br>\frac{\partial J(\theta)}{\partial \theta_1}  \\<br>\vdots\\<br>\frac{\partial J(\theta)}{\partial \theta_n}<br>\end{pmatrix}<br>$$<br>and $\alpha$ is a step-size parameter.</p><p><strong>Computing Gradients By Finite Differences (Numerical)</strong></p><p>To evaluate policy gradient of $\pi_\theta(s, a)$. </p><ul><li><p>For each dimension $k\in[1, n]$:</p><ul><li><p>Estimate $k$th partial derivative of objective function w.r.t. $\theta$</p></li><li><p>By perturbing $\theta$ by small amount $\epsilon$ in $k$th dimension<br>$$<br>\frac{\partial J(\theta)}{\partial \theta_k}\approx \frac{J(\theta+\epsilon u_k)-J(\theta)}{\epsilon}<br>$$<br>where $u_k$ is unit vector with 1 in $k$th component, 0 elsewhere</p></li></ul></li><li><p>Uses $n$ evaluations to compute policy gradient in $n$ dimensions</p></li></ul><p>This is a simple, noisy, inefficient, but sometimes effective method. It works for <strong>arbitrary</strong> policies, even if policy is <strong>not</strong> differentiable.</p><p>The algorithm is efficient when the dimension of $\theta$ is low.</p><h2><span id="monte-carlo-policy-gradient">Monte-Carlo Policy Gradient</span></h2><p><strong>Score Function</strong></p><p>We now compute the policy gradient <em>analytically</em>.</p><p>Assume policy $\pi_\theta$ is differentiable whenever it is non-zero and we know the gradient $\nabla_\theta\pi_\theta(s, a)$.</p><p>$\color{red}{\mbox{Likelihood ratios}}$ exploit the following identity<br>$$<br>\begin{align}<br>\nabla_\theta\pi_\theta(s, a) &amp; =\pi_\theta(s, a) \frac{\nabla_\theta\pi_\theta(s, a) }{\pi_\theta(s, a) } \\<br>&amp; = \pi_\theta(s, a) \nabla_\theta\log \pi_\theta(s, a)  \\<br>\end{align}<br>$$<br>The $\color{red}{\mbox{score function}}$ is $\nabla_\theta\log\pi_\theta(s, a) $. Let’s take two examples to see what the score function looks like.</p><p><em>Softmax Policy</em></p><p>We will use a softmax policy as a running example. Weight actions using linear combination of features $\phi(s, a)^T\theta$. Probability of action is proportional to exponentiated weight:<br>$$<br>\pi_\theta(s, a)\varpropto e^{\phi(s, a)^T\theta}<br>$$<br>The score function is<br>$$<br>\nabla_\theta\log\pi_\theta(s, a)=\phi(s, a)-\mathbb{E}_{\pi_\theta}[\phi(s, \cdot)]<br>$$<br>(Intuition: log gradient = the feature for the action that we actually took minus the average feature for all actions.)</p><p><em>Gaussian Policy</em></p><p>In continuous action spaces, a Gaussian policy is natural. </p><ul><li>Mean is a linear combination of state features $\mu(s) = \phi(s)^T\theta$.</li><li>Variance may be fixed $\sigma^2$, or can also parametrised</li></ul><p>Policy is Gaussian, $a\sim \mathcal{N}(\mu(s), \sigma^2)$. The score function is<br>$$<br>\nabla_\theta\log\pi_\theta(s, a)=\frac{(a-\mu(s))\phi(s)}{\sigma^2}<br>$$<br>So far we just have a sense of what does the score function look like. Now we step into policy gradient theorem.</p><p><strong>One-Step MDPs</strong></p><p>Consider a simple class of one-step MDPs:</p><ul><li>Starting in state $s\sim d(s)$</li><li>Terminating after one time-step with reward $r=\mathcal{R}_{s,a}$</li></ul><p>Use likelihood ratios to compute the policy gradient<br>$$<br>\begin{align}<br>J(\theta) &amp;=\mathbb{E}_{\pi_\theta}[r]\\<br>&amp;=\sum_{s\in\mathcal{S}}d(s)\sum_{a\in\mathcal{A}}\pi_\theta(s, a)\mathcal{R}_{s,a}<br>\end{align}<br>$$</p><p>$$<br>\begin{align}<br>\nabla_\theta J(\theta) &amp;=\sum_{s\in\mathcal{S}}d(s)\sum_{a\in\mathcal{A}}\pi_\theta(s, a)\nabla_\theta\log\pi_\theta(s, a)\mathcal{R}_{s,a}\\<br>&amp;=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s, a)r]<br>\end{align}<br>$$</p><p>The policy gradient theorem generalises the likelihood ratio approach to multi-step MDPs.</p><ul><li>Replaces instantaneous reward $r$ with long-term value $Q^\pi(s, a)$</li></ul><p>Policy gradient theorem applies to start state objective, average reward, and average value objective.</p><blockquote><p>Theorem</p><p>For any differentiable policy $\pi_\theta(s,a)$, for any of the policy objective functions mentioned earlier, the policy gradient is<br>$$<br>\nabla_\theta J(\theta)=\color{red}{\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s, a)Q^{\pi_\theta}(s, a)]}<br>$$</p></blockquote><p><strong>Demonstration</strong></p><blockquote><p>Settings: The initial state $s_0$ is sampled from distribution $\rho_0$. A trajectory $\tau = (s_0, a_0, s_1, a_1, …, s_{t+1})$ is sampled from policy $\pi_\theta$.</p><p>The target function would be<br>$$<br>J(\theta) = E_{\tau\sim\pi}[R(\tau)]<br>$$<br>The probability of trajectory $\tau$ is sampled from $\pi$ is<br>$$<br>P(\tau|\theta) = \rho_0(s_0)+\prod_{t=0}^TP(s_{t+1}|s_t, a_t)\pi_\theta(a_t|s_t)<br>$$<br>Using the log prob trick:<br>$$<br>\triangledown_\theta P(\tau|\theta) = P(\tau|\theta)\triangledown_\theta\log P(\tau|\theta)<br>$$<br>Expand the trajectory:<br>$$<br>\begin{align}<br>\require{cancel}\triangledown_\theta \log P(\tau|\theta) &amp;= \cancel{\triangledown_\theta \log\rho_0(s_0)}+\sum_{t=0}^T\cancel{\triangledown_\theta \log P(s_{t+1}|s_t,a_t)}+ \triangledown_\theta\log\pi_\theta(a_t|s_t)\\<br>&amp;= \sum_{t=0}^T\triangledown_\theta\log\pi_\theta(a_t|s_t)<br>\end{align}<br>$$<br>The gradient of target function<br>$$<br>\begin{align}<br>\triangledown_\theta J(\theta) &amp;= \triangledown_\theta E_{\tau\sim\pi}[R(\tau)]\\<br>&amp;= \int_\tau \triangledown_\theta P(\tau|\theta)R(\tau)\\<br>&amp;= \int_\tau P(\tau|\theta)\triangledown_\theta \log P(\tau|\theta)R(\tau)\\<br>&amp;= E_{\tau\sim\pi}[\triangledown_\theta \log P(\tau|\theta)R(\tau)]\\<br>&amp;= E_{\tau\sim\pi}[\sum_{t=0}^T\triangledown_\theta\log\pi_\theta(a_t|s_t)R(\tau)]\\<br>&amp;= E_{\tau\sim\pi}[\sum_{t=0}^T \color{red}{\Phi_t}\triangledown_\theta\log\pi_\theta(a_t|s_t)]<br>\end{align}<br>$$</p></blockquote><p><strong>Monte-Carlo Policy Gradient (REINFORCE)</strong></p><p>Update parameters by stochastic gradient ascent using policy gradient theorem. And using return $v_t$ as an <strong>unbiased sample</strong> of $Q^{\pi_\theta}(s_t,a_t)$:<br>$$<br>\triangle\theta_t=\alpha\nabla_\theta\log\pi_\theta(s_t, a_t)v_t<br>$$<br><img src="/images/mcpseudo.png" alt=""></p><p>(Note: MCPG is slow.)</p><h2><span id="actor-critic-policy-gradient">Actor-Critic Policy Gradient</span></h2><p><strong>Reducing Variance Using a Critic</strong></p><p>Monte-Carlo policy gradient still has high variance, we use a $\color{red}{critic}$ to estimate the action-value function:<br>$$<br>Q_w(s, a)\approx Q^{\pi_\theta}(s, a)<br>$$<br>Actor-critic algorithms maintain two sets of parameters:</p><ul><li>Critic: Updates action-value function parameters $w$</li><li>Actor: Updates policy parameters $\theta$, in direction suggested by critic</li></ul><p>Actor-critic algorithms follow an <em>approximate</em> policy gradient:<br>$$<br>\nabla_\theta J(\theta)\approx \mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s, a)Q_w(s, a)]\\<br>\triangle\theta= \alpha\nabla_\theta\log\pi_\theta(s, a)Q_w(s, a)<br>$$<br>The critic is solving a familiar problem: policy evaluation. This problem was explored in previous lectures:</p><ul><li>Monte-Carlo policy evaluation</li><li>Temporal-Difference learning</li><li>TD($\lambda$)</li><li>Least Squares policy evaluation</li></ul><p>Simple actor-critic algorithm based on action-value critic using linear value function approximation. $Q_w(s, a)=\phi(s,a)^Tw$</p><ul><li>Critic: Updates $w$ by linear TD(0)</li><li>Actor: Updates $\theta$ by policy gradient</li></ul><p><img src="/images/qacpseudo.png" alt=""></p><p><strong>Bias in Actor-Critic Algorithms</strong></p><p>Approximating the policy gradient introduces bias. A biased policy gradient may not find the right solution. Luckily, if we choose value function approximation carefully, then we can avoid introducing any bias. That is we can still follow the exact policy gradient.</p><blockquote><p><strong>Compatible Function Approximation Theorem</strong></p><p>If the following two conditions are satisdied:</p><ol><li><p>Value function approximator is <strong>compatible</strong> to the policy<br>$$<br>\nabla_w Q_w(s, a)=\nabla_\theta \log\pi_\theta(s, a)<br>$$</p></li><li><p>Value function parameters $w$ minimise the mean-squared error<br>$$<br>\epsilon=\mathbb{E}_{\pi_\theta}[(Q^{\pi_\theta}(s, a)-Q_w(s, a))^2]<br>$$</p></li></ol><p>Then the policy gradient is exact,<br>$$<br>\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)Q_w(s,a)]<br>$$</p></blockquote><p><strong>Trick: Reducing Variance Using a Baseline</strong></p><p>We substract a baseline function $B(s)$ from the policy gradient. This can <strong>reduce variance, without changing expectation</strong>:<br>$$<br>\begin{align}<br>\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)B(s)]&amp;=\sum_{s\in\mathcal{S}}d^{\pi_\theta}(s)\sum_a\nabla_\theta\pi_\theta(s,a)B(s)\\<br>&amp;= \sum_{s\in\mathcal{S}}d^{\pi_\theta}B(s)\nabla_\theta\sum_{a\in\mathcal{A}}\pi_\theta(s,a)\\<br>&amp;=  \sum_{s\in\mathcal{S}}d^{\pi_\theta}B(s)\nabla_\theta 1 \\<br>&amp;=0<br>\end{align}<br>$$<br>A good baseline is the state value function $B(s)=V^{\pi_\theta}(s)$. So we can rewrite the policy gradient using the $\color{red}{\mbox{advantage function}} A^{\pi_\theta}(s,a)$.<br>$$<br>A^{\pi_\theta}(s,a)=Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s)\\<br>\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)\color{red}{A^{\pi_\theta}(s,a)}]<br>$$<br>where $V^{\pi_\theta}(s)​$ is the state value function of $s​$. </p><p><strong>Intuition</strong>: The advantage function $A^{\pi_\theta}(s,a)$ tells us how much better than usual is it to take action $a$.</p><p><strong>Estimating the Advantage Function</strong></p><p>How do we know the state value function $V$?</p><p>One way to do that is to estimate both $V^{\pi_\theta}(s)$ and $Q^{\pi_\theta}(s,a)$. Using two function approximators and two parameter vectors,<br>$$<br>V_v(s)\approx V^{\pi_\theta}(s)\\<br>Q_w(s,a)\approx Q^{\pi_\theta}(s,a)\\<br>A(s,a)=Q_w(s,a)-V_v(s)<br>$$<br>And updating both value functions by e.g. TD learning.</p><p>Another way is to use the TD error to compute the policy gradient. For the true value function $V^{\pi_\theta}(s)$, the TD error $\delta^{\pi_\theta}$<br>$$<br>\delta^{\pi_\theta}=r+\gamma V^{\pi_\theta}(s’)-V^{\pi_\theta}(s)<br>$$<br>is an unbiased estimate of the advantage function:<br>$$<br>\begin{align}<br>\mathbb{E}_{\pi_\theta}[\delta^{\pi_\theta}|s, a] &amp;= \mathbb{E}_{\pi_\theta}[r+\gamma V^{\pi_\theta}(s’)|s, a]-V^{\pi_\theta}(s)\\<br>&amp;= Q^{\pi_\theta}(s, a) - V^{\pi_\theta}(s)\\<br>&amp;= \color{red}{A^{\pi_\theta}(s,a)}<br>\end{align}<br>$$<br>So we can use the TD error to compute the policy gradient<br>$$<br>\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)\color{red}{\delta^{\pi_\theta}}]<br>$$<br>In practice we can use an approximate TD error:<br>$$<br>\delta_v=r+\gamma V_v(s’)-V_v(s)<br>$$<br>This approach only requires one set of critic parameters $v$.</p><p><strong>Critics and Actors at Different Time-Scales</strong></p><p>Critic can estimate value function $V_\theta(s)$ from many targets at different time-scales</p><ul><li><p>For MC, the target is return $v_t$<br>$$<br>\triangle \theta=\alpha(\color{red}{v_t}-V_\theta(s))\phi(s)<br>$$</p></li><li><p>For TD(0), the target is the TD target $r+\gamma V(s’)$<br>$$<br>\triangle \theta=\alpha(\color{red}{r+\gamma V(s’)}-V_\theta(s))\phi(s)<br>$$</p></li><li><p>For forward-view TD($\lambda$), the target is the return $_vt^\lambda$<br>$$<br>\triangle \theta=\alpha(\color{red}{v_t^\lambda}-V_\theta(s))\phi(s)<br>$$</p></li><li><p>For backward-view TD($\lambda$), we use eligibility traces<br>$$<br>\begin{align}<br>\delta_t &amp;= r_{t+1}+\gamma V(s_{t+1})-V(s_t) \\<br>e_t&amp; = \gamma\lambda e_{t-1} +\phi(s_t) \\<br>\triangle\theta&amp;=\alpha\delta_te_t<br>\end{align}<br>$$</p></li></ul><p>The policy gradient can also be estimated at many time-scales<br>$$<br>\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)\color{red}{A^{\pi_\theta}(s,a)}]<br>$$</p><ul><li><p>MC policy gradient uses error from complete return<br>$$<br>\triangle\theta=\alpha(\color{red}{v_t}-V_v(s_t))\nabla_\theta\log\pi_\theta(s_t,a_t)<br>$$</p></li><li><p>Actor-critic policy gradient uses the one-step TD error<br>$$<br>\triangle\theta=\alpha(\color{red}{r+\gamma V_v(s_{t+1})}-V_v(s_t))\nabla_\theta\log\pi_\theta(s_t,a_t)<br>$$</p></li><li><p>Just like forward-view TD($\lambda$), we can mix over time-scale<br>$$<br>\triangle \theta=\alpha(\color{red}{v_t^\lambda}-V_v(s_t))\nabla_\theta\log\pi_\theta(s_t,a_t)<br>$$<br>where $v_t^\lambda-V_v(s_t)$ is a biased estimate of advantage function.</p></li><li><p>Like backward-view TD($\lambda$), we can also use eligibility traces by substituting $\phi(s)=\nabla_\theta\log\pi_\theta(s,a)$<br>$$<br>\begin{align}<br>\delta_t &amp;= r_{t+1}+\gamma V_v(s_{t+1})-V_v(s_t) \\<br>e_{t+1}&amp; = \gamma\lambda e_{t} +\nabla_\theta\log\pi_\theta(s,a) \\<br>\triangle\theta&amp;=\alpha\delta_te_t<br>\end{align}<br>$$</p></li></ul><h2><span id="summary-of-policy-gradient-algorithms">Summary of Policy Gradient Algorithms</span></h2><p>The policy gradient has many equivalent forms</p><p><img src="/images/sumpg.png" alt=""></p><p>Each leads a stochastic gradient ascent algorithm. Critic uses policy evaluation to estimate $Q^\pi(s, a)$, $A^\pi(s, a)$ or $V^\pi(s)$.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;This lecture talks about methods that optimise policy directly. Instead of working with value function as we consider so far, we seek experience and use the experience to update our policy in the direction that makes it better.&lt;/p&gt;
&lt;p&gt;In the last lecture, we approximated the value or action-value function using parameters $\theta$,&lt;br&gt;$$&lt;br&gt;V_\theta(s)\approx V^\pi(s)\\&lt;br&gt;Q_\theta(s, a)\approx Q^\pi(s, a)&lt;br&gt;$$&lt;br&gt;A policy was generated directly from the value function using $\epsilon$-greedy.&lt;/p&gt;
&lt;p&gt;In this lecture we will directly parametrise the policy&lt;br&gt;$$&lt;br&gt;\pi_\theta(s, a)=\mathbb{P}[a|s, \theta]&lt;br&gt;$$&lt;br&gt;We will focus again on $\color{red}{\mbox{model-free}}$ reinforcement learning.&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy Gradient" scheme="http://www.52coding.com.cn/tags/Policy-Gradient/"/>
    
      <category term="REINFORCE" scheme="http://www.52coding.com.cn/tags/REINFORCE/"/>
    
      <category term="Actor-Critic" scheme="http://www.52coding.com.cn/tags/Actor-Critic/"/>
    
  </entry>
  
  <entry>
    <title>RL - Value Function Approximation</title>
    <link href="http://www.52coding.com.cn/2018/01/03/RL%20-%20Value%20Function%20Approximation/"/>
    <id>http://www.52coding.com.cn/2018/01/03/RL - Value Function Approximation/</id>
    <published>2018-01-03T10:09:09.000Z</published>
    <updated>2018-11-06T03:47:58.984Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="introduction">Introduction</span></h2><p>This lecture will introduce how to scale up our algorithm to real practical RL problems by value function approximation.</p><p>Reinforcement learning can be used to solve <em>large</em> problems, e.g.</p><ul><li>Backgammon: <span class="math inline">\(10^{20}\)</span> states</li><li>Computer Go: <span class="math inline">\(10^{170}\)</span> states</li><li>Helicopter: continuous state space</li></ul><a id="more"></a><p>How can we scale up the model-free methods for prediction and control from the last two lectures?</p><p>So far we have represented value function by a <strong>lookup</strong> table:</p><ul><li>Every state <span class="math inline">\(s\)</span> has an entry <span class="math inline">\(V(s)\)</span></li><li>Or every state-action pair <span class="math inline">\(s, a\)</span> has an entry <span class="math inline">\(Q(s, a)\)</span></li></ul><p>Problems with large MDPs:</p><ul><li>There are too many states and/or actions to store in memory</li><li>It is too slow to learn the value of each state individually</li></ul><p>Solution for large MDPs: Estimate value function with <em>function approximation</em> <span class="math display">\[\hat{v}(s, \mathbb{w})\approx v_\pi(s)\\\mbox{or }\hat{q}(s, a, \mathbb{w})\approx q_\pi(s, a)\]</span> where <span class="math inline">\(\hat{v}\)</span> or <span class="math inline">\(\hat{q}\)</span> are function approximations of real <span class="math inline">\(v_\pi\)</span> or <span class="math inline">\(q_\pi\)</span>, and <span class="math inline">\(\mathbb{w}\)</span> are the parameters. This apporach has a major advantage:</p><ul><li><strong>Generalise</strong> from seen state to unseen states</li></ul><p>We can fit the <span class="math inline">\(\hat{v}\)</span> or <span class="math inline">\(\hat{q}\)</span> to <span class="math inline">\(v_\pi\)</span> or <span class="math inline">\(q_\pi\)</span> by MC or TD learning.</p><p><strong>Types of Value Function Approximation</strong></p><p><img src="/images/vftypes.png"></p><p>We consider <span class="math inline">\(\color{red}{\mbox{differentiable}}\)</span> function approximators, e.g.</p><ul><li>Linear combinations of features</li><li>Neural network</li></ul><p>Futhermore, we require a training method that is suitable for <span class="math inline">\(\color{red}{\mbox{non-stationary}}\)</span>, <span class="math inline">\(\color{red}{\mbox{non-idd}}\)</span> (idd = independent and identical distributed) data.</p><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#incremental-methods">Incremental Methods</a><ul><li><a href="#value-function-approximation">Value Function Approximation</a></li><li><a href="#action-value-function-approximation">Action-Value Function Approximation</a></li></ul></li><li><a href="#batch-methods">Batch Methods</a><ul><li><a href="#least-square-prediction">Least Square Prediction</a></li><li><a href="#least-squares-control">Least Squares Control</a></li></ul></li></ul><!-- tocstop --><h2><span id="incremental-methods">Incremental Methods</span></h2><h3><span id="value-function-approximation">Value Function Approximation</span></h3><p><strong>Gradient Descent</strong></p><p>Let <span class="math inline">\(J(\mathbb{w})\)</span> be a differentiable function of parameter vector <span class="math inline">\(\mathbb{w}\)</span>.</p><p>Define the gradient of <span class="math inline">\(J(\mathbb{w})\)</span> to be <span class="math display">\[\bigtriangledown_wJ(\mathbb{w})=\begin{pmatrix}\frac{\partial J(\mathbb{w})}{\partial \mathbb{w}_1} \\\vdots\\\frac{\partial J(\mathbb{w})}{\partial \mathbb{w}_n} \end{pmatrix}\]</span> To find a local minimum of <span class="math inline">\(J(\mathbb{w})\)</span>, adjust <span class="math inline">\(\mathbb{w}\)</span> in direction of -ve gradient <span class="math display">\[\triangle \mathbb{w}=-\frac{1}{2}\alpha \bigtriangledown_\mathbb{w}J(\mathbb{w})\]</span> where <span class="math inline">\(\alpha\)</span> is a step-size parameter.</p><p>So let's apply the <em>stochastic gradient descent</em> to <strong>value fucntion approximation</strong>.</p><p>Goal: find parameter vector <span class="math inline">\(\mathbb{w}\)</span> minimising mean-squared error between approximate value function <span class="math inline">\(\hat{v}(s, \mathbb{w})\)</span> and true value function <span class="math inline">\(v_\pi(s)\)</span>. <span class="math display">\[J(\mathbb{w})=\mathbb{E}_\pi[(v_\pi(S)-\hat{v}(S, \mathbb{w}))^2]\]</span> Gradient descent finds a local minimum <span class="math display">\[\begin{align}\triangle\mathbb{w}&amp;=-\frac{1}{2}\alpha \bigtriangledown_\mathbb{w}J(\mathbb{w})\\&amp; = \alpha\mathbb{E}_\pi[(v_\pi(S)-\hat{v}(S, \mathbb{w}))\bigtriangledown_\mathbb{w}\hat{v}(S, \mathbb{w})] \\\end{align}\]</span> Stochastic gradient descent <em>samples</em> the gradient <span class="math display">\[\triangle\mathbb{w}=\alpha(v_\pi(S)-\hat{v}(S, \mathbb{w}))\bigtriangledown_\mathbb{w}\hat{v}(S, \mathbb{w})\]</span> Expected update is equal to full gradient update.</p><p><strong>Feature Vectors</strong></p><p>Let's make this idea more concrete.</p><p>Represent state by a <em>feature vector</em>: <span class="math display">\[x(S) =\begin{pmatrix}x_1(S) \\\vdots\\x_n(S)\end{pmatrix}\]</span> For example:</p><ul><li>Distance of robot from landmarks</li><li>Trend in the stock market</li><li>Piece and pawn configurations in chess</li></ul><p><strong>Linear Value Function Approximation</strong></p><p>Represent value function by a linear combination of features <span class="math display">\[\hat{v}(S, \mathbb{w})=x(S)^T\mathbb{w}=\sum^n_{j=1}x_j(S)\mathbb{w}_j\]</span> Objective function is quadratic in parameters <span class="math inline">\(\mathbb{w}\)</span> <span class="math display">\[J(\mathbb{w})=\mathbb{E}_\pi[(v_\pi(S)-x(S)^T\mathbb{w})^2]\]</span> Stochastic gradient descent converges on global optimum.</p><p>Update rule is particularly simple <span class="math display">\[\bigtriangledown_\mathbb{w}\hat{v}(S, \mathbb{w})=x(S)\\\triangle \mathbb{w}=\alpha(v_\pi(S)-\hat{v}(S, \mathbb{w}))x(S)\]</span> Update = step-size <span class="math inline">\(\times\)</span> prediction error <span class="math inline">\(\times\)</span> feature value.</p><p>So far we have assumed true value function <span class="math inline">\(v_\pi(s)\)</span> given by supervisor. But in RL there is <strong>no supervisor, only rewards</strong>.</p><p>In practice, we substitute a <em>target</em> for <span class="math inline">\(v_\pi(s)\)</span>:</p><ul><li><p>For MC, the target is return <span class="math inline">\(G_t​\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{G_t}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w})\]</span></p></li><li><p>For TD(0), the target is the TD target <span class="math inline">\(R_{t+1}+\gamma\hat{v}(S_{t+1}, \mathbb{w})\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{R_{t+1}+\gamma\hat{v}(S_{t+1}, \mathbb{w})}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w})\]</span></p></li><li><p>For TD(<span class="math inline">\(\lambda\)</span>), the target is the return <span class="math inline">\(G_t^\lambda\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{G_t^\lambda}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w})\]</span></p></li></ul><p><strong>Monte-Carlo with Value Function Approximation</strong></p><p>Return <span class="math inline">\(G_t\)</span> is unbiased, noisy sample of true value <span class="math inline">\(v_\pi(S_t)\)</span>. We can build our &quot;training data&quot; to apply supervised learning: <span class="math display">\[&lt;S_1, G_1&gt;, &lt;S_2, G_2&gt;, ..., &lt;S_T, G_T&gt;\]</span> For example, using <em>linear Monte-Carlo policy evaluation</em> <span class="math display">\[\begin{align}\triangle \mathbb{w}&amp;=\alpha(\color{red}{G_t}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w}) \\&amp; = \alpha(G_t-\hat{v}(S_t, \mathbb{w}))x(S_t)\\\end{align}\]</span> Monte-Carlo evaluation converges to a local optimum even when using non-linear value function approximation.</p><p><strong>TD Learning with Value Function Approximation</strong></p><p>The TD-target <span class="math inline">\(R_{t+1}+\gamma \hat{v}(S_{t+1}, \mathbb{w})\)</span> is a biased sample of true value <span class="math inline">\(v_\pi(S_t)\)</span>. We can still apply supervised learning to &quot;traning data&quot;: <span class="math display">\[&lt;S_1, R_2 +\gamma\hat{v}(S_2, \mathbb{w})&gt;,&lt;S_2, R_3 +\gamma\hat{v}(S_3, \mathbb{w})&gt;,...,&lt;S_{T-1}, R_T&gt;\]</span> For example, using <em>linear TD(0)</em> <span class="math display">\[\begin{align}\triangle \mathbb{w}&amp;=\alpha(\color{red}{R+\gamma\hat{v}(S&#39;, \mathbb{w})}-\hat{v}(S, \mathbb{w}))\bigtriangledown_w \hat{v}(S, \mathbb{w}) \\&amp; = \alpha\delta x(S)\\\end{align}\]</span> Linear TD(0) converges (close) to global optimum.</p><p><strong>TD(<span class="math inline">\(\lambda\)</span>) with Value Function Approximation</strong></p><p>The <span class="math inline">\(\lambda\)</span>-return <span class="math inline">\(G_t^\lambda\)</span> is also a biased sample of true value <span class="math inline">\(v_\pi(s)\)</span>. We can also apply supervised learning to &quot;training data&quot;: <span class="math display">\[&lt;S_1, G_1^\lambda&gt;, &lt;S_2, G_2^\lambda&gt;, ..., &lt;S_{T-1}, G_{T-1}^\lambda&gt;\]</span> Can use either forward view linear TD(<span class="math inline">\(\lambda\)</span>): <span class="math display">\[\begin{align}\triangle \mathbb{w}&amp;=\alpha(\color{red}{G_t^\lambda}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w}) \\&amp; = \alpha(G_t-\hat{v}(S_t, \mathbb{w}))x(S_t)\\\end{align}\]</span> or backward view linear TD(<span class="math inline">\(\lambda\)</span>): <span class="math display">\[\begin{align}\delta_t &amp;= R_{t+1}+\gamma \hat{v}(S_{t+1}, \mathbb{w})-\hat{v}(S_t, \mathbb{w}) \\E_t&amp; = \gamma\lambda E_{t-1} +x(S_t) \\\triangle\mathbb{w}&amp;=\alpha\delta_tE_t\end{align}\]</span></p><h3><span id="action-value-function-approximation">Action-Value Function Approximation</span></h3><p><img src="/images/avfa.png"></p><p>Approximate the action-value function: <span class="math display">\[\hat{q}(S, A, \mathbb{w}) \approx q_\pi(S, A)\]</span> Minimise mean-squared error between approximate action-value function <span class="math inline">\(\hat{q}(S, A, \mathbb{w})\)</span> and true action-value function <span class="math inline">\(q_\pi(S, A)\)</span>: <span class="math display">\[J(\mathbb{w})=\mathbb{E}_\pi[(q_\pi(S, A)-\hat{q}(S, A, \mathbb{w}))^2]\]</span> Use stochastic gradient descent to find a local minimum: <span class="math display">\[-\frac{1}{2}\bigtriangledown_w J(\mathbb{w})=(q_\pi(S, A)-\hat{q}(S, A, \mathbb{w}))\bigtriangledown_w\hat{q}(S, A, \mathbb{w})\\\triangle\mathbb{w}=\alpha (q_\pi(S, A)-\hat{q}(S, A, \mathbb{w}))\bigtriangledown_w\hat{q}(S, A, \mathbb{w})\]</span> Represent state and action by a feature vector: <span class="math display">\[\mathbb{x}(S, A)=\begin{pmatrix}x_1(S, A) \\\vdots\\x_n(S, A)\end{pmatrix}\]</span> Represent action-value function by linear combination of features: <span class="math display">\[\hat{q}(S, A, \mathbb{w})=\mathbb{x}(S, A)^T\mathbb{w}=\sum^n_{j=1}x_j (S, A)\mathbb{w}_j\]</span> Stochastic gradient descent update: <span class="math display">\[\bigtriangledown_w\hat{q}(S, A, \mathbb{w})=\mathbb{x}(S, A)\\\triangle \mathbb{w}=\alpha(q_\pi(S, A)-\hat{q}(S,  A, \mathbb{w}))\mathbb{x}(S, A)\]</span> Like prediction, we must subsitute a target for <span class="math inline">\(q_\pi(S, A)\)</span>:</p><ul><li><p>For MC, the target is the return <span class="math inline">\(G_t\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{G_t}-\hat{q}(S_t,  A_t, \mathbb{w}))\bigtriangledown_w\hat{q}(S_t, A_t, \mathbb{w})\]</span></p></li><li><p>For TD(0), the target is the TD target <span class="math inline">\(R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})​\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{R_{t+1}+\gamma \hat{q}(S_{t+1},  A_{t+1}, \mathbb{w})}-\hat{q}(S_t,  A_t, \mathbb{w}))\bigtriangledown_w\hat{q}(S_t, A_t, \mathbb{w})\]</span></p></li><li><p>For forward-view TD(<span class="math inline">\(\lambda\)</span>), target is the action-value <span class="math inline">\(\lambda\)</span>-return <span class="math display">\[\triangle\mathbb{w}=\alpha(\color{red}{q_t^\lambda}-\hat{q}(S_t, A_t,\mathbb{w}))\bigtriangledown\hat{q}(S_t, A_t, \mathbb{w})\]</span></p></li><li><p>For backward-view TD(<span class="math inline">\(\lambda\)</span>), equivalent update is <span class="math display">\[\begin{align}\delta_t&amp; =R_{t+1}+\gamma\hat{q}(S_{t+1}, A_{t+1}, \mathbb{w})-\hat{q}(S_t, A_t, \mathbb{w}) \\E_t&amp; = \gamma\lambda E_{t-1}+\bigtriangledown_w\hat{q}(S_t, A_t, \mathbb{w}) \\\triangle\mathbb{w}&amp;= \alpha\delta_t E_t\end{align}\]</span></p></li></ul><p><strong>Linear Sarsa with Coarse Coding in Mountain Car</strong></p><p><img src="/images/linsarsa.png"></p><p>The goal is to control our car to reach the top of the mountain. We represent state by the car's position and velocity. The height of the diagram shows the value of each state. Finally, the value function is like:</p><p><img src="/images/linsarfin.png"></p><p><strong>Study of <span class="math inline">\(\lambda\)</span>: Should We Bootstrap?</strong></p><p><img src="/images/lambdastudy.png"></p><p>The answer is <strong>yes</strong>. We can see from above picture, choose some approprite <span class="math inline">\(\lambda\)</span> can certainly reduce the training steps as well as the cost.</p><p>However, temporal-difference learning in many cases doesn't guarantee to converge. It may also diverge.</p><p><strong>Convergence of Prediction Algorithms</strong></p><p><img src="/images/converge.png"></p><p>TD dose not follow the gradient of <em>any</em> objective function. This is why TD can diverge when off-policy or using non-linear function approximation. <strong>Gradient TD</strong> follows true gradient of projected Bellman error.</p><p><img src="/images/gtd.png"></p><p><strong>Convergence of Control Algorithms</strong></p><p><img src="/images/convca.png"></p><h2><span id="batch-methods">Batch Methods</span></h2><p>Gradient descent is simple and appealing. But it is not <strong>sample efficient</strong>. Batch methods seek to find the best fitting value function given the agent's experience.</p><h3><span id="least-square-prediction">Least Square Prediction</span></h3><p>Give value function approximation <span class="math inline">\(\hat{v}(s, \mathbb{w})\approx v_\pi(s)\)</span> and experience <span class="math inline">\(\mathcal{D}\)</span> consisting of <em>&lt;state, value&gt;</em> pairs: <span class="math display">\[\mathcal{D} = \{&lt;s_1, v_1^\pi&gt;, &lt;s_2, v_2^\pi&gt;, ..., &lt;s_T, v_T^\pi&gt; \}\]</span> Which parameters <span class="math inline">\(\mathbb{w}\)</span> give the best fitting value function <span class="math inline">\(\hat{v}(s, \mathbb{w})\)</span> ?</p><p><span class="math inline">\(\color{red}{\mbox{Least squares}}\)</span> algorithms find parameter vector <span class="math inline">\(\mathbb{w}\)</span> minimising sum-squared error between <span class="math inline">\(\hat{v}(s_t, \mathbb{w})\)</span> and target values <span class="math inline">\(v_t^\pi\)</span>, <span class="math display">\[\begin{align}LS(\mathbb{w}) &amp; = \sum^T_{t=1}(v_t^\pi-\hat{v}(s_t, \mathbb{w}))^2 \\&amp; = \mathbb{E}_\mathcal{D}[(v^\pi-\hat{v}(s, \mathbb{w}))^2] \\\end{align}\]</span> Given experience consisting of <em>&lt;state, value&gt;</em> pairs <span class="math display">\[\mathcal{D}=\{&lt;s_1, v_1^\pi&gt;, &lt;s_2, v_2^\pi&gt;, ..., &lt;s_T, v_T^\pi&gt;\}\]</span> Repeat:</p><ol type="1"><li><p>Sample state, value from experience <span class="math display">\[&lt;s, v^\pi&gt; \sim \mathcal{D}\]</span></p></li><li><p>Apply stochastic gradient descent update <span class="math display">\[\triangle \mathbb{w}=\alpha(v^\pi-\hat{v}(s, \mathbb{w}))\bigtriangledown_w\hat{v}(s, w)\]</span></p></li></ol><p>Converges to least squares solution <span class="math display">\[\mathbb{w}^\pi=\arg\min_w LS(w)\]</span> <strong>Deep Q-Networks (DQN)</strong></p><p>DQN uses <span class="math inline">\(\color{red}{\mbox{experience replay}}\)</span> and <span class="math inline">\(\color{red}{\mbox{fixed Q-targets}}\)</span>:</p><ul><li><p>Take action <span class="math inline">\(a_t\)</span> according to <span class="math inline">\(\epsilon\)</span>-greedy policy</p></li><li><p>Store transition <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> in replay memory <span class="math inline">\(\mathcal{D}\)</span></p></li><li><p>Sample random mini-batch of transitions <span class="math inline">\((s, a, r, s&#39;)\)</span> from <span class="math inline">\(\mathcal{D}\)</span></p></li><li><p>Compute Q-learning targets w.r.t. old, fixed parameters <span class="math inline">\(w^-\)</span></p></li><li><p>Optimise MSE between Q-network and Q-learning target <span class="math display">\[\mathcal{L}(w_i)=\mathbb{E}_{s,a,r,s&#39;\sim\mathcal{D}_i}[(r+\gamma\max_{a&#39;}Q(s&#39;,a&#39;;w^-_i)-Q(s, a;w_i))^2]\]</span></p></li><li><p>Using variant of stochastic gradient descent</p></li></ul><p>Note: <span class="math inline">\(\color{red}{\mbox{fixed Q-targets}}\)</span> means we use two Q-networks. One of it using fixed old parameters to generate the Q-target to update the fresh Q-network, which can keep the update <strong>stable</strong>. Otherwise, when you update the Q-network, you also update the Q-target, which can cause diverge.</p><p>DQN in Atari</p><ul><li>End-to-end learning of values <span class="math inline">\(Q(s, a)\)</span> from pixels <span class="math inline">\(s\)</span></li><li>Input state <span class="math inline">\(s\)</span> is stack of raw pixels from last 4 frames</li><li>Output is <span class="math inline">\(Q(s, a)\)</span> for 18 joystick/button positions</li><li>Rewards is change in score for that step</li></ul><p><img src="/images/ataridqn.png"></p><p>Results</p><p><img src="/images/dqnres.png"></p><p>How much does DQN help?</p><p><img src="/images/dqnhelp.png"></p><p><strong>Linear Least Squares Prediction - Normal Equation</strong></p><p>Experience replay finds least squares solution but it may take many iterations. Using <em>linear value function approximation</em> <span class="math inline">\(\hat{v}(s, w) = x(s)^Tw\)</span>, we can solve squares soluton directly.</p><p>At minimum of <span class="math inline">\(LS(w)\)</span>, the expected update must be zero:</p><p><img src="/images/llsp.png"></p><p>For <span class="math inline">\(N\)</span> features, direct solution time is <span class="math inline">\(O(N^3)\)</span>. Incremental solution time is <span class="math inline">\(O(N^2)\)</span> using Shermann-Morrison.</p><p>We do not know true values <span class="math inline">\(v_t^\pi\)</span>. In practice, our &quot;training data&quot; must be noisy or biased samples of <span class="math inline">\(v_t^\pi\)</span>:</p><p><img src="/images/llspa.png"></p><p>In each case solve directly for fixed point of MC / TD / TD(<span class="math inline">\(\lambda\)</span>).</p><p><strong>Convergence of Linear Least Squares Prediction Algorithms</strong></p><p><img src="/images/cllspa.png"></p><h3><span id="least-squares-control">Least Squares Control</span></h3><p><strong>Least Squares Policy Iteration</strong></p><p><img src="/images/lspi.png"></p><p><strong>Least Squares Action-Value Function Approximation</strong></p><p>Approximate action-value function <span class="math inline">\(q_\pi(s, a)\)</span> using linear combination of features <span class="math inline">\(\mathbb{x}(s, a)\)</span>: <span class="math display">\[\hat{q}(s, a, \mathbb{w})=\mathbb{x}(s, a)^T\mathbb{w}\approx q_\pi(s, a)\]</span> Minimise least squares error between <span class="math inline">\(\hat{q}(s, a, \mathbb{w})\)</span> and <span class="math inline">\(q_\pi(s, a)\)</span> from experience generated using policy <span class="math inline">\(\pi\)</span> consisting of <em>&lt;(state, action), value&gt;</em> pairs: <span class="math display">\[\mathcal{D}=\{&lt;(s_1,a_1),v_1^\pi&gt;,&lt;(s_2,a_2),v_2^\pi&gt;,...,&lt;(s_T,a_T),v_T^\pi&gt;\}\]</span> <strong>Least Squares Control</strong></p><p>For policy evaluation, we want to efficiently use all experience. For control, we also want to improve the policy. This experience is generated from many policies. So to evaluate <span class="math inline">\(q_\pi(S, A)\)</span> we must learn <span class="math inline">\(\color{red}{\mbox{off-policy}}\)</span>.</p><p>We use the same idea as Q-learning:</p><ul><li>Use experience generated by old policy <span class="math inline">\(S_t, A_t, R_{t+1}, S_{t+1} \sim \pi_{old}\)</span></li><li>Consider alternative successor action <span class="math inline">\(A&#39;=\pi_{new}(S_{t+1})\)</span></li><li>Update <span class="math inline">\(\hat{q}(S_t, A_t,\mathbb{w})\)</span> towards value of alternative action <span class="math inline">\(R_{t+1}+\gamma \hat{q}(S_{t+1}, A&#39;, \mathbb{w})\)</span></li></ul><p>Consider the following linear Q-learning update <span class="math display">\[\delta=R_{t+1}+\gamma \hat{q}(S_{t+1}, \color{red}{\pi(S_{t+1})}, \mathbb{w})-\hat{q}(S_t, A_t, \mathbb{w})\\\triangle \mathbb{w}=\alpha\delta\mathbb{x}(S_t, A_t)\]</span> LSTDQ algorithm: solve for total update = zero:</p><p><img src="/images/lstdq.png"></p><p>The following pseudocode uses LSTDQ for policy evaluation. It repeatedly re-evaluates experience <span class="math inline">\(\mathcal{D}\)</span> with different policies.</p><p><img src="/images/lspipseudo.png"></p><p><strong>Convergence of Control Algorithms</strong></p><p><img src="/images/ccal.png"></p><p>End.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This lecture will introduce how to scale up our algorithm to real practical RL problems by value function approximation.&lt;/p&gt;
&lt;p&gt;Reinforcement learning can be used to solve &lt;em&gt;large&lt;/em&gt; problems, e.g.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Backgammon: &lt;span class=&quot;math inline&quot;&gt;\(10^{20}\)&lt;/span&gt; states&lt;/li&gt;
&lt;li&gt;Computer Go: &lt;span class=&quot;math inline&quot;&gt;\(10^{170}\)&lt;/span&gt; states&lt;/li&gt;
&lt;li&gt;Helicopter: continuous state space&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="DQN" scheme="http://www.52coding.com.cn/tags/DQN/"/>
    
      <category term="Neural Network" scheme="http://www.52coding.com.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>RL - Model-Free Control</title>
    <link href="http://www.52coding.com.cn/2017/12/21/RL%20-%20Model-Free%20Control/"/>
    <id>http://www.52coding.com.cn/2017/12/21/RL - Model-Free Control/</id>
    <published>2017-12-21T12:00:09.000Z</published>
    <updated>2018-11-06T03:47:38.964Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="introduction">Introduction</span></h2><p>Last lecture:</p><ul><li>Model-free prediction</li><li><em>Estimate</em> the value function of an <em>unknown</em> MDP</li></ul><p>This lecture:</p><ul><li>Model-free control</li><li><strong>Optimise</strong> the value function of an unknown MDP</li></ul><a id="more"></a><p>Why we care about model-free control? So, let's see some example problems that can be modelled as MDPs:</p><ul><li>Helicopter, Robocup Soccer, Quake</li><li>Portfolio management, Game of Go...</li></ul><p>For most of these problems, either:</p><ul><li>MDP model is <strong>unknown</strong>, but experience can be sampled</li><li>MDP model is known, but is <strong>too big to use</strong>, except by samples</li></ul><p><span class="math inline">\(\color{red}{\mbox{Model-free control}}\)</span> can sovlve these problems.</p><p>There are two branches of model-free control:</p><ul><li><span class="math inline">\(\color{red}{\mbox{On-policy}}\)</span> learning<ul><li>&quot;Learn on the job&quot;</li><li>Learn about policy <span class="math inline">\(\pi\)</span> from experience sampled from <span class="math inline">\(\pi\)</span></li></ul></li><li><span class="math inline">\(\color{red}{\mbox{Off-policy}}\)</span> learning<ul><li>&quot;Look over someone's shoulder&quot;</li><li>Learn about policy <span class="math inline">\(\pi\)</span> from experience sampled from <span class="math inline">\(\mu\)</span></li></ul></li></ul><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#on-policy-monte-carlo-control">On-Policy Monte-Carlo Control</a></li><li><a href="#on-policy-temporal-difference-learning">On-Policy Temporal-Difference Learning</a><ul><li><a href="#sarsalambda">Sarsa(<span class="math inline">\(\lambda\)</span>)</a></li></ul></li><li><a href="#off-policy-learning">Off-Policy Learning</a><ul><li><a href="#q-learning">Q-Learning</a></li></ul></li><li><a href="#summary">Summary</a></li></ul><!-- tocstop --><h2><span id="on-policy-monte-carlo-control">On-Policy Monte-Carlo Control</span></h2><p>In previous lectures, we have seen that using policy iteration to find the best policy. Today, we also use this central idea plugging in MC or TD algorithm.</p><p><img src="/images/pi.png"></p><p><strong>Generalised Policy Iteration With Monte-Carlo Evaluation</strong></p><p>A simple idea is</p><ul><li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(V = v_\pi\)</span>?</li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: Greedy policy improvement?</li></ul><p>Well, this idea has two major problems:</p><ul><li><p>Greedy policy improvement over <span class="math inline">\(V(s)\)</span> requires <strong>model of MDP</strong> <span class="math display">\[\pi^\prime(s) = \arg\max_{a\in\mathcal{A}}\mathcal{R}^a_s+\mathcal{P}^a_{ss&#39;}V(s&#39;)\]</span> since, we do not know the state transition probability matrix <span class="math inline">\(\mathcal{P}\)</span>.</p></li><li><p>Exploration issue: cannot guarantee to explore all states</p></li></ul><p>So, the alternative is to use action-value function <span class="math inline">\(Q\)</span>:</p><ul><li>Greedy policy improvement over <span class="math inline">\(Q(s, a)\)</span> is model-free <span class="math display">\[\pi^\prime=\arg\max_{a\in\mathcal{A}}Q(s,a)\]</span></li></ul><p>Let's replace it in the algorithm:</p><p><img src="/images/avf.png"></p><ul><li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(\color{red}{Q = q_\pi}\)</span></li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: Greedy policy improvement?</li></ul><p>We still have one problems about the algorithm, which is exploration issue. Here is a example of greedy action selection:</p><p><img src="/images/gaseg.png"></p><p>The reward of the two doors are stochastic. However, because of the greedy action selection, we always choose the right door without exploring the value of the left one.</p><p>One simple algorithm to ensure keeping exploration is <strong><span class="math inline">\(\epsilon\)</span>-greedy exploration</strong>.</p><p><strong><span class="math inline">\(\epsilon\)</span>-Greedy Exploration</strong></p><p>All <span class="math inline">\(m\)</span> actions are tried with non-zero probalility,</p><ul><li>With probability <span class="math inline">\(1-\epsilon\)</span> choose the greedy action</li><li>With probability <span class="math inline">\(\epsilon\)</span> choose an action at <strong>random</strong></li></ul><p><span class="math display">\[\pi(a|s)=\begin{cases} \epsilon/m+1-\epsilon,  &amp; \mbox{if } a^* = \arg\max_{a\in\mathcal{A}}Q(s,a) \\\epsilon/m, &amp; \mbox{otherwise }\end{cases}\]</span></p><blockquote><p>Theorem</p><p>For any <span class="math inline">\(\epsilon\)</span>-greedy policy <span class="math inline">\(\pi\)</span>, the <span class="math inline">\(\epsilon\)</span>-greedy policy <span class="math inline">\(\pi^\prime\)</span> with respect to <span class="math inline">\(q_\pi\)</span> is an improvement, <span class="math inline">\(v_{\pi^\prime}≥v_\pi(s)\)</span>.</p></blockquote><p><img src="/images/egpi.png"></p><p>Therefore from policy improvement theorem, <span class="math inline">\(v_{\pi^\prime}(s) ≥ v_\pi(s)\)</span>.</p><p><strong>Monte-Carlo Policy Iteration</strong></p><p><img src="/images/mcpi.png"></p><ul><li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(Q = q_\pi\)</span></li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: <span class="math inline">\(\color{red}{\epsilon}\)</span>-greedy policy improvement</li></ul><p><strong>Monte-Carlo Control</strong></p><p><img src="/images/mcc.png"></p><p><span class="math inline">\(\color{red}{\mbox{Every episode}}\)</span>:</p><ul><li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(\color{red}{Q \approx q_\pi}\)</span></li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: <span class="math inline">\(\epsilon\)</span>-greedy policy improvement</li></ul><p>The method is once evaluate over an episode, immediately improve the policy. The idea is since we already have a better evaluation, why waiting to update the policy after numerous episodes. That is improving the policy right after evaluating one episode.</p><p><strong>GLIE</strong></p><blockquote><p>Definition</p><p><strong>Greedy in the Limit with Infinite Exploration</strong> (GLIE)</p><ul><li><p>All state-action pairs are explored infinitely many times, <span class="math display">\[\lim_{k\rightarrow\infty}N_k(s,a)=\infty\]</span></p></li><li><p>The policy converges on a greedy policy, <span class="math display">\[\lim_{k\rightarrow\infty}\pi_k(a|s)=1(a=\arg\max_{a^\prime \in\mathcal{A}}Q_k(s, a^\prime))\]</span></p></li></ul></blockquote><p>For example, <span class="math inline">\(\epsilon\)</span>-greedy is GLIE if <span class="math inline">\(\epsilon\)</span> reduces to zero at <span class="math inline">\(\epsilon_k=\frac{1}{k}\)</span>.</p><p><strong>GLIE Monte-Carlo Control</strong></p><p>Sample <span class="math inline">\(k\)</span>th episode using <span class="math inline">\(\pi\)</span>: <span class="math inline">\(\{S_1, A_1, R_2, …, S_T\} \sim \pi\)</span></p><ul><li><p><span class="math inline">\(\color{red}{\mbox{Evaluation}}\)</span></p><ul><li>For each state <span class="math inline">\(S_t\)</span> and action <span class="math inline">\(A_t\)</span> in the episode, <span class="math display">\[\begin{array}{lcl}N(S_t, A_t) \leftarrow N(S_t, A_t)+1 \\Q(S_t, A_t) \leftarrow Q(S_t, A_t)+\frac{1}{N(S_t, A_t)}(G_t-Q(S_t, A_t))\end{array}\]</span></li></ul></li><li><p><span class="math inline">\(\color{red}{\mbox{Improvement}}\)</span></p><ul><li>Improve policy based on new action-value function <span class="math display">\[\begin{array}{lcl}\epsilon\leftarrow \frac{1}{k} \\\pi \leftarrow \epsilon\mbox{-greedy}(Q)\end{array}\]</span></li></ul></li></ul><p>GLIE Monte-Carlo control converges to the optimal action-value function, <span class="math inline">\(Q(s,a) \rightarrow q_*(s,a)\)</span>.</p><p><strong>Blackjack Example</strong></p><p><img src="/images/mccb.png"></p><p>Using Monte-Carlo control, we can get the optimal policy above.</p><h2><span id="on-policy-temporal-difference-learning">On-Policy Temporal-Difference Learning</span></h2><p>Temporal-difference (TD) learning has several advantages over Monte-Carlo (MC):</p><ul><li>low variance</li><li>Online</li><li>Incomplete sequences</li></ul><p>A natural idea is using TD instead of MC in our control loop:</p><ul><li>Apply TD to <span class="math inline">\(Q(S, A)\)</span></li><li>Use <span class="math inline">\(\epsilon\)</span>-greedy policy improvement</li><li>Update every <em>time-step</em></li></ul><p><strong>Sarsa Update</strong></p><p><img src="/images/sarsa.png"></p><p>Updating action-value functions with Sarsa: <span class="math display">\[Q(S,A) \leftarrow Q(S, A) + \alpha(R+\gamma Q(S^\prime, A^\prime)-Q(S, A))\]</span> <img src="/images/mcc.png"></p><p>So, the full algorithm is:</p><ul><li>Every <span class="math inline">\(\color{red}{\mbox{time-step}}\)</span>:<ul><li><span class="math inline">\(\color{blue}{\mbox{Policy evaluation}}\)</span> <span class="math inline">\(\color{red}{\mbox{Sarsa}}\)</span>, <span class="math inline">\(Q\approx q_\pi\)</span></li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span> <span class="math inline">\(\epsilon\)</span>-greedy policy improvement</li></ul></li></ul><p><img src="/images/pesedotd.png"></p><p><strong>Windy Gridworld Example</strong></p><p><img src="/images/wweg.png"></p><p>The 'S' represents start location and 'G' marks the goal. There is a number at the bottom of each column which represents the wind will blow the agent up how many grids if the agent stays at that column.</p><p>The result of apply Sarsa to the problem is</p><p><img src="/images/wwegres.png"></p><h3><span id="sarsalambda">Sarsa(<span class="math inline">\(\lambda\)</span>)</span></h3><p><strong>n-Step Sarsa</strong></p><p>Consider the following <span class="math inline">\(n\)</span>-step returns for <span class="math inline">\(n=1,2,..\infty\)</span>:</p><p><img src="/images/nsarsa.png"></p><p>Define the <span class="math inline">\(n\)</span>-step <span class="math inline">\(Q\)</span>-return <span class="math display">\[q_t^{(n)}=R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^n Q(S_{t+n})\]</span> <span class="math inline">\(n\)</span>-step Sarsa updates <span class="math inline">\(Q(s, a)\)</span> towards the <span class="math inline">\(n\)</span>-step <span class="math inline">\(Q\)</span>-return <span class="math display">\[Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha(q_t^{(n)}-Q(S_t,A_t))\]</span> <strong>Forward View Sarsa(<span class="math inline">\(\lambda\)</span>)</strong></p><p><img src="/images/sarsalam.png"></p><p>The <span class="math inline">\(q^\lambda\)</span> return combines all <span class="math inline">\(n\)</span>-step Q-returns <span class="math inline">\(q_t^{(n)}\)</span> using weight <span class="math inline">\((1-\lambda)\lambda^{n-1}\)</span>: <span class="math display">\[q_t^\lambda = (1-\lambda)\sum^\infty_{n=1}\lambda^{n-1}q_t^{(n)}\]</span> Forward-view Sarsa(<span class="math inline">\(\lambda\)</span>): <span class="math display">\[Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha(q_t^\lambda-Q(S_t, A_t))\]</span> <strong>Backward View Sarsa(<span class="math inline">\(\lambda\)</span>)</strong></p><p>Just like TD(<span class="math inline">\(\lambda\)</span>), we use <span class="math inline">\(\color{red}{\mbox{eligibility traces}}\)</span> in an online algorithm, but Sarsa(<span class="math inline">\(\lambda\)</span>) has one eligibility trace for each state-action pair: <span class="math display">\[E_0(s, a) = 0\]</span></p><p><span class="math display">\[E_t(s, a) = \gamma\lambda E_{t-1}(s,a)+1(S_t=s, A_t=a)\]</span></p><p><span class="math inline">\(Q(s, a)\)</span> is updated for every state <span class="math inline">\(s\)</span> and action <span class="math inline">\(a\)</span> in proportion to TD-error <span class="math inline">\(\delta_t\)</span> and eligibility trace <span class="math inline">\(E_t(s, a)\)</span>: <span class="math display">\[\delta_t=R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})-Q(S_t, A_t)\]</span></p><p><span class="math display">\[Q(s, a) \leftarrow Q(s, a) +\alpha \delta_t E_t(s, a)\]</span></p><p><img src="/images/sarcode.png"></p><p>The difference between Sarsa and Sarsa(<span class="math inline">\(\lambda\)</span>):</p><p><img src="/images/sarsadiff.png"></p><p>If we initial all <span class="math inline">\(Q(s, a) = 0\)</span>, then we first do a random walk and reach the goal. Using Sarsa, we can only update the Q-value of the previous state before reaching the goal since all other <span class="math inline">\(Q\)</span> are zero. So the reward can only propagate one state. On the contrary, if we using Sarsa(<span class="math inline">\(\lambda\)</span>), the reward can propagate from the last state to the first state with a exponential decay.</p><h2><span id="off-policy-learning">Off-Policy Learning</span></h2><p>Evaluate target policy <span class="math inline">\(\pi(a|s)\)</span> to compute <span class="math inline">\(v_\pi(s)\)</span> or <span class="math inline">\(q_\pi(s, a)\)</span> while following behaviour policy <span class="math inline">\(\mu(a|s)\)</span> <span class="math display">\[\{S_1, A_1, R_2, ..., S_T\}\sim \mu\]</span> So, why is this important? There are several reasons:</p><ul><li>Learn from observing hunman or other agents</li><li>Re-use experience generated from old policies <span class="math inline">\(\pi_1, \pi_2, …, \pi_{t-1}\)</span></li><li>Learn about <strong>optimal</strong> policy while following <span class="math inline">\(\color{red}{\mbox{exploratory policy}}\)</span></li><li>Learn about <strong>multiple</strong> policies while following one policy</li></ul><p><strong>Importance Sampling</strong></p><p>Estimate the expectation of a different distribution <span class="math display">\[\mathbb{E}_{X\sim P}[f(X)] = \sum P(X)f(X)=\sum Q(X)\frac{P(X)}{Q(X)}f(X)=\mathbb{E}_{X\sim Q}[\frac{P(X)}{Q(X)}f(X)]\]</span> <strong>Off-Policy Monte-Carlo</strong></p><p>Use returns generated from <span class="math inline">\(\mu\)</span> to evaluate <span class="math inline">\(\pi\)</span>. Weight return <span class="math inline">\(G_t\)</span> according to <strong>similarity</strong> between policies. Multiply importance sampling corrections along whole episode: <span class="math display">\[G_t^{\pi/\mu}=\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}\frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})}...\frac{\pi(A_T|S_T)}{\mu(A_T|S_T)}G_t\]</span> Update value towards <em>corrected</em> return: <span class="math display">\[V(S_t)\leftarrow V(S_t)+\alpha (\color{red}{G_t^{\pi/\mu}}-V(S_t))\]</span> But it has two major problems:</p><ul><li>Cannot use if <span class="math inline">\(\mu\)</span> is zero when <span class="math inline">\(\pi\)</span> is non-zero</li><li>Importance sampling can dramatically increase variance, so it is useless in practice</li></ul><p><strong>Off-Policy TD</strong></p><p>Use TD targets generated from <span class="math inline">\(\mu\)</span> to evaluate <span class="math inline">\(\pi\)</span>. Weight TD target <span class="math inline">\(R+\gamma V(S&#39;)\)</span> by importance sampling. Only need a single importance sampling correction: <span class="math display">\[V(S_t)\leftarrow V(S_t)+\alpha \left(\color{red}{\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1}+\gamma V(S_{t+1}))}-V(S_t)\right)\]</span> This algorithm has much lower variance than Monte-Carlo importance sampling because policies only need to be similar over a single step.</p><h3><span id="q-learning">Q-Learning</span></h3><p>We now consider off-policy learning of action-values <span class="math inline">\(Q(s, a)\)</span>. The benefit of it is no importance sampling is required.</p><p>The next action is chosen using <strong>behaviour</strong> policy <span class="math inline">\(A_{t+1}\sim\mu(\cdot|S_t)\)</span>. But we consider <strong>alternative</strong> successor action <span class="math inline">\(A&#39;\sim \pi(\cdot|S_t)\)</span>. And update <span class="math inline">\(Q(S_t, A_t)\)</span> towards value of alternative action <span class="math display">\[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1}+\gamma Q(S_{t+1}, \color{red}{A&#39;})-Q(S_t, A_t))\]</span> We now allow both behaviour and target policies to <strong>improve</strong>.</p><p>The <strong>target</strong> policy <span class="math inline">\(\pi\)</span> is <span class="math inline">\(\color{red}{\mbox{greedy}}\)</span> w.r.t <span class="math inline">\(Q(s, a)\)</span>: <span class="math display">\[\pi(S_{t+1})=\arg\max_{a&#39;}Q(S_{t+1}, a&#39;)\]</span> The <strong>behaviour</strong> policy <span class="math inline">\(\mu\)</span> is e.g. <span class="math inline">\(\color{red}{\epsilon \mbox{-greedy}}\)</span> w.r.t. <span class="math inline">\(Q(s,a)\)</span>.</p><p>The <strong>Q-learning</strong> target then simplifies: <span class="math display">\[\begin{align}\mbox{Q-learning Target} &amp;= R_{t+1}+\gamma Q(S_{t+1}, A&#39;) \\&amp; = R_{t+1}+\gamma Q(S_{t+1}, \arg\max_{a&#39;}Q(S_{t+1}, a&#39;)) \\&amp;= R_{t+1}+\max_{a&#39;}\gamma Q(S_{t+1}, a&#39;)\end{align}\]</span> So the Q-learning control algorithm is</p><p><img src="/images/qlalg.png"></p><p>Of course, the Q-learning control still converges to the optimal action-value function, <span class="math inline">\(Q(s, a)\rightarrow q_*(s,a)\)</span>.</p><p><img src="/images/qlcode.png"></p><h2><span id="summary">Summary</span></h2><p><strong>Relationship Between DP and TD</strong></p><p><img src="/images/rbtddp.png"></p><p><img src="/images/rbtddp2.png"></p><p>In a word, TD backup can be seen as the sample of corresponding DP backup. This lecture introduces model-free control which is optimise the value function of an unknown MDP with on-policy and off-policy methods. Next lecture will introduce function approximation which is easy to scale up and can be applied into big MDPs.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Last lecture:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model-free prediction&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Estimate&lt;/em&gt; the value function of an &lt;em&gt;unknown&lt;/em&gt; MDP&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This lecture:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model-free control&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimise&lt;/strong&gt; the value function of an unknown MDP&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Monte-Carlo Control" scheme="http://www.52coding.com.cn/tags/Monte-Carlo-Control/"/>
    
      <category term="Sarsa" scheme="http://www.52coding.com.cn/tags/Sarsa/"/>
    
      <category term="Q-learning" scheme="http://www.52coding.com.cn/tags/Q-learning/"/>
    
  </entry>
  
  <entry>
    <title>RL - Model-Free Prediction</title>
    <link href="http://www.52coding.com.cn/2017/12/16/RL%20-%20Model-Free%20Prediction/"/>
    <id>http://www.52coding.com.cn/2017/12/16/RL - Model-Free Prediction/</id>
    <published>2017-12-16T07:00:09.000Z</published>
    <updated>2018-11-06T03:47:43.363Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="introduction">Introduction</span></h2><p>Last lecture, David taught us how to solve a <em>known</em> MDP, which is <em>planning by dynamic programming</em>. In this lecture, we will learn how to estimate the value function of an <strong>unknown</strong> MDP, which is <em>model-free prediction</em>. And in the next lecture, we will <em>optimise</em> the value function of an unknown MDP.</p><a id="more"></a><p>In summary:</p><ul><li>Planning by dynamic programming<ul><li>Solve a <em>known MDP</em></li></ul></li><li><strong>Model-Free prediction</strong><ul><li>Estimate the value function of an <em>unknown</em> MDP</li></ul></li><li>Model-Free control<ul><li>Optimise the value function of an <em>unknown</em> MDP</li></ul></li></ul><p>We have two major methods to estimate the value function of an unknown MDP:</p><ul><li>Monte-Carlo Learning</li><li>Temporal-Difference Learning</li></ul><p>We will introduce the two methods and combine them to a general method.</p><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#monte-carlo-learning">Monte-Carlo Learning</a></li><li><a href="#temporal-difference-learning">Temporal-Difference Learning</a><ul><li><a href="#unified-view">Unified View</a></li></ul></li><li><a href="#tdlambda">TD(<span class="math inline">\(\lambda\)</span>)</a><ul><li><a href="#forward-view-tdlambda">Forward View TD(<span class="math inline">\(\lambda\)</span>)</a></li><li><a href="#backward-view-tdlambda">Backward View TD(<span class="math inline">\(\lambda\)</span>)</a></li><li><a href="#relationship-between-forward-and-backward-td">Relationship Between Forward and Backward TD</a></li></ul></li></ul><!-- tocstop --><h2><span id="monte-carlo-learning">Monte-Carlo Learning</span></h2><p>MC (Monte-Carlo) methods learn directly from <strong>episodes of experience</strong>, which means:</p><ul><li>MC is <em>model-free</em>: no knowledge of MDP transitions / rewards</li><li>MC learns from complete episodes: no bootstrapping</li><li>MC uses the simplest possible idea: value = mean return</li><li>Can only apply MC to <em>episodic</em> MDPs: all episodes must terminate</li></ul><p><strong>Goal</strong>: learn <span class="math inline">\(v_\pi\)</span> from episodes of experience under policy <span class="math inline">\(\pi\)</span> <span class="math display">\[S_1, A_1, R_2, ..., S_k \sim \pi\]</span> Recall that the <strong>return</strong> is the total discounted reward: <span class="math display">\[G_t = R_{t+1}+\gamma R_{t+2} + ... + \gamma^{T-1}R_T\]</span> Recall that the <strong>value function</strong> is the expected return: <span class="math display">\[v_\pi(s) = \mathbb{E}_\pi[G_t|S_t=s]\]</span> Monte-Carlo policy evaluation uses <em>empirical mean</em> return instead of <em>expected</em> return.</p><p><strong>First-Visit Monte-Carlo Policy Evaluation</strong></p><p>To evaluate state <span class="math inline">\(s\)</span>, the <strong>first</strong> time-step <span class="math inline">\(t\)</span> that state <span class="math inline">\(s\)</span> is visited in <strong>an episode</strong>:</p><ul><li>Increment counter <span class="math inline">\(N(s) \leftarrow N(s) + 1\)</span></li><li>Increment total return <span class="math inline">\(S(s) \leftarrow S(s) + G_t\)</span></li></ul><p>Value is estimated by mean return <span class="math inline">\(V(s) = S(s) / N(s)\)</span>, by <em>law of large numbers</em>, <span class="math inline">\(V(s) \rightarrow v_\pi(s)\)</span> as <span class="math inline">\(N(s) \rightarrow \infty\)</span>.</p><p><strong>Every-Visit Monte-Carlo Policy Evaluation</strong></p><p>To evaluate state <span class="math inline">\(s\)</span>, <strong>every</strong> time-step <span class="math inline">\(t\)</span> that state <span class="math inline">\(s\)</span> is visited in <strong>an episode</strong>:</p><ul><li>Increment counter <span class="math inline">\(N(s) \leftarrow N(s) + 1\)</span></li><li>Increment total return <span class="math inline">\(S(s) \leftarrow S(s) + G_t\)</span></li></ul><p>Value is estimated by mean return <span class="math inline">\(V(s) = S(s) / N(s)\)</span>. Again, by <em>law of large numbers</em>, <span class="math inline">\(V(s) \rightarrow v_\pi(s)\)</span> as <span class="math inline">\(N(s) \rightarrow \infty\)</span>.</p><p><strong>Blackjack Example</strong></p><p>Please refer to https://www.wikiwand.com/en/Blackjack to learn the rule of <em>Blackjack</em>.</p><p>If we build an RL agent to play blackjack, the <strong>states</strong> would have 3-dimension:</p><ul><li>Current sum (12 - 21)<ul><li>We just consider this range because if the current sum is lower than 12, we will always take another card.</li></ul></li><li>Dealer's showing card (ace - 10)</li><li>Do I have a &quot;useable&quot; ace? (yes - no)</li></ul><p>So there would be 200 different states.</p><p>The actions are:</p><ul><li><strong>Stick</strong>: stop receiving cards and terminate</li><li><strong>Twist</strong>: take another card (no replacement)</li></ul><p>And <em>reward</em> for action</p><ul><li><strong>Stick</strong><ul><li><span class="math inline">\(+1\)</span> if sum of cards <span class="math inline">\(&gt;\)</span> sum of dealer cards</li><li><span class="math inline">\(0\)</span> if sum of cards <span class="math inline">\(=\)</span> sum of dealer cards</li><li><span class="math inline">\(-1\)</span> if sum of cards <span class="math inline">\(&lt;\)</span> sum of dealer cards</li></ul></li><li><strong>Twist</strong><ul><li><span class="math inline">\(-1\)</span> if sum of cards <span class="math inline">\(&gt; 21\)</span> and terminate</li><li><span class="math inline">\(0\)</span> otherwise</li></ul></li></ul><p>Transitions: automatically <em>twist</em> if sum of cards &lt; 12.</p><p>Policy: <strong>stick</strong> if sum of cards <span class="math inline">\(≥ 20\)</span>, otherwise <strong>twist</strong>.</p><p><img src="/images/backjack.png"></p><p>In the above diagrams, the height represents the value function of that point. Since it's a simple policy, the value funtion achieves high value only if player sum is higher than 20.</p><p><strong>Incremental Mean</strong></p><p>The mean <span class="math inline">\(\mu_1, \mu_2, …\)</span> of a sequence <span class="math inline">\(x_1, x_2, …\)</span> can be computed incrementally, <span class="math display">\[\begin{align}\mu_k &amp; = \frac{1}{k}\sum^k_{j=1}x_j \\&amp; = \frac{1}{k}(x_k+\sum^{k-1}_{j=1}x_j) \\&amp;= \frac{1}{k}(x_k+(k-1)\mu_{k-1}) \\&amp;= \mu_{k-1}+\frac{1}{k}(x_k-\mu_{k-1}) \\\end{align}\]</span> which means the current mean equals to previous mean plus some error. The error is <span class="math inline">\(x_k - \mu_{k-1}\)</span> and the step-size is <span class="math inline">\(\frac{1}{k}\)</span>, which is dynamic.</p><p><strong>Incremental Monte-Carlo Updates</strong></p><p>Update <span class="math inline">\(V(s)\)</span> incrementally after episode <span class="math inline">\(S_1, A_1, R_2, …, S_T\)</span>, for each state <span class="math inline">\(S_t\)</span> with return <span class="math inline">\(G_t\)</span>, <span class="math display">\[N(S_t) \leftarrow N(S_t) + 1\]</span></p><p><span class="math display">\[V(S_t)\leftarrow V(S_t)+\frac{1}{N(S_t)}(G_t-V(S_t))\]</span></p><p>In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes, <span class="math display">\[V(S_t)\leftarrow V(S_t) + \alpha(G_t-V(S_t))\]</span> So, that's the part for Monte-Carlo learning. It's a very simple idea: you run out an episode, look the complete return and update the mean value of the sample return for each state you have visited.</p><h2><span id="temporal-difference-learning">Temporal-Difference Learning</span></h2><p>Temporal-Difference (TD) methods learn directly from episodes of experiences, which means</p><ul><li>TD is <em>model-free</em>: no knowledge of MDP transitions / rewards</li><li>TD learns from <strong>incomplete</strong> episodes, by <em>bootstrapping</em>. (A major difference from MC method)</li><li>TD updates a guess towards a guess.</li></ul><p>Goal: learn <span class="math inline">\(v_\pi\)</span> online from experience under policy <span class="math inline">\(\pi\)</span>.</p><p><em>Incremental every-visit Monte-Carlo</em></p><ul><li>Update value <span class="math inline">\(V(S_t)\)</span> toward actual return <span class="math inline">\(\color{Red}{G_t}\)</span> <span class="math display">\[V(S_t)\leftarrow V(S_t)+\alpha (\color{Red}{G_t}-V(S_t))\]</span></li></ul><p>Simplest temporal-difference learning algorithm: <strong>TD(0)</strong></p><ul><li><p>Update value <span class="math inline">\(V(S_t)\)</span> towards <em>estimated</em> return <span class="math inline">\({\color{Red}{R_{t+1}+\gamma V(S_{t+1})}}\)</span> <span class="math display">\[V(S_t)\leftarrow V(S_t)+ \alpha ({\color{Red}{R_{t+1}+\gamma V(S_{t+1})}}-V(S_t))\]</span></p></li><li><p><span class="math inline">\(R_{t+1}+\gamma V(S_{t+1})​\)</span> is called the <em>TD target</em>;</p></li><li><p><span class="math inline">\(\delta = R_{t+1}+\gamma V(S_{t+1})-V(S_t)\)</span> is called the <em>TD error</em>.</p></li></ul><p>Let's see a concret <strong>driving home example</strong>.</p><p><img src="/images/dheg.png"></p><p>The <em>Elapsed Time</em> shows the actual time that has spent, the <em>Predicted Time to Go</em> represents the predicted time to arrive home from current state, and the <em>Predicted Total Time</em> means the predicted time to arrive home from leaving office.</p><p><strong>Advantages and Disadvantages of MC vs. TD</strong></p><p>TD can learn <em>before</em> knowing the final outcome</p><ul><li>TD can learn online after every step</li><li>MC must wait until end of episode before return is known</li></ul><p>TD can learn <em>without</em> the final outcome</p><ul><li>TD can learn from incomplete sequences</li><li>MC can only learn from complete sequences</li><li>TD works in continuing (non-terminating) environments</li><li>MC only works for episodic (terminating) environments</li></ul><p>MC has high variance, zero bias</p><ul><li>Good convergence properties (even with function approximation)</li><li>Not very sensitive to initial value</li><li>Very simple to understand and use</li></ul><p>TD has low variance, some bias</p><ul><li>Usually more efficient than MC</li><li>TD(0) converges to <span class="math inline">\(v_\pi(s)\)</span> (but not always with function approximation)</li><li>More sensitive to initial value</li></ul><p><strong>Bias/Variance Trade-Off</strong></p><p>Return <span class="math inline">\(G_t = R_{t+1} + \gamma R_{t+2}+…+\gamma^{T-1}R_T\)</span> is <strong>unbiased</strong> estimate of <span class="math inline">\(v_\pi(S_t)\)</span>.</p><p>True TD target <span class="math inline">\(R_{t+1}+\gamma v_\pi(S_{t+1})\)</span> is <strong>unbiased</strong> estimate of <span class="math inline">\(v_\pi(S_t)\)</span></p><blockquote><p>Explanation of bias and variance:</p><ul><li>The <a href="https://www.wikiwand.com/en/Bias_of_an_estimator" target="_blank" rel="noopener">bias of an estimator</a> is the difference between an estimator's expected value and the true value of the parameter being estimated.</li><li>A <strong>variance</strong> value of zero indicates that all values within a set of numbers are identical; all variances that are non-zero will be positive numbers. A large variance indicates that numbers in the set are far from the mean and each other, while a small variance indicates the opposite. Read more: <a href="https://www.investopedia.com/terms/v/variance.asp#ixzz51J8RTueh" target="_blank" rel="noopener">Variance</a></li></ul></blockquote><p>While TD target <span class="math inline">\(R_{t+1}+\gamma V(S_{t+1})\)</span> is <strong>biased</strong> estimate of <span class="math inline">\(v_\pi(S_t)\)</span>.</p><p>However, TD target is much lower <em>variance</em> than the return, since</p><ul><li>Return depends on <em>many</em> random actions, transitions, rewards</li><li>TD target depends on <em>one</em> random actions, transition, reward</li></ul><p><strong>Random Walk Example</strong></p><p><img src="/images/rweg.png"></p><p>There are several states on a street, the black rectangles are terminate states. Each transition has 0.5 probability and the reward is marked on the line. The question is what is the value function of each state?</p><p>Using <em>TD</em> to solve the problem:</p><p><img src="/images/rwtd.png"></p><p>The x-axis represents each state, y-axis represent the estimated value. Each line represents the result of TD algorithm that run different episodes. We can see, at the begining, all states have initial value <span class="math inline">\(0.5\)</span>. After 100 episodes, the line converges to diagonal, which is the true values.</p><p>Using <em>MC</em> to solve the problem:</p><p><img src="/images/rwmc.png"></p><p>The x-axis represents the number of episodes that algorithm takes. The y-axis shows the error of the algorithm. The black lines shows using MC methods with different step-size, while the grey lines below represents using TD methods with different step-size. We can see TD methods are more efficient than MC methods.</p><p><strong>Batch MC and TD</strong></p><p>We know that MC and TD converge: <span class="math inline">\(V(s) \rightarrow v_\pi(s)\)</span> as experience <span class="math inline">\(\rightarrow \infty\)</span>. But what about batch solution for finite experience? If we <strong>repeatly</strong> train some <em>finite</em> sample episodes with MC and TD respectively, do the two algorithms give <strong>same</strong> result?</p><p><em>AB Example</em></p><p>To get more intuition, let's see the <em>AB</em> example.</p><p>There are two states in a MDP, <span class="math inline">\(A, B\)</span> with no discounting. And we have 8 episodes of experience:</p><ul><li>A, 0, B, 0</li><li>B, 1</li><li>B, 1</li><li>B, 1</li><li>B, 1</li><li>B, 1</li><li>B, 1</li><li>B, 0</li></ul><p>For example, the first episode means we in state <span class="math inline">\(A\)</span> and get <span class="math inline">\(0\)</span> reward, then transit to state <span class="math inline">\(B\)</span> getting <span class="math inline">\(0\)</span> reward, and then terminate.</p><p>So, What is <span class="math inline">\(V(A), V(B)\)</span> ?</p><p>First, let's consider <span class="math inline">\(V(B)\)</span>. <span class="math inline">\(B\)</span> state shows 8 times and 6 of them get reward <span class="math inline">\(1\)</span>, 2 of them get reward <span class="math inline">\(0\)</span>. So <span class="math inline">\(V(B) = \frac{6}{8} = 0.75\)</span> according to TD and MC.</p><p>However, if we consider <span class="math inline">\(V(A)\)</span>, MC method will give <span class="math inline">\(V(A) = 0\)</span>, since <span class="math inline">\(A\)</span> just shows in one episode and the reward of that episode is <span class="math inline">\(0\)</span>. TD method will give <span class="math inline">\(V(A) = 0 + V(B) = 0.75\)</span>.</p><p>The MDP of these experiences can be illustrated as</p><p><img src="/images/abmdp.png"></p><p><strong>Certainty Equivalence</strong></p><p>As we show above,</p><ul><li><p><strong>MC</strong> converges to solution with <strong>minimum mean-squared error</strong></p><ul><li><p>Best fit to the <strong>observed returns</strong> <span class="math display">\[\sum^K_{k=1}\sum^{T_k}_{t=1}(G^k_t-V(s^k_t))^2\]</span></p></li><li><p>In the AB example, <span class="math inline">\(V(A) = 0\)</span></p></li></ul></li><li><p><strong>TD(0)</strong> converges to solution of <strong>max likelihood Markov model</strong></p><ul><li><p>Solution to the <strong>MDP <span class="math inline">\(&lt;\mathcal{S, A, P, R, }\gamma&gt;\)</span> that best fits the data</strong></p><p><img src="/images/cemath.png"></p><p>(First, count the transitions. Then compute rewards.)</p></li><li><p>In the AB example, <span class="math inline">\(V(A) = 0.75\)</span></p></li></ul></li></ul><p><strong>Advantages and Disadvantages of MC vs. TD (2)</strong></p><ul><li>TD exploits <strong>Markov property</strong><ul><li>Usually more efficient in Markov environments</li></ul></li><li>MC does <strong>not</strong> exploit Markov property<ul><li>Usually more effective in non-Markov environments</li></ul></li></ul><h3><span id="unified-view">Unified View</span></h3><p><strong>Monte-Carlo Backup</strong></p><p><img src="/images/mcbackup.png"></p><p>We start from <span class="math inline">\(S_t\)</span> to look-ahead and build a look-ahead tree. What Monte-Carlo do is to sample a episode until it terminates and use the episode to update the value of state <span class="math inline">\(S_t\)</span>.</p><p><strong>Temporal-Difference Backup</strong></p><p><img src="/images/tdbackup.png"></p><p>On the contrary, TD backup just sample one-step ahead and use the value of <span class="math inline">\(S_{t+1}\)</span> to update <span class="math inline">\(S_t\)</span>.</p><p><strong>Dynamic Programming Backup</strong></p><p><img src="/images/dpbackup.png"></p><p>In dynamic programming backup, we do not sample. Since we know the environment, we look all possible one-step ahead and weighted them to update the value of <span class="math inline">\(S_t\)</span>.</p><p><strong>Bootstrapping and Sampling</strong></p><ul><li><strong>Bootstrapping</strong>: update involves an estimate<ul><li>MC does not bootstrap</li><li>DP bootstraps</li><li>TD bootstraps</li></ul></li><li><strong>Sampling</strong>: update samples an expectation<ul><li>MC samples</li><li>DP does not sample</li><li>TD samples</li></ul></li></ul><p><strong>Unified View of Reinforcement Learning</strong></p><p><img src="/images/uvrl.png"></p><h2><span id="tdlambda">TD(<span class="math inline">\(\lambda\)</span>)</span></h2><p>Let TD target look <span class="math inline">\(n\)</span> steps into the future,</p><figure><img src="/images/tdlam.png" alt="ds"><figcaption>ds</figcaption></figure><p>Consider the following <span class="math inline">\(n\)</span>-step returns for <span class="math inline">\(n = 1, 2, …, \infty\)</span>:</p><p><img src="/images/tdlamret.png"></p><p>Define the <span class="math inline">\(n\)</span>-step return <span class="math display">\[G_t^{(n)} = R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^n V(S_{t+n})\]</span> <span class="math inline">\(n\)</span>-step temporal-difference learning: <span class="math display">\[V(S_t)\leftarrow V(S_t)+\alpha (G_t^{(n)}-V(S_t))\]</span> We know that <span class="math inline">\(n \in [1, \infty)\)</span>, but which <span class="math inline">\(n\)</span> is the best?</p><p>There are some experiments about that:</p><p><img src="/images/rmn.png"></p><p>So, you can see that the optimal <span class="math inline">\(n\)</span> changes with on-line learning and off-line leanring. If the MDP changes, the best <span class="math inline">\(n\)</span> also changes. Is there a robust algorithm to fit any different situation?</p><h3><span id="forward-view-tdlambda">Forward View TD(<span class="math inline">\(\lambda\)</span>)</span></h3><p><strong>Averaging n-step Returns</strong></p><p>We can average n-step returns over different <span class="math inline">\(n\)</span>, e.g. average the 2-step and 4-step returns: <span class="math display">\[\frac{1}{2}G^{(2)}+\frac{1}{2}G^{(4)}\]</span> But can we efficiently combine information from all time-steps?</p><p>The answer is yes.</p><p><strong><span class="math inline">\(\lambda\)</span>-return</strong></p><p><img src="/images/tdlambda.png"></p><p>The <span class="math inline">\(\lambda\)</span>-return <span class="math inline">\(G_t^{\lambda}\)</span> combines all n-step returns <span class="math inline">\(G_t^{(n)}\)</span> using weight <span class="math inline">\((1-\lambda)\lambda^{n-1}\)</span>: <span class="math display">\[G_t^\lambda = (1-\lambda)\sum^\infty_{n=1}\lambda^{n-1}G_t^{(n)}\]</span> <strong>Forward-view</strong> <span class="math inline">\(TD(\lambda)\)</span>, <span class="math display">\[V(S_t) \leftarrow V(S_t) + \alpha (G_t^\lambda-V(S_t))\]</span> <img src="/images/tdgeo.png"></p><p>We can see the weight decay geometrically and the weights sum to 1.</p><p>The reason we use geometrical decay rather than other weight because it's efficient to compute, we can compute TD(<span class="math inline">\(\lambda\)</span>) as efficient as TD(0).</p><p><img src="/images/forwardtd.png"></p><p><strong>Forward-view</strong> <span class="math inline">\(TD(\lambda)\)</span></p><ul><li>Updates value function towards the <span class="math inline">\(\lambda\)</span>-return</li><li>Looks into the future to compute <span class="math inline">\(G_t^\lambda\)</span></li><li>Like MC, can only be computed from <strong>complete episodes</strong></li></ul><p><img src="/images/fortdlam.png"></p><h3><span id="backward-view-tdlambda">Backward View TD(<span class="math inline">\(\lambda\)</span>)</span></h3><p><strong>Eligibility Traces</strong></p><p><img src="/images/bellexe.png"></p><p>Recall the <a href="https://www.52coding.com.cn/index.php?/Articles/single/69#header-n50">rat example</a> in lecture 1, credit assignment problem: did bell or light cause shock?</p><ul><li><strong>Frequency heuristic</strong>: assign credit to most frequent states</li><li><strong>Recency heuristic</strong>: assign credit to most recent states</li></ul><p><em>Eligibility traces</em> combine both heuristics.</p><p><img src="/images/egt.png"></p><p>If visit state <span class="math inline">\(s\)</span>, <span class="math inline">\(E_t(s)\)</span> plus <span class="math inline">\(1\)</span>; otherwise <span class="math inline">\(E_t(s)\)</span> decay exponentially.</p><p><strong>Backward View TD(<span class="math inline">\(\lambda\)</span>)</strong></p><ul><li>Keep an eligibility trace for every state <span class="math inline">\(s\)</span></li><li>Update value <span class="math inline">\(V(s)\)</span> for every state <span class="math inline">\(s\)</span> in proportion to TD-error <span class="math inline">\(\delta_t\)</span> and eligibility trace <span class="math inline">\(E_t(s)\)</span></li></ul><p><span class="math display">\[\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)\]</span></p><p><span class="math display">\[V(s)\leftarrow V(s)+\alpha \delta_tE_t(s)\]</span></p><p><img src="/images/bvtdlam.png"></p><p>When <span class="math inline">\(\lambda = 0\)</span>, only current state is updated, which is exactly equivalent to TD(0) update: <span class="math display">\[E_t(s) = 1(S_t = s)\]</span></p><p><span class="math display">\[V(s)\leftarrow V(s)+\alpha\delta_tE_t(s) = V(S_t)+\alpha\delta_t\]</span></p><p>When <span class="math inline">\(\lambda = 1\)</span>, credit is deferred until end of episode, total update for TD(1) is the same as total update for MC.</p><h3><span id="relationship-between-forward-and-backward-td">Relationship Between Forward and Backward TD</span></h3><blockquote><p><strong>Theorem</strong></p><p>The sum of offline updates is identical for forward-view and backward-view TD(<span class="math inline">\(\lambda\)</span>) <span class="math display">\[\sum^T_{t=1}\alpha\delta_tE_t(s)=\sum^T_{t=1}\alpha(G_t^\lambda-V(S_t))1(S_t=s)\]</span></p></blockquote><p><strong>MC and TD(1)</strong></p><p>Consider an episode where <span class="math inline">\(s\)</span> is visited once at time-step <span class="math inline">\(k\)</span>, TD(1) eligiblity trace discounts time since visit, <span class="math display">\[E_t(s) = \gamma E_{t-1}(s)+1(S_t = s) = \begin{cases} 0,  &amp; \mbox{if }t&lt;k \\\gamma^{t-k}, &amp; \mbox{if }t≥k\end{cases}\]</span> TD(1) updates accumulate error <em>online</em> <span class="math display">\[\sum^{T-1}_{t=1}\alpha\delta_tE_t(s)=\alpha\sum^{T-1}_{t=k}\gamma^{t-k}\delta_t\]</span> By end of episode it accumulates total error <span class="math display">\[\begin{align}\mbox{TD(1) Error}&amp;= \delta_k+\gamma\delta_{k+1}+\gamma^2\delta_{k+2}+...+\gamma^{T-1-k}\delta_{T-1} \\&amp; = R_{t+1}+\gamma V(S_{t+1}) -V(S_t) \\&amp;+ \gamma R_{t+2}+\gamma^2V(S_{t+2}) - \gamma V(S_{t+1})\\&amp;+ \gamma^2 R_{t+3}+\gamma^3V(S_{t+3}) - \gamma^2 V(S_{t+2})\\&amp;+\ ... \\&amp;+ \gamma^{T-1-t}R_T+\gamma^{T-t}V(S_T)-\gamma^{T-1-t}V(S_{T-1})\\&amp;= R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3} ... + \gamma^{T-1-t}R_T-V(S_t)\\&amp;= G_t-V(S_t)\\&amp;= \mbox{MC Error}\end{align}\]</span> TD(1) is roughly equivalent to every-visit Monte-Carlo, error is accumulated online, step-by-step.</p><p>If value function is only updated offline at end of episode, then total update is exactly the same as MC.</p><p><strong>Forward and Backward Equivalence</strong></p><p>For general <span class="math inline">\(\lambda\)</span>, TD errors also telescope to <span class="math inline">\(\lambda\)</span>-error, <span class="math inline">\(G_t^\lambda-V(S_t)\)</span></p><p><img src="/images/teltdlam.png"></p><p>Consider an episode where <span class="math inline">\(s\)</span> is visited once at time-step <span class="math inline">\(k\)</span>, TD(<span class="math inline">\(\lambda\)</span>) eligibility trace discounts time since visit, <span class="math display">\[E_t(s) = \gamma\lambda E_{t-1}(s)+1(S_t = s) = \begin{cases} 0,  &amp; \mbox{if }t&lt;k \\(\gamma\lambda)^{t-k}, &amp; \mbox{if }t≥k\end{cases}\]</span> Backward TD(<span class="math inline">\(\lambda\)</span>) updates accumulate error <em>online</em> <span class="math display">\[\sum^{T-1}_{t=1}\alpha\delta_tE_t(s)=\alpha\sum^{T-1}_{t=k}(\gamma\lambda)^{t-k}\delta_t = \alpha(G_k^\lambda-V(S_k))\]</span> By end of episode it accumulates total error for <span class="math inline">\(\lambda\)</span>-return.</p><p>For multiple visits to <span class="math inline">\(s\)</span>, <span class="math inline">\(E_t(s)\)</span> accumulates many errors.</p><p><strong>Offline</strong> Updates</p><ul><li>Updates are accumulated within episode but applied in batch at the end of episode</li></ul><p><strong>Online</strong> Updates</p><ul><li>TD(<span class="math inline">\(\lambda\)</span>) updates are applied online at each step within episode, forward and backward view TD(<span class="math inline">\(\lambda\)</span>) are slightly different.</li></ul><p>In summary,</p><p><img src="/images/tdsum.png"></p><ul><li>Forward view provides <strong>theory</strong></li><li>Backward view provids <strong>mechanism</strong><ul><li>update online, every step, from incomplete sequences</li></ul></li></ul><p>This lecture just talks about how to evaluate a policy given an unknown MDP. Next lecture will introduce Model-free Control.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Last lecture, David taught us how to solve a &lt;em&gt;known&lt;/em&gt; MDP, which is &lt;em&gt;planning by dynamic programming&lt;/em&gt;. In this lecture, we will learn how to estimate the value function of an &lt;strong&gt;unknown&lt;/strong&gt; MDP, which is &lt;em&gt;model-free prediction&lt;/em&gt;. And in the next lecture, we will &lt;em&gt;optimise&lt;/em&gt; the value function of an unknown MDP.&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Model-Free" scheme="http://www.52coding.com.cn/tags/Model-Free/"/>
    
      <category term="Monte-Carlo Learning" scheme="http://www.52coding.com.cn/tags/Monte-Carlo-Learning/"/>
    
      <category term="TD" scheme="http://www.52coding.com.cn/tags/TD/"/>
    
  </entry>
  
  <entry>
    <title>RL - Planning by Dynamic Programming</title>
    <link href="http://www.52coding.com.cn/2017/12/07/RL%20-%20Planning%20by%20Dynamic%20Programming/"/>
    <id>http://www.52coding.com.cn/2017/12/07/RL - Planning by Dynamic Programming/</id>
    <published>2017-12-07T07:48:19.000Z</published>
    <updated>2018-11-06T03:47:48.747Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#introduction">Introduction</a></li><li><a href="#policy-evaluation">Policy Evaluation</a></li><li><a href="#policy-iteration">Policy Iteration</a></li><li><a href="#value-iteration">Value Iteration</a></li><li><a href="#extentions-to-dynamic-programming">Extentions to Dynamic Programming</a></li><li><a href="#contraction-mapping">Contraction Mapping</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="introduction">Introduction</span></h2><p><strong>What is Dynamic Programming?</strong></p><p><strong>Dynamic</strong>: sequential or temporal component to the problem <strong>Programming</strong>: optimising a &quot;program&quot;, i.e. a policy</p><ul><li>c.f. linear programming</li></ul><p>So, Dynamic Programming is a method for solving complex problems by breaking them down into <strong>subproblems</strong>.</p><ul><li>Solve the subproblems</li><li>Combine solutions to subproblems</li></ul><p>Dynamic Programming is a very general solution method for problems which have two properties:</p><ul><li><strong>Optimal substructure</strong><ul><li><em>Principle of optimality applies</em></li><li>Optimal solution can be decomposed into subproblems</li></ul></li><li><strong>Overlapping subproblems</strong><ul><li>Subproblems recur many times</li><li>Solution can be cached and reused</li></ul></li></ul><p><strong>Markov decision processes</strong> satisfy both properties:</p><ul><li><strong>Bellman euqtion</strong> gives recursive decomposition</li><li><strong>Value function</strong> stores and reuses solutions</li></ul><p><strong>Planning by Dynamic Programming</strong></p><p><strong>Planning</strong> means dynamic programming assumes full knowledge of the MDP.</p><ul><li>For prediction (Policy Evaluation):<ul><li>Input: MDP<span class="math inline">\(&lt;S,A,P,R,\gamma&gt;\)</span> and policy <span class="math inline">\(\pi\)</span></li><li>Output: value function <span class="math inline">\(v_{\pi}\)</span></li></ul></li><li>For <strong>control</strong>:<ul><li>Input: MDP<span class="math inline">\(&lt;S,A,P,R,\gamma&gt;\)</span></li><li>Output: optimal value function <span class="math inline">\(v_*\)</span> and optimal policy <span class="math inline">\(\pi_*\)</span></li></ul></li></ul><p>We first learn how to evaluate a policy and then put it into a loop to find the optimal policy.</p><h2><span id="policy-evaluation">Policy Evaluation</span></h2><ul><li><p>Problem: evaluate a given policy <span class="math inline">\(\pi\)</span></p></li><li><p>Solution: iterative application of <strong>Bellman expectation equation</strong></p></li><li><p><span class="math inline">\(v_1\)</span> -&gt; <span class="math inline">\(v_2\)</span> -&gt; <span class="math inline">\(v_3\)</span> -&gt; … -&gt; <span class="math inline">\(v_\pi\)</span></p></li><li><p><em>Synchronous</em> backups</p><ul><li><p>At each iteration <span class="math inline">\(k+1\)</span></p></li><li><p>For all states <span class="math inline">\(s \in S\)</span></p></li><li><p>Update <span class="math inline">\(v_{k+1}(s)\)</span> from <span class="math inline">\(v_k(s&#39;)\)</span>, where <span class="math inline">\(s&#39;\)</span> is a successor state of <span class="math inline">\(s\)</span></p><p><img src="/images/vpi2.png"> <span class="math display">\[v_{k+1}(s)=\sum_{a\in\mathcal{A}}\pi(a|s)(\mathcal{R}^a_s+\gamma\sum_{s&#39;\in\mathcal{S}}P^a_{ss&#39;}v_k(s&#39;))\]</span></p><p><span class="math display">\[v^{k+1}=\mathcal{R}^\pi+\gamma \mathcal{P}^\pi v^k\]</span></p></li></ul></li></ul><p><em>Example</em>: Evaluating a Random Policy in the Small Gridworld</p><p><img src="/images/grid.png"></p><ul><li><p>Actions are move North/East/South/West for one grid.</p></li><li><p>Undiscounted episodic MDP (<span class="math inline">\(\gamma = 1\)</span>)</p></li><li><p>Nontermial states <span class="math inline">\(1, …, 14\)</span></p></li><li><p>One terminal State (shown twice as shaded squares)</p></li><li><p>Reward is <span class="math inline">\(-1\)</span> until the terminal state is reahed</p></li><li><p>Agent follows uniform random policy <span class="math display">\[\pi(n|\cdot)=\pi(e|\cdot)=\pi(s|\cdot)=\pi(w|\cdot) = 0.25\]</span></p></li></ul><p>Let's use dynamic programming to solve the MDP.</p><p><img src="/images/ipe1.png"></p><p><img src="/images/ipe2.png"></p><p>The grids on the left show the value function of each state, the update rule shown by the illustration. Finally, it converges to the true value function of the policy. It basically tell us <em>if we take the random walk under the policy, how much reward on average we will get when we reach the terminal state</em>.</p><p>The right-hand column shows how to find better policy with respect to the value funtions.</p><h2><span id="policy-iteration">Policy Iteration</span></h2><p>How to improve a Policy</p><ul><li><p>Given a policy <span class="math inline">\(\pi\)</span></p><ul><li><p><strong>Evaluate</strong> the policy <span class="math inline">\(\pi\)</span> <span class="math display">\[v_\pi(s) = E[R_{t+1}+\gamma R_{t+2} + ... | S_t = s]\]</span></p></li><li><p><strong>Improve</strong> the policy by acting <em>greedily</em> with respect to <span class="math inline">\(v_\pi\)</span> <span class="math display">\[\pi&#39;=greddy(v_\pi)\]</span></p></li></ul></li><li><p>In general, need more iterations of improvement / evaluation</p></li><li><p>But this process of <em>policy iteration</em> always converges to <span class="math inline">\(\pi_*\)</span></p></li></ul><p><img src="/images/pi.png"></p><p><strong>Demonstration</strong></p><p>Consider a deterministic policy <span class="math inline">\(a = \pi(s)\)</span>, we can improve the policy by acting greedily <span class="math display">\[\pi&#39;(s) = \arg\max_{a\in\mathcal{A}}q_\pi(s, a)\]</span> (Note: <span class="math inline">\(q_\pi\)</span> is the action value function following policy <span class="math inline">\(\pi\)</span>)</p><p>This improves the value from any state <span class="math inline">\(s\)</span> over one step, <span class="math display">\[q_\pi(s, \pi&#39;(s)) = \max_{a\in\mathcal{A}}q_\pi(s,a)≥q_\pi(s, \pi(s))=v_\pi(s)\]</span> (Note: <span class="math inline">\(q_\pi(s, \pi&#39;(s))\)</span> means the action value of taking one step following policy <span class="math inline">\(\pi&#39;\)</span> then following policy <span class="math inline">\(\pi\)</span> forever.)</p><p>If therefore improves the value function, <span class="math inline">\(v_{\pi&#39;}(s) ≥ v_\pi (s)\)</span> <span class="math display">\[\begin{align}v_\pi(s) &amp; ≤ q_\pi(s, \pi&#39;(s))=E_{\pi&#39;}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t = s]  \\&amp; ≤ E_{\pi&#39;}[R_{t+1} + \gamma q_\pi(S_{t+1}, \pi&#39;(S_{t+1}))|S_t=s] \\&amp;≤ E_{\pi&#39;}[R_{t+1} + \gamma R_{t+2} + \gamma^2q_\pi(S_{t+2}, \pi&#39;(S_{t+2}))|S_t = s]\\&amp;≤ E_{\pi&#39;}[R_{t+1} + \gamma R_{t+2} + ..... | S_t = s] = v_{\pi&#39;}(s)\end{align}\]</span> (Unroll the equation to the second, third … step by taking the Bellman euqation into it.)</p><p>If improvements stop, <span class="math display">\[q_\pi(s, \pi&#39;(s)) = \max_{a\in\mathcal{A}}q_\pi(s,a) = q_\pi(s, \pi(s)) = v_\pi(s)\]</span> Then the <strong>Bellman optimality</strong> equation has been satisfied <span class="math display">\[v_\pi(s) = \max_{a\in\mathcal{A}}q_\pi(s, a)\]</span> Therefore <span class="math inline">\(v_\pi(s) = v_*(s)\)</span> for all <span class="math inline">\(s \in \mathcal{S}\)</span>, so <span class="math inline">\(\pi\)</span> is an optimal policy.</p><p><strong>Early Stopping</strong></p><p>Question: Does policy evaluation need to converge to <span class="math inline">\(v_\pi\)</span> ?</p><ul><li>e.g. in the small gridworld <span class="math inline">\(k = 3\)</span> was sufficient to acheive optimal policy</li></ul><p>Or shoule we introduce a stopping condition</p><ul><li>e.g. <span class="math inline">\(\epsilon\)</span>-convergence of value function</li></ul><p>Or simply stop after <span class="math inline">\(k\)</span> iterations of iterative policy evaluation?</p><h2><span id="value-iteration">Value Iteration</span></h2><p><strong>Principle of Optimality</strong></p><p>Any optimal policy can be subdivided into two components:</p><ul><li>An optimal first action <span class="math inline">\(A_*\)</span></li><li>Followed by an optimal policy from successor state <span class="math inline">\(S&#39;\)</span></li></ul><blockquote><p>Theorem: Principle of Optimality</p><p>A policy <span class="math inline">\(\pi(a|s)\)</span> achieves the optimal value from state <span class="math inline">\(s\)</span>, <span class="math inline">\(v_\pi(s) = v_*(s)\)</span> if and only if</p><ul><li>For any state <span class="math inline">\(s&#39;\)</span> reachable from <span class="math inline">\(s\)</span>, <span class="math inline">\(\pi\)</span> achieves the optimal value from state <span class="math inline">\(s&#39;\)</span></li></ul></blockquote><p>If we know the solution to subproblems <span class="math inline">\(v_\ast(s&#39;)\)</span>, then solution <span class="math inline">\(v_\ast(s)\)</span> can be found by one-step look ahead: <span class="math display">\[v_\ast(s) \leftarrow \max_{a\in\mathcal{A}}\mathcal{R}^a_s+\gamma \sum_{s&#39;\in \mathcal{S}}P^a_{ss&#39;}v_\ast(s&#39;)\]</span> The idea of value iteration is to apply these updates iteratively.</p><ul><li>Intuition: start with final rewards and work backwards</li><li>Still works with loopy, stochatis MDPs</li></ul><p><strong>Example: Shortest Path</strong></p><p><img src="/images/grid2.png"></p><ul><li>The goal state is on the left-up corner</li><li>Each step get -1 reward</li><li>The number showed in each grid is the value of that state</li><li>At each iteration, update all states</li></ul><p><strong>Value Iteration</strong></p><ul><li>Problem: find optimal policy <span class="math inline">\(\pi\)</span></li><li>Solution: iterative application of Bellman optimality backup</li><li><span class="math inline">\(v_1 \rightarrow v_2 \rightarrow … \rightarrow v_*\)</span></li><li><em>Synchronous</em> backups<ul><li>At each iteration <span class="math inline">\(k+1\)</span></li><li>For all states <span class="math inline">\(s\in \mathcal{S}\)</span></li><li>Update <span class="math inline">\(v_{k+1}(s)\)</span> from <span class="math inline">\(v_k(s&#39;)\)</span></li></ul></li><li>Convergence to <span class="math inline">\(v_*\)</span> will be proven later</li><li>Unlike policy iteration, there is no explicit policy</li><li>Intermediate value functions may not correspond to any policy</li></ul><p><strong>Synchronous Dynamic Programming Algorithms</strong></p><table><colgroup><col style="width: 12%"><col style="width: 51%"><col style="width: 35%"></colgroup><thead><tr class="header"><th>problem</th><th>bellman equation</th><th>algorithm</th></tr></thead><tbody><tr class="odd"><td>Prediction</td><td>Bellman Expectation Equation</td><td>Iterative Policy Evaluation</td></tr><tr class="even"><td>Control</td><td>Bellman Expectation Equation + Greedy Policy Improvement</td><td>Policy Iteration</td></tr><tr class="odd"><td>Control</td><td>Bellman Optimatility Equation</td><td>Value Iteration</td></tr></tbody></table><p>Algorithms are based on state-value function <span class="math inline">\(v_\pi(s)\)</span> or <span class="math inline">\(v_*(s)\)</span></p><ul><li><span class="math inline">\(O(mn^2)\)</span> per iteration, for <span class="math inline">\(m\)</span> actions and <span class="math inline">\(n\)</span> states</li></ul><p>Could also apply to action-value function <span class="math inline">\(q_\pi(s, a)\)</span> or <span class="math inline">\(q_*(s, a)\)</span></p><ul><li><span class="math inline">\(O(m^2n^2)\)</span> per iteration</li></ul><h2><span id="extentions-to-dynamic-programming">Extentions to Dynamic Programming</span></h2><p><strong>Asynchronous Dynamic Programming</strong></p><p><em>Asynchronous DP</em> backs up states individually, in any order. For each selected state, apply the appropriate backup, which can significantly reduce computation. It also guaranteed to converge if all states continue to be selected.</p><p>Three simple ideas for asynchronous dynamic programming:</p><ul><li><p><em>In-place</em> dynamic programming</p><p><img src="/images/in-place.png"></p></li><li><p><em>Prioritised sweeping</em></p><p><img src="/images/ps.png"></p></li><li><p><em>Real-time</em> dynamic programming</p><p><img src="/images/real-time.png"></p></li></ul><p><strong>Full-Width Backups</strong></p><p>DP uses <em>full-width</em> backups</p><ul><li><em>full-width</em> means when we look aheah, we consider all branches(actions) that could happen</li><li>For each backup (sync or async)<ul><li>Every successor state and action is considered</li><li>Using knowledge of the MDP transitions and reward function</li></ul></li></ul><p><img src="/images/fw.png"></p><h2><span id="contraction-mapping">Contraction Mapping</span></h2><p>Information about <em>contraction mapping theorem</em>, please refer to http://www.math.uconn.edu/~kconrad/blurbs/analysis/contraction.pdf</p><p>Consider the vector space <span class="math inline">\(\mathcal{V}\)</span> over value functions. There are <span class="math inline">\(|\mathcal{S}|\)</span> dimensions.</p><ul><li>Each point in this space fully specifies a value function <span class="math inline">\(v(s)\)</span></li></ul><p>We will measure distance between state-value functions <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> by the <span class="math inline">\(\infty\)</span>-norm. <span class="math display">\[||u-v||_\infty = \max_{s\in\mathcal{S}}|u(s)-v(s)|\]</span> <em>Bellman Expectation Backup is a Contraction</em></p><p>Define the <em>Bellman expectation backup operator</em> <span class="math inline">\(T^\pi\)</span>, <span class="math display">\[T^\pi(v) = \mathcal{R}^\pi + \gamma\mathcal{P}^\pi v\]</span> This operator is a <span class="math inline">\(\gamma\)</span>-contraction, it makes value functions closer bt at least <span class="math inline">\(\gamma\)</span>, <span class="math display">\[\begin{align}||T^\pi(u)-T^\pi(v)||_\infty &amp;= ||(\mathcal{R}^\pi + \gamma\mathcal{P}^\pi v) - (\mathcal{R}^\pi + \gamma\mathcal{P}^\pi u)||_\infty \\&amp;= ||\gamma P^\pi(u-v)||_\infty\\&amp;≤||\gamma P^\pi||u-v||_\infty||_\infty\\&amp;≤\gamma||u-v||_\infty\end{align}\]</span></p><blockquote><p>Theorem: <strong>Contraction Mapping Theorem</strong></p><p>For any metric space <span class="math inline">\(\mathcal{V}\)</span> that is complete (closed) under an operator <span class="math inline">\(T(v)\)</span>, where <span class="math inline">\(T\)</span> is a <span class="math inline">\(\gamma\)</span>-contraction,</p><ul><li><span class="math inline">\(T\)</span> converges to a unique fixed point</li><li>At a linear convergence rate of <span class="math inline">\(\gamma\)</span></li></ul></blockquote><p><strong>Convergence of Iterative Policy Evaluation and Policy Iteration</strong></p><p>The Bellman expectation operator <span class="math inline">\(T^\pi\)</span> has a unique fixed point <span class="math inline">\(v_\pi\)</span>.</p><p>By contraction mapping theorem,</p><ul><li>Iterative policy evaluation converges on <span class="math inline">\(v_\pi\)</span>;</li><li>Policy iteration converges on <span class="math inline">\(v_*\)</span>.</li></ul><p><em>Bellman Optimality Backup is a Contraction</em></p><p>Define the <em>Bellman Optimality backup operator</em> <span class="math inline">\(T^\ast\)</span>, <span class="math display">\[T^\ast(v) = \max_{a\in\mathcal{A}}\mathcal{R}^a+\gamma \mathcal{P}^av\]</span> This operator is a <span class="math inline">\(\gamma\)</span>-contraction, it makes value functions closer by at least <span class="math inline">\(\gamma\)</span>, <span class="math display">\[||T^\ast(u)-T\ast(v)||_\infty≤\gamma ||u-v||_\infty\]</span> <strong>Convergence of Value Iteration</strong></p><p>The Bellman optimality operator <span class="math inline">\(T^∗\)</span> has a unique fixed point <span class="math inline">\(v_*\)</span>.</p><p>By contraction mapping theorem, value iteration converges on <span class="math inline">\(v_*\)</span>.</p><p>In summary, what does a Bellman backup do to points in value function space is to bring value functions <em>closer</em> to a unique fixed point. And therefore the backups must converge on a unique solution.</p><p>This lecture (note) introduces how to use dynamic programming to solve <em>planning</em> problems. Next lecture will introduce model-free prediction, which is a really RL problem.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#policy-evaluation&quot;&gt;Policy Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#policy-iteration&quot;&gt;Policy Iteration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#value-iteration&quot;&gt;Value Iteration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#extentions-to-dynamic-programming&quot;&gt;Extentions to Dynamic Programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#contraction-mapping&quot;&gt;Contraction Mapping&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="MDP" scheme="http://www.52coding.com.cn/tags/MDP/"/>
    
      <category term="Dynamic Programming" scheme="http://www.52coding.com.cn/tags/Dynamic-Programming/"/>
    
      <category term="Policy Iteration" scheme="http://www.52coding.com.cn/tags/Policy-Iteration/"/>
    
      <category term="Value Iteration" scheme="http://www.52coding.com.cn/tags/Value-Iteration/"/>
    
  </entry>
  
  <entry>
    <title>RL - Markov Decision Processes</title>
    <link href="http://www.52coding.com.cn/2017/08/18/RL%20-%20Markov%20Decision%20Processes/"/>
    <id>http://www.52coding.com.cn/2017/08/18/RL - Markov Decision Processes/</id>
    <published>2017-08-18T07:02:19.000Z</published>
    <updated>2018-11-06T03:47:34.671Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#markov-processes">Markov Processes</a></li><li><a href="#markov-reward-process">Markov Reward Process</a></li><li><a href="#markov-decision-process">Markov Decision Process</a></li></ul><!-- tocstop --><h2><span id="markov-processes">Markov Processes</span></h2><p>Basically, <strong>Markov decision processes</strong> formally describe an environment for reinforcement learning, where the environment is <strong>fully observable</strong>, which means the current state completely characterises the process.</p><a id="more"></a><p>Almost all RL problems can be formalised as MDPs, e.g.</p><ul><li>Optimal control primarily deals with continuous MDPs</li><li>Partially observable problems can be converted into MDPs</li><li>Bandits are MDPs with one state</li></ul><p>So, if we solve MDP, we can solve all above RL problems.</p><p><strong>Markov Property</strong></p><p>Markov Property is &quot;The future is independent of the past given the present&quot;, like <a href="https://www.52coding.com.cn/index.php?/Articles/single/69">last note</a> said. The formal definition is: <span class="math display">\[P[S_{t+1}|S_t]=P[S_{t+1}|S_1, ..., S_t]\]</span> where <span class="math inline">\(S\)</span> represents a state.</p><p>The formula means the current can capture all relevant information from the history. Once the state is known, the history may be thrown away, i.e. the state is a sufficient statistic of the future.</p><p><strong>State Transition Matrix</strong></p><p>We know that given the current state, we can use its information to reach the next state, but how? — It is characterized by the <em>state transition probability</em>.</p><p>For a Markov state <span class="math inline">\(s\)</span> and successor state <span class="math inline">\(s&#39;\)</span> , the state transition probability is deﬁned by <span class="math display">\[P_{ss&#39;}=P[S_{t+1}=s&#39;|S_t=s]\]</span> We can put all of the probabities into a matrix called transition matrix, denoted by <span class="math inline">\(P\)</span> : <span class="math display">\[P = \begin{bmatrix}P_{11}     &amp; \cdots &amp; P_{1n}      \\\vdots &amp; \ddots &amp; \vdots \\P_{n1}     &amp; \cdots &amp; P_{nn}\end{bmatrix}\]</span> where each row of the matrix sums to 1.</p><p><strong>Markov Process</strong></p><p>Formally, a Markov process is a <strong>memoryless</strong> random process, i.e. a sequence of random states <span class="math inline">\(S_1, S_2, …\)</span> with the <strong>Markov property</strong>.</p><blockquote><p>Definition</p><p><strong>A Markov Process (or Markov Chain) is tuple</strong> <span class="math inline">\(&lt;S, P&gt;\)</span></p><ul><li><strong><span class="math inline">\(S\)</span> is a (finite) set of states</strong></li><li><strong><span class="math inline">\(P\)</span> is a state transition probability matrix, <span class="math inline">\(P_{ss&#39;} = P[S_{t+1}=s&#39;|S_t=s]\)</span></strong></li></ul></blockquote><p><em>Example</em></p><p><img src="/images/markov.png"></p><p>The above figure show a markov chains of a student's life. Process starts from <em>Class 1</em>, taking class 1 may be boring, so he have either 50% probability to look <em>Facebook</em> or to move to <em>Class 2</em>. …. And finally, he reach the final state <em>Sleep</em>. It's a final state just because it is a self-loop with probability 1 which is nothing special.</p><p>We can sample sequences from such process. Sample <strong>episodes</strong> for Student Markov Chain starting from <span class="math inline">\(S_1 = C_1\)</span>: <span class="math display">\[S_1, S_2, ..., S_T\]</span></p><ul><li>C1 C2 C3 Pass Sleep</li><li>C1 FB FB C1 C2 Sleep</li><li>C1 C2 C3 Pub C2 C3 Pass Sleep</li><li>C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep</li></ul><p>Also, we can make the transition matrix from such markov chain:</p><p><img src="/images/trans_mat.png"></p><p>If we have this matrix, we can fully describe the Markov process.</p><h2><span id="markov-reward-process">Markov Reward Process</span></h2><p>So far, we have never talked about Reinforcement Learning, there is no reward at all. So, let's talk about the <em>Markov Reward Process</em>.</p><p>The most important is adding reward to Markov process, so a Markov reward process is a Markov chain with values.</p><blockquote><p>Definition</p><p>A Markov Reward Process is tuple <span class="math inline">\(&lt;S, P, R, \gamma&gt;\)</span></p><ul><li><span class="math inline">\(S\)</span> is a (finite) set of states</li><li><span class="math inline">\(P\)</span> is a state transition probability matrix, <span class="math inline">\(P_{ss&#39;} = P[S_{t+1}=s&#39;|S_t=s]\)</span></li><li><strong><span class="math inline">\(R\)</span> is a reward function, <span class="math inline">\(R_s=E[R_{t+1}|S_t=s]\)</span></strong></li><li><strong><span class="math inline">\(\gamma\)</span> is a discount factor, <span class="math inline">\(\gamma \in [0, 1]\)</span></strong></li></ul></blockquote><p>Note that <span class="math inline">\(R\)</span> is the <strong>immediate reward</strong>, it characterize the reward you will get if you currently stay on state <span class="math inline">\(s\)</span>.</p><p><em>Example</em></p><p>Let's back to the student example:</p><p><img src="/images/mrp.png"></p><p>At each state, we have corresponding reward represents the goodness/badness of that state.</p><p><strong>Return</strong></p><p>We don't actually care about the immediate reward, we care about the whole random sequence's total reward. So we define the term <em>return</em>:</p><blockquote><p>Definition</p><p><strong>The return <span class="math inline">\(G_t\)</span> is the total dicounted reward from time-step <span class="math inline">\(t\)</span>.</strong> <span class="math display">\[G_t=R_{t+1}+\gamma R_{t+2}+...=\sum_{k=0}^\infty \gamma^kR_{t+k+1}\]</span></p></blockquote><p>The discount <span class="math inline">\(\gamma \in [0,1]\)</span> is the present value of future rewards. So the value of receiving reward <span class="math inline">\(R\)</span> after <span class="math inline">\(k+1\)</span> time-steps is <span class="math inline">\(\gamma^k R\)</span>.</p><p><strong>Note</strong>: <span class="math inline">\(R_{t+1}\)</span> is the immediate reward of state <span class="math inline">\(S_t\)</span>.</p><p>This values <strong>immediate reward</strong> above <strong>delayed reward</strong>:</p><ul><li><span class="math inline">\(\gamma\)</span> closes to <span class="math inline">\(0\)</span> leads to &quot;myopic&quot; evaluation</li><li><span class="math inline">\(\gamma\)</span> closes to <span class="math inline">\(1\)</span> leads to &quot;far-sighted&quot; evaluation</li></ul><p>Most Markov reward and decision processes are discounted. <strong>Why?</strong></p><ul><li><strong>Mathematically convenient</strong> to discount rewards</li><li><strong>Avoids inﬁnite returns</strong> in cyclic Markov processes</li><li><strong>Uncertainty</strong> about the future may not be fully represented</li><li>If the reward is ﬁnancial, immediate rewards may earn more interest than delayed rewards</li><li><strong>Animal/human behaviour</strong> shows preference for immediate reward</li><li>It is sometimes possible to use undiscounted Markov reward processes (i.e. γ = 1), e.g. if all sequences terminate.</li></ul><p><strong>Value Function</strong></p><p>The value function <span class="math inline">\(v(s)\)</span> gives the long-term value of state <span class="math inline">\(s\)</span>.</p><blockquote><p>Definition</p><p><strong>The state value funtion <span class="math inline">\(v(s)\)</span> of an MRP is the expected return starting from state <span class="math inline">\(s\)</span></strong> <span class="math display">\[v(s) = E[G_t|S_t=s]\]</span></p></blockquote><p>We use expectation because it is a random process, we want to figure out the expected value of a state, not such a sequence sampled starts it.</p><p><em>Example</em></p><p>Sample <strong>returns</strong> from Student MRP, starting from <span class="math inline">\(S_1 = C1\)</span> with <span class="math inline">\(\gamma = \frac{1}{2}\)</span>: <span class="math display">\[G_1=R_2+\gamma R_3+...+\gamma^{T-2}R_T\]</span> <img src="/images/samret.png"></p><p>The <em>return</em> is random, but the <em>value function</em> is not random, rather, it is expectation of all samples' return.</p><p>Let's see the example of state <em>value function</em>:</p><p><img src="/images/svf.png"></p><p>When <span class="math inline">\(\gamma = 0\)</span>, the value function just consider the reward of current state no matter how it changes future.</p><p><img src="/images/gamma0.9.png"></p><p><img src="/images/gamma1.png"></p><p><strong>Bellman Equation for MRPs</strong></p><p>The value function can be decomposed into two parts:</p><ul><li>immediate reward <span class="math inline">\(R_{t+1}\)</span></li><li>discounted value of successor state <span class="math inline">\(\gamma v(S_{t+1})\)</span></li></ul><p>So as to we can apply dynamic programming to solve the value function.</p><p>Here is the demonstration: <span class="math display">\[\begin{align}v(s) &amp; = \mathbb{E}[G_t|S_t=s] \\&amp; = \mathbb{E}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...|S_t=s] \\&amp;= \mathbb{E}[R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+...)|S_t=s]\\&amp;= \mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t=s]\\&amp;= \mathbb{E}[R_{t+1}+\gamma v(S_{t+1})|S_t=s]\end{align}\]</span> Here we get: <span class="math display">\[v(s) =\mathbb{E}[R_{t+1}+\gamma v(S_{t+1})|S_t=s]=R_{t+1}+\gamma\mathbb{E}[ v(S_{t+1})|S_t=s]\]</span> We look ahead one-step, and averaging all value function of next possible state:</p><p><img src="/images/bf2.png"> <span class="math display">\[v(s) = R_s+\gamma\sum_{s&#39;\in S}P_{ss&#39;}v(s&#39;)\]</span> We can use the Bellman equation to vertify a MRP:</p><p><img src="/images/verMRP.png"></p><p><em>Bellman Equation in Matrix Form</em></p><p>The Bellman equation can be expressed concisely using matrices, <span class="math display">\[v = R + \gamma Pv\]</span> where <span class="math inline">\(v\)</span> is a column vector with one entry per state: <span class="math display">\[\begin{bmatrix}v(1)         \\\vdots  \\v(n)    \end{bmatrix}=\begin{bmatrix}R_1         \\\vdots  \\R_n   \end{bmatrix}+\gamma \begin{bmatrix}P_{11}     &amp; \cdots &amp; P_{1n}      \\\vdots &amp; \ddots &amp; \vdots \\P_{n1}     &amp; \cdots &amp; P_{nn}\end{bmatrix}\begin{bmatrix}v(1)         \\\vdots  \\v(n)    \end{bmatrix}\]</span> Because the Bellman equation is a linear equation, it can be solved directly: <span class="math display">\[\begin{align}v &amp; = R+\gamma Pv \\(I-\gamma P)v&amp; =R \\v &amp;= (I-\gamma P)^{-1}R\end{align}\]</span> However, the Computational complexity is <span class="math inline">\(O(n^3)\)</span> for <span class="math inline">\(n\)</span> states, because of the inverse operation. This method can be applied to solve small MRPs.</p><p>There are many iterative methods for large MRPs, e.g.</p><ul><li>Dynamic programming</li><li>Monte-Carlo evaluation</li><li>Temporal-Diﬀerence learning</li></ul><p>So far with MRP, all we want to do is to make decisions, so let's move on <em>Markov Decision Process</em>, which we actually using in RL.</p><h2><span id="markov-decision-process">Markov Decision Process</span></h2><p>A <strong>Markov decision process (MDP)</strong> is a Markov reward process with decisions. It is an <em>environment</em> in which all states are Markov.</p><blockquote><p>Definition</p><p><strong>A Markov Decision Process is a tuple</strong> <span class="math inline">\(&lt;S, A,P,R,\gamma&gt;\)</span></p><ul><li><span class="math inline">\(S\)</span> is a finite set of states</li><li><span class="math inline">\(A\)</span> <strong>is a finite set of actions</strong></li><li><span class="math inline">\(P\)</span> is a state transition probability matrix, <span class="math inline">\(P^a_{ss&#39;}=\mathbb{P}[S_{t+1}=s&#39;|S_t=s, A_t=a]\)</span></li><li><span class="math inline">\(R\)</span> is a reward function, <span class="math inline">\(R^a_s=\mathbb{E}[R_{t+1}|S_t=s,A_t=t]\)</span></li><li><span class="math inline">\(\gamma\)</span> is a discount factor <span class="math inline">\(\gamma \in[0,1]\)</span></li></ul></blockquote><p><em>Example</em></p><p><img src="/images/mdpstu.png"></p><p>Red marks represents the actions or decisions, what we want to do is to find the best path that maximize the value function.</p><p><strong>Policy</strong></p><p>Formally, the decision can be defined as <em>policy</em>:</p><blockquote><p>Definition</p><p><strong>A policy π is a distribution over actions given states,</strong> <span class="math display">\[\pi(a|s)=\mathbb{P}[A_t=a|S_t=s]\]</span></p></blockquote><p>A policy fully deﬁnes the behaviour of an agent.</p><p>Note that MDP policies depend on the current state (not the history), i.e. policies are stationary (time-independent), <span class="math inline">\(A_t~\pi(\cdot|S_t), \forall t&gt;0\)</span>.</p><p>An MDP can transform into a Markov process or an MRP:</p><ul><li><p>Given an MDP <span class="math inline">\(\mathcal{M}=&lt;\mathcal{S,A,P,R}, \gamma&gt;\)</span> and a policy <span class="math inline">\(\pi\)</span></p></li><li><p>The state sequence <span class="math inline">\(&lt;S_1 , S_2 , ... &gt;\)</span> is a Markov process <span class="math inline">\(&lt;\mathcal{S, P}^π&gt;\)</span></p></li><li><p>The state and reward sequence <span class="math inline">\(&lt;S_1 , R_2 , S_2 , …&gt;\)</span> is a Markov reward process <span class="math inline">\(&lt;\mathcal{S,P}^\pi,\mathcal{R}^\pi,\gamma&gt;\)</span> where, <span class="math display">\[\mathcal{P}^\pi_{s,s&#39;}=\sum_{a\in\mathcal{A}}\pi(a|s)P^a_{ss&#39;}\]</span></p><p><span class="math display">\[\mathcal{R}^\pi_s=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{R}^a_s\]</span></p></li></ul><p><strong>Value Function</strong></p><p>There are two value functions: the first one is called <em>state-value function</em> which represents the expected return following policy <span class="math inline">\(\pi\)</span>, the other is called <em>action-value function</em> which measures the goodness/badness of an action following policy <span class="math inline">\(\pi\)</span>.</p><blockquote><p>Definition</p><p>The <strong>state-value function</strong> <span class="math inline">\(v_π (s)\)</span> of an MDP is the expected return starting from state <span class="math inline">\(s\)</span>, and then following policy π <span class="math display">\[v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]\]</span></p></blockquote><blockquote><p>Definition</p><p>The <strong>action-value function</strong> <span class="math inline">\(q_π (s, a)\)</span> is the expected return starting from state <span class="math inline">\(s\)</span>, taking action <span class="math inline">\(a\)</span>, and then following policy <span class="math inline">\(π\)</span> <span class="math display">\[q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]\]</span></p></blockquote><p><em>Example</em></p><p><img src="/images/svformdp.png"></p><p><strong>Bellman Expectation Equation</strong></p><p>The <strong>state-value function</strong> can again be decomposed into immediate reward plus discounted value of successor state, <span class="math display">\[v_\pi(s)=\mathbb{E}_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\]</span> The <strong>action-value function</strong> can similarly be decomposed, <span class="math display">\[q_\pi(s,a)=\mathbb{E}_\pi[R_{t+1}+\gamma q_\pi (S_{t+1},A_{t+1})|S_t=s,A_t=a]\]</span> <em>Bellman Expectation Equation for <span class="math inline">\(V_π\)</span></em></p><p><img src="/images/vpi.png"> <span class="math display">\[v_\pi(s)=\sum_{a\in\mathcal{A}}\pi(a|s)q_\pi(s,a)\]</span> The look-ahead approach is taking an action and computing its reward, all we need to do is to averaging all possible actions' rewards, which is equal to current state-value.</p><p><em>Bellman Expectation Equation for <span class="math inline">\(Q_π\)</span></em></p><p><img src="/images/qpi.png"> <span class="math display">\[q_\pi(s,a)=\mathcal{R}_s^a + \gamma \sum_{s&#39;\in\mathcal{S}}P^a_{ss&#39;}v_\pi(s&#39;)\]</span> It is identical to the immediate reward of taking action <span class="math inline">\(a\)</span> plus the average of the reward/value of all possible states which the action could lead to.</p><p><em>Bellman Expectation Equation for <span class="math inline">\(V_π\)</span></em></p><p><img src="/images/vpi2.png"> <span class="math display">\[v_\pi(s)=\sum_{a\in\mathcal{A}}\pi(a|s)(\mathcal{R}^a_s+\gamma\sum_{s&#39;\in\mathcal{S}}P^a_{ss&#39;}v_\pi(s&#39;))\]</span> This is a two-step look-ahead approach, just combining the last two equations.</p><p><img src="/images/qpi2.png"> <span class="math display">\[q_\pi(s,a)=\mathcal{R}^a_s+\gamma\sum_{s&#39;\in\mathcal{S}}P^a_{ss&#39;}\sum_{a&#39;\in\mathcal{A}}\pi(a&#39;|s&#39;)q_\pi(s&#39;,a&#39;)\]</span> <em>Example</em>: State-value function</p><p><img src="/images/qgbee.png"></p><p><em>Bellman Expectation Equation (Matrix Form)</em></p><p>The Bellman expectation equation can be expressed concisely using the induced MRP, <span class="math display">\[v_\pi=R^\pi+\gamma P^\pi v_\pi\]</span> with direct solution <span class="math display">\[v_\pi=(I-\gamma P^{\pi-1})^{-1}R^\pi\]</span> <strong>Optimal Value Function</strong></p><blockquote><p>Definition</p><p><strong>The optimal state-value function <span class="math inline">\(v_∗(s)\)</span> is the maximum value function over all policies</strong> <span class="math display">\[v_\ast(s)=\max_{\pi}v_\pi(s)\]</span> <strong>The optimal action-value function <span class="math inline">\(q_∗ (s, a)\)</span> is the maximum action-value function over all policies</strong> <span class="math display">\[q_\ast(s,a)=\max_\pi q_\pi(s,a)\]</span></p></blockquote><p>The optimal value function speciﬁes the best possible performance in the MDP.</p><p>If we know the <span class="math inline">\(q_*(s,a)\)</span>, we &quot;solve&quot; MDP because we know what actions should take at each state to maximize the reward.</p><p><em>Example</em></p><p><img src="/images/egvalue.png"></p><p><img src="/images/egstar.png"></p><p><strong>Optimal Policy</strong></p><p>Deﬁne a partial ordering over policies <span class="math display">\[\pi≥\pi&#39;\ if\ v_\pi(s)≥v_{\pi&#39;}, \forall s\]</span></p><blockquote><p>Theorem</p><ul><li><strong>There exists an optimal policy <span class="math inline">\(π_∗\)</span> that is better than or equal to all other policies,</strong>, <span class="math inline">\(\pi_* ≥ \pi, \forall \pi\)</span></li><li><strong>All optimal policies achieve the optimal value function,</strong> <span class="math inline">\(v_{pi_*}=v_*(s)\)</span></li><li><strong>All optimal policies achieve the optimal action-value function,</strong> <span class="math inline">\(q_{\pi_*}(s,a)=q_*(s,a)\)</span></li></ul></blockquote><p><em>Finding an Optimal Policy</em></p><p>An optimal policy can be found by maximising over <span class="math inline">\(q_∗ (s, a)\)</span>,</p><p><span class="math display">\[\pi_\ast(a|s)=\begin{cases}1\ if\ a=\arg\max_{a\in\mathcal{A}}q_\ast(s,a) \\0 \ otherwise\end{cases}\]</span></p><p>There is always a deterministic optimal policy for any MDP. So if we know <span class="math inline">\(q_∗ (s, a)\)</span>, we immediately have the optimal policy.</p><p><img src="/images/optpol.png"></p><p>The optimal policy is highlight in red.</p><p><strong>Bellman Optimality Equation</strong></p><p>The optimal value functions are recursively related by the Bellman optimality equations:</p><p><img src="/images/boev.png"></p><p><span class="math display">\[v_\ast(s)=\max_aq_\ast(s,a)\]</span></p><p><img src="/images/boeq.png"></p><p><span class="math display">\[q_\ast(s,a)=\mathcal{R}^a_s+\gamma\sum_{s&#39;\in\mathcal{S}}\mathcal{P}^a_{ss&#39;}v_\ast(s&#39;)\]</span></p><p><img src="/images/boev2.png"></p><p><span class="math display">\[v_\ast(s)=\max_a\mathcal{R}^a_s+\gamma\sum_{s&#39;\in\mathcal{S}}\mathcal{P}^a_{ss&#39;}v_\ast(s&#39;)\]</span></p><p><img src="/images/boeq2.png"></p><p><span class="math display">\[q_\ast(s,a)=\mathcal{R}^a_s+\gamma\sum_{s&#39;\in\mathcal{S}}P^a_{ss&#39;}\max_{a&#39;}q_\ast(s&#39;,s)\]</span></p><p><em>Example</em></p><p><img src="/images/boe_in_stu_mdp.png"></p><p><em>Solving the Bellman Optimality Equation</em></p><p>Bellman Optimality Equation is non-linear, so it is not able to be sovle as solving linear equation. And there is no closed from solution (in general).</p><p>Many <strong>iterative</strong> solution methods</p><ul><li>Value Iteration</li><li>Policy Iteration</li><li>Q-learning</li><li>Sarsa</li></ul><p>End. Next note will introduce how to solve the Bellman Optimality Equation by dynamic programming.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#markov-processes&quot;&gt;Markov Processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#markov-reward-process&quot;&gt;Markov Reward Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#markov-decision-process&quot;&gt;Markov Decision Process&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
&lt;h2 id=&quot;markov-processes&quot;&gt;Markov Processes&lt;/h2&gt;
&lt;p&gt;Basically, &lt;strong&gt;Markov decision processes&lt;/strong&gt; formally describe an environment for reinforcement learning, where the environment is &lt;strong&gt;fully observable&lt;/strong&gt;, which means the current state completely characterises the process.&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="MDP" scheme="http://www.52coding.com.cn/tags/MDP/"/>
    
  </entry>
  
  <entry>
    <title>RL - Introduction to Reinforcement Learning</title>
    <link href="http://www.52coding.com.cn/2017/08/15/RL%20-%20Introduction%20to%20Reinforcement%20Learning/"/>
    <id>http://www.52coding.com.cn/2017/08/15/RL - Introduction to Reinforcement Learning/</id>
    <published>2017-08-15T07:55:19.000Z</published>
    <updated>2018-11-06T03:47:27.932Z</updated>
    
    <content type="html"><![CDATA[<p>RL, especially DRL (Deep Reinforcement Learning) has been an fervent research area during these years. One of the most famous RL work would be AlphaGo, who has beat <a href="https://www.wikiwand.com/en/Lee_Sedol" target="_blank" rel="noopener">Lee Sedol</a>, one of the best players at Go, last year. And in this year (2017), AlphaGo won three games with Ke Jie, the world No.1 ranked player. Not only in Go, AI has defeated best human play in many games, which illustrates the powerful of the combination of Deep Learning and Reinfocement Learning. However, despite AI plays better games than human, AI takes more time, data and energy to train which cannot be said to be very intelligent. Still, there are numerous unexplored and unsolved problems in RL research, that's also why we want to learn RL.</p><p>This is the first note of David Silver's <a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">RL course</a>.</p><a id="more"></a><!-- toc --><ul><li><a href="#about-reinforcement-learning">About Reinforcement Learning</a></li><li><a href="#the-reinforcement-learning-problem">The Reinforcement Learning Problem</a></li><li><a href="#inside-an-rl-agent">Inside An RL Agent</a></li><li><a href="#problems-within-reinforcement-learning">Problems within Reinforcement Learning</a></li></ul><!-- tocstop --><h2><span id="about-reinforcement-learning">About Reinforcement Learning</span></h2><p>Reinforcement Learning is one of the three major branches of Machine Learning, and is also the intersect of many different disciplines, as the following two figure illustrated.</p><p><img src="/images/branches.png"></p><p><img src="/images/faces.png"></p><p>There are several reasons that makes reinforcement learning different from other machine learning paradigms:</p><ul><li>There is no supervisor, only a <em>reward</em> signal</li><li>Feedback is delayed, not instantaneous</li><li>Time really matters (sequential, non i.i.d data)</li><li>Agent's actions affect the subsequent data it receives</li></ul><p>There are some examples of Reinforcement Learning:</p><ul><li>Fly stunt manoeuvres in a helicopter</li><li>Defeat the world champion at Backgammon</li><li>Manage an investment portfolio</li><li>Control a power station</li><li>Make a humanoid robot walk</li><li>Play many diﬀerent Atari games better than humans</li></ul><h2><span id="the-reinforcement-learning-problem">The Reinforcement Learning Problem</span></h2><p><strong>Rewards</strong></p><p>We say RL do not have a supervisor, just a <em>reward</em> signal. Then what is <em>reward</em> ?</p><ul><li>A <strong>reward</strong> <span class="math inline">\(R_t\)</span> is a scalar feedback signal</li><li>Indicates how well agent is doing at step <span class="math inline">\(t\)</span></li><li>The agent's job is to maximise cumulative reward</li></ul><p>Reinforcement Learning is based on the <strong>reward hypothesis</strong>, which is</p><blockquote><p>Definition of reward hypothesis</p><p><strong>All goals can be described by the maximisation of expected cumulative reward.</strong></p></blockquote><p>There are some examples of <em>rewards</em> :</p><ul><li>Fly stunt manoeuvres in a helicopter<ul><li>+ve reward for following desired trajectory</li><li>−ve reward for crashing</li></ul></li><li>Defeat the world champion at Backgammon<ul><li>+/−ve reward for winning/losing a game</li></ul></li><li>Manage an investment portfolio<ul><li>+ve reward for each $ in bank</li></ul></li><li>Control a power station<ul><li>+ve reward for producing power</li><li>−ve reward for exceeding safety thresholds</li></ul></li><li>Make a humanoid robot walk<ul><li>+ve reward for forward motion</li><li>−ve reward for falling over</li></ul></li><li>Play many diﬀerent Atari games better than humans<ul><li>+/−ve reward for increasing/decreasing score</li></ul></li></ul><p><strong>Sequential Decision Making</strong></p><p>So, according to the <em>reward hypothesis</em>, our goal is to <strong>select actions to maximise total future reward</strong>. Actions may have long term consequences as well as reward may be delayed. It may be better to sacrifice immediate reward to gain more long-term reward. For instance, a ﬁnancial investment may take months to mature and a helicopter might prevent a crash in several hours.</p><p><strong>Agent and Environment</strong></p><p><img src="/images/aae.png"></p><p>Agents and envrionments are big concepts in RL. They have following relationships:</p><ul><li>At each step <span class="math inline">\(t\)</span> the agent:<ul><li>Excutes action <span class="math inline">\(A_t\)</span></li><li>Receives observation <span class="math inline">\(O_t\)</span></li><li>Receives scalar reward <span class="math inline">\(R_t\)</span></li></ul></li><li>The environment:<ul><li>Receives action <span class="math inline">\(A_t\)</span></li><li>Emits observation <span class="math inline">\(O_{t+1}\)</span></li><li>Emits scalar reward <span class="math inline">\(R_{t+1}\)</span></li></ul></li><li><span class="math inline">\(t\)</span> increments at env. step</li></ul><p><strong>State</strong></p><p><em>History and State</em></p><p>The <strong>history</strong> is the sequence of observations, actions, rewards: <span class="math display">\[H_t = O_1, R_1, A_1, ..., A_{t-1}, O_t, R_t\]</span> which means all observable variables up to time <span class="math inline">\(t\)</span>, i.e. the sensorimotor stream of a robot or embodied agent.</p><p>What happens next depends on the history:</p><ul><li>The agent selects actions</li><li>The environments select observations/rewards</li></ul><p><strong>State</strong> is the information used to determine what happens next. Formally, state is a function of the history: <span class="math display">\[S_t = f(H_t)\]</span> <em>Environment State</em></p><p>The <strong>environment state</strong> <span class="math inline">\(S^e_t\)</span> is the environment's private representation, i.e. whatever data the environment uses to pick the next observation /reward. The environment state is not usually visible to the agent. Even if <span class="math inline">\(S^e_t\)</span> is visible, it may contain irrelevant information.</p><p><em>Agent State</em></p><p>The <strong>agent state</strong> <span class="math inline">\(S^a_t\)</span> is the agent's internal representation, i.e. whatever information the agent uses to pick the next action. It is the information used by reinforcement learning algotithms. It can be any function of history: <span class="math display">\[S^a_t=f(H_t)\]</span> <em>Information State</em></p><p>An <strong>information state</strong> (a.k.a. <strong>Markov state</strong>) contains all useful information from the history.</p><blockquote><p>Definition</p><p><strong>A state <span class="math inline">\(S_t\)</span> is Markov if and only if</strong> <span class="math display">\[P[S_{t+1}|S_t]=P[S_{t+1}|S_1,...,S_t]\]</span></p></blockquote><p>The above formular means:</p><ul><li><p>&quot;The future is independent of the past given the present&quot; <span class="math display">\[H_{1:t}\rightarrow S_t\rightarrow H_{t+1:\infty}\]</span></p></li><li><p>Once the state is known, the history may be thrown away.</p></li><li><p>The state is a sufficient statistic of the future</p></li><li><p>The environment state <span class="math inline">\(S^e_t\)</span> is Markov</p></li><li><p>The history <span class="math inline">\(H_t\)</span> is Markov</p></li></ul><p><em>Rat Example</em></p><p>Here is an example to explain what is state, imagine you are a rat and your master would decide whether to excuted you or give you a pice of cheese. The master makes decisions according to a sequence of signals, the first two sequence and the result are shown in the figure below:</p><p><img src="/images/firsttwo.png"></p><p>The question is, what would you get if the sequence is like below:</p><p><img src="/images/que.png"></p><p>Well, the answer you may give is decided by what is your agent state：</p><ul><li>If agent state = last 3 items in sequence, then the answer would be being excuted.</li><li>If agent state = counters for lights, bells and levers, then the answer would be given a piece of cheese.</li><li>What if agent state = complete sequence?</li></ul><p><em>Fully Observable Environments</em></p><p><strong>Full observability</strong>: agent <strong>directly</strong> observes environment state: <span class="math display">\[O_t = S^a_t=S^e_t\]</span></p><ul><li>Agent state = environment state = information state</li><li>Formally, this is a <strong>Markov decision process</strong> (MDP)</li></ul><p><em>Partially Observable Environments</em></p><p><strong>Partial observability</strong>: agent <strong>indirectly</strong> observes environment:</p><ul><li>A robot with camera vision isn't told its absolute location</li><li>A trading agent only observes current prices</li><li>A poker playing agent only observes public cards</li></ul><p>With partial observability, agent state ≠ environment state, formally this is a <strong>partially observable Markov decision process</strong> (POMDP).</p><p>In this situation, agent must construct its own state representation <span class="math inline">\(S^a_t\)</span>, e.g.</p><ul><li>Complete history: <span class="math inline">\(S^a_t = H_t\)</span></li><li><strong>Beliefs</strong> of environment state: <span class="math inline">\(S_t^a = (P[S^e_t=s^1], …, P[S^e_t=s^n])\)</span></li><li>Recurrent neural network: <span class="math inline">\(S^a_t=\sigma(S^a_{t-1}W_s+O_tW_o)\)</span></li></ul><h2><span id="inside-an-rl-agent">Inside An RL Agent</span></h2><p>There are three major components of an RL agent, actually, an agent may include one or more of these:</p><ul><li>Policy: agent's behaviour function</li><li>Value function：how good is each state and/or action</li><li>Model：agent's representation of the environment</li></ul><p><strong>Policy</strong></p><p>A <strong>Policy</strong> is the agent's behaviour, it is a map from state to action, e.g.</p><ul><li>Deterministic policy：<span class="math inline">\(a = \pi(s)\)</span></li><li>Stochastic policy: <span class="math inline">\(\pi(a|s)=P[A_t=a|S_t=s]\)</span></li></ul><p><strong>Value Function</strong></p><p>Value function is a prediction of future reward, it is used to evaluate the goodness/badness of states. And therefore to select between actions: <span class="math display">\[v_\pi(s)=E_\pi[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+...|S_t=s]\]</span> <strong>Model</strong></p><p>A <strong>model</strong> predicts what the environment will do next, denoted by <span class="math inline">\(P\)</span> which predicts the next state and by <span class="math inline">\(R\)</span> predicts the next (immediate) reward: <span class="math display">\[P^a_{ss&#39;}=P[S_{t+1}=s&#39;|S_t=s,A_t=a]\]</span></p><p><span class="math display">\[R^a_s=E[R_{t+1}|S_t=s, A_t=a]\]</span></p><p><em>Maze Example</em></p><p><img src="/images/maze.png"></p><p>Let an RL agent to solve the maze, the parameters are:</p><ul><li>Rewards: -1 per time-step</li><li>Actions: N, E, S, W</li><li>States: Agent's location</li></ul><p>Then the policy map would be (arrows represent policy <span class="math inline">\(\pi(s)\)</span> for each state <span class="math inline">\(s\)</span>):</p><p><img src="/images/mpolicy.png"></p><p>And the value function at each state would be (numbers represent value <span class="math inline">\(v_\pi(s)\)</span> of each state <span class="math inline">\(s\)</span>):</p><p><img src="/images/mvf.png"></p><p>Model could be visualize as following:</p><ul><li>Grid layout represents transition model <span class="math inline">\(P^a_{ss&#39;}\)</span></li><li>Numbers represent immediate reward <span class="math inline">\(R^a_s\)</span> from each state <span class="math inline">\(s\)</span> (same for all <span class="math inline">\(a\)</span>)</li></ul><p><img src="/images/mm.png"></p><ul><li>Agent may have an internal model of the environment</li><li>Dynamics: how actions change the state</li><li>Rewards: how much reward from each state</li><li>The model may be imperfect</li></ul><p><strong>Categorizing RL agents</strong></p><p>RL agents could be categorized into several categories:</p><ul><li>Value Based<ul><li>No Policy (Implicit)</li><li>Value Function</li></ul></li><li>Policy Based<ul><li>Policy</li><li>No Value Function</li></ul></li><li>Actor Critic<ul><li>Policy</li><li>Value Function</li></ul></li><li>Model Free<ul><li>Policy and/or Value Function</li><li>No Model</li></ul></li><li>Model Based<ul><li>Policy and/or Value Function</li><li>Model</li></ul></li></ul><p><img src="/images/agcat.png"></p><h2><span id="problems-within-reinforcement-learning">Problems within Reinforcement Learning</span></h2><p>This section only proposes questions without providing the solutions.</p><p><strong>Learning and Planning</strong></p><p>Two fundamental problems in sequential decision making:</p><ul><li>Reinforcement Learning<ul><li>The environment is initially unknown</li><li>The agent interacts with the environment</li><li>The agent improves its policy</li></ul></li><li>Planning:<ul><li>A model of the environment is known</li><li>The agent performs computations with its model (without any external interaction)</li><li>The agent improves its policy a.k.a. deliberation, reasoning, introspection, pondering, thought, search</li></ul></li></ul><p><em>Atari Example: Reinforcement Learning</em></p><p><img src="/images/atari.png"></p><p>In atari games, rules of the game are unknown, the agent learns directly from interactive game-play by picking actions on joystick and seeing pixels and scores.</p><p><em>Atari Example: Planning</em></p><p><img src="/images/plan.png"></p><p>In such case, rules of the game are known, which means the agent could query the emulator as known as a perfect model inside agent's brain. Agents need plan ahead to ﬁnd optimal policy, e.g. tree search.</p><p><strong>Exploration and Exploitation</strong></p><ul><li>Reinforcement learning is like trial-and-error learning</li><li>The agent should discover a good policy</li><li>From its experiences of the environment</li><li>Without losing too much reward along the way</li><li><strong>Exploration</strong> ﬁnds more information about the environment</li><li><strong>Exploitation</strong> exploits known information to maximise reward</li><li>It is usually important to explore as well as exploit</li></ul><p><em>Examples</em></p><ul><li>Restaurant Selection<ul><li>Exploitation Go to your favourite restaurant</li><li>Exploration Try a new restaurant</li></ul></li><li>Online Banner Advertisements<ul><li>Exploitation Show the most successful advert</li><li>Exploration Show a diﬀerent advert</li></ul></li><li>Game Playing<ul><li>Exploitation Play the move you believe is best</li><li>Exploration Play an experimental move</li></ul></li></ul><p><strong>Prediction and Control</strong></p><p><strong>Prediction</strong>: evaluate the future</p><ul><li>Given a policy</li></ul><p><strong>Control</strong>: optimise the future</p><ul><li>Find the best policy</li></ul><p>End. Next note will introduce the Markov Decision Processes.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RL, especially DRL (Deep Reinforcement Learning) has been an fervent research area during these years. One of the most famous RL work would be AlphaGo, who has beat &lt;a href=&quot;https://www.wikiwand.com/en/Lee_Sedol&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Lee Sedol&lt;/a&gt;, one of the best players at Go, last year. And in this year (2017), AlphaGo won three games with Ke Jie, the world No.1 ranked player. Not only in Go, AI has defeated best human play in many games, which illustrates the powerful of the combination of Deep Learning and Reinfocement Learning. However, despite AI plays better games than human, AI takes more time, data and energy to train which cannot be said to be very intelligent. Still, there are numerous unexplored and unsolved problems in RL research, that&#39;s also why we want to learn RL.&lt;/p&gt;
&lt;p&gt;This is the first note of David Silver&#39;s &lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;RL course&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="AlphaGo" scheme="http://www.52coding.com.cn/tags/AlphaGo/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="DRL" scheme="http://www.52coding.com.cn/tags/DRL/"/>
    
  </entry>
  
  <entry>
    <title>Paper Reading - Stacked Attention Networks for Image QA</title>
    <link href="http://www.52coding.com.cn/2017/08/15/SAN%20for%20Image%20QA/"/>
    <id>http://www.52coding.com.cn/2017/08/15/SAN for Image QA/</id>
    <published>2017-08-15T01:30:19.000Z</published>
    <updated>2018-11-06T03:48:03.977Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Zichao Yang, Xiaodong He, Jianfeng Gao , Li Deng , Alex Smola <a href="https://arxiv.org/abs/1511.02274" target="_blank" rel="noopener">Stacked Attention Networks for Image Question Answering</a></p></blockquote><p>这篇文章发表在CVPR2016，作者把 attention 机制应用在 Visual QA，不但能理解神经网络生成答案的 multiple resoning，而且获得了当时最好的效果。</p><p>SAN总共由三部分组成：</p><ul><li>Image Model：用来编码图片信息</li><li>Question Moel：用来编码问题信息</li><li>Stacked Attention Networks：通过多层 attention layer 不断优化对问题的编码</li></ul><a id="more"></a><p><img src="/images/san.png"></p><h2><span id="image-model">Image Model</span></h2><p>Image model 使用 VGGNet 处理源图片，用最后一个池化层作为提取的图片特征: <span class="math display">\[f_I = CNN_{vgg}(I)\]</span> 把输入图片转化为 <span class="math inline">\(448 \times 448\)</span> 大小， 输出的特征即为 <span class="math inline">\(512 \times 14 \times 14\)</span>，其中 <span class="math inline">\(512\)</span> 为特征向量（feature vector） <span class="math inline">\(f_i\)</span> 的维度，<span class="math inline">\(14 \times 14\)</span> 是区域（特征向量）的个数，每个特征向量 <span class="math inline">\(f_i\)</span> 代表源图片中 <span class="math inline">\(32 \times 32\)</span> 大小的区域。</p><p><img src="/images/vgg.png"></p><p>为了后面方便处理，通过一个线性层把图片特征转化为和问题特征一样的维度： <span class="math display">\[v_I = \tanh(W_If_I + b_I)\]</span> 其中，<span class="math inline">\(v_I\)</span> 是个矩阵，它的第 <span class="math inline">\(i\)</span> 列 <span class="math inline">\(v_i\)</span> 是区域 <span class="math inline">\(i\)</span> 的特征向量（feature vector）。</p><h2><span id="question-model">Question Model</span></h2><p>作者采用了两种模型对问题进行编码，分别基于 LSTM 和 CNN。</p><h3><span id="lstm-based-question-model">LSTM based question model</span></h3><p>基于 LSTM 的模型很简单，就是用一个普通的 LSTM 对问题进行编码（没准扩展成bi-LSTM效果会更好一些），每个时刻处理一个词，把最后一个词对应的 hidden state 作为编码结果: <span class="math display">\[\begin{alignat}{3}x_t &amp;= W_eq_t, \ t\in\{1, 2, ... T\} \\h_t &amp;= LSTM(x_t), \ t\in\{1, 2, ... T\}\\v_Q &amp;= h_T\end{alignat}\]</span> 其中， <span class="math inline">\(q_t\)</span> 为词的 one-hot encoding，<span class="math inline">\(W_e\)</span> 为 embedding 矩阵，<span class="math inline">\(x_t\)</span> 就为词的 word embedding（总觉得这样的词编码太简单了），<span class="math inline">\(v_Q\)</span> 为对问题的编码。</p><p><img src="/images/lstmq.png"></p><h3><span id="cnn-based-question-model">CNN based question model</span></h3><p><img src="/images/cnn_basedq.png"></p><p>这应该算是CNN的一个变种，它的 filter 有三种，分别为 unigram, bigram, trigram，分别对应窗口大小 <span class="math inline">\(c = 1, 2, 3\)</span>。定义符号 <span class="math inline">\(x_{i:j}\)</span> 为 <span class="math inline">\(x_i, x_{i+1}, …, x_j\)</span> 的连接，所以问题向量可表示为: <span class="math display">\[x_{1:T} = [x1, x2, ..., x_T]\]</span></p><p>然后对每一个 filter 分别在 <span class="math inline">\(x_{1:T}\)</span> 上进行卷积操作，第 <span class="math inline">\(t\)</span> 次卷积操作的输出为： <span class="math display">\[h_{c,t} = \tanh(W_cx_{t:t+c-1}+b_c)\]</span> 窗口大小为 <span class="math inline">\(c\)</span> 的 feature map 为： <span class="math display">\[h_c = [h_{c,1}, h_{c,2}, ..., h_{c,T-c+1}]\]</span> 然后对每个 feature map 进行 max pooling，得到最终的问题特征： <span class="math display">\[\hat{h}_c = \max_t(h_{c,1}, h_{c, 2} ..., h_{c,T-c+1})\]</span></p><p><span class="math display">\[v_Q = h = [\hat{h}_1,\hat{h}_2,\hat{h}_3]\]</span></p><h2><span id="stacked-attention-networks">Stacked Attention Networks</span></h2><p><strong>第一层 attention network</strong></p><p><img src="/images/san1st.png"></p><p>首先根据图像特征矩阵 <span class="math inline">\(v_I\)</span> 和问题特征向量 <span class="math inline">\(v_Q\)</span> 计算 attention map： <span class="math display">\[\begin{alignat}{3}h_A &amp;= \tanh(W_{I,A}v_I\oplus (W_{Q,A}v_Q+b_A))\\p_I &amp;= softmax(W_Ph_A+b_P)\end{alignat}\]</span> 其中，<span class="math inline">\(v_I\in R^{d\times m}\)</span>, <span class="math inline">\(d\)</span> 是图像特征的维度，<span class="math inline">\(m\)</span> 是图像区域个数；<span class="math inline">\(v_Q \in R^d\)</span>；<span class="math inline">\(W_{I, A}, W_{Q,A} \in R^{k \times d}\)</span>，<span class="math inline">\(b_A \in R^{k}\)</span>；定义 <span class="math inline">\(\oplus\)</span> 为矩阵和向量的加法，其运算规则为矩阵的每一列分别和该向量相加，所以 <span class="math inline">\(h_A \in R^{k\times m}\)</span>。<span class="math inline">\(W_P \in R^{1\times k}, b_P\in R^{1\times m}\)</span>，<span class="math inline">\(p_I \in R^{1\times m}\)</span> 为 attention vector，它每一项都是一个概率，表示该问题的答案所在某个区域的概率，或者说问了回答这个问题，注意力应该集中在哪里。</p><p>之后用 attention vector 计算 图像特征的加权和，然后与问题特征相加，得到<strong>优化的问题特征</strong>： <span class="math display">\[\begin{alignat}{3}\widehat{v}_I &amp;= \sum_ip_iv_i \\u &amp;= \hat{v}_I+v_Q\end{alignat}\]</span> 后面的每层 attention network 结构都是一样的，区别在于不再使用原始的问题特征 <span class="math inline">\(v_Q\)</span>，而是用优化后的 <span class="math inline">\(u\)</span>:</p><p><img src="/images/san2nd.png"></p><p><strong>第 k 层 attention network</strong> <span class="math display">\[\begin{alignat}{3}h_A^k &amp;= \tanh(W_{I,A}^kv_I\oplus (W_{Q,A}^ku_{k-1}+b_A^k))\\p_I^k &amp;= softmax(W_P^kh_A^k+b_P^k)\\\widehat{v}_I^k &amp;= \sum_ip_i^kv_i \\u^k &amp;= \hat{v}_I^k+u^{k-1}\end{alignat}\]</span> 作者通过实验发现，第一层 attention 可以识别问题中出现的实体，第二层则可以消除无关的，只关心与答案相关的实体，多加几层对识别效果没有明显提升。</p><p><strong>输出层</strong></p><p><img src="/images/vqa_out.png"></p><p>由于输出只是一个词，所以可以转化为分类问题，在所有候选答案里挑一个词出来： <span class="math display">\[p_{ans} = softmax(W_uu^K+b_u)\]</span> 其中 <span class="math inline">\(K\)</span> 为 attention 的层数。</p><h2><span id="可视化-attention-layer">可视化 Attention Layer</span></h2><ul><li>正确结果</li></ul><p><img src="/images/true_vqa.png"></p><ul><li>错误结果</li></ul><p><img src="/images/false_vqa.png"></p><h2><span id="总结">总结</span></h2><p>这篇文章的主要工作在于把 attention 机制应用在 Visual QA 问题中，效果卓群，可解释性强。但也有可改进的地方，如图片编码选择 ResNet 而不是 VGGNet；问题的 word embedding 采用 word2vec；对问题的编码采用 bi-LSTM 等，也许会进一步提高整体的表现。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Zichao Yang, Xiaodong He, Jianfeng Gao , Li Deng , Alex Smola &lt;a href=&quot;https://arxiv.org/abs/1511.02274&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Stacked Attention Networks for Image Question Answering&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这篇文章发表在CVPR2016，作者把 attention 机制应用在 Visual QA，不但能理解神经网络生成答案的 multiple resoning，而且获得了当时最好的效果。&lt;/p&gt;
&lt;p&gt;SAN总共由三部分组成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Image Model：用来编码图片信息&lt;/li&gt;
&lt;li&gt;Question Moel：用来编码问题信息&lt;/li&gt;
&lt;li&gt;Stacked Attention Networks：通过多层 attention layer 不断优化对问题的编码&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="Attention" scheme="http://www.52coding.com.cn/tags/Attention/"/>
    
      <category term="CV" scheme="http://www.52coding.com.cn/tags/CV/"/>
    
      <category term="CNN" scheme="http://www.52coding.com.cn/tags/CNN/"/>
    
      <category term="VQA" scheme="http://www.52coding.com.cn/tags/VQA/"/>
    
  </entry>
  
  <entry>
    <title>Paper Reading - Neural Machine Translation In Linear Time (ByteNet)</title>
    <link href="http://www.52coding.com.cn/2017/08/14/Neural%20Machine%20Translation%20In%20Linear%20Time%20(ByteNet)/"/>
    <id>http://www.52coding.com.cn/2017/08/14/Neural Machine Translation In Linear Time (ByteNet)/</id>
    <published>2017-08-14T01:30:19.000Z</published>
    <updated>2018-11-06T03:46:52.613Z</updated>
    
    <content type="html"><![CDATA[<p>ByteNet 可用于<strong>字符级</strong>的机器翻译模型并且有着很好的表现，它的特点在于可以在线性时间 (linear time) 完成翻译而且能够处理长距离依赖。它也采用编码器-解码器架构，并且编码器和解码器都由CNN组成。</p><p>ByteNet 之所以有上述的这些特性，是因为使用了如下一些技术：</p><ul><li>Dynamic Unfolding<ul><li>解决了生成不同长度翻译的问题</li></ul></li><li>Dilated Convolution<ul><li>缩短了依赖传播的距离</li></ul></li><li>Masked 1D Convolution<ul><li>保证训练时只用过去的信息生成当前字符</li></ul></li><li>Residual Blocks<ul><li>解决梯度消失问题</li></ul></li></ul><a id="more"></a><p><img src="/images/byte.png"></p><h2><span id="dynamic-unfolding">Dynamic Unfolding</span></h2><p><img src="/images/dy_unfold.png"></p><p>Encoder 的输出的句子编码的长度固定为 <span class="math inline">\(|\hat{t}|\)</span> (不够会补零)，是目标句子长度 <span class="math inline">\(|t|\)</span> 的上界，可以通过下式得到： <span class="math display">\[|\hat{t}| = a|s| + b\]</span> 其中，<span class="math inline">\(|s|\)</span> 表示源句子长度，通过选择适当的参数 <span class="math inline">\(a\)</span> 和 <span class="math inline">\(b\)</span>，使得 <span class="math inline">\(|\hat{t}|\)</span> 基本都大于实际长度 <span class="math inline">\(|t|\)</span>，并且没有太多冗余。</p><p>在每一步，decoder 根据当前输入字符和句子特征输出下一个字符，直到生成EOS。至于 decoder 怎么接收输入并 conditioned on 编码器的输出，论文在并没有提及，不过从一个<a href="https://github.com/paarthneekhara/byteNet-tensorflow/blob/master/ByteNet/model.py" target="_blank" rel="noopener">开源实现</a>中看出是直接把输入和编码器在对应位置的输出连接起来，如上图所示。</p><h2><span id="dilated-convolution">Dilated Convolution</span></h2><p>一维离散卷积的定义为： <span class="math display">\[(f*g)[n] = \sum_{m=-\infty}^{\infty}f[m]g[n-m] = \sum_{m=-M}^Mf[n - m]g[m]\]</span> 例：如果 <span class="math inline">\(f = [0, 1, 2, -1, 1, -3, 0]\)</span>, <span class="math inline">\(g = [1, 0, -1]\)</span>，则按照上式，卷积计算如下所示： <span class="math display">\[\begin{array}{lcl}(f*g)[2]        &amp; = &amp; f[2]g[0] + f[1]g[1] + f[0]g[2] = -2 \\(f*g)[3]  &amp; = &amp; f[3]g[0] + f[2]g[1] + f[1]g[2] = 2 \\...\\(f*g)[6]  &amp; = &amp; f[6]g[0] + f[5]g[1] + f[4]g[2] = 1 \\\end{array}\]</span> 和 stride = 1 的普通卷积网络计算一致。</p><p><img src="/images/stride.jpeg"></p><p><strong>Dilated Convolution</strong>的定义为： <span class="math display">\[(f*_lg)[n] = \sum_{m=-\infty}^{\infty}f[m]g[n-lm] = \sum_{m=-M}^Mf[n - lm]g[m]\]</span> 其中，<span class="math inline">\(l\)</span> 为 dilation factor，控制扩张大小，这样 <span class="math inline">\(l = 2\)</span> 时上面例子中的卷积就变成了： <span class="math display">\[\begin{array}{lcl}(f*_2g)[4]        &amp; = &amp; f[4]g[0] + f[2]g[1] + f[0]g[2]  \\(f*_2g)[5]  &amp; = &amp; f[5]g[0] + f[3]g[1] + f[1]g[2] \\(f*_2g)[6]  &amp; = &amp; f[6]g[0] + f[4]g[1] + f[2]g[2]  \\\end{array}\]</span> 当 <span class="math inline">\(l = 3\)</span> 时，相应卷积就为: <span class="math display">\[(f*_3g)[6] = f[6]g[0] + f[3]g[1] + f[0]g[2]\]</span> 这样虽然卷积核都为3，但 receptive field 的大小却大了很多，所以使用 dialted conv 能使 <strong>receptive field</strong> 的大小呈<strong>指数增长</strong>，而相应<strong>参数</strong>却是<strong>线性增长</strong>的，如下图所示。使用 dilated conv 就可以有效地缩短依赖传播的距离。</p><p><img src="/images/dilated.png"></p><p>参考：<a href="https://arxiv.org/abs/1511.07122" target="_blank" rel="noopener">MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS</a></p><h2><span id="residual-block">Residual Block</span></h2><p><img src="/images/residual.png"></p><p>每一层（包括 Encoder 和 Decoder）都封装了一个 residual block（上图），其中每个黄色的格子代表一个卷积层，里面的数字是相应的 filter size。中间的 Masked 1 x K 是这层的主力，其他都是为了使他发挥更大效果的陪衬。</p><h2><span id="linear-time">Linear Time</span></h2><p><img src="/images/lt.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ByteNet 可用于&lt;strong&gt;字符级&lt;/strong&gt;的机器翻译模型并且有着很好的表现，它的特点在于可以在线性时间 (linear time) 完成翻译而且能够处理长距离依赖。它也采用编码器-解码器架构，并且编码器和解码器都由CNN组成。&lt;/p&gt;
&lt;p&gt;ByteNet 之所以有上述的这些特性，是因为使用了如下一些技术：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dynamic Unfolding
&lt;ul&gt;
&lt;li&gt;解决了生成不同长度翻译的问题&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Dilated Convolution
&lt;ul&gt;
&lt;li&gt;缩短了依赖传播的距离&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Masked 1D Convolution
&lt;ul&gt;
&lt;li&gt;保证训练时只用过去的信息生成当前字符&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Residual Blocks
&lt;ul&gt;
&lt;li&gt;解决梯度消失问题&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="NLP" scheme="http://www.52coding.com.cn/tags/NLP/"/>
    
      <category term="NMT" scheme="http://www.52coding.com.cn/tags/NMT/"/>
    
      <category term="Deep NLP" scheme="http://www.52coding.com.cn/tags/Deep-NLP/"/>
    
      <category term="CNN" scheme="http://www.52coding.com.cn/tags/CNN/"/>
    
      <category term="ByteBet" scheme="http://www.52coding.com.cn/tags/ByteBet/"/>
    
  </entry>
  
</feed>
