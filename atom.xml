<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>NIUHE</title>
  
  <subtitle>日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.52coding.com.cn/"/>
  <updated>2019-05-05T08:08:02.508Z</updated>
  <id>http://www.52coding.com.cn/</id>
  
  <author>
    <name>NIUHE</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PyTorch源码浅析(5)：Python扩展</title>
    <link href="http://www.52coding.com.cn/2019/05/05/PyTorch5/"/>
    <id>http://www.52coding.com.cn/2019/05/05/PyTorch5/</id>
    <published>2019-05-05T07:58:01.000Z</published>
    <updated>2019-05-05T08:08:02.508Z</updated>
    
    <content type="html"><![CDATA[<p>这篇是本系列最后一篇博客了，介绍一下前面的C++代码怎么与Python交互，或者说Python里怎么调用C++代码进行高效的计算。首先简单介绍一下预备知识，既Python的C扩展通常怎么写；然后以比较核心的数据结构 Tensor 和 Storage 为例看一下它们怎么转换为Python类型的；最后稍带点儿Python自微分函数的实现。</p><a id="more"></a><!-- toc --><ul><li><a href="#python的cc扩展">Python的C/C++扩展</a><ul><li><a href="#扩展模块">扩展模块</a></li><li><a href="#自定义python类型">自定义Python类型</a></li></ul></li><li><a href="#torch_c">torch._C</a><ul><li><a href="#storage">Storage</a></li><li><a href="#tensor">Tensor</a></li><li><a href="#nn">NN</a></li></ul></li></ul><!-- tocstop --><h2><span id="python的cc扩展">Python的C/C++扩展</span></h2><h3><span id="扩展模块">扩展模块</span></h3><p>对于简单的C代码，构建一个自定义扩展模块是很容易的。<code>C/C++</code>部分基本上只需要做以下几件事：</p><ul><li>包含头文件 <code>Python.h</code></li><li>正确的声明函数，即<ul><li>函数必须是 <code>static</code></li><li>返回类型必须是 <code>PyObject *</code></li><li>参数列表必须是 <code>PyObject *self, PyObject *args</code></li></ul></li><li>定义一个Method Table，把模块需要包括的函数都放进去</li><li>定义模块和初始化函数</li></ul><p>举个🌰，下面的代码构建了一个只有一个函数的Python模块，该函数的功能是求最大公约数（<code>py_gcd</code>）:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"Python.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"sample.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* 把普通C语言实现的gcd()封装成Python可以调用的函数 */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> PyObject *<span class="title">py_gcd</span><span class="params">(PyObject *self, PyObject *args)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> x, y, result;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 从 args 里解析实际参数 */</span></span><br><span class="line">  <span class="keyword">if</span> (!PyArg_ParseTuple(args,<span class="string">"ii"</span>, &amp;x, &amp;y)) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/* 调用普通C语言实现的gcd() */</span></span><br><span class="line">  result = gcd(x,y);</span><br><span class="line">  <span class="comment">/* 把 int 转化为 PyObject* */</span></span><br><span class="line">  <span class="keyword">return</span> Py_BuildValue(<span class="string">"i"</span>, result);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 定义模块的 method table */</span></span><br><span class="line"><span class="keyword">static</span> PyMethodDef SampleMethods[] = &#123;</span><br><span class="line">  &#123;<span class="string">"gcd"</span>,  py_gcd, METH_VARARGS, <span class="string">"Greatest common divisor"</span>&#125;,</span><br><span class="line">  &#123; <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">0</span>, <span class="literal">NULL</span>&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 定义模块结构 */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">struct</span> <span class="title">PyModuleDef</span> <span class="title">samplemodule</span> = &#123;</span></span><br><span class="line">  PyModuleDef_HEAD_INIT,</span><br><span class="line">  <span class="string">"sample"</span>,           <span class="comment">/* name of module */</span></span><br><span class="line">  <span class="string">"A sample module"</span>,  <span class="comment">/* Doc string (may be NULL) */</span></span><br><span class="line">  <span class="number">-1</span>,                 <span class="comment">/* Size of per-interpreter state or -1 */</span></span><br><span class="line">  SampleMethods       <span class="comment">/* Method table */</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 模块初始化函数 */</span></span><br><span class="line"><span class="function">PyMODINIT_FUNC <span class="title">PyInit_sample</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> PyModule_Create(&amp;samplemodule);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>具体细节就不展开了，有兴趣的童鞋可以参考下面的链接。</p><p>要绑定这个扩展模块，像下面这样创建一个 <code>setup.py</code> 文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># setup.py</span></span><br><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup, Extension</span><br><span class="line"></span><br><span class="line">setup(name=<span class="string">'sample'</span>,</span><br><span class="line">      ext_modules=[</span><br><span class="line">        Extension(<span class="string">'sample'</span>,</span><br><span class="line">                  [<span class="string">'pysample.c'</span>],</span><br><span class="line">                  include_dirs = [<span class="string">'/some/dir'</span>],</span><br><span class="line">                  define_macros = [(<span class="string">'FOO'</span>,<span class="string">'1'</span>)],</span><br><span class="line">                  undef_macros = [<span class="string">'BAR'</span>],</span><br><span class="line">                  library_dirs = [<span class="string">'/usr/local/lib'</span>],</span><br><span class="line">                  libraries = [<span class="string">'sample'</span>]</span><br><span class="line">                  )</span><br><span class="line">        ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>为了构建最终的函数库，只需简单的使用 <code>python3 buildlib.py build_ext --inplace</code> 命令即可。它会创建一个名字叫 <code>sample.so</code> 的共享库，当被编译后，你就能将它作为一个模块导入进来了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> sample</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sample.gcd(<span class="number">35</span>, <span class="number">42</span>)</span><br><span class="line"><span class="number">7</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><blockquote><p>注：这部分主要参考<a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/c15/p02_write_simple_c_extension_module.html" target="_blank" rel="noopener">Python3-Cookbook</a>.</p></blockquote><h3><span id="自定义python类型">自定义Python类型</span></h3><p>在Python代码中如果要创建一个自定义类使用<code>class</code>关键字即可，但是在C代码中就没那么方便了。首先简单介绍下Python中的类型。在Python中一切皆对象，Python中有两种对象：</p><ul><li>一种是类型对象（class对象）：表示Python定义的类型，例如<code>int, str, object</code>等；</li><li>另一种是实例对象（instance对象）：表示由class对象创建的实例。</li></ul><p>Python中的所有对象都是直接或者间接继承<code>object</code>，然后<code>object</code>又是<code>type</code>类型。Python对象的C语言实现也是分为两部分，一部分表示实例对象，存储对象实际的数据；另一部分是类型对象，存储对象的元数据。也就是说，自定义类型也要实现这两部分，举个例子：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 实例对象 */</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    PyObject_HEAD</span><br><span class="line">    <span class="comment">/* 类型实际的数据在这里定义 */</span></span><br><span class="line">    <span class="keyword">int</span> value;</span><br><span class="line">&#125; noddy_NoddyObject;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 类型对象 */</span></span><br><span class="line"><span class="keyword">static</span> PyTypeObject noddy_NoddyType = &#123;</span><br><span class="line">    PyVarObject_HEAD_INIT(<span class="literal">NULL</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="string">"noddy.Noddy"</span>,             <span class="comment">/*tp_name*/</span></span><br><span class="line">    <span class="keyword">sizeof</span>(noddy_NoddyObject), <span class="comment">/*tp_basicsize*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_itemsize*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_dealloc*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_print*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_getattr*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_setattr*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_compare*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_repr*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_as_number*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_as_sequence*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_as_mapping*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_hash */</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_call*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_str*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_getattro*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_setattro*/</span></span><br><span class="line">    <span class="number">0</span>,                         <span class="comment">/*tp_as_buffer*/</span></span><br><span class="line">    Py_TPFLAGS_DEFAULT,        <span class="comment">/*tp_flags*/</span></span><br><span class="line">    <span class="string">"Noddy objects"</span>,           <span class="comment">/*tp_doc*/</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>然后创建一个新扩展模块，并完成初始化：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 定义模块的 method table */</span></span><br><span class="line"><span class="keyword">static</span> PyMethodDef noddy_methods[] = &#123;</span><br><span class="line">    &#123;<span class="literal">NULL</span>&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 模块初始化函数 */</span></span><br><span class="line"><span class="function">PyMODINIT_FUNC <span class="title">initnoddy</span><span class="params">(<span class="keyword">void</span>)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    PyObject* m;</span><br><span class="line"><span class="comment">/* tp_new 相当于Python里的 __new__ */</span></span><br><span class="line">    noddy_NoddyType.tp_new = PyType_GenericNew;</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> (PyType_Ready(&amp;noddy_NoddyType) &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    m = Py_InitModule3(<span class="string">"noddy"</span>, noddy_methods,</span><br><span class="line">                       <span class="string">"Example module"</span>);</span><br><span class="line"></span><br><span class="line">    Py_INCREF(&amp;noddy_NoddyType);</span><br><span class="line">  <span class="comment">/* 向模块添加类型 */</span></span><br><span class="line">    PyModule_AddObject(m, <span class="string">"Noddy"</span>, (PyObject*)&amp;noddy_NoddyType);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>注：这部分介绍的比较简略，详细请参考 <a href="http://www.xefan.com/archives/84091.html" class="uri" target="_blank" rel="noopener">http://www.xefan.com/archives/84091.html</a>.</p></blockquote><h2><span id="torch_c">torch._C</span></h2><p>有了上面的预备知识之后，我们就能看ATen还有autograd等模块的代码是怎么导入Python了。构建Python扩展模块的代码在 <code>torch/csrc/Module.cpp</code> 里，主要部分如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">/* method table */</span></span><br><span class="line"><span class="keyword">static</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;PyMethodDef&gt; methods;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 初始化模块 */</span></span><br><span class="line"><span class="function">PyObject* <span class="title">initModule</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  HANDLE_TH_ERRORS</span><br><span class="line">  THInferNumThreads();</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 向 method table 添加函数 */</span></span><br><span class="line">  THPUtils_addPyMethodDefs(methods, TorchMethods);</span><br><span class="line">  THPUtils_addPyMethodDefs(methods, DataLoaderMethods);</span><br><span class="line">  THPUtils_addPyMethodDefs(methods, </span><br><span class="line">                           torch::autograd::python_functions());</span><br><span class="line">  THPUtils_addPyMethodDefs(methods, </span><br><span class="line">                       torch::multiprocessing::python_functions());</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 构建 torch._C 模块 */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> PY_MAJOR_VERSION == 2</span></span><br><span class="line">  ASSERT_TRUE(<span class="keyword">module</span> = Py_InitModule(<span class="string">"torch._C"</span>, methods.data()));</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">  <span class="keyword">static</span> <span class="class"><span class="keyword">struct</span> <span class="title">PyModuleDef</span> <span class="title">torchmodule</span> = &#123;</span></span><br><span class="line">      PyModuleDef_HEAD_INIT, <span class="string">"torch._C"</span>, <span class="literal">nullptr</span>, <span class="number">-1</span>, </span><br><span class="line">      methods.data()</span><br><span class="line">  &#125;;</span><br><span class="line">  ASSERT_TRUE(<span class="keyword">module</span> = PyModule_Create(&amp;torchmodule));</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* 各种初始化 */</span></span><br><span class="line">  ASSERT_TRUE(THPWrapper_init(<span class="keyword">module</span>));</span><br><span class="line">  ...</span><br><span class="line">  torch::autograd::initNNFunctions(<span class="keyword">module</span>);<span class="comment">// 初始化自微分相关API</span></span><br><span class="line">  torch::autograd::init_legacy_variable(<span class="keyword">module</span>);<span class="comment">// 初始化Tensor类型</span></span><br><span class="line">  torch::python::init_bindings(<span class="keyword">module</span>);<span class="comment">// 初始化NN相关函数</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_CUDA</span></span><br><span class="line">  torch::cuda::initModule(<span class="keyword">module</span>);<span class="comment">// 初始化CUDA模块</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">  <span class="comment">/* 初始化各种Storage类型 */</span></span><br><span class="line">  ASSERT_TRUE(THPDoubleStorage_init(<span class="keyword">module</span>));</span><br><span class="line">  ASSERT_TRUE(THPFloatStorage_init(<span class="keyword">module</span>));</span><br><span class="line">  ASSERT_TRUE(THPHalfStorage_init(<span class="keyword">module</span>));</span><br><span class="line">  ASSERT_TRUE(THPLongStorage_init(<span class="keyword">module</span>));</span><br><span class="line">  ASSERT_TRUE(THPIntStorage_init(<span class="keyword">module</span>));</span><br><span class="line">  ASSERT_TRUE(THPShortStorage_init(<span class="keyword">module</span>));</span><br><span class="line">  ASSERT_TRUE(THPCharStorage_init(<span class="keyword">module</span>));</span><br><span class="line">  ASSERT_TRUE(THPByteStorage_init(<span class="keyword">module</span>));</span><br><span class="line">  ASSERT_TRUE(THPBoolStorage_init(<span class="keyword">module</span>));</span><br><span class="line"> </span><br><span class="line">  ...</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">module</span>;</span><br><span class="line">  END_HANDLE_TH_ERRORS</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>基本上所有C/C++实现的API都被绑定在 <code>torch._C</code> 扩展模块中，下面以Storage和Tensor为例，看一下 <code>torch.Storage</code>和<code>torch.Tensor</code> 类型的绑定方法，比较有意思的是它们两个的绑定方式区别还挺大的。</p><h3><span id="storage">Storage</span></h3><p>从上面的 <code>initModule()</code> 函数中可以看到，里面有许多初始化各种Storage类型的代码，它们的目的就是创建各种Storage类型，如 <code>torch._C._FloatStorageBase</code>, <code>torch._C._LongStorageBase</code>等，而 <code>torch.FloatStorage</code> 等类型是从Python端创建的，继承自 <code>torch._C._FloatStorageBase</code> 等类型，这部分代码可以在<code>torch/__init__.py</code>中找到。</p><p>回到绑定过程，<code>THPDoubleStorage_init()</code> 等函数其实是用C范式生成的，和<a href="要加链接">第一篇</a>里的TH库中用的方法一样。它实际调用的函数是 <code>bool THPStorage_(init)(PyObject *module)</code>，实现 <code>torch/csrc/generic/Storage.cpp</code>里，这个函数会根据不同类型展开：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">THPStorage_</span><span class="params">(init)</span><span class="params">(PyObject *<span class="keyword">module</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">static</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;PyMethodDef&gt; methods;</span><br><span class="line">  THPUtils_addPyMethodDefs(methods, THPStorage_(methods));</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> THD_GENERIC_FILE</span></span><br><span class="line">  THPUtils_addPyMethodDefs(methods, THPStorage_(sharingMethods));</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 绑定类方法 */</span></span><br><span class="line">  THPStorageType.tp_methods = methods.data();</span><br><span class="line">  <span class="comment">/* 绑定类成员 */</span></span><br><span class="line">  THPStorageType.tp_members = THPStorage_(members);</span><br><span class="line">  <span class="keyword">if</span> (PyType_Ready(&amp;THPStorageType) &lt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  Py_INCREF(&amp;THPStorageType);</span><br><span class="line">  <span class="comment">/* 向模块添加 THPStorage 类型 */</span></span><br><span class="line">  PyModule_AddObject(<span class="keyword">module</span>, THPStorageBaseStr, </span><br><span class="line">                     (PyObject*)&amp;THPStorageType);</span><br><span class="line">  THPStorage_(initCopyMethods)();</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个函数初始化了 <code>THPStorageType</code> 类型并添加到<code>torch._C</code>模块中，该类型的定义如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 实例对象 */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">THPStorage</span> &#123;</span></span><br><span class="line">  PyObject_HEAD</span><br><span class="line">  <span class="comment">/* THWStorage 为宏定义，会转换为 THxxxStorage */</span></span><br><span class="line">  THWStorage *cdata;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 类型对象 */</span></span><br><span class="line">PyTypeObject THPStorageType = &#123;</span><br><span class="line">  PyVarObject_HEAD_INIT(<span class="literal">nullptr</span>, <span class="number">0</span>)</span><br><span class="line">  <span class="string">"torch._C."</span> THPStorageBaseStr,         <span class="comment">/* tp_name */</span></span><br><span class="line">  <span class="keyword">sizeof</span>(THPStorage),                    <span class="comment">/* tp_basicsize */</span></span><br><span class="line">  <span class="number">0</span>,                                     <span class="comment">/* tp_itemsize */</span></span><br><span class="line">  (destructor)THPStorage_(dealloc),      <span class="comment">/* tp_dealloc */</span></span><br><span class="line">  ...</span><br><span class="line">  THPStorage_(pynew),                    <span class="comment">/* tp_new */</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>Python里类型的名字由 <code>tp_name</code> 域确定，也就是 <code>&quot;torch._C.&quot; THPStorageBaseStr</code>，后者一看就是宏定义，它展开后会变成 <code>_xxxStorageBase</code>，其中<code>xxx</code>为各种类型，所以最后就变成了 <code>torch._C._FloatStorageBase</code> 等类型。</p><h3><span id="tensor">Tensor</span></h3><p><code>torch.Tensor</code>的实现就与<code>torch.Storage</code> 不一样了，因为ATen的存在，绑定的话也是绑定ATen里的Tensor，不会绑定THTensor。还有一点，就是Python里的Tensor和Variable合并了，所以<code>torch.Tensor</code>直接和 <code>autograd::Variable</code> 绑定在一起了。不过准确来说是 <code>torch._C._TensorBase</code> 和 <code>autograd::Variable</code> 绑定在一起了，而 <code>torch.Tensor</code> 继承自 <code>torch._C._TensorBase</code>。</p><p>绑定的代码在 <code>torch/csrc/autograd/python_variable.cpp</code>中：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// python_variable.h</span></span><br><span class="line"><span class="comment">/* 实例对象 */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">THPVariable</span> &#123;</span></span><br><span class="line">    PyObject_HEAD</span><br><span class="line">    <span class="comment">// Payload</span></span><br><span class="line">    torch::autograd::Variable cdata;</span><br><span class="line">    PyObject* backward_hooks = <span class="literal">nullptr</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// python_variable.cpp</span></span><br><span class="line">...</span><br><span class="line"><span class="comment">/* 类型对象 */</span></span><br><span class="line">PyTypeObject THPVariableType = &#123;</span><br><span class="line">  PyVarObject_HEAD_INIT(<span class="literal">nullptr</span>, <span class="number">0</span>)</span><br><span class="line">  <span class="string">"torch._C._TensorBase"</span>,                <span class="comment">/* tp_name */</span></span><br><span class="line">  <span class="keyword">sizeof</span>(THPVariable),                   <span class="comment">/* tp_basicsize */</span></span><br><span class="line">  <span class="number">0</span>,                                     <span class="comment">/* tp_itemsize */</span></span><br><span class="line">  (destructor)THPVariable_dealloc,       <span class="comment">/* tp_dealloc */</span></span><br><span class="line">  ...</span><br><span class="line">  THPVariable_pynew                      <span class="comment">/* tp_new */</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 初始化类型 */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">THPVariable_initModule</span><span class="params">(PyObject *<span class="keyword">module</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">/* 获取 method table */</span></span><br><span class="line">  <span class="keyword">static</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;PyMethodDef&gt; methods;</span><br><span class="line">  THPUtils_addPyMethodDefs(methods, </span><br><span class="line">                           torch::autograd::variable_methods);</span><br><span class="line">  THPUtils_addPyMethodDefs(methods, extra_methods);</span><br><span class="line">  <span class="comment">/* 绑定类方法 */</span></span><br><span class="line">  THPVariableType.tp_methods = methods.data();</span><br><span class="line">  <span class="keyword">if</span> (PyType_Ready(&amp;THPVariableType) &lt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  Py_INCREF(&amp;THPVariableType);</span><br><span class="line">  <span class="comment">/* 向模块中添加类型 */</span></span><br><span class="line">  PyModule_AddObject(<span class="keyword">module</span>, <span class="string">"_TensorBase"</span>, </span><br><span class="line">                     (PyObject *)&amp;THPVariableType);</span><br><span class="line">  torch::autograd::initTorchFunctions(<span class="keyword">module</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意到，这里只绑定了 <code>_TensorBase</code> 一种类型，而不像Storage那样利用宏把各种类型的StorageBase都定义了。</p><p>其他类型的Tensor，如 <code>torch.FloatTensor</code> 等在 <code>torch/tensor/python_tensor.cpp</code> 中的 <code>initialize_python_bindings()</code> 函数里动态绑定：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initialize_python_bindings</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">/* 把ATen里的Tensor类型转化为Python里的PyTypeObject */</span></span><br><span class="line">  initialize_aten_types(tensor_types);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 初始化上面转化来的PyTypeObject */</span></span><br><span class="line">  py_initialize_metaclass(metaclass);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 获取 torch.Tensor 的所有方法 */</span></span><br><span class="line">  <span class="keyword">auto</span> tensor_dict = get_tensor_dict();</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 把torch.Tensor的方法复制给每个类型，如torch.FloatTensor等 */</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; tensor_type : tensor_types) &#123;</span><br><span class="line">    py_initialize_tensor_type(tensor_type.py_type, </span><br><span class="line">                              tensor_type.name, tensor_dict.get());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 向torch模块绑定这些各种类型的Tensor */</span></span><br><span class="line">  py_bind_tensor_types(tensor_types);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 设置 torch.Tensor 的默认类型为 torch.FloatTensor */</span></span><br><span class="line">  set_default_tensor_type(at::globalContext().getVariableType(</span><br><span class="line">    at::Backend::CPU, at::kFloat));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个函数由Python调用，调用的代码在 <code>torch/__init__.py</code> 中（<code>_C._initExtension()</code>）。调用时 <code>torch.Tensor</code> 已经定义，这个函数要做的就是定义其他Tensor类型，然后把Tensor类型的方法直接拷贝给它们，最后在设置一下默认类型的Tensor。为什么数据类型不同却可以直接拷贝？因为 <code>at::Tensor</code> 可以针对不同数据类型调用不同的方法，类型多态已经在ATen内部实现了。</p><p>在上面的代码中，Tensor 绑定的方法来自 <code>torch::autograd::variable_methods</code>，这个列表在 <code>csrc/autograd/generated/python_variable_methods.cpp</code> 中：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">PyMethodDef variable_methods[] = &#123;</span><br><span class="line">  &#123;<span class="string">"__add__"</span>, (PyCFunction)THPVariable_add, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"__radd__"</span>, (PyCFunction)THPVariable_add, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"__iadd__"</span>, (PyCFunction)THPVariable_add_, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"__rmul__"</span>, (PyCFunction)THPVariable_mul, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"__mul__"</span>, (PyCFunction)THPVariable_mul, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"__imul__"</span>, (PyCFunction)THPVariable_mul_, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"__sub__"</span>, (PyCFunction)THPVariable_sub, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"addcmul"</span>, (PyCFunction)THPVariable_addcmul, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从文件路径可以看出这也是根据 <code>derivatives.yaml</code> 自动生成的代码，拿 <code>addcmul</code> 举个例子看看这些函数的实现方法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> PyObject * <span class="title">THPVariable_addcmul</span><span class="params">(PyObject* self_, PyObject* args, PyObject* kwargs)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  HANDLE_TH_ERRORS</span><br><span class="line">  <span class="function"><span class="keyword">static</span> PythonArgParser <span class="title">parser</span><span class="params">(&#123;</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="string">"addcmul(Scalar value, Tensor tensor1, Tensor tensor2)|deprecated"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="string">"addcmul(Tensor tensor1, Tensor tensor2, *, Scalar value=1)"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">  &#125;, <span class="comment">/*traceable=*/</span><span class="literal">true</span>)</span></span>;</span><br><span class="line">  <span class="comment">/* 获取autograd::Variable实例 */</span></span><br><span class="line">  <span class="keyword">auto</span>&amp; self = <span class="keyword">reinterpret_cast</span>&lt;THPVariable*&gt;(self_)-&gt;cdata;</span><br><span class="line">  <span class="comment">/* 解析函数参数 */</span></span><br><span class="line">  ParsedArgs&lt;<span class="number">4</span>&gt; parsed_args;</span><br><span class="line">  <span class="keyword">auto</span> r = parser.parse(args, kwargs, parsed_args);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 调用 dispatch */</span></span><br><span class="line">  <span class="keyword">if</span> (r.idx == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> wrap(dispatch_addcmul(self, r.scalar(<span class="number">0</span>), r.tensor(<span class="number">1</span>), r.tensor(<span class="number">2</span>)));</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (r.idx == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> wrap(dispatch_addcmul(self, r.tensor(<span class="number">0</span>), r.tensor(<span class="number">1</span>), r.scalar(<span class="number">2</span>)));</span><br><span class="line">  &#125;</span><br><span class="line">  Py_RETURN_NONE;</span><br><span class="line">  END_HANDLE_TH_ERRORS</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终函数调用了 <code>dispatch_addcmul()</code> 进行下一步计算，该函数在同文件夹的 <code>python_variable_methods_dispatch.h</code> ，可见这个文件也是自动生成的，函数声明如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> Tensor <span class="title">dispatch_addcmul</span><span class="params">(Tensor &amp; self, Scalar value, <span class="keyword">const</span> Tensor &amp; tensor1, <span class="keyword">const</span> Tensor &amp; tensor2)</span> </span>&#123;</span><br><span class="line">  <span class="comment">/* 释放GIL锁 */</span></span><br><span class="line">  AutoNoGIL no_gil;</span><br><span class="line">  <span class="comment">/* 调用 autograd::Variable.addcmul */</span></span><br><span class="line">  <span class="keyword">return</span> self.addcmul(tensor1, tensor2, value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个函数首先获取了GIL锁，然后调用C++前端 <code>Variable.addcmul()</code> 进行计算，由于后者实现了自动微分，所以Python调用也具有自动微分功能。为什么要释放GIL锁？因为这样才能使其与Python解释器中的其他进程一起正确的执行。</p><p>也就是说，Python Tensor的方法的封装思路是：</p><ul><li>生成dispatch系列函数，该函数用于释放GIL锁，然后调用Variable的对应实现</li><li>生成可被Python调用的API，该函数解析Python参数，并调用dispatch系列函数进行实际计算</li></ul><h3><span id="nn">NN</span></h3><p>神经网络的部分函数也有部分函数是直接从 ATen 绑定而来，绑定到 <code>torch._C._nn</code> 模块中，代码在 <code>csrc/autograd/generate/python_nn_functions.cpp</code> 中：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> PyMethodDef nn_functions[] = &#123;</span><br><span class="line">  &#123;<span class="string">"_parse_to"</span>, (PyCFunction)THPVariable__parse_to, METH_VARARGS | METH_KEYWORDS, <span class="literal">nullptr</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"adaptive_avg_pool2d"</span>, (PyCFunction)THPVariable_adaptive_avg_pool2d, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"adaptive_avg_pool3d"</span>, (PyCFunction)THPVariable_adaptive_avg_pool3d, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"adaptive_max_pool2d"</span>, (PyCFunction)THPVariable_adaptive_max_pool2d, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"adaptive_max_pool3d"</span>, (PyCFunction)THPVariable_adaptive_max_pool3d, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"avg_pool2d"</span>, (PyCFunction)THPVariable_avg_pool2d, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"avg_pool3d"</span>, (PyCFunction)THPVariable_avg_pool3d, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"binary_cross_entropy"</span>, (PyCFunction)THPVariable_binary_cross_entropy, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"elu"</span>, (PyCFunction)THPVariable_elu, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"elu_"</span>, (PyCFunction)THPVariable_elu_, METH_VARARGS | METH_KEYWORDS, <span class="literal">NULL</span>&#125;,</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> initNNFunctions(PyObject* <span class="keyword">module</span>) &#123;</span><br><span class="line">#<span class="keyword">if</span> PY_MAJOR_VERSION == <span class="number">2</span></span><br><span class="line">  PyObject* nn = Py_InitModule(<span class="string">"torch._C._nn"</span>, nn_functions);</span><br><span class="line">  Py_XINCREF(nn);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">  <span class="keyword">static</span> <span class="class"><span class="keyword">struct</span> <span class="title">PyModuleDef</span> <span class="title">def</span> = &#123;</span></span><br><span class="line">     PyModuleDef_HEAD_INIT,</span><br><span class="line">     <span class="string">"torch._C._nn"</span>,</span><br><span class="line">     <span class="literal">NULL</span>,</span><br><span class="line">     <span class="number">-1</span>,</span><br><span class="line">     nn_functions</span><br><span class="line">  &#125;;</span><br><span class="line">  PyObject* nn = PyModule_Create(&amp;def);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">  <span class="keyword">if</span> (!nn) &#123;</span><br><span class="line">    <span class="keyword">throw</span> python_error();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (PyModule_AddObject(<span class="keyword">module</span>, <span class="string">"_nn"</span>, nn) != <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">throw</span> python_error();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里面有pooling, loss, conv等相关函数，供Python里的<code>nn.functional</code>调用。</p><p>此外，为了方便在Python中自定义的自微分的函数，Python里也实现了上一篇对应的Function：<code>torch.autograd.Function</code>。继承它，重载 <code>forward()</code> 和 <code>backward()</code> 方法就可以用Python实现自定义的自微分函数，详见<a href="https://pytorch.org/docs/stable/notes/extending.html" target="_blank" rel="noopener">文档</a>。</p><p><code>torch.autograd.Function</code> 的定义在 <code>torch/autograd/function.py</code> 中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Function</span><span class="params">(with_metaclass<span class="params">(FunctionMeta, _C._FunctionBase, _ContextMethodMixin, _HookMixin)</span>)</span>:</span></span><br><span class="line">    <span class="comment"># only for backward compatibility</span></span><br><span class="line">    __call__ = _C._FunctionBase._do_forward</span><br><span class="line"></span><br><span class="line">    <span class="comment"># for the tracer</span></span><br><span class="line">    is_traceable = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, *args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, *grad_outputs)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>它继承自 <code>torch._C.FunctionBase</code>，该类型在 <code>csrc/autograd/python_function.cpp</code>中绑定，实现的逻辑和 <code>autograd::Function</code> 一样，也是在前向计算时建立反向计算图，这里就不读赘述了。绑定部分的代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">THPFunction</span> &#123;</span></span><br><span class="line">    PyObject_HEAD</span><br><span class="line"></span><br><span class="line">    PyObject *needs_input_grad;</span><br><span class="line">    PyObject *to_save;</span><br><span class="line">    PyObject *non_differentiable;</span><br><span class="line">    PyObject *dirty_tensors;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;torch::autograd::VariableInfo&gt; output_info;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;torch::autograd::VariableInfo&gt; input_info;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;torch::autograd::SavedVariable&gt; saved_variables;</span><br><span class="line">    <span class="comment">// For each input, true if the input is a THPVariable</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; is_variable_input;</span><br><span class="line">    <span class="keyword">char</span> has_freed_buffers;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* PyFunction继承自Function，实际调用最终转发到Function中 */</span></span><br><span class="line">    torch::autograd::PyFunction cdata;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">struct</span> <span class="title">PyGetSetDef</span> <span class="title">THPFunction_properties</span>[] = &#123;</span></span><br><span class="line">  &#123;<span class="string">"saved_tensors"</span>, (getter)THPFunction_saved_tensors, <span class="literal">nullptr</span>, <span class="literal">nullptr</span>, <span class="literal">nullptr</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"saved_variables"</span>, (getter)THPFunction_saved_variables, <span class="literal">nullptr</span>, <span class="literal">nullptr</span>, <span class="literal">nullptr</span>&#125;,</span><br><span class="line">  &#123;<span class="string">"next_functions"</span>, (getter)THPFunction_next_functions, <span class="literal">nullptr</span>, <span class="literal">nullptr</span>, <span class="literal">nullptr</span>&#125;,</span><br><span class="line">  ...</span><br><span class="line">  &#123;<span class="literal">nullptr</span>&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">struct</span> <span class="title">PyMethodDef</span> <span class="title">THPFunction_methods</span>[] = &#123;</span></span><br><span class="line">  &#123;(<span class="keyword">char</span>*)<span class="string">"apply"</span>, (PyCFunction)THPFunction_apply, METH_CLASS | METH_VARARGS, <span class="literal">nullptr</span>&#125;,</span><br><span class="line">  &#123;(<span class="keyword">char</span>*)<span class="string">"_do_forward"</span>, (PyCFunction)THPFunction_do_forward, METH_VARARGS, <span class="literal">nullptr</span>&#125;,</span><br><span class="line">  &#123;(<span class="keyword">char</span>*)<span class="string">"_do_backward"</span>, (PyCFunction)THPFunction_do_backward, METH_VARARGS, <span class="literal">nullptr</span>&#125;,</span><br><span class="line">  &#123;(<span class="keyword">char</span>*)<span class="string">"_register_hook_dict"</span>, (PyCFunction)THPFunction__register_hook_dict, METH_O, <span class="literal">nullptr</span>&#125;,</span><br><span class="line">  &#123;(<span class="keyword">char</span>*)<span class="string">"register_hook"</span>, (PyCFunction)THPFunction_register_hook, METH_O, <span class="literal">nullptr</span>&#125;,</span><br><span class="line">  &#123;<span class="literal">nullptr</span>&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">PyTypeObject THPFunctionType = &#123;</span><br><span class="line">  PyVarObject_HEAD_INIT(<span class="literal">nullptr</span>, <span class="number">0</span>)</span><br><span class="line">  <span class="string">"torch._C._FunctionBase"</span>,              <span class="comment">/* tp_name */</span></span><br><span class="line">  <span class="keyword">sizeof</span>(THPFunction),                   <span class="comment">/* tp_basicsize */</span></span><br><span class="line">  <span class="number">0</span>,                                     <span class="comment">/* tp_itemsize */</span></span><br><span class="line">  ...</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">THPFunction_initModule</span><span class="params">(PyObject *<span class="keyword">module</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (PyType_Ready(&amp;THPFunctionType) &lt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  Py_INCREF(&amp;THPFunctionType);</span><br><span class="line">  PyModule_AddObject(<span class="keyword">module</span>, <span class="string">"_FunctionBase"</span>, </span><br><span class="line">                     (PyObject *)&amp;THPFunctionType);</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>总结一下：</p><ul><li><p><code>THPFunction</code> = <code>_C._FunctionBase</code></p></li><li><p>Python的 <code>autograd.Function</code> 继承自上面实现自动微分</p></li></ul><p>完</p><table><thead><tr class="header"><th style="text-align: left;">上一篇：<a href="https://www.52coding.com.cn/2019/05/05/PyTorch4/">Autograd</a></th><th style="text-align: right;"></th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"></td><td style="text-align: right;"></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇是本系列最后一篇博客了，介绍一下前面的C++代码怎么与Python交互，或者说Python里怎么调用C++代码进行高效的计算。首先简单介绍一下预备知识，既Python的C扩展通常怎么写；然后以比较核心的数据结构 Tensor 和 Storage 为例看一下它们怎么转换为Python类型的；最后稍带点儿Python自微分函数的实现。&lt;/p&gt;
    
    </summary>
    
      <category term="博客" scheme="http://www.52coding.com.cn/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="C++" scheme="http://www.52coding.com.cn/tags/C/"/>
    
      <category term="PyTorch" scheme="http://www.52coding.com.cn/tags/PyTorch/"/>
    
      <category term="Python扩展" scheme="http://www.52coding.com.cn/tags/Python%E6%89%A9%E5%B1%95/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch源码浅析(4)：Autograd</title>
    <link href="http://www.52coding.com.cn/2019/05/05/PyTorch4/"/>
    <id>http://www.52coding.com.cn/2019/05/05/PyTorch4/</id>
    <published>2019-05-05T07:57:01.000Z</published>
    <updated>2019-05-05T08:07:27.766Z</updated>
    
    <content type="html"><![CDATA[<p>这篇博客介绍 PyTorch 中自动微分引擎的实现，主要分为三部分：首先简要介绍一下计算图的原理；然后介绍 PyTorch 中与 autograd 的相关数据结构和 <code>backward()</code> 函数的实现，数据结构包括 <code>torch::autograd::Variable</code>, <code>torch::autograd::Function</code> 等；最后讲一下动态建立计算图的实现，这部分代码涉及到动态派发机制，而且都是用脚本生成的，不太容易理解。</p><a id="more"></a><!-- toc --><ul><li><a href="#计算图简介">计算图简介</a></li><li><a href="#autograd-engine">Autograd Engine</a><ul><li><a href="#variable">Variable</a></li><li><a href="#autogradmeta">AutogradMeta</a></li><li><a href="#function-edge">Function &amp; Edge</a></li><li><a href="#engine">Engine</a></li></ul></li><li><a href="#动态建立计算图">动态建立计算图</a></li></ul><!-- tocstop --><h2><span id="计算图简介">计算图简介</span></h2><p>计算图是一个有向图，它的每个节点都表示一个函数（如加减乘除等）或者输入数据（叶子节点）。计算图的边代表数据流向：指向某个节点的边为该节点的输入，由该节点流出的边表示它的输出。计算图可以用来描述神经网络的计算，如下图描述了 <span class="math inline">\(y = \sin(xa+b)​\)</span> 的计算过程：</p><p><img src="/images/pytorch/comp_graph.png"></p><p>计算图的求值分为前向传播和反向传播，分别用于计算输出和梯度。前向传播的过程就是从叶子节点开始遍历计算图，直到整个图都被遍历过；而反向传播就是从输出节点开始遍历，节点表示的函数也变为原函数对输入的导数。举个栗子，下面是<code>addcmul()</code>函数的计算图的前向和反向计算过程：</p><p><img src="/images/pytorch/forward.gif"></p><h2><span id="autograd-engine">Autograd Engine</span></h2><p>介绍完了标准的计算图结构，那么PyTorch里面是怎么实现的呢？在回答这个问题之前首先要看几个相关的数据结构，分别是<code>Variable</code>, <code>AutogradMeta</code>, <code>Function</code>, 和 <code>Edge</code>。</p><h3><span id="variable">Variable</span></h3><p>相信用过老版本的PyTorch的小伙伴对<code>Variable</code>一定不会陌生，它是用来实现自动微分的核心数据结构，代码在<code>torch/csrc/autograd/variable.h</code>。<code>Variable</code> 可以表示计算图中的叶子节点，如权重，也可以表示图中的中间变量，虽然在新版本中 <code>Tensor</code> 与 <code>Variable</code> 合并了，在前端中可以直接用 <code>Tensor</code> 代替 <code>Variable</code>，但是 <code>Variable</code> 并没有消失。</p><p>它的实现确实改变了，但功能依旧。<code>Variable</code> 继承自 <code>at::Tensor</code>，重载了<code>Tensor</code>里与梯度计算相关的方法，同时也提供了和 <code>Tensor</code> 隐式转换的构造函数。</p><p><code>Variable</code> 的底层实现 <code>Variable::Impl</code> 也继承 <code>at::TensorImpl</code>，这个类在<a href="">此系列的第一篇</a>中介绍过，它里面有一个成员 <code>bool is_variable</code>，用于识别这个 Tensor 到底是 <code>at::Tensor</code> 还是 <code>Variable</code>。<code>TensorImpl</code> 里还有一个重要的成员：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;c10::AutogradMetaInterface&gt; autograd_meta_=<span class="literal">nullptr</span>;</span><br></pre></td></tr></table></figure><p><code>autograd_meta_</code> 记录了当前与 <code>Variable</code> 相关的计算图信息，在<code>Variable::Impl</code>里它被强制转换成 <code>AutogradMeta*</code> 类型：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Variable::<span class="function">AutogradMeta* <span class="title">get_autograd_meta</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;Variable::AutogradMeta*&gt;(autograd_meta());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中 <code>AutogradMeta</code> 类型实现了 <code>c10::AutogradMetaInterface</code> 接口。</p><h3><span id="autogradmeta">AutogradMeta</span></h3><p><code>AutogradMeta</code> 的声明也在 <code>variable.h</code> 中，它的声明如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TORCH_API</span> <span class="title">Variable</span>:</span>:AutogradMeta : <span class="keyword">public</span> c10::AutogradMetaInterface &#123;</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">string</span> name;</span><br><span class="line"></span><br><span class="line">  Variable grad_;<span class="comment">// 存储梯度</span></span><br><span class="line">  <span class="comment">// 反向传播函数 for 中间节点</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Function&gt; grad_fn_;</span><br><span class="line">  <span class="comment">// 反向传播函数 for 叶节点（权重），只是把梯度累加起来用来更新</span></span><br><span class="line">  <span class="built_in">std</span>::weak_ptr&lt;Function&gt; grad_accumulator_;</span><br><span class="line"></span><br><span class="line">  VariableVersion version_counter_;</span><br><span class="line">  <span class="comment">// 预处理</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;FunctionPreHook&gt;&gt; hooks_;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">bool</span> requires_grad_;</span><br><span class="line">  <span class="keyword">bool</span> is_view_;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 若该Variable是某个函数的输出，那么output_nr_记录它是第几个输出</span></span><br><span class="line">  <span class="keyword">uint32_t</span> output_nr_;</span><br><span class="line">  PyObject* pyobj_ = <span class="literal">nullptr</span>; <span class="comment">// weak reference</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::mutex mutex_;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">set_requires_grad</span><span class="params">(<span class="keyword">bool</span> requires_grad, at::TensorImpl* self_impl)</span> override </span>&#123;</span><br><span class="line"><span class="comment">/* check */</span></span><br><span class="line">    requires_grad_ = requires_grad;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">requires_grad</span><span class="params">()</span> <span class="keyword">const</span> override </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> requires_grad_ || grad_fn_;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">Variable&amp; <span class="title">grad</span><span class="params">()</span> override </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> grad_;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">const</span> Variable&amp; <span class="title">grad</span><span class="params">()</span> <span class="keyword">const</span> override </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> grad_;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>从声明中可以看出 <code>AutogradMeta</code> 不但存储了梯度，还存储了该<code>Variable</code>对应的反向传播函数，也就是计算图的节点。</p><p>PyTorch只建立反向传播的计算图，因为其实前向传播是用户自己定义的，不用PyTorch干什么。但是在用户在定义前向连接的时候，PyTorch需要偷偷建立反向连接，具体怎么操作见下一节。PyTorch里计算图的节点全部都是<code>Function</code>，由于只计算图只用于反向传播，所以 <code>Function</code> 实现都是反向传播函数。对于计算图中的中间结果，对应的 <code>grad_fn_</code> 是相应的反向传播函数；而对于叶节点（权重），对应的 <code>grad_fn_</code> 为 <code>nullptr</code>，而 <code>grad_accumulator_</code> 为其相应的处理函数，就是把梯度累加存储到 <code>grad_</code> 里。除此之外，不需要梯度的输入是不会进入计算图中的。</p><p>仔细观察我们会发现，<code>Variable</code>里拥有<code>AutogradMeta</code>，而后者里用有 <code>Function</code>，所以实际上 <code>Variable</code> 和它的 <code>grad_fn_</code> 是绑定的，也就是说，一个 <code>Variable</code> 只能有一个 <code>grad_fn_</code>，但反过来则不一定，一个 <code>grad_fn_</code> 也可能属于多个 <code>Variable</code>（如果正向传播函数输出很多的话，那么这些输出共享一个反向传播函数）。</p><h3><span id="function-amp-edge">Function &amp; Edge</span></h3><p><code>Function</code> 和 <code>Edge</code> 实现是紧密相连的，<code>Function</code> 本质是一个函数对象，可以当作反向传播的函数来用。除此之外，<code>Function</code> 里还有一个成员 <code>edge_list next_edges_;</code>，是与该节点相连的边。PyTorch的计算图中的边其实就是 <code>{function, input_nr}</code> pair：前者表示这条边指向哪个节点，后者表示本节点是目标节点的第几个输入（从0开始）。</p><p><code>Function</code> 的实现在 <code>csrc/autograd/function.h</code>，大致声明如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TORCH_API</span> <span class="title">Function</span> :</span> <span class="built_in">std</span>::enable_shared_from_this&lt;Function&gt; &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">/* 构造函数略 */</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 不可拷贝或移动</span></span><br><span class="line">  Function(<span class="keyword">const</span> Function&amp; other) = <span class="keyword">delete</span>;</span><br><span class="line">  Function(Function&amp;&amp; other) = <span class="keyword">delete</span>;</span><br><span class="line">  Function&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> Function&amp; other) = <span class="keyword">delete</span>;</span><br><span class="line">  Function&amp; <span class="keyword">operator</span>=(Function&amp;&amp; other) = <span class="keyword">delete</span>;</span><br><span class="line">  <span class="keyword">virtual</span> ~Function() = <span class="keyword">default</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 重载()运算符实现函数对象功能，需重载apply函数</span></span><br><span class="line">  <span class="comment">// 该函数接收一系列variable，返回一系列variable</span></span><br><span class="line">  <span class="function">variable_list <span class="title">operator</span><span class="params">()</span><span class="params">(variable_list&amp;&amp; inputs)</span> </span>&#123;</span><br><span class="line">    profiler::<span class="function">RecordFunction <span class="title">rec</span><span class="params">(<span class="keyword">this</span>)</span></span>;</span><br><span class="line">    <span class="keyword">return</span> apply(<span class="built_in">std</span>::move(inputs));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 有关计算图的 API</span></span><br><span class="line">  <span class="comment">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/* ... */</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">const</span> Edge&amp; <span class="title">next_edge</span><span class="params">(<span class="keyword">size_t</span> index)</span> <span class="keyword">const</span> <span class="keyword">noexcept</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> next_edges_[index];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">set_next_edge</span><span class="params">(<span class="keyword">size_t</span> index, Edge edge)</span> </span>&#123;</span><br><span class="line">    next_edges_[index] = <span class="built_in">std</span>::move(edge);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">add_next_edge</span><span class="params">(Edge edge)</span> </span>&#123;</span><br><span class="line">    next_edges_.push_back(<span class="built_in">std</span>::move(edge));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">set_next_edges</span><span class="params">(edge_list&amp;&amp; next_edges)</span> </span>&#123;</span><br><span class="line">    next_edges_ = <span class="built_in">std</span>::move(next_edges);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">const</span> edge_list&amp; <span class="title">next_edges</span><span class="params">()</span> <span class="keyword">const</span> <span class="keyword">noexcept</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> next_edges_;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">edge_list&amp; <span class="title">next_edges</span><span class="params">()</span> <span class="keyword">noexcept</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> next_edges_;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">uint32_t</span> num_outputs() <span class="keyword">const</span> <span class="keyword">noexcept</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> next_edges_.size();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* ... */</span></span><br><span class="line">  </span><br><span class="line"> <span class="keyword">protected</span>:</span><br><span class="line">  <span class="function"><span class="keyword">static</span> uint64_t&amp; <span class="title">get_next_sequence_nr</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 需要重载的apply函数，实现实际功能</span></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> variable_list <span class="title">apply</span><span class="params">(variable_list&amp;&amp; inputs)</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function">variable_list <span class="title">traced_apply</span><span class="params">(variable_list inputs)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 函数序列号</span></span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">uint64_t</span> sequence_nr_;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 保存邻边</span></span><br><span class="line">  edge_list next_edges_;</span><br><span class="line">  PyObject* pyobj_ = <span class="literal">nullptr</span>; <span class="comment">// weak reference</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;AnomalyMetadata&gt; anomaly_metadata_ = <span class="literal">nullptr</span>;</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;FunctionPreHook&gt;&gt; pre_hooks_;</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;FunctionPostHook&gt;&gt; post_hooks_;</span><br><span class="line">  at::SmallVector&lt;InputMetadata, <span class="number">2</span>&gt; input_metadata_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><code>Edge</code> 的声明在 <code>csrc/autograd/edge.h</code>，它的声明就很简单了：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Edge</span> &#123;</span></span><br><span class="line">  Edge() <span class="keyword">noexcept</span> : function(<span class="literal">nullptr</span>), input_nr(<span class="number">0</span>) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  Edge(<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Function&gt; function_, <span class="keyword">uint32_t</span> input_nr_) <span class="keyword">noexcept</span></span><br><span class="line">      : function(<span class="built_in">std</span>::move(function_)), input_nr(input_nr_) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/// Convenience method to test if an edge is valid.</span></span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">is_valid</span><span class="params">()</span> <span class="keyword">const</span> <span class="keyword">noexcept</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> function != <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Required for use in associative containers.</span></span><br><span class="line">  <span class="keyword">bool</span> <span class="keyword">operator</span>==(<span class="keyword">const</span> Edge&amp; other) <span class="keyword">const</span> <span class="keyword">noexcept</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>-&gt;function == other.function &amp;&amp; <span class="keyword">this</span>-&gt;input_nr == other.input_nr;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">bool</span> <span class="keyword">operator</span>!=(<span class="keyword">const</span> Edge&amp; other) <span class="keyword">const</span> <span class="keyword">noexcept</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> !(*<span class="keyword">this</span> == other);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 目标函数</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Function&gt; function;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 第几个输入</span></span><br><span class="line">  <span class="keyword">uint32_t</span> input_nr;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>总结一下，<code>Variable</code>，<code>AutogradMeta</code>，<code>Function</code>，和<code>Edge</code> 的大致关系如下图所示：</p><p><img src="/images/pytorch/Variable.svg"></p><h3><span id="engine">Engine</span></h3><p>我们先不看怎么建立计算图，而是先假设图已经建立好，考虑具体怎么执行反向传播。你可能觉得答案已经很明显了，不就是对图进行遍历么，没错，是对图进行遍历，但考虑到效率以及要在不同设备上计算，实际操作起来还是有点麻烦的，这部分代码主要由 <code>autograd::Engine</code>实现，声明和实现分别在 <code>csrc/autograd/engine.h</code> 和 <code>csrc/autograd/engine.cpp</code> 中。</p><p>用 PyTorch 建立神经网络的时候，相信你一定用过 <code>loss.backward()</code> 来进行反向传播，那就从<code>Variable::backward()</code> 函数开始一步一步看反向传播是怎么执行的：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> Variable::backward(</span><br><span class="line">    c10::optional&lt;Tensor&gt; gradient,</span><br><span class="line">    <span class="keyword">bool</span> keep_graph,</span><br><span class="line">    <span class="keyword">bool</span> create_graph) <span class="keyword">const</span> &#123;</span><br><span class="line">  <span class="comment">// 获取 AutogradMeta</span></span><br><span class="line">  <span class="keyword">auto</span> autograd_meta = get_autograd_meta();</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 构造起始边，做为遍历的起点（图的root）</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Edge&gt; edges;</span><br><span class="line">  edges.emplace_back(autograd_meta-&gt;grad_fn_, autograd_meta-&gt;output_nr_);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 构造输入：variable list</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Variable&gt; inputs;</span><br><span class="line">  inputs.push_back(<span class="built_in">std</span>::move(as_variable_ref(*gradient)));</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 调用 execute 进行反向传播</span></span><br><span class="line">  Engine::get_default_engine().execute(edges, inputs, keep_graph, create_graph);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数首先构造遍历的起点和输入，然后调用 <code>Engine::execute()</code> 进行计算，其中 <code>Engine::get_default_engine()</code> 返回的是 <code>Engine</code> 的实例。</p><p>接着研究 <code>Engine::execute()</code>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> Engine::execute(<span class="keyword">const</span> edge_list&amp; roots,</span><br><span class="line">                     <span class="keyword">const</span> variable_list&amp; inputs,</span><br><span class="line">                     <span class="keyword">bool</span> keep_graph,</span><br><span class="line">                     <span class="keyword">bool</span> create_graph,</span><br><span class="line">                     <span class="keyword">const</span> edge_list&amp; outputs) -&gt; variable_list &#123;</span><br><span class="line">  <span class="comment">// 启动多线程（每个设备一个线程）</span></span><br><span class="line">  <span class="built_in">std</span>::call_once(start_threads_flag, &amp;Engine::start_threads, <span class="keyword">this</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 验证outputs */</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 记录计算图的任务</span></span><br><span class="line">  <span class="function">GraphTask <span class="title">graph_task</span><span class="params">(keep_graph, create_graph)</span></span>;</span><br><span class="line">  <span class="built_in">std</span>::unique_lock&lt;<span class="built_in">std</span>::mutex&gt; lock(graph_task.mutex);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 遍历一遍计算图，计算每个函数的依赖</span></span><br><span class="line">  <span class="comment">// 所谓某函数的依赖就是有几条边指向该函数，用图的术语说就是节点的入度</span></span><br><span class="line">  <span class="comment">// 依赖大于零（入度&gt;0）的节点不能够执行</span></span><br><span class="line">  <span class="keyword">auto</span> graph_root = <span class="built_in">std</span>::make_shared&lt;GraphRoot&gt;(roots, inputs);</span><br><span class="line">  compute_dependencies(graph_root.get(), graph_task);</span><br><span class="line">  <span class="keyword">if</span> (!outputs.empty()) &#123;</span><br><span class="line">    graph_task.init_to_execute(*graph_root, outputs);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 把root加入准备队列，每个设备有一个准备队列，-1代表CPU</span></span><br><span class="line">  ready_queue(<span class="number">-1</span>).push(FunctionTask(&amp;graph_task, <span class="built_in">std</span>::move(graph_root), InputBuffer(<span class="number">0</span>)));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (worker_device == NO_DEVICE) &#123;</span><br><span class="line">    <span class="comment">// 非工作线程：老老实实等待图计算完毕</span></span><br><span class="line">    graph_task.not_done.wait(lock, [&amp;graph_task]&#123;</span><br><span class="line">      <span class="keyword">return</span> graph_task.outstanding_tasks.load() == <span class="number">0</span>;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 工作线程：996!</span></span><br><span class="line">    graph_task.owner = worker_device;</span><br><span class="line">    lock.unlock();</span><br><span class="line">    <span class="comment">// 线程主循环</span></span><br><span class="line">    thread_main(&amp;graph_task);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* check exceptions */</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> graph_task.captured_vars;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>Engine::execute()</code> 里做了这几件事（可以把它想象成一个公司）：</p><ul><li>招聘员工（启动多线程）</li><li>明确整体任务（计算依赖）</li><li>准备第一份工作（把root加入准备队列）</li><li>开始工作！</li></ul><p>首先看启动多线程的代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> Engine::start_threads() -&gt; <span class="keyword">void</span> &#123;</span><br><span class="line">  <span class="comment">// 获取GPU数目</span></span><br><span class="line">  <span class="keyword">int</span> num_devices = at::getNumGPUs();</span><br><span class="line">  <span class="comment">// 线程数 = GPU数 + 1 (for CPU)</span></span><br><span class="line">  <span class="keyword">int</span> num_threads = num_devices + <span class="number">1</span>;</span><br><span class="line">  ready_queues = <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;ReadyQueue&gt;&gt;(num_threads);</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; <span class="built_in">queue</span> : ready_queues)</span><br><span class="line">    <span class="comment">// 初始化每个设备的准备队列</span></span><br><span class="line">    <span class="built_in">queue</span>.reset(<span class="keyword">new</span> ReadyQueue());</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_threads; ++i) &#123;</span><br><span class="line">    <span class="comment">// 赋予每个线程对应的 device，然后进入 thread_main()</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="function">thread <span class="title">t</span><span class="params">(&amp;Engine::thread_init, <span class="keyword">this</span>, i - <span class="number">1</span>)</span></span>;</span><br><span class="line">    t.detach();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来看如何计算依赖：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> Engine::compute_dependencies(Function* root, GraphTask&amp; task) -&gt; <span class="keyword">void</span> &#123;</span><br><span class="line">  <span class="comment">// 记录已访问过节点</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">unordered_set</span>&lt;Function*&gt; seen;</span><br><span class="line">  <span class="comment">// 节点队列，用于BFS</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Function*&gt; <span class="built_in">queue</span> &#123; root &#125;;</span><br><span class="line"><span class="comment">// 记录依赖的数据结构，类型为 unordered_map&lt;Function*, int&gt;</span></span><br><span class="line">  <span class="keyword">auto</span>&amp; dependencies = task.dependencies;</span><br><span class="line">  <span class="comment">// BFS</span></span><br><span class="line">  <span class="keyword">while</span> (!<span class="built_in">queue</span>.empty()) &#123;</span><br><span class="line">    <span class="keyword">auto</span> fn = <span class="built_in">queue</span>.back(); <span class="built_in">queue</span>.pop_back();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; edge : fn-&gt;next_edges()) &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="keyword">auto</span> next_ptr = edge.function.get()) &#123;</span><br><span class="line">        <span class="comment">// 目标节点的依赖+1</span></span><br><span class="line">        dependencies[next_ptr] += <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">bool</span> was_inserted = seen.insert(next_ptr).second;</span><br><span class="line">        <span class="keyword">if</span> (was_inserted) <span class="built_in">queue</span>.push_back(next_ptr);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后来看最主要的 <code>thread_main()</code>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> Engine::thread_main(GraphTask *graph_task) -&gt; <span class="keyword">void</span> &#123;</span><br><span class="line">  <span class="comment">// 获取当前进程的准备队列</span></span><br><span class="line">  <span class="keyword">auto</span> <span class="built_in">queue</span> = ready_queues[worker_device + <span class="number">1</span>];</span><br><span class="line">  <span class="comment">// 工作没完成之前不能下班：</span></span><br><span class="line">  <span class="keyword">while</span> (!graph_task || graph_task-&gt;outstanding_tasks &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// 获取最新工作，pop()是阻塞的，只有来工作了才会继续执行</span></span><br><span class="line">    <span class="comment">// 生产者消费者模型</span></span><br><span class="line">    FunctionTask task = <span class="built_in">queue</span>-&gt;pop();</span><br><span class="line">    <span class="keyword">if</span> (task.fn &amp;&amp; !task.base-&gt;has_error.load()) &#123;</span><br><span class="line">      GradMode::set_enabled(task.base-&gt;grad_mode);</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 执行该任务</span></span><br><span class="line">        evaluate_function(task);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (<span class="built_in">std</span>::exception&amp; e) &#123;</span><br><span class="line">        thread_on_exception(task, e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 找到发布任务的人</span></span><br><span class="line">    <span class="keyword">auto</span> base_owner = task.base-&gt;owner;</span><br><span class="line">    <span class="comment">// 若该任务来自非工作线程（i.e.s 领导线程，发号施令）：</span></span><br><span class="line">    <span class="keyword">if</span> (base_owner == NO_DEVICE) &#123;</span><br><span class="line">      <span class="comment">// 自减剩余任务数</span></span><br><span class="line">      <span class="keyword">if</span> (--task.base-&gt;outstanding_tasks == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// 所有任务完毕，通知大伙下班</span></span><br><span class="line">        <span class="built_in">std</span>::lock_guard&lt;<span class="built_in">std</span>::mutex&gt; lock(task.base-&gt;mutex);</span><br><span class="line">        task.base-&gt;not_done.notify_all();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 如果任务发布者是自己：</span></span><br><span class="line">      <span class="keyword">if</span> (base_owner == worker_device) &#123;</span><br><span class="line">        <span class="comment">// 自减剩余任务数</span></span><br><span class="line">        --task.base-&gt;outstanding_tasks;</span><br><span class="line">      <span class="comment">// 如果任务发布自其他工人：</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (base_owner != worker_device) &#123;</span><br><span class="line">        <span class="comment">// 自减剩余任务数</span></span><br><span class="line">        <span class="keyword">if</span> (--task.base-&gt;outstanding_tasks == <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="comment">// 提醒他们那个任务做完了</span></span><br><span class="line">          <span class="built_in">std</span>::atomic_thread_fence(<span class="built_in">std</span>::memory_order_release);</span><br><span class="line">          ready_queue(base_owner).push(FunctionTask(task.base, <span class="literal">nullptr</span>, InputBuffer(<span class="number">0</span>)));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>thread_main()</code> 里没有具体执行任务的代码，而是把它单独抽出变成一个方法，下面看该方法都干了什么：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> Engine::evaluate_function(FunctionTask&amp; task) -&gt; <span class="keyword">void</span> &#123;</span><br><span class="line">  <span class="comment">/* exec info blabla... */</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 调用 task 里的函数获取输出，基本相当于 (*task.fn)()</span></span><br><span class="line">  <span class="keyword">auto</span> outputs = call_function(task);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span>&amp; fn = *task.fn;</span><br><span class="line">  <span class="comment">// 如果不保留图的话就把当前节点释放了</span></span><br><span class="line">  <span class="keyword">if</span> (!task.base-&gt;keep_graph) &#123;</span><br><span class="line">    fn.release_variables();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获取输出个数</span></span><br><span class="line">  <span class="keyword">int</span> num_outputs = outputs.size();</span><br><span class="line">  <span class="comment">// 如果没有输出这个任务就完成了，直接返回</span></span><br><span class="line">  <span class="keyword">if</span> (num_outputs == <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* ... */</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::lock_guard&lt;<span class="built_in">std</span>::mutex&gt; lock(task.base-&gt;mutex);</span><br><span class="line">  <span class="comment">// 遍历每个输出（遍历与之相邻的节点）：</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_outputs; ++i) &#123;</span><br><span class="line">    <span class="keyword">auto</span>&amp; output = outputs[i];</span><br><span class="line">    <span class="comment">// 获取第i条边（指向第i个输出所对应的函数）</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">auto</span>&amp; next = fn.next_edge(i);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!next.is_valid()) <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 检查这条边是否准备好（依赖是否为0）</span></span><br><span class="line">    <span class="keyword">bool</span> is_ready = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">auto</span>&amp; dependencies = task.base-&gt;dependencies;</span><br><span class="line">    <span class="comment">// 获取这条边的依赖数</span></span><br><span class="line">    <span class="keyword">auto</span> it = dependencies.find(next.function.get());</span><br><span class="line">    <span class="keyword">if</span> (it == dependencies.end()) &#123;</span><br><span class="line">      <span class="comment">/* 没找到，抛出异常 */</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (--it-&gt;second == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// 依赖自减后等于零，说明该任务已准备好</span></span><br><span class="line">      dependencies.erase(it);</span><br><span class="line">      is_ready = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取/创建该节点的 input_buffer</span></span><br><span class="line">    <span class="keyword">auto</span>&amp; not_ready = task.base-&gt;not_ready;</span><br><span class="line">    <span class="keyword">auto</span> not_ready_it = not_ready.find(next.function.get());</span><br><span class="line">    <span class="comment">// 还没有为该节点分配 input_buffer:</span></span><br><span class="line">    <span class="keyword">if</span> (not_ready_it == not_ready.end()) &#123;</span><br><span class="line">      <span class="comment">/* 跳过那些不该被执行的函数 */</span></span><br><span class="line">      </span><br><span class="line">      <span class="comment">// 用 output 构造该节点的 input_buffer</span></span><br><span class="line">      <span class="function">InputBuffer <span class="title">input_buffer</span><span class="params">(next.function-&gt;num_inputs())</span></span>;</span><br><span class="line">      input_buffer.add(next.input_nr, <span class="built_in">std</span>::move(output));</span><br><span class="line">      <span class="keyword">if</span> (is_ready) &#123;</span><br><span class="line">        <span class="comment">// 准备好就加入准备队列</span></span><br><span class="line">        <span class="keyword">auto</span>&amp; <span class="built_in">queue</span> = ready_queue(input_buffer.device());</span><br><span class="line">        <span class="built_in">queue</span>.push(FunctionTask(task.base, next.function, <span class="built_in">std</span>::move(input_buffer)));</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 没准备好就缓存 input_buffer</span></span><br><span class="line">        not_ready.emplace(next.function.get(), <span class="built_in">std</span>::move(input_buffer));</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="comment">// 该节点已有 input_buffer:</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 向已有 input_buffer 里添加新的输入</span></span><br><span class="line">      <span class="keyword">auto</span> &amp;input_buffer = not_ready_it-&gt;second;</span><br><span class="line">      input_buffer.add(next.input_nr, <span class="built_in">std</span>::move(output));</span><br><span class="line">      <span class="keyword">if</span> (is_ready) &#123;</span><br><span class="line">        <span class="comment">// 准备好就加入准备队列</span></span><br><span class="line">        <span class="keyword">auto</span>&amp; <span class="built_in">queue</span> = ready_queue(input_buffer.device());</span><br><span class="line">        <span class="built_in">queue</span>.push(FunctionTask(task.base, next.function, <span class="built_in">std</span>::move(input_buffer)));</span><br><span class="line">        <span class="comment">// 从输入缓存里删除该节点</span></span><br><span class="line">        not_ready.erase(not_ready_it);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>好，至此执行计算图的代码就梳理完了，简单总结一下：</p><ul><li>把调用 <code>backward()</code> 的节点设为根节点，从该节点开始遍历</li><li>首先BFS一遍计算图，计算每个节点的依赖</li><li>为每个设备建立一个工作线程，每个线程里有一个准备队列<ul><li>每个线程等待任务到来直到任务全部完成</li><li>完成一个任务后遍历与之相邻的节点，更新他们的依赖(-1)，若依赖为0则加入准备队列</li></ul></li></ul><h2><span id="动态建立计算图">动态建立计算图</span></h2><p>这一小节介绍 PyTorch 是怎么建立用于反向传播的计算图的。根据前面的分析，计算图一定是在定义前向传播是建立的，那么机密一定是藏在前向传播API里了，这个猜想是没错，但是由于 ATen 复杂的动态派发机制以及使用脚本生成代码，我也是费了九牛二虎之力才找到具体实现以及理解其中的逻辑。</p><p>神经网络的具体计算：每一层的前向和反向传播，其实都是在 ATen 里实现的，但回去看 ATen 的API实现，并没有发现其中有任何建立计算图的代码。这其中的机密实际在 <code>tools/autograd</code> 目录里。这个目录里有 <code>derivatives.yaml</code> 和用于生成代码的脚本，前者记录了所有需要自动微分的ATen API，后者为它们生成一层wrapper代码，这些代码主要干两件事：</p><ul><li>把ATen的反向传播API转换成 <code>Function</code></li><li>在ATen的正向传播API中加入建图过程</li></ul><p>举个例子，<code>derivatives.yaml</code> 第119行的 <code>addcmul()</code> API：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">- name:</span> <span class="string">addcmul(Tensor</span> <span class="string">self,</span> <span class="string">Tensor</span> <span class="string">tensor1,</span> <span class="string">Tensor</span> <span class="string">tensor2,</span> <span class="string">*,</span> <span class="string">Scalar</span> <span class="string">value)</span></span><br><span class="line"><span class="attr">  self:</span> <span class="string">grad</span></span><br><span class="line"><span class="attr">  tensor1:</span> <span class="string">grad</span> <span class="string">*</span> <span class="string">tensor2</span> <span class="string">*</span> <span class="string">value</span></span><br><span class="line"><span class="attr">  tensor2:</span> <span class="string">grad</span> <span class="string">*</span> <span class="string">tensor1</span> <span class="string">*</span> <span class="string">value</span></span><br></pre></td></tr></table></figure><p>这里指明了函数名、参数、反向计算方法。执行 <code>gen_autograd.py</code> 可以自动为其生成代码，生成的代码在 <code>torch/csrc/autograd/generated</code> 中，有关 <code>addcmul()</code> 的前向传播的代码在 <code>VariableType0.cpp</code> 中， 反向传播的代码在 <code>Functions.h</code> 和 <code>Functionss.cpp</code> 中。</p><p><strong>前向传播</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">Tensor VariableType::addcmul(<span class="keyword">const</span> Tensor &amp; self, <span class="keyword">const</span> Tensor &amp; tensor1, <span class="keyword">const</span> Tensor &amp; tensor2, Scalar value) <span class="keyword">const</span> </span><br><span class="line">&#123;</span><br><span class="line">  profiler::<span class="function">RecordFunction <span class="title">profiler</span><span class="params">(<span class="string">"addcmul"</span>, Function::peek_at_next_sequence_nr())</span></span>;</span><br><span class="line">  <span class="comment">// 获取输入</span></span><br><span class="line">  <span class="keyword">auto</span>&amp; self_ = unpack(self, <span class="string">"self"</span>, <span class="number">0</span>);</span><br><span class="line">  <span class="keyword">auto</span>&amp; tensor1_ = unpack(tensor1, <span class="string">"tensor1"</span>, <span class="number">1</span>);</span><br><span class="line">  <span class="keyword">auto</span>&amp; tensor2_ = unpack(tensor2, <span class="string">"tensor2"</span>, <span class="number">2</span>);</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;AddcmulBackward&gt; grad_fn;</span><br><span class="line">  <span class="keyword">if</span> (compute_requires_grad( self, tensor1, tensor2 )) &#123;</span><br><span class="line">    <span class="comment">// 建立反向传播节点 AddcmulBackward</span></span><br><span class="line">    grad_fn = <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;AddcmulBackward&gt;(<span class="keyword">new</span> AddcmulBackward(), deleteFunction);</span><br><span class="line">    <span class="comment">// 新建边指向与 self, tensor1, tensor2 绑定的节点</span></span><br><span class="line">    grad_fn-&gt;set_next_edges(collect_next_edges(self, tensor1, </span><br><span class="line">                                               tensor2));</span><br><span class="line">    <span class="keyword">if</span> (grad_fn-&gt;should_compute_output(<span class="number">1</span>)) &#123;</span><br><span class="line">      grad_fn-&gt;tensor2_ = SavedVariable(tensor2, <span class="literal">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    grad_fn-&gt;value = value;</span><br><span class="line">    <span class="keyword">if</span> (grad_fn-&gt;should_compute_output(<span class="number">2</span>)) &#123;</span><br><span class="line">      grad_fn-&gt;tensor1_ = SavedVariable(tensor1, <span class="literal">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* ... */</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 调用 ATen API 进行实际计算</span></span><br><span class="line">  <span class="keyword">auto</span> tmp = ([&amp;]() &#123;</span><br><span class="line">    at::AutoNonVariableTypeMode non_var_type_mode(<span class="literal">true</span>);</span><br><span class="line">    <span class="keyword">return</span> baseType-&gt;addcmul(self_, tensor1_, tensor2_, value);</span><br><span class="line">  &#125;)();</span><br><span class="line">  <span class="keyword">auto</span> result = as_variable(tmp);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* ... */</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 把 grad_fn 与输出绑定</span></span><br><span class="line">  set_history(flatten_tensor_args(result), grad_fn);</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个函数是前向计算的API，在具体计算之前先建立反向传播函数（节点），并且把该节点与<strong>输入</strong>的节点相连；然后调用下层API计算结果；最后把结果和新建立的节点绑定，这样用于反向传播的计算图就建立完成了。由于这是反向传播计算图，所以前向传播中的输入节点变成反向的输出，前向的输出节点变成反向的输入，如下图所示。</p><p><img src="/images/pytorch/addcmul.png"></p><p><strong>反向传播</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 声明在 Functions.h</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">AddcmulBackward</span> :</span> <span class="keyword">public</span> TraceableFunction &#123;</span><br><span class="line">  <span class="keyword">using</span> TraceableFunction::TraceableFunction;</span><br><span class="line">  <span class="function">variable_list <span class="title">apply</span><span class="params">(variable_list&amp;&amp; grads)</span> override</span>;</span><br><span class="line">  <span class="built_in">std</span>::<span class="function"><span class="built_in">string</span> <span class="title">name</span><span class="params">()</span> <span class="keyword">const</span> override </span>&#123; <span class="keyword">return</span> <span class="string">"AddcmulBackward"</span>; &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">release_variables</span><span class="params">()</span> override </span>&#123;</span><br><span class="line">    tensor2_.reset_data();</span><br><span class="line">    tensor2_.reset_grad_function();</span><br><span class="line">    tensor1_.reset_data();</span><br><span class="line">    tensor1_.reset_grad_function();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  SavedVariable tensor2_;</span><br><span class="line">  Scalar value;</span><br><span class="line">  SavedVariable tensor1_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实现在 Functions.cpp</span></span><br><span class="line"><span class="comment">// 重载 Function::apply 实现梯度计算</span></span><br><span class="line">variable_list AddcmulBackward::apply(variable_list&amp;&amp; grads) &#123;</span><br><span class="line">  IndexRangeGenerator gen;</span><br><span class="line">  <span class="keyword">auto</span> self_ix = gen.range(<span class="number">1</span>);</span><br><span class="line">  <span class="keyword">auto</span> tensor1_ix = gen.range(<span class="number">1</span>);</span><br><span class="line">  <span class="keyword">auto</span> tensor2_ix = gen.range(<span class="number">1</span>);</span><br><span class="line">  <span class="function">variable_list <span class="title">grad_inputs</span><span class="params">(gen.size())</span></span>;</span><br><span class="line">  <span class="keyword">auto</span>&amp; grad = grads[<span class="number">0</span>];</span><br><span class="line">  <span class="keyword">auto</span> tensor2 = tensor2_.unpack();</span><br><span class="line">  <span class="keyword">auto</span> tensor1 = tensor1_.unpack();</span><br><span class="line">  <span class="comment">// 计算梯度，与 derivatives.yaml 中的定义相同</span></span><br><span class="line">  <span class="keyword">if</span> (should_compute_output(&#123; self_ix &#125;)) &#123;</span><br><span class="line">    <span class="keyword">auto</span> grad_result = grad;</span><br><span class="line">    copy_range(grad_inputs, self_ix, grad_result);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (should_compute_output(&#123; tensor1_ix &#125;)) &#123;</span><br><span class="line">    <span class="keyword">auto</span> grad_result = grad * tensor2 * value;</span><br><span class="line">    copy_range(grad_inputs, tensor1_ix, grad_result);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (should_compute_output(&#123; tensor2_ix &#125;)) &#123;</span><br><span class="line">    <span class="keyword">auto</span> grad_result = grad * tensor1 * value;</span><br><span class="line">    copy_range(grad_inputs, tensor2_ix, grad_result);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> grad_inputs;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这些代码确确实实把 ATen API 转换成了计算图节点 <code>Function</code>。</p><p><strong>动态派发</strong></p><p>最后还有一个问题就是代码中调用的API是 <code>torch.addcmul()</code> 或 <code>variable.addcmul()</code> 怎么就会执行到上面的 <code>VariableType::addcmul()</code> 呢？这就要归功于 ATen 的动态派发了，以第二种调用举例分析一下，也就是通过一个<code>Variable</code>类型变量调用<code>addcmul()</code>。</p><p>首先，上一篇说过，ATen 的API都记录在<code>ATen/native/native_functions.yaml</code>里，这些API如果有生成 method 的需求的话，会把声明通过脚本自动添加到 <code>at::Tensor</code> 类里，这样就可以通过 <code>tensor.addcmul()</code> 调用该API，而 <code>Variable</code> 继承自 <code>at::Tensor</code>，自然也就拥有了该方法。</p><p>但这还没完，如果查看这个方法的实现会发现：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> Tensor Tensor::addcmul(<span class="keyword">const</span> Tensor &amp; tensor1, <span class="keyword">const</span> Tensor &amp; tensor2, Scalar value) <span class="keyword">const</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> type().addcmul(*<span class="keyword">this</span>, tensor1, tensor2, value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它调用了 <code>type()</code> 的相应方法，这个 <code>type()</code> 返回的是 <code>Type</code> 类型的实例。<code>Type</code> 类型同样声明了所有 ATen API，并且有 <code>TensorType</code> 和 <code>VariableType</code> 继承自它。根据ATen的动态派发机制，如果调用者是 <code>Tensor</code> 的话，并且<code>is_variable == False</code>，就会返回 <code>TensorType</code> 实例；如果调用者是 <code>Variable</code> 的话，或者<code>is_variable == True</code>，就会返回上述的 <code>VariableType</code> 实例。所以一个<code>Variable</code>类型的变量调用 <code>addcmul()</code> 的话实际会执行 <code>VariableType::addcmul()</code>。</p><p>つづく</p><table><thead><tr class="header"><th style="text-align: left;">上一篇：<a href="https://www.52coding.com.cn/2019/05/05/PyTorch3/">NN</a></th><th style="text-align: right;">下一篇：<a href="https://www.52coding.com.cn/2019/05/05/PyTorch5/">Python扩展</a></th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"></td><td style="text-align: right;"></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇博客介绍 PyTorch 中自动微分引擎的实现，主要分为三部分：首先简要介绍一下计算图的原理；然后介绍 PyTorch 中与 autograd 的相关数据结构和 &lt;code&gt;backward()&lt;/code&gt; 函数的实现，数据结构包括 &lt;code&gt;torch::autograd::Variable&lt;/code&gt;, &lt;code&gt;torch::autograd::Function&lt;/code&gt; 等；最后讲一下动态建立计算图的实现，这部分代码涉及到动态派发机制，而且都是用脚本生成的，不太容易理解。&lt;/p&gt;
    
    </summary>
    
      <category term="博客" scheme="http://www.52coding.com.cn/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="PyTorch" scheme="http://www.52coding.com.cn/tags/PyTorch/"/>
    
      <category term="Autograd" scheme="http://www.52coding.com.cn/tags/Autograd/"/>
    
      <category term="自动微分引擎" scheme="http://www.52coding.com.cn/tags/%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E5%BC%95%E6%93%8E/"/>
    
      <category term="Variable" scheme="http://www.52coding.com.cn/tags/Variable/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch源码浅析(3)：NN</title>
    <link href="http://www.52coding.com.cn/2019/05/05/PyTorch3/"/>
    <id>http://www.52coding.com.cn/2019/05/05/PyTorch3/</id>
    <published>2019-05-05T07:55:01.000Z</published>
    <updated>2019-05-05T08:07:04.555Z</updated>
    
    <content type="html"><![CDATA[<p>THNN是一个用C语言实现的神经网络模块的库，提供的功能非常底层。它实现了许多基础的神经网络模块，包括线性层，卷积层，Sigmoid等各种激活层，一些基本的loss函数，这些API都声明在<code>THNN/generic/THNN.h</code>中。每个模块都实现了前向传导（forward）和后向传导（backward）的功能。THCUNN则是对应模块的CUDA实现。</p><a id="more"></a><!-- toc --><ul><li><a href="#thnn-thcunn">THNN &amp; THCUNN</a><ul><li><a href="#tanh">Tanh</a></li><li><a href="#2d-convolution">2D Convolution</a><ul><li><a href="#前向传播">前向传播</a></li><li><a href="#反向传播">反向传播</a></li></ul></li></ul></li><li><a href="#aten">ATen</a></li></ul><!-- tocstop --><h2><span id="thnn-amp-thcunn">THNN &amp; THCUNN</span></h2><p>我们通过几个例子具体看一下几种模块是怎么实现的。</p><h3><span id="tanh">Tanh</span></h3><p>首先从最简单的激活层开始看，以 <span class="math inline">\(\tanh​\)</span> 为代表，代码在 <code>THNN/generic/Tanh.c</code>。这个模块只有两个函数，分别是 <code>THNN_(Tanh_updateOutput)()</code> 和 <code>THNN_(Tanh_updateGradInput)()</code>，其中前者实现了前向传播，后者实现了反向传播。注意到函数的命名方式，依旧是使用宏实现范式，<code>THNN_</code> 是宏定义，<code>Tanh_</code>是具体模块名，<code>updateOutput</code> 表示前向传播，<code>updateGradInput</code>表示反向传播，与之前不同的是，这里只需生成浮点类型（ <code>float</code> 和 <code>double</code> ）的实现即可。</p><p>前向传播的实现很简单，直接调用之前实现的tanh函数就行了：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THNN_</span><span class="params">(Tanh_updateOutput)</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">          THNNState *state,</span></span></span><br><span class="line"><span class="function"><span class="params">          THTensor *input,</span></span></span><br><span class="line"><span class="function"><span class="params">          THTensor *output)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  THTensor_(<span class="built_in">tanh</span>)(output, input);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它有三个参数，分别是THNN状态（暂不知有何用），本层的输入（<code>input</code>）和本层的输出（<code>output</code>），输出存储在<code>output</code>参数中。</p><p>反向传播的实现为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THNN_</span><span class="params">(Tanh_updateGradInput)</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">          THNNState *state,</span></span></span><br><span class="line"><span class="function"><span class="params">          THTensor *gradOutput,</span></span></span><br><span class="line"><span class="function"><span class="params">          THTensor *gradInput,</span></span></span><br><span class="line"><span class="function"><span class="params">          THTensor *output)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  THNN_CHECK_SHAPE(output, gradOutput);</span><br><span class="line">  THTensor_(resizeAs)(gradInput, output);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (<span class="comment">/* output, gradInput, 和 gradOutput 的内存是连续的 */</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    TH_TENSOR_APPLY3(<span class="keyword">scalar_t</span>, gradInput, <span class="keyword">scalar_t</span>, gradOutput,</span><br><span class="line">                     <span class="keyword">scalar_t</span>, output,</span><br><span class="line">      <span class="keyword">scalar_t</span> z = *output_data;            \</span><br><span class="line">      *gradInput_data = *gradOutput_data * (<span class="number">1.</span> - z*z);</span><br><span class="line">    );</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">scalar_t</span>* ptr_gradOutput = gradOutput-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;();</span><br><span class="line">    <span class="keyword">scalar_t</span>* ptr_gradInput  = gradInput-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;();</span><br><span class="line">    <span class="keyword">scalar_t</span>* ptr_output     = output-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;();</span><br><span class="line">    <span class="keyword">int64_t</span> i;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for private(i)</span></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; THTensor_(nElement)(gradInput); i++)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="keyword">scalar_t</span> z = ptr_output[i];</span><br><span class="line">      ptr_gradInput[i] = ptr_gradOutput[i] * (<span class="number">1.</span> - z*z);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>反向传播接收5个参数，分别是THNN状态，从后面传回来的梯度（<code>gradOutput</code>），本层往回传的梯度（<code>gradInput</code>），本层前向传播的输出（<code>output</code>），这个函数计算的是本层的梯度，然后存储在<code>gradInput</code>中。</p><p><span class="math inline">\(\tanh\)</span> 的导数为： <span class="math display">\[f(z) = \tanh(z)\\f&#39;(z) = 1 - (f(z))^2\]</span> 其中，<span class="math inline">\(f(z)​\)</span> 就是前向传播时本层的输出，也就是 <code>output</code> 参数（循环里的<code>z</code>），根据链式法则，再乘以后面层传回来的梯度（<code>gradOutput</code>）就是本层应该往回传的梯度了（相对于本层输入的梯度），所以循环里的代码为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">*gradInput_data = *gradOutput_data * (<span class="number">1.</span> - z*z);</span><br></pre></td></tr></table></figure><p>注：<code>_data</code> 后缀表示数据指针，具体可以看<a href="此处应该加第一篇的链接">Apply宏</a>的实现。</p><h3><span id="2d-convolution">2D Convolution</span></h3><p>最普通的2D卷积，CPU实现在<code>THNN/generic/SpatialConvolutionMM.c</code>，CUDA实现在<code>THCUNN/generic/SpatialConvolutionMM.cu</code>，大体的算法是把输入展开成一个特殊的矩阵，然后把卷积转化为矩阵相乘（MM = Matrix Multiplication）。模块里主要包含三个函数，前向传播（<code>THNN_(SpatialConvolutionMM_updateOutput)()</code>），反向传播（<code>THNN_(SpatialConvolutionMM_updateGradInput)()</code>），和更新层内参数（<code>THNN_(SpatialConvolutionMM_accGradParameters)()</code>）。</p><h4><span id="前向传播">前向传播</span></h4><p>PyTorch实现卷积的做法是用im2col算法把输入展开成一个大矩阵，然后用kernel乘以这个大矩阵，就得了卷积的结果。这里不具体介绍im2col算法是怎么做的，但会解释为什么可以这么做。</p><p>首先定义符号，为了和代码中的符号一致，首先来看一下updateOutput的声明：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THNN_</span><span class="params">(SpatialConvolutionMM_updateOutput)</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">           THCState *state,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *input,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *output,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *weight,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *bias,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *columns,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *ones,</span></span></span><br><span class="line"><span class="function"><span class="params">           <span class="keyword">int</span> kW, <span class="keyword">int</span> kH,</span></span></span><br><span class="line"><span class="function"><span class="params">           <span class="keyword">int</span> dW, <span class="keyword">int</span> dH,</span></span></span><br><span class="line"><span class="function"><span class="params">           <span class="keyword">int</span> padW, <span class="keyword">int</span> padH)</span></span>;</span><br></pre></td></tr></table></figure><p>其中，</p><ul><li><code>input</code> 是输入的4D Tensor，大小为 <span class="math inline">\(\text{batch}\times\text{nInputPlane}\times\text{inputHeight}\times\text{inputWidth}\)</span>，batch维也可以没有，就变为3D Tensor；</li><li><code>output</code> 是输出的4D或3D Tensor，大小为 <span class="math inline">\(\text{batch}\times\text{nOutputPlane}\times\text{outputHeight}\times\text{outputWidth}\)</span>，其中，</li></ul><p><span class="math display">\[\text{outputHeight}=\frac{\text{inputHeight}+2*\text{padH}-\text{kH}}{\text{dH}}+1\\\text{outputWidth}=\frac{\text{inputWidth}+2*\text{padW}-\text{kW}}{\text{dW}}+1\]</span></p><ul><li><code>weight</code> 是权重，也就是卷积核，大小为 <span class="math inline">\(\text{nOutputPlane}\times\text{nInputPlane}\times\text{kH}\times\text{kW}\)</span>；</li><li><code>bias</code> 是偏置，大小为 <span class="math inline">\(\text{nOutputPlane}\times1\times1\times1\)</span>；</li><li><code>columns</code> 用于存储im2col的结果；</li><li><code>ones</code> 是一个值全为1的矩阵，大小为 <span class="math inline">\(\text{outputHeight}\times\text{outputWidth}\)</span>，用于计算偏置；</li><li><code>kW</code> 和 <code>kH</code> 是卷积核（kernel）的宽和高；</li><li><code>dW</code> 和 <code>dH</code> 是步长（stride）；</li><li><code>padW</code> 和 <code>padH</code> 是补零的宽和高。</li></ul><p>定义好了符号之后来看一下卷积是怎么转化为一个矩阵乘法的。首先来看卷积是怎么做的，下图是一个简单的例子，其中输入和输出深度 <span class="math inline">\(\text{nInputPlane}=\text{nOutputPlane}=1​\)</span>，卷积核大小 <span class="math inline">\(\text{kH}=\text{kW}=3​\)</span>，输入大小 <span class="math inline">\(\text{inputHeight}=\text{inputWidth}=7​\)</span>，步长 <span class="math inline">\(\text{dW}=\text{dH}=2​\)</span>，补零 <span class="math inline">\(\text{padW}=\text{padH}=1​\)</span>，可以算出 <span class="math inline">\(\text{outputHeight}=\text{outputWidth}=3​\)</span>。</p><p><img src="/images/pytorch/conv.png"></p><p>要想把卷积变成一次矩阵乘法运算，就需要<strong>把输入中每个卷积窗口变为单独一列</strong>，这也就是im2col做的事情，见下图：</p><p><img src="/images/pytorch/im2col.png"></p><p>上图中右边矩阵的每一列都是原输入矩阵的一个卷积窗口，转换后的矩阵大小为 <span class="math display">\[(\text{nInputPlane}\times\text{kH}\times\text{kW})\times(\text{outputHeight}\times\text{outputWidth})\]</span> 得到上述矩阵之后，只需把kernel的大小也resize成 <span class="math display">\[(\text{nOutputPlane})\times(\text{nInputPlane}\times\text{kH}\times\text{kW})\]</span> 就可以直接用kernel乘以该矩阵得到卷积结果了，见下图。</p><p><img src="/images/pytorch/convmm.png"></p><p><strong>代码</strong>（CUDA版）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THNN_</span><span class="params">(SpatialConvolutionMM_updateOutput)</span><span class="params">(<span class="comment">/* 参数列表前面有 */</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">/* 省略了一些常规check */</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* resize 卷积核 */</span></span><br><span class="line">  weight = THNN_(newViewWeightMM2d)(state, weight);</span><br><span class="line">  <span class="comment">/* 对输入的维度进行check */</span></span><br><span class="line">  THNN_(SpatialConvolutionMM_shapeCheck)</span><br><span class="line">       (state, input, <span class="literal">NULL</span>, weight, bias, kH, kW, dH, dW, padH, </span><br><span class="line">        padW, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> ndim = input-&gt;dim();</span><br><span class="line">  <span class="keyword">int</span> dimf = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> dimh = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">int</span> dimw = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (ndim == <span class="number">4</span>) &#123;<span class="comment">/* 如果输入是4D的话，第0维是batch size */</span></span><br><span class="line">    dimf++;</span><br><span class="line">    dimh++;</span><br><span class="line">    dimw++;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 计算输入输出大小 */</span></span><br><span class="line">  <span class="keyword">int64_t</span> nInputPlane = input-&gt;size(dimf);</span><br><span class="line">  <span class="keyword">int64_t</span> inputHeight  = input-&gt;size(dimh);</span><br><span class="line">  <span class="keyword">int64_t</span> inputWidth   = input-&gt;size(dimw);</span><br><span class="line">  <span class="keyword">int64_t</span> nOutputPlane = weight-&gt;size(<span class="number">0</span>);</span><br><span class="line">  <span class="keyword">int64_t</span> outputHeight = (inputHeight + <span class="number">2</span>*padH - kH) / dH + <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">int64_t</span> outputWidth  = (inputWidth + <span class="number">2</span>*padW - kW) / dW + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  input = THCTensor_(newContiguous)(state, input);</span><br><span class="line">  <span class="keyword">int</span> is_batch = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">if</span> (input-&gt;dim() == <span class="number">3</span>) &#123;</span><br><span class="line">    <span class="comment">/* 强行加入 batch 维度，把输入变为4D Tensor（batch size = 1）*/</span></span><br><span class="line">    is_batch = <span class="number">0</span>;</span><br><span class="line">    THCTensor_(resize4d)(state, input, <span class="number">1</span>, input-&gt;size(<span class="number">0</span>), </span><br><span class="line">                         input-&gt;size(<span class="number">1</span>), input-&gt;size(<span class="number">2</span>));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 获取 batch size */</span></span><br><span class="line">  <span class="keyword">int64_t</span> batchSize = input-&gt;size(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Resize output</span></span><br><span class="line">  THCTensor_(resize4d)(state, output, batchSize, nOutputPlane,</span><br><span class="line">                       outputHeight, outputWidth);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* Resize columns 矩阵 */</span></span><br><span class="line">  THCTensor_(resize2d)(state, columns, nInputPlane*kW*kH,</span><br><span class="line">                       outputHeight*outputWidth);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 定义 buffer `ones`，代码略 */</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Helpers</span></span><br><span class="line">  THCTensor *input_n = THCTensor_(<span class="keyword">new</span>)(state);</span><br><span class="line">  THCTensor *output_n = THCTensor_(<span class="keyword">new</span>)(state);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 对每个batch单独计算： */</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> elt = <span class="number">0</span>; elt &lt; batchSize; elt ++) &#123;</span><br><span class="line">    <span class="comment">/* 取出对应batch的输入和输出buffer */</span></span><br><span class="line">    <span class="comment">/* input_n = input[elt] */</span></span><br><span class="line">    THCTensor_(select)(state, input_n, input, <span class="number">0</span>, elt);</span><br><span class="line">    THCTensor_(select)(state, output_n, output, <span class="number">0</span>, elt);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 首先计算偏置 bias，即把 bias 存到 output_n 中 */</span></span><br><span class="line">    <span class="comment">// M,N,K are dims of matrix A and B</span></span><br><span class="line">    <span class="keyword">int64_t</span> m_ = nOutputPlane;</span><br><span class="line">    <span class="keyword">int64_t</span> n_ = outputHeight * outputWidth;</span><br><span class="line">    <span class="keyword">int64_t</span> k_ = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 调用 GEMM 计算 output_n = 1 * ones * bias + 0 * output_n */</span></span><br><span class="line">    <span class="keyword">if</span> (bias) &#123;</span><br><span class="line">      <span class="meta">#<span class="meta-keyword">ifdef</span> THC_REAL_IS_FLOAT</span></span><br><span class="line">      THCudaBlas_Sgemm(</span><br><span class="line">      #elif defined(THC_REAL_IS_HALF)</span><br><span class="line">      THCudaBlas_Hgemm(</span><br><span class="line">      #elif defined(THC_REAL_IS_DOUBLE)</span><br><span class="line">      THCudaBlas_Dgemm(</span><br><span class="line">      #endif</span><br><span class="line">          state,</span><br><span class="line">          <span class="string">'t'</span>, <span class="string">'n'</span>,</span><br><span class="line">          n_, m_, k_,</span><br><span class="line">          ScalarConvert&lt;<span class="keyword">int</span>, <span class="keyword">scalar_t</span>&gt;::to(<span class="number">1</span>),</span><br><span class="line">          THCTensor_(data)(state, ones), k_,</span><br><span class="line">          THCTensor_(data)(state, bias), k_,</span><br><span class="line">          ScalarConvert&lt;<span class="keyword">int</span>, <span class="keyword">scalar_t</span>&gt;::to(<span class="number">0</span>),</span><br><span class="line">          THCTensor_(data)(state, output_n), n_</span><br><span class="line">      );</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">/* 如果没有 bias 就把 output_n 填零 */</span></span><br><span class="line">      THCTensor_(zero)(state, output_n);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 用 im2col 把输入 input_n 转化为列矩阵存储在 columns */</span></span><br><span class="line">    im2col(</span><br><span class="line">      THCState_getCurrentStream(state),</span><br><span class="line">      THCTensor_(data)(state, input_n),</span><br><span class="line">      nInputPlane, inputHeight, inputWidth,</span><br><span class="line">      outputHeight, outputWidth,</span><br><span class="line">      kH, kW, padH, padW, dH, dW,</span><br><span class="line">      <span class="number">1</span>, <span class="number">1</span>, THCTensor_(data)(state, columns)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 接下来计算 kernel * columns: */</span></span><br><span class="line">    <span class="keyword">int64_t</span> m = nOutputPlane;</span><br><span class="line">    <span class="keyword">int64_t</span> n = columns-&gt;size(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">int64_t</span> k = nInputPlane*kH*kW;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 计算 output_n = 1 * weight * columns + 1 * bias */</span></span><br><span class="line">    <span class="comment">/* 代码中 columns 在 weight 前面是因为 GEMM 假设矩阵是列主序 */</span></span><br><span class="line">    #ifdef THC_REAL_IS_FLOAT</span><br><span class="line">    THCudaBlas_Sgemm(</span><br><span class="line">    #elif defined(THC_REAL_IS_HALF)</span><br><span class="line">    THCudaBlas_Hgemm(</span><br><span class="line">    #elif defined(THC_REAL_IS_DOUBLE)</span><br><span class="line">    THCudaBlas_Dgemm(</span><br><span class="line">    #endif</span><br><span class="line">        state,</span><br><span class="line">        <span class="string">'n'</span>, <span class="string">'n'</span>,</span><br><span class="line">        n, m, k,</span><br><span class="line">        ScalarConvert&lt;<span class="keyword">int</span>, <span class="keyword">scalar_t</span>&gt;::to(<span class="number">1</span>),</span><br><span class="line">        THCTensor_(data)(state, columns), n,</span><br><span class="line">        THCTensor_(data)(state, weight), k,</span><br><span class="line">        ScalarConvert&lt;<span class="keyword">int</span>, <span class="keyword">scalar_t</span>&gt;::to(<span class="number">1</span>),</span><br><span class="line">        THCTensor_(data)(state, output_n), n</span><br><span class="line">    );</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* free 临时变量 input_n, output_n 等 */</span></span><br><span class="line">  <span class="comment">/* resize 输出矩阵，代码略 */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4><span id="反向传播">反向传播</span></h4><p>首先来看一下卷积层反向传播的公式，设第 <span class="math inline">\(l\)</span> 层的输入为 <span class="math inline">\(a^{l-1}\)</span>，卷积核为 <span class="math inline">\(W\)</span>，偏置为 <span class="math inline">\(b\)</span>，输出为 <span class="math inline">\(z^l = a^{l-1}*W+b\)</span>，相对于输出的误差梯度为 <span class="math inline">\(\delta^l = \frac{\partial \text{Loss}}{\partial z^l}\)</span>，则相对于输入的梯度为： <span class="math display">\[\delta^{l-1}=\delta^l*\text{rot180}(W^l)\]</span> 相对于权重和偏置的梯度为： <span class="math display">\[\frac{\partial\text{Loss}}{\partial W^l}=\frac{\partial\text{Loss}}{\partial z^l}\frac{\partial z^l}{\partial W^l}=a^{l-1}*\delta^l\\\frac{\partial\text{Loss}}{\partial b^l}=\sum_{u,v}(\delta^l)_{u,v}\]</span> <strong>注</strong>：<span class="math inline">\(*\)</span> 为卷积的意思，<span class="math inline">\(\text{rot180}(x)\)</span> 为把矩阵 <span class="math inline">\(x\)</span> 旋转180度的意思。公式推导可以看<a href="https://www.cnblogs.com/pinard/p/6494810.html" target="_blank" rel="noopener">这篇博客</a>。</p><p>从公式可以看出反向传播分为两个部分：计算对输入的梯度和计算对参数的梯度，这两部分也分别对应了模块里的两个函数，我们一个一个分析。</p><p><strong>THNN_(SpatialConvolutionMM_updateGradInput)</strong></p><p>这部分是计算对输入的梯度，虽然公式摆在了那里，但Torch的代码实现并不是直接翻译公式，我也一直没能把这部分的实现和公式对上，不过倒是可以通过图示的方式来理解，有点像前向传播的逆过程，把卷积后的梯度分配回卷积前。</p><p>在画图之前还是先定义一些符号：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THNN_</span><span class="params">(SpatialConvolutionMM_updateGradInput)</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">           THCState *state,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *input,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *gradOutput,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *gradInput,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *weight,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *gradColumns,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *ones,</span></span></span><br><span class="line"><span class="function"><span class="params">           <span class="keyword">int</span> kW, <span class="keyword">int</span> kH,</span></span></span><br><span class="line"><span class="function"><span class="params">           <span class="keyword">int</span> dW, <span class="keyword">int</span> dH,</span></span></span><br><span class="line"><span class="function"><span class="params">           <span class="keyword">int</span> padW, <span class="keyword">int</span> padH)</span></span>;</span><br></pre></td></tr></table></figure><p>其中（与前向传播相同的参数就不重复介绍了），</p><ul><li><code>gradOutput</code> 是相对于输出的梯度，即 <span class="math inline">\(\delta^l\)</span>，大小与前向传播的输出 <code>output</code> 相同；</li><li><code>gradInput</code> 是相对于输入的梯度，即 <span class="math inline">\(\delta^{l-1}\)</span>，是这个函数需要计算的对象，大小与输入 <code>input</code> 相同；</li><li><code>gradColumns</code> 是个列矩阵，用于存储临时的梯度，大小为 <span class="math inline">\((\text{nInputPlane}\times\text{kH}\times\text{kW})\times(\text{outputHeight}\times\text{outputWidth})\)</span>；</li></ul><p>代码的实现逻辑是用权重的转置乘以输出梯度，得到<code>gradColumns</code>，然后通过col2im还原到输入的大小，即得到了相对输入的梯度 <code>gradInput</code>。这个操作看起来就是前向传播的逆操作，但是搞不懂为什么这样就实现了 <span class="math inline">\(\delta^l*\text{rot180}(W^l)\)</span>，<strong>如果有大佬知道希望评论区指点一下</strong>。使用前面例子的设定，这波操作的示意图如下所示：</p><p><img src="/images/pytorch/convmm2.png"></p><p><img src="/images/pytorch/col2im.png"></p><p><strong>核心部分代码</strong>（其余部分与前向传播类似）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 循环取每个批次： */</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> elt = <span class="number">0</span>; elt &lt; batchSize; elt ++) &#123;</span><br><span class="line">  <span class="comment">/* gradInput_n = gradInput[elt]; gradInput_n 同理 */</span></span><br><span class="line">  THCTensor_(select)(state, gradInput_n, gradInput, <span class="number">0</span>, elt);</span><br><span class="line">  THCTensor_(select)(state, gradOutput_n, gradOutput, <span class="number">0</span>, elt);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// M,N,K are dims of matrix A and B</span></span><br><span class="line">  <span class="keyword">int64_t</span> m = nInputPlane*kW*kH;</span><br><span class="line">  <span class="keyword">int64_t</span> n = gradColumns-&gt;size(<span class="number">1</span>);</span><br><span class="line">  <span class="keyword">int64_t</span> k = nOutputPlane;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 调用GEMM计算 gradColumns = weight' * gradOutput_n */</span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">ifdef</span> THC_REAL_IS_FLOAT</span></span><br><span class="line">  THCudaBlas_Sgemm(</span><br><span class="line">  #elif defined(THC_REAL_IS_HALF)</span><br><span class="line">  THCudaBlas_Hgemm(</span><br><span class="line">  #elif defined(THC_REAL_IS_DOUBLE)</span><br><span class="line">  THCudaBlas_Dgemm(</span><br><span class="line">  #endif</span><br><span class="line">      state,</span><br><span class="line">      <span class="string">'n'</span>, <span class="string">'t'</span>,</span><br><span class="line">      n, m, k,</span><br><span class="line">      ScalarConvert&lt;<span class="keyword">int</span>, <span class="keyword">scalar_t</span>&gt;::to(<span class="number">1</span>),</span><br><span class="line">      THCTensor_(data)(state, gradOutput_n), n,</span><br><span class="line">      THCTensor_(data)(state, weight), m,</span><br><span class="line">      ScalarConvert&lt;<span class="keyword">int</span>, <span class="keyword">scalar_t</span>&gt;::to(<span class="number">0</span>),</span><br><span class="line">      THCTensor_(data)(state, gradColumns), n</span><br><span class="line">  );</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 调用 col2im 把 gradColumns 还原为 gradInput_n */</span></span><br><span class="line">  col2im&lt;<span class="keyword">scalar_t</span>, accreal&gt;(</span><br><span class="line">    THCState_getCurrentStream(state),</span><br><span class="line">    THCTensor_(data)(state, gradColumns),</span><br><span class="line">    nInputPlane, inputHeight, inputWidth, outputHeight,</span><br><span class="line">    outputWidth, kH, kW, padH, padW, dH, dW,</span><br><span class="line">    <span class="number">1</span>, <span class="number">1</span>, THCTensor_(data)(state, gradInput_n)</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>THNN_(SpatialConvolutionMM_accGradParameters)</strong></p><p>接下来看计算参数梯度的部分，这部分相对好理解，因为代码和公式一样，还是先看函数声明：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THNN_</span><span class="params">(SpatialConvolutionMM_accGradParameters)</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">           THCState *state,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *input,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *gradOutput,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *gradWeight,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *gradBias,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *columns,</span></span></span><br><span class="line"><span class="function"><span class="params">           THCTensor *ones,</span></span></span><br><span class="line"><span class="function"><span class="params">           <span class="keyword">int</span> kW, <span class="keyword">int</span> kH,</span></span></span><br><span class="line"><span class="function"><span class="params">           <span class="keyword">int</span> dW, <span class="keyword">int</span> dH,</span></span></span><br><span class="line"><span class="function"><span class="params">           <span class="keyword">int</span> padW, <span class="keyword">int</span> padH,</span></span></span><br><span class="line"><span class="function"><span class="params">           accreal scale_)</span></span>;</span><br></pre></td></tr></table></figure><p>其中，</p><ul><li><code>gradWeight</code> 是权重的梯度，是这个函数需要计算的对象，大小和权重相同；</li><li><code>gradBias</code> 是偏置的梯度，也是函数需要计算的对象，大小与偏置相同（就是一个Vector）</li><li><code>columns</code> 是用来存储 im2col 的结果，因为要计算卷积，所以要把输入展开</li><li><code>scale_</code> 是学习速率（learning rate）</li></ul><p>已知<strong>权重梯度</strong>的计算公式为 <span class="math inline">\(\frac{\partial\text{Loss}}{\partial W^l}=a^{l-1}*\delta^l\)</span>，这个卷积的计算方式和前向传播相同：首先把输入（<span class="math inline">\(a^{l-1}\)</span>）通过im2col展开为列矩阵，存储到<code>columns</code>里，然后用 <code>gradOutput</code> 乘以 <code>columns</code> 计算卷积。而<strong>偏置梯度</strong>的计算就是把 <code>gradOutput</code> 累加成一个长度为 <span class="math inline">\(\text{nOutputPlane}​\)</span> 的 Tensor 即可。</p><p><strong>核心代码</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THNN_</span><span class="params">(SpatialConvolutionMM_accGradParameters)</span><span class="params">(<span class="comment">/* 略 */</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">/* ... */</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* 循环取每个batch： */</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> elt = <span class="number">0</span>; elt &lt; batchSize; elt ++) &#123;</span><br><span class="line">    <span class="comment">/* gradOutput_n = gradOutput[elt] */</span></span><br><span class="line">    THCTensor_(select)(state, gradOutput_n, gradOutput, <span class="number">0</span>, elt);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 计算权重梯度 */</span></span><br><span class="line">    <span class="keyword">if</span> (gradWeight) &#123;</span><br><span class="line">      <span class="comment">/* input_n = input[elt] */</span></span><br><span class="line">      THCTensor_(select)(state, input_n, input, <span class="number">0</span>, elt);</span><br><span class="line"></span><br><span class="line">      <span class="comment">/* 把 input_n 展开为 columns */</span></span><br><span class="line">      im2col(</span><br><span class="line">        THCState_getCurrentStream(state),</span><br><span class="line">        THCTensor_(data)(state, input_n),</span><br><span class="line">        nInputPlane, inputHeight, inputWidth,</span><br><span class="line">        outputHeight, outputWidth,</span><br><span class="line">        kH, kW, padH, padW, dH, dW,</span><br><span class="line">        <span class="number">1</span>, <span class="number">1</span>, THCTensor_(data)(state, columns)</span><br><span class="line">      );</span><br><span class="line"></span><br><span class="line">      </span><br><span class="line">      <span class="keyword">int64_t</span> m = nOutputPlane;</span><br><span class="line">      <span class="keyword">int64_t</span> n = nInputPlane*kW*kH;</span><br><span class="line">      <span class="keyword">int64_t</span> k = columns-&gt;size(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">      <span class="comment">/* 调用GEMM计算gradWeight += scale * gradOutput_n * columns'*/</span></span><br><span class="line">      <span class="meta">#<span class="meta-keyword">ifdef</span> THC_REAL_IS_FLOAT</span></span><br><span class="line">      THCudaBlas_Sgemm(</span><br><span class="line">      #elif defined(THC_REAL_IS_HALF)</span><br><span class="line">      THCudaBlas_Hgemm(</span><br><span class="line">      #elif defined(THC_REAL_IS_DOUBLE)</span><br><span class="line">      THCudaBlas_Dgemm(</span><br><span class="line">      #endif</span><br><span class="line">          state,</span><br><span class="line">          <span class="string">'t'</span>, <span class="string">'n'</span>,</span><br><span class="line">          n, m, k,</span><br><span class="line">          scale,</span><br><span class="line">          THCTensor_(data)(state, columns), k,</span><br><span class="line">          THCTensor_(data)(state, gradOutput_n), k,</span><br><span class="line">          ScalarConvert&lt;<span class="keyword">int</span>, <span class="keyword">scalar_t</span>&gt;::to(<span class="number">1</span>),</span><br><span class="line">          THCTensor_(data)(state, gradWeight), n</span><br><span class="line">      );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 计算偏置梯度 */</span></span><br><span class="line">    <span class="keyword">if</span> (gradBias) &#123;</span><br><span class="line">     </span><br><span class="line">      <span class="keyword">int64_t</span> m_ = nOutputPlane;</span><br><span class="line">      <span class="keyword">int64_t</span> k_ = outputHeight * outputWidth;</span><br><span class="line"></span><br><span class="line">      <span class="comment">/* 调用GEMV计算 gradBias += scale * gradOutput_n * ones */</span></span><br><span class="line">      #<span class="keyword">if</span> defined(THC_REAL_IS_FLOAT) || defined(THC_REAL_IS_DOUBLE)</span><br><span class="line">      #ifdef THC_REAL_IS_FLOAT</span><br><span class="line">      THCudaBlas_Sgemv(</span><br><span class="line">      #elif defined(THC_REAL_IS_DOUBLE)</span><br><span class="line">      THCudaBlas_Dgemv(</span><br><span class="line">      #endif</span><br><span class="line">          state,</span><br><span class="line">          <span class="string">'t'</span>,</span><br><span class="line">          k_, m_,</span><br><span class="line">          scale,</span><br><span class="line">          THCTensor_(data)(state, gradOutput_n), k_,</span><br><span class="line">          THCTensor_(data)(state, ones), <span class="number">1</span>,</span><br><span class="line">          ScalarConvert&lt;<span class="keyword">int</span>, <span class="keyword">scalar_t</span>&gt;::to(<span class="number">1</span>),</span><br><span class="line">          THCTensor_(data)(state, gradBias), <span class="number">1</span></span><br><span class="line">      );</span><br><span class="line">      #endif</span><br><span class="line">      #ifdef THC_REAL_IS_HALF</span><br><span class="line">      THCudaBlas_Hgemm(</span><br><span class="line">          state,</span><br><span class="line">          <span class="string">'t'</span>, <span class="string">'n'</span>,</span><br><span class="line">          m_, <span class="number">1</span>, k_,</span><br><span class="line">          scale,</span><br><span class="line">          THCTensor_(data)(state, gradOutput_n), k_,</span><br><span class="line">          THCTensor_(data)(state, ones), k_,</span><br><span class="line">          ScalarConvert&lt;<span class="keyword">int</span>, <span class="keyword">scalar_t</span>&gt;::to(<span class="number">1</span>),</span><br><span class="line">          THCTensor_(data)(state, gradBias), m_</span><br><span class="line">      );</span><br><span class="line">      #endif</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* ... */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="aten">ATen</span></h2><p>看了<code>THNN.h</code>的读者可能会发现，THNN和THCUNN只定义了少量的神经网络相关的函数，其实大部分都定义在ATen中，这个ATen是指<code>pytorch/aten/src/ATen</code>文件夹（下同）。说到底，TH系列库都是torch lua时代留下的产物，是用C语言实现的，后来PyTorch开发者觉得cpp大法好，就用C++写了ATen，把TH里的接口都封装了，同时新的API直接在ATen里实现。</p><p>这个ATen有点意思，它大概干了这么几件事情：</p><ul><li>在<code>ATen/core/Tensor.h</code>定义了 <code>at::Tensor</code> 类型，这个是C++前端以及更上层的API都在用的Tensor类型，它的成员内有一个<code>TensorImpl impl_</code>，提供底层实现；</li><li>实现和封装了有关Tensor的所有操作，并根据数据类型和设备进行自动派发；</li><li>使用Python脚本生成ATen API。</li></ul><p>其中从TH（包括THC、THNN等）里封装的函数叫做 legacy函数，而在ATen直接实现的函数叫 native函数。native函数的实现全在 <code>ATen/native</code> 文件夹中，实现了THNN里没有的神经网络和Tensor操作，比如RNN什么的，API列表在<code>ATen/native/native_functions.yaml</code>里，感兴趣的童鞋可以自己阅读。</p><p>了解了神经网络每一层的前向传播和反向传播的实现之后，下一步就是控制执行顺序了，也就是自动微分（Autograd），下一章将介绍PyTorch自动微分的实现。</p><p>つづく</p><table><thead><tr class="header"><th style="text-align: left;">上一篇：<a href="https://www.52coding.com.cn/2019/05/05/PyTorch2/">THC</a></th><th style="text-align: right;">下一篇：<a href="https://www.52coding.com.cn/2019/05/05/PyTorch4/">Autograd</a></th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"></td><td style="text-align: right;"></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;THNN是一个用C语言实现的神经网络模块的库，提供的功能非常底层。它实现了许多基础的神经网络模块，包括线性层，卷积层，Sigmoid等各种激活层，一些基本的loss函数，这些API都声明在&lt;code&gt;THNN/generic/THNN.h&lt;/code&gt;中。每个模块都实现了前向传导（forward）和后向传导（backward）的功能。THCUNN则是对应模块的CUDA实现。&lt;/p&gt;
    
    </summary>
    
      <category term="博客" scheme="http://www.52coding.com.cn/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="Neural Network" scheme="http://www.52coding.com.cn/tags/Neural-Network/"/>
    
      <category term="PyTorch" scheme="http://www.52coding.com.cn/tags/PyTorch/"/>
    
      <category term="THNN" scheme="http://www.52coding.com.cn/tags/THNN/"/>
    
      <category term="CONV" scheme="http://www.52coding.com.cn/tags/CONV/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch源码浅析(2)：THC</title>
    <link href="http://www.52coding.com.cn/2019/05/05/PyTorch2/"/>
    <id>http://www.52coding.com.cn/2019/05/05/PyTorch2/</id>
    <published>2019-05-05T07:53:01.000Z</published>
    <updated>2019-05-05T08:06:43.284Z</updated>
    
    <content type="html"><![CDATA[<p>这篇主要看 Torch CUDA 部分，对应源码目录 <code>aten/src/THC</code>，里面包含了许多C++和CUDA代码。这部分实现了操作 THCTensor 和 THCStorage 的接口，不过底层用的数据结构还是 <code>TensorImpl</code> 和 <code>StorageImpl</code>。THC里的接口也是通过C语言范式实现的，但是Apply系列操作不再由宏来实现，而是使用了C++模板。其他的区别还有allocator不同，以及多了 THCState 结构。</p><a id="more"></a><p>记号：</p><ul><li>TH = TorcH</li><li>THC = TorcH Cuda</li></ul><!-- toc --><ul><li><a href="#thcstate">THCState</a></li><li><a href="#thcallocator">THCAllocator</a><ul><li><a href="#thccachingallocator">THCCachingAllocator</a></li><li><a href="#thccachinghostallocator">THCCachingHostAllocator</a></li></ul></li><li><a href="#thcapply">THCApply</a></li><li><a href="#总结">总结</a></li></ul><!-- tocstop --><h2><span id="thcstate">THCState</span></h2><p>通过观察THC里面实现的接口不难发现，几乎每个接口都需要传入一个 <code>THCState*</code> 参数，而这是在TH中没有的，这个THCState的声明在<code>THCGeneral.hpp</code>中：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">THCState</span> &#123;</span></span><br><span class="line">  <span class="comment">// 貌似是用来生成随机数的</span></span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">THCRNGState</span>* <span class="title">rngState</span>;</span></span><br><span class="line"><span class="comment">// 记录每个CUDA设备的cuBLAS句柄和cuSparse句柄</span></span><br><span class="line">  THCCudaResourcesPerDevice* resourcesPerDevice;</span><br><span class="line">  </span><br><span class="line"><span class="comment">// cuda设备个数</span></span><br><span class="line">  <span class="keyword">int</span> numDevices; </span><br><span class="line"></span><br><span class="line">  at::Allocator* cudaHostAllocator;<span class="comment">// 内存分配器</span></span><br><span class="line">  at::Allocator* cudaDeviceAllocator;<span class="comment">// 显存分配器</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 二维数组，大小为 numDevices * numDevices</span></span><br><span class="line">  <span class="comment">// 数组中的每一项 i, j 记录了 CUDA_i 能否直接从 CUDA_j 复制数据</span></span><br><span class="line">  <span class="comment">// 如果值为 1 代表允许拷贝，0 代表不允许，-1 代表不知道（默认值）</span></span><br><span class="line">  <span class="keyword">int</span>** p2pAccessEnabled;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>THCState是一个全局CUDA状态，记录了CUDA设备的所有有用的信息，它的初始化在<code>THCGeneral.cpp</code>中，代码并不难理解。</p><blockquote><p>注 <strong>host和device</strong>：在CUDA编程中，<em>host</em> 指的是CPU和它的内存，而 <em>device</em> 指GPU及其显存。在 <em>host</em> 上运行的代码可以操控内存和显存，还可以启动 <em>kernels</em> 在GPU上计算。因为CUDA编程的这些特性，一个典型CUDA程序的执行过程为：</p><ol type="1"><li>分配内存和显存空间</li><li>初始化内存数据（host）</li><li>把数据从内存 (host) 传送到显存 (device)</li><li>执行 kernels</li><li>把结果从显存 (device) 传回内存 (host)</li></ol></blockquote><h2><span id="thcallocator">THCAllocator</span></h2><h3><span id="thccachingallocator">THCCachingAllocator</span></h3><p>查看THCState初始化的代码不难发现，显存分配器<code>cudaDeviceAllocator</code>的类型为<code>CudaCachingAllocator</code>，它的实现在<code>c10/cuda/CUDACachingAllocator.cpp</code>中。进一步观察后发现，<code>CudaCachingAllocator</code>调用的分配内存函数实际上是<code>THCCachingAllocator</code>实现的，<code>THCCachingAllocator</code>不光向系统请求分配显存，还实现了缓存管理系统，我们主要来看一下<code>THCCachingAllocator</code>的实现。</p><p>首先声明一个内存区块的结构体，里面存储设备、stream、区块大小等信息：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Block</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span>           device;      <span class="comment">// gpu</span></span><br><span class="line">  cudaStream_t  stream;      <span class="comment">// allocation stream</span></span><br><span class="line">  stream_set    stream_uses; <span class="comment">// streams on which the block was used</span></span><br><span class="line">  <span class="keyword">size_t</span>        size;        <span class="comment">// block size in bytes</span></span><br><span class="line">  <span class="keyword">char</span>*         ptr;         <span class="comment">// memory address</span></span><br><span class="line">  <span class="keyword">bool</span>          allocated;   <span class="comment">// in-use flag      </span></span><br><span class="line">  <span class="keyword">int</span>           event_count; <span class="comment">// number of outstanding CUDA events</span></span><br><span class="line">  <span class="comment">// prev block if split from a larger allocation</span></span><br><span class="line">  Block*        prev;</span><br><span class="line">  <span class="comment">// next block if split from a larger allocation</span></span><br><span class="line">  Block*        next;</span><br><span class="line">  </span><br><span class="line">  Block(<span class="keyword">int</span> device, cudaStream_t stream, <span class="keyword">size_t</span> size) :</span><br><span class="line">      device(device), stream(stream), stream_uses(), size(size),</span><br><span class="line">      ptr(ptr), allocated(<span class="number">0</span>), prev(<span class="literal">NULL</span>), next(<span class="literal">NULL</span>),</span><br><span class="line">      event_count(<span class="number">0</span>) &#123; &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>为了方便比较查找，为<code>Block</code>定义比较大小的方法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">bool</span> <span class="title">BlockComparator</span><span class="params">(<span class="keyword">const</span> Block* a, <span class="keyword">const</span> Block* b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">// 首先比较设备ID</span></span><br><span class="line">  <span class="keyword">if</span> (a-&gt;device != b-&gt;device) &#123;</span><br><span class="line">    <span class="keyword">return</span> a-&gt;device &lt; b-&gt;device;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 再比较stream id</span></span><br><span class="line">  <span class="keyword">if</span> (a-&gt;stream != b-&gt;stream) &#123;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">uintptr_t</span>)a-&gt;stream &lt; (<span class="keyword">uintptr_t</span>)b-&gt;stream;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 再比较区块大小</span></span><br><span class="line">  <span class="keyword">if</span> (a-&gt;size != b-&gt;size) &#123;</span><br><span class="line">    <span class="keyword">return</span> a-&gt;size &lt; b-&gt;size;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 最后比较内存地址大小</span></span><br><span class="line">  <span class="keyword">return</span> (<span class="keyword">uintptr_t</span>)a-&gt;ptr &lt; (<span class="keyword">uintptr_t</span>)b-&gt;ptr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用此比较函数的话 <code>Block(device, NULL, 0)</code> 就是设备device上区块大小的下限，而<code>Block(device + 1, NULL, 0)</code>则是设备device上的区块大小上限。</p><p>接下来看<code>THCCachingAllocator</code>定义的属性：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">THCCachingAllocator</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">typedef</span> <span class="title">bool</span> <span class="params">(*Comparison)</span><span class="params">(<span class="keyword">const</span> Block*, <span class="keyword">const</span> Block*)</span></span>;</span><br><span class="line">  <span class="keyword">typedef</span> <span class="built_in">std</span>::<span class="built_in">set</span>&lt;Block*, Comparison&gt; FreeBlocks;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// device statistics</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;DeviceStats&gt; device_stats;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// lock around all operations</span></span><br><span class="line">  <span class="built_in">std</span>::mutex mutex;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// lock around calls to cudaFree (to prevent deadlocks with NCCL)</span></span><br><span class="line">  <span class="built_in">std</span>::mutex cuda_free_mutex;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// cached blocks larger than 1 MB</span></span><br><span class="line">  FreeBlocks large_blocks;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// cached blocks 1 MB or smaller</span></span><br><span class="line">  FreeBlocks small_blocks;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// allocated blocks by device pointer</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">unordered_map</span>&lt;<span class="keyword">void</span>*, Block*&gt; allocated_blocks;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// outstanding cuda events</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">deque</span>&lt;<span class="built_in">std</span>::pair&lt;cudaEvent_t, Block*&gt;&gt; cuda_events;</span><br><span class="line"></span><br><span class="line">  THCCachingAllocator() :</span><br><span class="line">      large_blocks(BlockComparator),</span><br><span class="line">      small_blocks(BlockComparator) &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每个属性是干什么的注释已经写的很清楚了，注意到<code>FreeBlocks</code>类型是Block的有序集合，曾经分配过但是用完了的内存会被缓存起来，大于1M的分块会进入<code>large_blocks</code>，小于等于1M的分块进入<code>small_blocks</code>。之后再向分配器申请内存时会先从缓存里面分配，分配策略为Best-fit，即返回大于等于所需大小的最小分块（使用<code>set::lower_bound</code>实现）。如果缓存中的所有分块都小于目标大小，那么会尝试<code>cudaMalloc</code>分配目标大小的内存，如果还是失败的话就把缓存release了再试<code>cudaMalloc</code>，如果还是分配失败就会报内（显）存不足的错误。</p><p>如果上面某一步分配内存成功了，由于分配的块很有可能比实际需要的size大，所以还要进行切割操作。切割操作就是判断多余的大小是否大于1M，如果大于1M就插入到<code>large_blocks</code>，否则插入到<code>small_blocks</code>中，然后再设置一下<code>prev</code>和<code>next</code>指针即可。</p><p>分配内存的（简略）实现如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment"> * allocates a block which is safe to use from the provided stream </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">void</span> THCCachingAllocator::<span class="built_in">malloc</span>(<span class="keyword">void</span>** devPtr, <span class="keyword">size_t</span> size, cudaStream_t stream)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">// 加锁，函数返回时自动释放</span></span><br><span class="line">  <span class="built_in">std</span>::lock_guard&lt;<span class="built_in">std</span>::mutex&gt; lock(mutex);</span><br><span class="line"><span class="comment">// 获取设备ID</span></span><br><span class="line">  <span class="keyword">int</span> device;</span><br><span class="line">  C10_CUDA_CHECK(cudaGetDevice(&amp;device));</span><br><span class="line"><span class="comment">// 类似四舍五入</span></span><br><span class="line">  size = round_size(size);</span><br><span class="line">  <span class="keyword">bool</span> small = size &lt;= kSmallAlloc;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 搜索目标</span></span><br><span class="line">  <span class="function">Block <span class="title">search_key</span><span class="params">(device, stream, size)</span></span>;</span><br><span class="line">  <span class="comment">// 判断应该从哪个集合分配</span></span><br><span class="line">  <span class="keyword">auto</span>&amp; free_blocks = small ? small_blocks : large_blocks;</span><br><span class="line"></span><br><span class="line">  Block* block = <span class="literal">NULL</span>;</span><br><span class="line">  Block* remaining = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 搜索大于等于目标的第一个块</span></span><br><span class="line">  <span class="keyword">auto</span> it = free_blocks.lower_bound(&amp;search_key);</span><br><span class="line">  <span class="keyword">if</span> (it != free_blocks.end() &amp;&amp; (*it)-&gt;device == device &amp;&amp; (*it)-&gt;stream == stream) &#123;</span><br><span class="line">    <span class="comment">// 如果device和stream和目标相同的话就分配该块内存</span></span><br><span class="line">    block = *it;</span><br><span class="line">    free_blocks.erase(it);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">void</span>* ptr;</span><br><span class="line">    <span class="keyword">size_t</span> alloc_size = small ? kSmallAlloc : size;</span><br><span class="line">    <span class="comment">// 尝试向系统申请内存</span></span><br><span class="line">    cudaError_t err = cuda_malloc_retry(device, &amp;ptr, alloc_size);</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess) &#123;</span><br><span class="line">      <span class="keyword">if</span> (err == cudaErrorMemoryAllocation) &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">auto</span>&amp; stats = get_stats_for_device(device);</span><br><span class="line"><span class="comment">// 报错：分配内存失败！</span></span><br><span class="line">        AT_ERROR(<span class="string">"CUDA out of memory. Tried to allocate"</span>);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        C10_CUDA_CHECK(err);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    block = <span class="keyword">new</span> Block(device, stream, alloc_size, (<span class="keyword">char</span>*)ptr);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (block-&gt;size - size &gt;= (small? kRoundSmall : kSmallAlloc+<span class="number">1</span>)) &#123;</span><br><span class="line">    remaining = block;</span><br><span class="line"><span class="comment">// 切割多余内存块</span></span><br><span class="line">    block = <span class="keyword">new</span> Block(device, stream, size, block-&gt;ptr);</span><br><span class="line">    block-&gt;prev = remaining-&gt;prev;</span><br><span class="line">    <span class="keyword">if</span> (block-&gt;prev) &#123;</span><br><span class="line">      block-&gt;prev-&gt;next = block;</span><br><span class="line">    &#125;</span><br><span class="line">    block-&gt;next = remaining;</span><br><span class="line"></span><br><span class="line">    remaining-&gt;prev = block;</span><br><span class="line">    remaining-&gt;ptr += size;</span><br><span class="line">    remaining-&gt;size -= size;</span><br><span class="line">    <span class="comment">// 把剩余块插入缓存</span></span><br><span class="line">    free_blocks.insert(remaining);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  block-&gt;allocated = <span class="literal">true</span>;</span><br><span class="line">  allocated_blocks[block-&gt;ptr] = block;</span><br><span class="line"></span><br><span class="line">  *devPtr = (<span class="keyword">void</span>*)block-&gt;ptr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>释放内存的时候不会把内存直接还给系统，而是缓存起来根据块大小插入<code>large_blocks</code>或<code>small_blocks</code>，在插入之前还会检查能否和之前被切割的部分合并。</p><blockquote><p>注 <strong>CUDA架构</strong>：从硬件上看，CUDA设备含有 SP (Streaming Processor) 和 SM (Streaming Multiprocessor)；从软件上看，有 thread, block, grid, 和 warp 等概念。</p><ul><li>SP：最基本的处理单元，最后具体的指令都是在SP上处理的。</li><li>SM：多个SP加上其他资源，如：warp scheduler, register, shared memory等组成的大核，相当于CPU的核心。</li><li>thread：普通的线程，运行在SP上。</li><li>block：多个线程会组成一个block，同一个block中的线程可以通过shared memory通信。同一个block中的线程在同一个SM中执行。</li><li>grid：多个block构成一个grid。</li><li>warp：调度和运行的基本单元，包含多个线程。每个线程执行相同指令，但是数据不同（SIMT）。一个warp需要占用一个SM运行，多个warps需要轮流进入SM。</li><li>stream：一个GPU操作队列，该队列中的操作将以添加到流中的先后顺序而依次执行。</li></ul><p><img src="/images/pytorch/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/cuda.png"></p></blockquote><h3><span id="thccachinghostallocator">THCCachingHostAllocator</span></h3><p><code>THCState</code>中还有一个内存分配器<code>cudaHostAllocator</code>用来分配main memory，这个分配器指针指向<code>HostAllocator</code>，实现在<code>aten/src/THC/THCCachingHostAllocator.cpp</code>中。这个分配器的实现和上面的类似，也提供了缓存功能。值得注意的是，它向系统分配内存是用的<code>cudaHostAlloc()</code>函数，这是cuda库函数，它与<code>malloc()</code>不同。<code>malloc()</code>分配的标准的、可分页的主机内存，而<code>cudaHostAlloc()</code>分配的是页锁定的主机内存，也称作固定内存 (pinned memory)。它的一个重要特点是操作系统将不会对这块内存分页并交换到磁盘上，从而保证了内存始终驻留在物理内存中。由于GPU知道内存的物理地址，因此就可以使用DMA技术来在GPU和CPU之间复制数据，加快复制速度。</p><p><img src="/images/pytorch/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/cpugpu.png"></p><h2><span id="thcapply">THCApply</span></h2><p>和TH中一样，THC也有Apply系列函数，不同的是THC中的Apply不再用宏实现，而是用C++模板函数实现，代码在<code>src/THC/THCApply.cuh</code>。</p><p>首先来看直接运行在GPU上的kernel的实现：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Op,</span><br><span class="line">          <span class="keyword">typename</span> Ta,</span><br><span class="line">          <span class="keyword">typename</span> IndexType,</span><br><span class="line">          <span class="keyword">int</span> ADims&gt;</span><br><span class="line">__global__ <span class="keyword">void</span></span><br><span class="line">kernelPointwiseApply1(<span class="keyword">const</span> OffsetInfo&lt;Ta, IndexType, ADims&gt; a,</span><br><span class="line">                      IndexType totalElements,</span><br><span class="line">                      Op op) &#123;</span><br><span class="line">  <span class="keyword">for</span> (IndexType linearIndex = (IndexType) blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">       linearIndex &lt; totalElements;</span><br><span class="line">       linearIndex += (IndexType) gridDim.x * blockDim.x) &#123;</span><br><span class="line">    op(a.get(linearIndex));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的kernel实现了对Tensor的一元操作，也就是对Tensor中的每一个都数据都执行<code>op</code>操作，二元和三元操作与一元操作代码类似，就不列出了。这段理解起来不难，只有三点需要特殊说明一下：</p><p><strong>函数对象</strong></p><p>模板参数中的 <code>Op</code> 是<a href="https://www.wikiwand.com/zh-hans/%E5%87%BD%E6%95%B0%E5%AF%B9%E8%B1%A1" target="_blank" rel="noopener">函数对象</a>类型，它实现了 <code>operator()</code> 方法，这样的话它的实例对象就可以直接当函数来用，如：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">IntComparator</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> &amp;a, <span class="keyword">const</span> <span class="keyword">int</span> &amp;b)</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a &lt; b;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>这就是一个很常用的用来比较整数大小的函数对象类型，它可以这样用：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">IntComparator op;</span><br><span class="line">op(<span class="number">1</span>, <span class="number">2</span>);<span class="comment">// true</span></span><br></pre></td></tr></table></figure><p>函数对象类似于函数指针，但有两个优点：第一是编译器可以内联执行函数对象的调用；第二是函数对象内部可以保持状态。</p><p><strong>循环下标</strong></p><p>循环的下标每次增加 <code>gridDim.x * blockDim.x</code>，而不是通常的<code>1</code>，这就涉及到kernel是如何执行的了。首先kernel被分配到多个Grid上执行，每个Grid里有多个Block，每个Block里有多个Thread，这些线程(Thread)都执行相同的代码，也就是kernel。为了让这些线程分工合作，每个线程都记录了<code>gridDim</code>, <code>blockDim</code>, <code>blockIdx</code>, 和 <code>threadIdx</code>，分别代表Grid维度，Block维度，Block ID，和Thread ID。在这里，Grid和Block都是一维的，所以当前线程的全局ID可以通过</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blockIdx.x * blockDim.x + threadIdx.x</span><br></pre></td></tr></table></figure><p>得到，其中 <code>blockIdx.x</code> 表示第几个Block，<code>blockDim.x</code> 是一个Block内有多少线程，<code>threadIdx.x</code> 是该线程在Block内的ID。</p><p>再看下标递增的间隔 <code>gridDim.x * blockDim.x</code> 实际上是执行这个kernel的线程个数，即一个Grid内的Block数 <span class="math inline">\(\times\)</span> 一个Block内线程数。这样的好处是，如果不同线程要读取的内存是连续的，则这些内存读取可以捆绑到一起进行，加快了内存读取速度，如下图。</p><p><img src="/images/pytorch/coalesced.png"></p><p><strong>OffsetInfo</strong></p><p>注意到循环体里只有一句代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">op(a.get(linearIndex));</span><br></pre></td></tr></table></figure><p>意思对第<code>linearIndex</code>个数据进行<code>op</code>操作。由于<code>linearIndex</code>是线性地址，并不是数据真正的内存偏移，所以 <code>a.get()</code> 的作用是把线性地址转换为实际数据的存储地址，而 <code>a</code> 的类型是 <code>OffsetInfo&lt;Ta, IndexType, Dims&gt;</code>。</p><p>转换的算法其实很简单，线性地址就是第几个数据，比如<code>x[0][0]</code>是第0个数据，线性地址就是0，<code>x[0][1]</code> 的线性地址就是1，<code>x[i][j]</code> 的线性地址就是 <code>i * size[0] + j</code>。而且我们知道 <code>x[i][j]</code> 的内存偏移为 <code>i * strides[0] + j * strides[1]</code>，那么转换算法要做的就是把线性地址转换为 <code>ijk</code> 下标，然后再把下标转化为内存地址：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> IndexType&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">IndexToOffset</span>&lt;T, IndexType, -1&gt; &#123;</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">inline</span> __host__ __<span class="function">device__ IndexType <span class="title">get</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    IndexType linearId,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> TensorInfo&lt;T, IndexType&gt;&amp; info)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    IndexType offset = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从最外围下标开始计算</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = info.dims - <span class="number">1</span>; i &gt; <span class="number">0</span>; --i) &#123;</span><br><span class="line">    <span class="comment">// 求出第i维下标</span></span><br><span class="line">      IndexType curDimIndex = linearId % info.sizes[i];</span><br><span class="line">      <span class="comment">// 转换为内存偏移</span></span><br><span class="line">      IndexType curDimOffset = curDimIndex * info.strides[i];</span><br><span class="line">      <span class="comment">// 和总偏移相加</span></span><br><span class="line">      offset += curDimOffset;</span><br><span class="line">      <span class="comment">// 计算第i-1维的线性地址</span></span><br><span class="line">      linearId /= info.sizes[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> offset + linearId * info.strides[<span class="number">0</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>细心的读者可能注意到了，上面的函数是 <code>IndexToOffset</code> 的方法，并不是 <code>OffsetInfo</code> 的，实际上后者对非负整数除法和取余进行了优化，把除法转换为一系列乘法，从而加快计算速度。</p><p>大致思想是这样的，假设我们要计算 <span class="math inline">\(\lfloor\frac{n}{d}\rfloor\)</span>，对任意N位非负整数<span class="math inline">\(d\)</span>，总可以找到一个 magic number <span class="math inline">\(m\)</span> 和 <span class="math inline">\(s\)</span>，使得： <span class="math display">\[\lfloor\frac{n}{d}\rfloor=\lfloor\frac{m\times n}{2^{N+s}}\rfloor\]</span> 这样就把除法操作转化为乘法和右移了。</p><p>那么怎么找 <span class="math inline">\(m\)</span> 和 <span class="math inline">\(s\)</span> 呢，非常简单： <span class="math display">\[s=\lceil\log_2 d\rceil \\m = \lfloor 2^N\times\frac{(2^s-d)}{d}\rfloor+2^N+1\]</span> 对于固定的除数 <span class="math inline">\(d\)</span>，这两个参数是不会变的，如果需要多次除以 <span class="math inline">\(d\)</span>，则可以提前计算好 <span class="math inline">\(m\)</span> 和 <span class="math inline">\(s\)</span>，在计算时加快速度。注意到计算 <span class="math inline">\(m\)</span> 时也是需要除法运算的，所以如果这个除数只用一次的话，那么用快速除法是没意义的（除非<span class="math inline">\(m\)</span>是在编译期计算的）。关于这个式子的证明和推导看<a href="http://ridiculousfish.com/blog/posts/labor-of-division-episode-i.html" target="_blank" rel="noopener">这里</a>。</p><p>代码中，<code>OffsetInfo</code> 的实现实际上是跟模板参数 <code>Dims</code> 相关的，如果维度能在编译期给出的话（值非-1），则在生成 <code>OffsetInfo</code> 对象时计算 <span class="math inline">\(m\)</span> 和 <span class="math inline">\(s\)</span>，以备之后使用：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> IndexType, <span class="keyword">int</span> Dims&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">OffsetInfo</span> &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">OffsetInfo</span><span class="params">(<span class="keyword">const</span> TensorInfo&lt;T, IndexType&gt;&amp; tinfo)</span> </span>&#123;</span><br><span class="line">    assert(tinfo.dims == Dims);</span><br><span class="line">    data = tinfo.data;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; Dims; ++i) &#123;</span><br><span class="line">    <span class="comment">// 提前计算 m, s (实现在 IntDivider 类中)</span></span><br><span class="line">      sizes[i] = IntDivider&lt;IndexType&gt;(tinfo.sizes[i]);</span><br><span class="line">      strides[i] = tinfo.strides[i];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  __host__ __<span class="function">device__ T* <span class="title">get</span><span class="params">(IndexType linearIndex)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    IndexType offset = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = Dims - <span class="number">1</span>; i &gt; <span class="number">0</span>; --i) &#123;</span><br><span class="line">    <span class="comment">// 使用快速除法</span></span><br><span class="line">      DivMod&lt;IndexType&gt; divmod = sizes[i].divmod(linearIndex);</span><br><span class="line">      linearIndex = divmod.div;</span><br><span class="line">      offset += divmod.mod * strides[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &amp;data[offset + linearIndex * strides[<span class="number">0</span>]];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  T* data;</span><br><span class="line">  <span class="comment">// Dims 是编译期常量，所以 sizes 和 strides 是静态分配的</span></span><br><span class="line">  IntDivider&lt;IndexType&gt; sizes[Dims];</span><br><span class="line">  IndexType strides[Dims];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>如果创建 <code>OffsetInfo</code> 时，<code>Dims</code> 的值为-1的话，代表Tensor的大小不是固定的，这样的话 <code>OffsetInfo::sizes</code> 和 <code>OffsetInfo::strides</code> 是动态分配的，就会触发NVCC的一个bug：<em>if a kernel argument contains an array that is dynamically accessed, the whole array is first copied into the local memory. Pre-computation makes it worse because now we have more data to copy.</em></p><p>所以对于大小不固定（编译期不能给出具体维度）的Tensor采用之前的办法计算，这里使用了特化模板匹配 <code>Dims</code> 为-1的情况：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> IndexType&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">OffsetInfo</span>&lt;T, IndexType, -1&gt; &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">OffsetInfo</span><span class="params">(<span class="keyword">const</span> TensorInfo&lt;T, IndexType&gt;&amp; tinfo)</span></span></span><br><span class="line">    : tinfo(tinfo) &#123; &#125;</span><br><span class="line"></span><br><span class="line">  __host__ __<span class="function">device__ T* <span class="title">get</span><span class="params">(IndexType linearIndex)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 直接调用之前列出的 IndexToOffset::get 方法</span></span><br><span class="line">    IndexType offset = IndexToOffset&lt;T, IndexType, <span class="number">-1</span>&gt;::get(linearIndex, tinfo);</span><br><span class="line">    <span class="keyword">return</span> &amp;tinfo.data[offset];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  TensorInfo&lt;T, IndexType&gt; tinfo;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2><span id="总结">总结</span></h2><p>总结一下，THC中的API的声明和实现都在 <code>generic</code> 目录下，API的形式是C风格范式：<code>THCTensor_(xxx)(...)</code> 。这些函数几乎都会调用 apply 系列函数来在GPU中实现具体功能，而 apply 函数的核心在于传入的OP，这些OP都定义在 <code>THC/</code> 根目录下。</p><p>举个例子，来看一下 <code>THCTensor_fill(state, self, value)</code> 是怎么执行的，它的功能是把 <code>self</code> Tensor 的每个元素赋值为 <code>value</code>。这个API的定义在 <code>THC/generic/THCTensorMath.cu</code>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THCTensor_</span><span class="params">(fill)</span><span class="params">(THCState* state, THCTensor *self_, <span class="keyword">scalar_t</span> value)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  THCAssertSameGPU(THCTensor_(checkGPU)(state, <span class="number">1</span>, self_));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!THC_pointwiseApply1&lt;<span class="keyword">scalar_t</span>&gt;(</span><br><span class="line">        state, self_, TensorFillOp&lt;<span class="keyword">scalar_t</span>&gt;(value))) &#123;</span><br><span class="line">    THArgCheck(<span class="literal">false</span>, <span class="number">1</span>, CUTORCH_DIM_WARNING);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  THCudaCheck(cudaGetLastError());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心代码为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">THC_pointwiseApply1&lt;<span class="keyword">scalar_t</span>&gt;(state, self_, TensorFillOp&lt;<span class="keyword">scalar_t</span>&gt;(value))</span><br></pre></td></tr></table></figure><p>这句话调用一元 apply 函数，传入的参数为 THCState, 要操作的Tensor和相应OP。<code>THC_pointwiseApply1()</code> 会进行一些特殊情况的处理和优化，最后调用之前列出的apply kernel执行相应OP。</p><p><code>TensorFillOp</code> 定义在 <code>THC/THCTensorMath.cu</code> 中，</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TensorFillOp</span> &#123;</span></span><br><span class="line">  TensorFillOp(T v) : val(v) &#123;&#125;</span><br><span class="line">  __device__ __<span class="function">forceinline__ <span class="keyword">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(T* v)</span> </span>&#123; *v = val; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">const</span> T val;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>注意到填入Tensor的值被保存为类的常量，而不是作为 <code>operator()</code> 的参数，这样才能统一接口，才能被 <code>kernelPointwiseApply1()</code> 直接调用。</p><p>调用过程的示意图如下：</p><p><img src="/images/pytorch/THCTensor_fill.png"></p><p>つづく</p><table><thead><tr class="header"><th style="text-align: left;">上一篇：<a href="https://www.52coding.com.cn/2019/05/05/PyTorch1/">THTensor</a></th><th style="text-align: right;">下一篇：<a href="https://www.52coding.com.cn/2019/05/05/PyTorch3/">NN</a></th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"></td><td style="text-align: right;"></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇主要看 Torch CUDA 部分，对应源码目录 &lt;code&gt;aten/src/THC&lt;/code&gt;，里面包含了许多C++和CUDA代码。这部分实现了操作 THCTensor 和 THCStorage 的接口，不过底层用的数据结构还是 &lt;code&gt;TensorImpl&lt;/code&gt; 和 &lt;code&gt;StorageImpl&lt;/code&gt;。THC里的接口也是通过C语言范式实现的，但是Apply系列操作不再由宏来实现，而是使用了C++模板。其他的区别还有allocator不同，以及多了 THCState 结构。&lt;/p&gt;
    
    </summary>
    
      <category term="博客" scheme="http://www.52coding.com.cn/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="PyTorch" scheme="http://www.52coding.com.cn/tags/PyTorch/"/>
    
      <category term="Tensor" scheme="http://www.52coding.com.cn/tags/Tensor/"/>
    
      <category term="CUDA" scheme="http://www.52coding.com.cn/tags/CUDA/"/>
    
      <category term="THC" scheme="http://www.52coding.com.cn/tags/THC/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch源码浅析(1)：THTensor</title>
    <link href="http://www.52coding.com.cn/2019/05/05/PyTorch1/"/>
    <id>http://www.52coding.com.cn/2019/05/05/PyTorch1/</id>
    <published>2019-05-05T07:51:01.000Z</published>
    <updated>2019-05-05T08:13:05.281Z</updated>
    
    <content type="html"><![CDATA[<p>PyTorch中Tensor的存储和表示分开，多个THTensor可能共享一个THStorage，每个THTensor可能拥有不同的view（e.g. size, stride）。这样设计的好处是，有时看起来不一样的数据底层是共享的，比如矩阵与矩阵的转置、二维矩阵与二维矩阵变成一维时的矩阵。这部分的主力实现在 <code>pytorch/aten</code> 文件夹中，这里面既实现了底层的Tensor操作库，也封装了名为 <strong>ATen</strong> 的 C++11接口。</p><a id="more"></a><p><code>aten/src</code>里有几个重要的文件夹，它们实现的内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">src</span><br><span class="line">├── ATen        # Tensor操作的C++11接口</span><br><span class="line">├── TH          # CPU Tensor 的底层实现（本篇内容）</span><br><span class="line">├── THC         # GPU Tensor (CUDA) 的底层实现（在下一篇讲）</span><br><span class="line">├── THCUNN      # CUDA版底层神经网络实现(下下篇讲)</span><br><span class="line">└── THNN        # CPU版底层神经网络实现(下下篇讲)</span><br></pre></td></tr></table></figure><p>这篇讲的主要代码也都在TH文件夹内。</p><p>目录</p><!-- toc --><ul><li><a href="#thtensor-thstorage">THTensor &amp; THStorage</a><ul><li><a href="#tensorimpl">TensorImpl</a></li><li><a href="#storageimpl">StorageImpl</a></li></ul></li><li><a href="#智能指针-intrusive_ptr">智能指针 Intrusive_ptr</a></li><li><a href="#tensor-apply-dynamic-dispatch">Tensor Apply &amp; Dynamic Dispatch</a></li></ul><!-- tocstop --><h2><span id="thtensor-amp-thstorage">THTensor &amp; THStorage</span></h2><p> TH里面的核心类型就是<code>THTensor</code>和<code>THStorage</code>了，前者是Tensor的view，后者是Tensor数据的存储地址。由于Tensor的数据类型可以是多种多样的，而每种类型的API都一致，所以需要用到范型来减少重复代码，TH中是使用宏实现范型功能（因为Torch一开始是用C实现的），不了解的读者可以先参考一下<a href="https://zhuanlan.zhihu.com/p/34496542" target="_blank" rel="noopener">这篇知乎专栏</a>。</p><p><code>THTensor</code>的定义在<code>aten/src/TH/generic/THTensor.h</code>中：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THTensor at::TensorImpl</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THFloatTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THDoubleTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THHalfTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THByteTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THCharTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THShortTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THIntTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THLongTensor THTensor</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THBoolTensor THTensor</span></span><br></pre></td></tr></table></figure><p>同样的，<code>THStorage</code>的定义在<code>Aten/src/TH/generic/THStorage.h</code>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THStorage at::StorageImpl</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THFloatStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THDoubleStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THHalfStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THByteStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THCharStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THShortStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THIntStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THLongStorage THStorage</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THBoolStorage THStorage</span></span><br></pre></td></tr></table></figure><p>在 <code>THTensor.h</code>中有很多类似这样的API声明：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TH_API THStorage* <span class="title">THTensor_</span><span class="params">(storage)</span><span class="params">(<span class="keyword">const</span> THTensor *self)</span></span>;</span><br></pre></td></tr></table></figure><p>其中，<code>THTensor_</code>的宏定义为</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THTensor_(NAME)   TH_CONCAT_4(TH,Real,Tensor_,NAME)</span></span><br></pre></td></tr></table></figure><p>上面的 <code>TH_CONCAT_4</code> 宏就是把它的四个参数连接起来，其中 <code>Real</code> 会被定义为实际类型（Float, Bool等），所以上面的API会被展开成：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">at::<span class="function">StorageImpl <span class="title">THFloatTensor_storage</span><span class="params">(<span class="keyword">const</span> at::TensorImpl *self)</span></span>;</span><br><span class="line">at::<span class="function">StorageImpl <span class="title">THBoolTensor_storage</span><span class="params">(<span class="keyword">const</span> at::TensorImpl *self)</span></span>;</span><br><span class="line">at::<span class="function">StorageImpl <span class="title">THLongTensor_storage</span><span class="params">(<span class="keyword">const</span> at::TensorImpl *self)</span></span>;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>在这些API中还会用到 <code>scalar_t</code> 类型，这个也是一个宏，特化的时候会传入具体的类型（如用来确保 <code>THFloatTensor</code> 里的 <code>StorageImpl</code> 存储的float类型）。</p><p>不难发现，所有的THTensor类型最终都会替换成 <code>at::TensorImpl</code>，所有的THStorage类型也都会替换成 <code>at::StorageImpl</code>，前者的实现在<code>c10/core/TensorImpl.h</code>中，后者的实现在<code>c10/core/StorageImpl.h</code>中。这两个类型的实现在c10中，也就是说Tensor类型的实现转移到了c10中，但API的实现依然在TH中。</p><h3><span id="tensorimpl">TensorImpl</span></h3><p>接着具体来看一下<code>TensorImpl</code>的实现，首先来看一下它的声明和属性：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">TensorImpl</span> :</span> <span class="keyword">public</span> c10::intrusive_ptr_target &#123;</span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">  <span class="comment">// 指向实际数据存储位置，也就是指向StorageImpl</span></span><br><span class="line">  Storage storage_;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// 用于自动微分，只对Variable适用</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;c10::AutogradMetaInterface&gt; autograd_meta_ = <span class="literal">nullptr</span>;</span><br><span class="line"></span><br><span class="line">  SmallVector&lt;<span class="keyword">int64_t</span>,<span class="number">5</span>&gt; sizes_;  <span class="comment">// Tensor的实际大小</span></span><br><span class="line">  SmallVector&lt;<span class="keyword">int64_t</span>,<span class="number">5</span>&gt; strides_;<span class="comment">// 各个维度直接的间隔</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">int64_t</span> storage_offset_ = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int64_t</span> numel_ = <span class="number">1</span>;<span class="comment">// Tensor中元素个数，也就是sizes_数组中元素的乘积</span></span><br><span class="line"></span><br><span class="line">  caffe2::TypeMeta data_type_;<span class="comment">// 数据类型</span></span><br><span class="line"></span><br><span class="line">  TensorTypeId type_id_;</span><br><span class="line">  <span class="keyword">bool</span> is_contiguous_ = <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">bool</span> is_variable_ = <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">bool</span> is_wrapped_number_ = <span class="literal">false</span>;</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">bool</span> allow_tensor_metadata_change_ = <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">bool</span> reserved_ = <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>TensorImpl</code>继承自<code>intrusive_ptr_target</code>，后者是为了通过使用<code>intrusive_ptr&lt;T&gt;</code>实现引用计数而设计的基类，需要实现引用计数的类只需继承它即可。<code>TensorImpl</code>中的每个成员是干什么的基本都有注释，其中<code>strides_</code>是用来实现内存寻址的，即某个ijk脚标对应的内存地址为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">storage_offset_ + i * strides_[<span class="number">0</span>] + j * strides_[<span class="number">1</span>] + k * strides_[<span class="number">2</span>]</span><br></pre></td></tr></table></figure><p>正常情况下用<code>sizes_</code>代替<code>strides_</code>可以实现同样的功能，但是有的Tensor是由较大的Tensor分割而来，维度之间的间隔不是<code>sizes_</code>，所以需要用另一个数组<code>strides_</code>存储维度间隔。</p><p><code>TensorImpl</code>中的方法较多，就不一一列出了，实现了对Tensor（和Variable）的基本操作，Aten中的API也是基于这些基本操作实现的。</p><p><strong><code>Variable</code>与<code>Tensor</code>的合并</strong></p><p>在早期的PyTorch版本中，Variable与Tensor是不同的类，Variable用来保存需要计算梯度的Tensor，但Variable的实现并不美好：一方面<code>Variable::Impl</code>是Tensor的子类，而它的成员里又拥有一个Tensor（存储数据），这违反了<a href="https://www.wikiwand.com/zh-hans/%E9%87%8C%E6%B0%8F%E6%9B%BF%E6%8D%A2%E5%8E%9F%E5%88%99" target="_blank" rel="noopener">里氏替换原则</a>，而且让Tensor的实现变得很复杂。而在现版本中，已经把Variable变成Tensor了，把一些<code>Variable</code>特有的方法（e.g. <code>set_requires_grad</code>, <code>requires_grad</code>）移到了<code>TensorImpl</code>里。</p><h3><span id="storageimpl">StorageImpl</span></h3><p><code>StorageImpl</code>的声明和属性如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">StorageImpl</span> <span class="title">final</span> :</span> <span class="keyword">public</span> c10::intrusive_ptr_target &#123;</span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  caffe2::TypeMeta data_type_;  <span class="comment">// 数据类型</span></span><br><span class="line">  DataPtr data_ptr_;            <span class="comment">// 指向存储数据的内存块</span></span><br><span class="line">  <span class="keyword">int64_t</span> numel_;               <span class="comment">// 数据总数</span></span><br><span class="line">  <span class="keyword">bool</span> resizable_;</span><br><span class="line">  Allocator* allocator_;        <span class="comment">// 内存分配器</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中，<code>caffe2::TypeMeta data_type_</code> 存储了数据类型信息，包括：类型id、大小、类型名字等；<code>DataPtr</code> 其实就是 <em>unique_ptr</em>，但指针类型固定为 <code>void*</code>。</p><p><strong>分配内存</strong></p><p>在 <code>Allocator.cpp</code> 中定义了全局变量 <code>allocator_array</code> 来存储所有的 allocator，每个设备类型对应一个：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">C10_API at::Allocator* allocator_array[at::COMPILE_TIME_MAX_DEVICE_TYPES];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">SetAllocator</span><span class="params">(at::DeviceType t, at::Allocator* alloc)</span> </span>&#123;</span><br><span class="line">  allocator_array[<span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(t)] = alloc;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">at::<span class="function">Allocator* <span class="title">GetAllocator</span><span class="params">(<span class="keyword">const</span> at::DeviceType&amp; t)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span>* alloc = allocator_array[<span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(t)];</span><br><span class="line">  AT_ASSERTM(alloc, <span class="string">"Allocator for "</span>, t, <span class="string">" is not set."</span>);</span><br><span class="line">  <span class="keyword">return</span> alloc;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同时配备了 <code>SetAllocator</code> 和 <code>GetAllocator</code> 来设置和获取相应的分配器。</p><p><code>Allocator</code> 是一个虚基类，它的声明如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">Allocator</span> &#123;</span></span><br><span class="line">  <span class="keyword">virtual</span> ~Allocator() = <span class="keyword">default</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> DataPtr <span class="title">allocate</span><span class="params">(<span class="keyword">size_t</span> n)</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 重载这个函数很关键，用来释放申请到的内存</span></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> DeleterFnPtr <span class="title">raw_deleter</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span>* <span class="title">raw_allocate</span><span class="params">(<span class="keyword">size_t</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> dptr = allocate(n);</span><br><span class="line">    AT_ASSERT(dptr.get() == dptr.get_context());</span><br><span class="line">    <span class="keyword">return</span> dptr.release_context();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">raw_deallocate</span><span class="params">(<span class="keyword">void</span>* ptr)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> d = raw_deleter();</span><br><span class="line">    AT_ASSERT(d);</span><br><span class="line">    d(ptr);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>这个分配器有两种使用方法，第一种就是直接调用 <code>raw_allocate</code> 和 <code>raw_deallocate</code> 分配和释放内存。第二种方法是调用 <code>Allocator::allocate</code>，这个方法将返回一个 <code>DataPtr</code> 类型的指针，也就是 <em>unique_ptr</em>，由于deleter存储在指针中，在释放指针的时候会释放相应的内存。不过两种方法的正确性都依赖于 <code>Allocator::raw_deleter</code> 能正确返回相应的释放器（deleter），否则两种方法都不能正确释放内存。这里需要注意的是，释放器（deleter）未必就是C库函数<code>free</code>：根据操作系统的不同，也可能是 <code>_aligned_free</code>；根据设备的不同也可能是其他函数。</p><p>下面是在CPU上内存分配的具体实现：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">C10_API</span> <span class="title">DefaultCPUAllocator</span> <span class="title">final</span> :</span> at::Allocator &#123;</span><br><span class="line">  ...</span><br><span class="line">      </span><br><span class="line">  at::<span class="function">DataPtr <span class="title">allocate</span><span class="params">(<span class="keyword">size_t</span> nbytes)</span> <span class="keyword">const</span> override </span>&#123;</span><br><span class="line">    <span class="keyword">void</span>* data = alloc_cpu(nbytes);</span><br><span class="line">    <span class="keyword">if</span> (FLAGS_caffe2_report_cpu_memory_usage &amp;&amp; nbytes &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      getMemoryAllocationReporter().New(data, nbytes);</span><br><span class="line">      <span class="keyword">return</span> &#123;data, data, &amp;ReportAndDelete, </span><br><span class="line">              at::Device(at::DeviceType::CPU)&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 这四个参数用来构造一个DataPtr</span></span><br><span class="line">    <span class="keyword">return</span> &#123;data, data, &amp;free_cpu, </span><br><span class="line">            at::Device(at::DeviceType::CPU)&#125;;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  at::<span class="function">DeleterFnPtr <span class="title">raw_deleter</span><span class="params">()</span> <span class="keyword">const</span> override </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (FLAGS_caffe2_report_cpu_memory_usage) &#123;</span><br><span class="line">      <span class="keyword">return</span> &amp;ReportAndDelete;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> &amp;free_cpu;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span>* <span class="title">alloc_cpu</span><span class="params">(<span class="keyword">size_t</span> nbytes)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (nbytes == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">void</span>* data;</span><br><span class="line">  <span class="comment">// 分配64字节对齐的连续内存块</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> __ANDROID__</span></span><br><span class="line">  data = memalign(gAlignment, nbytes);<span class="comment">// gAlignment = 64</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">elif</span> defined(_MSC_VER)</span></span><br><span class="line">  data = _aligned_malloc(nbytes, gAlignment);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">  CAFFE_ENFORCE_EQ(posix_memalign(&amp;data, gAlignment, nbytes), <span class="number">0</span>);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 移动数据到线程的NUMA节点中</span></span><br><span class="line">  NUMAMove(data, nbytes, GetCurrentNUMANode());</span><br><span class="line"><span class="comment">// 填充内存</span></span><br><span class="line">  <span class="keyword">if</span> (FLAGS_caffe2_cpu_allocator_do_zero_fill) &#123;</span><br><span class="line">    <span class="built_in">memset</span>(data, <span class="number">0</span>, nbytes);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (FLAGS_caffe2_cpu_allocator_do_junk_fill) &#123;</span><br><span class="line">    memset_junk(data, nbytes);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> data;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">free_cpu</span><span class="params">(<span class="keyword">void</span>* data)</span> </span>&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> _MSC_VER</span></span><br><span class="line">  _aligned_free(data);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">  <span class="built_in">free</span>(data);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>分配时使用 <code>memalign/_aligned_malloc/posix_memalign</code> 函数确保内存是64字节对齐的。</p><h2><span id="智能指针-intrusive_ptr">智能指针 Intrusive_ptr</span></h2><p>PyTorch中使用intrusive_ptr来管理THTensor和THStorage的引用计数，其中引用分为强引用和弱引用（弱引用为了解决循环引用问题），对应的类名为 <code>intrusive_ptr</code> 和 <code>weak_intrusive_ptr</code>。由于弱引用和强引用的实现类似，为了简单起见，我把弱引用的代码都去掉了，简化intrusive_ptr的实现如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">intrusive_ptr_target</span> &#123;</span></span><br><span class="line">  <span class="keyword">mutable</span> <span class="built_in">std</span>::atomic&lt;<span class="keyword">size_t</span>&gt; refcount_;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 声明友元类使得只能指针可以访问 refcount_</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="keyword">friend</span> <span class="class"><span class="keyword">class</span> <span class="title">intrusive_ptr</span>;</span></span><br><span class="line"></span><br><span class="line"> <span class="keyword">protected</span>:</span><br><span class="line">  <span class="comment">// 隐藏析构函数，防止直接析构对象</span></span><br><span class="line">  <span class="keyword">virtual</span> ~intrusive_ptr_target() &#123;&#125;</span><br><span class="line"></span><br><span class="line">  constexpr intrusive_ptr_target() noexcept : refcount_(0) &#123;&#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line"><span class="comment">// 在摧毁对象时会调用次函数释放资源</span></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">release_resources</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">TTarget</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">intrusive_ptr</span> <span class="title">final</span> &#123;</span></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  TTarget* target_;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 增加引用计数</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">retain_</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (target_ != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">      <span class="keyword">size_t</span> new_refcount = ++target_-&gt;refcount_;</span><br><span class="line">      AT_ASSERTM(new_refcount != <span class="number">1</span>,</span><br><span class="line">                 <span class="string">"intrusive_ptr:Cannot increase refcount after it"</span></span><br><span class="line">                 <span class="string">"reached zero."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 销毁智能指针，减少引用计数，当引用计数为0时摧毁对象</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">reset_</span><span class="params">()</span> <span class="keyword">noexcept</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (target_ != <span class="literal">nullptr</span> &amp;&amp; --target_-&gt;refcount_ == <span class="number">0</span>) &#123;</span><br><span class="line">      target_-&gt;release_resources();</span><br><span class="line">      <span class="keyword">delete</span> target_;</span><br><span class="line">    &#125;</span><br><span class="line">    target_ = <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 隐藏通过普通指针初始化的构造函数（只能通过make_intrusive调用）</span></span><br><span class="line">  explicit intrusive_ptr(TTarget* target) noexcept : target_(target) &#123;&#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  intrusive_ptr() <span class="keyword">noexcept</span> : intrusive_ptr(<span class="literal">nullptr</span>) &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过其他智能指针初始化，需增加引用计数</span></span><br><span class="line">  intrusive_ptr(<span class="keyword">const</span> intrusive_ptr&amp; rhs) : target_(rhs.target_) &#123;</span><br><span class="line">  retain_(); </span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ~intrusive_ptr() <span class="keyword">noexcept</span> &#123; reset_(); &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">TTarget* <span class="title">get</span><span class="params">()</span> <span class="keyword">const</span> <span class="keyword">noexcept</span> </span>&#123; <span class="keyword">return</span> target_; &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 重载运算符使其能当作正常指针使用</span></span><br><span class="line">  <span class="keyword">const</span> TTarget&amp; <span class="keyword">operator</span>*() <span class="keyword">const</span> <span class="keyword">noexcept</span> &#123; <span class="keyword">return</span> *target_; &#125;</span><br><span class="line">  TTarget&amp; <span class="keyword">operator</span>*() <span class="keyword">noexcept</span> &#123; <span class="keyword">return</span> *target_; &#125;</span><br><span class="line">  <span class="keyword">const</span> TTarget* <span class="keyword">operator</span>-&gt;() <span class="keyword">const</span> <span class="keyword">noexcept</span> &#123; <span class="keyword">return</span> target_; &#125;</span><br><span class="line">  TTarget* <span class="keyword">operator</span>-&gt;() <span class="keyword">noexcept</span> &#123; <span class="keyword">return</span> target_; &#125;</span><br><span class="line">  <span class="function"><span class="keyword">operator</span> <span class="title">bool</span><span class="params">()</span> <span class="keyword">const</span> <span class="keyword">noexcept</span> </span>&#123; <span class="keyword">return</span> target_ != <span class="literal">nullptr</span>; &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取当前的引用计数</span></span><br><span class="line">  <span class="keyword">size_t</span> use_count() <span class="keyword">const</span> <span class="keyword">noexcept</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (target_ == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> target_-&gt;refcount_.load();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 把普通指针转换为智能指针（增加引用计数）</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span>... <span class="title">Args</span>&gt;</span></span><br><span class="line"><span class="class">  <span class="title">static</span> <span class="title">intrusive_ptr</span> <span class="title">make</span>(<span class="title">Args</span>&amp;&amp;... <span class="title">args</span>) &#123;</span></span><br><span class="line">    <span class="keyword">auto</span> result = intrusive_ptr(<span class="keyword">new</span> TTarget(<span class="built_in">std</span>::forward&lt;Args&gt;(args)...));</span><br><span class="line">    ++result.target_-&gt;refcount_;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 把智能指针转换为普通指针</span></span><br><span class="line">  <span class="function">TTarget* <span class="title">release</span><span class="params">()</span> <span class="keyword">noexcept</span> </span>&#123;</span><br><span class="line">    TTarget* result = target_;</span><br><span class="line">    target_ = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 把普通指针转换为智能指针（不增加引用计数）</span></span><br><span class="line">  <span class="function"><span class="keyword">static</span> intrusive_ptr <span class="title">reclaim</span><span class="params">(TTarget* owning_ptr)</span> </span>&#123;</span><br><span class="line">    AT_ASSERTM(</span><br><span class="line">        owning_ptr == <span class="literal">nullptr</span> || owning_ptr-&gt;refcount_.load() &gt; <span class="number">0</span>,</span><br><span class="line">        <span class="string">"intrusive_ptr: Can only intrusive_ptr::reclaim() owning pointers that were created using intrusive_ptr::release()."</span>);</span><br><span class="line">    <span class="keyword">return</span> intrusive_ptr(owning_ptr);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 用于构造智能指针</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">TTarget</span>, <span class="title">class</span>... <span class="title">Args</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">inline</span> <span class="title">intrusive_ptr</span>&lt;TTarget&gt; <span class="title">make_intrusive</span>(<span class="title">Args</span>&amp;&amp;... <span class="title">args</span>) &#123;</span></span><br><span class="line">  <span class="keyword">return</span> intrusive_ptr&lt;TTarget&gt;::make(<span class="built_in">std</span>::forward&lt;Args&gt;(args)...);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>intrusive_ptr_target</code> 是被引用对象的父类，<code>TensorImpl</code> 和 <code>StorageImpl</code> 都继承自它，主要工作是声明了一个引用计数 <code>refcount_</code>，并且把智能指针类声明为友元类，这样在 <code>intrusive_ptr</code>里面就可以直接操作 <code>refcount_</code>了。</p><p>再来看<code>intrusive_ptr</code>的实现，它有一个私有成员<code>TTarget* target_</code>，是被引用对象的普通指针；还有两个私有方法<code>retain_</code>和<code>reset_</code>，分别用来增加和减少引用计数。需要注意的是：通过普通指针初始化<code>intrusive_ptr</code>的构造函数也是私有的，不能被外部调用，只能通过静态函数<code>make</code>来调用（详见下文）；在通过其他智能指针初始化的时候需要增加引用计数。</p><p>最后还有两个方法<code>release</code>和<code>reclaim</code>，它们实现了普通指针和智能指针间的相互转换，用于C风格的API中（在Aten中经常用到）。除此之外，<code>intrusive_ptr</code>里还重载了许多运算符，让它可以像普通指针一样使用，还有许多其他方法就不一一介绍了。</p><p><strong>创建Tensor</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">THTensor *<span class="title">THTensor_</span><span class="params">(<span class="keyword">new</span>)</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> c10::make_intrusive&lt;at::TensorImpl&gt;(</span><br><span class="line">    <span class="comment">// 下面三个参数将通过intrusive_ptr::make传给TensorImpl的构造函数，然后</span></span><br><span class="line">    <span class="comment">// 通过TensorImpl的指针构造intrusive_ptr</span></span><br><span class="line">    c10::intrusive_ptr&lt;at::StorageImpl&gt;::reclaim(THStorage_(<span class="keyword">new</span>)()),<span class="comment">// Storage&amp;&amp; storage</span></span><br><span class="line">    at::CPUTensorId(),<span class="comment">// TensorTypeId type_id</span></span><br><span class="line">    <span class="literal">false</span><span class="comment">// bool is_variable</span></span><br><span class="line">  ).release();<span class="comment">// release返回普通指针，TensorImpl*</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">THStorage* <span class="title">THStorage_</span><span class="params">(<span class="keyword">new</span>)</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> THStorage_new(caffe2::TypeMeta::Make&lt;<span class="keyword">scalar_t</span>&gt;());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">THStorage* <span class="title">THStorage_new</span><span class="params">(caffe2::TypeMeta data_type)</span> </span>&#123;</span><br><span class="line">  THStorage* storage = c10::make_intrusive&lt;at::StorageImpl&gt;(</span><br><span class="line">    <span class="comment">// 同理下面四个参数将通过intrusive_ptr::make传给StorageImpl的构造函</span></span><br><span class="line">      <span class="comment">// 数，然后通过StorageImpl的指针构造intrusive_ptr</span></span><br><span class="line">      data_type,</span><br><span class="line">      <span class="number">0</span>,</span><br><span class="line">      getTHDefaultAllocator(),</span><br><span class="line">      <span class="literal">true</span></span><br><span class="line">  ).release();</span><br><span class="line">  <span class="keyword">return</span> storage;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面是新建一个tensor的过程，通过<code>c10::make_instrusive</code>构造智能指针，然后调用<code>release</code>返回普通指针。传入的三个参数：storage智能指针，tensortype，is_variable会被转发到<code>intrusive_ptr::make</code>函数中，然后用这三个参数构造一个<code>TensorImpl</code>对象：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TensorImpl(Storage&amp;&amp; storage, TensorTypeId type_id, <span class="keyword">bool</span> is_variable);</span><br></pre></td></tr></table></figure><p>再用该对象的指针初始化智能指针：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">intrusive_ptr(TTarget* target);</span><br></pre></td></tr></table></figure><p>同时增加引用计数，最后<code>make_intrusive</code>返回智能指针。THStorage的构造过程同理。</p><h2><span id="tensor-apply-amp-dynamic-dispatch">Tensor Apply &amp; Dynamic Dispatch</span></h2><p>TH中的Tensor Apply就相当于map函数，把一个函数应用到tensor的每个数据中。举个例子来看一下<code>THTensor_(cadd)</code>的具体实现（简化过的）：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THTensor_</span><span class="params">(cadd)</span><span class="params">(THTensor *r_, THTensor *t, <span class="keyword">scalar_t</span> value, THTensor *src)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  THTensor_(resizeAs)(r_, t);</span><br><span class="line">  <span class="keyword">int64_t</span> r_Size = THTensor_(nElement)(r_);</span><br><span class="line">  <span class="keyword">int64_t</span> srcSize = THTensor_(nElement)(src);</span><br><span class="line">  <span class="keyword">int</span> r_Contig = THTensor_(isContiguous)(r_);</span><br><span class="line">  <span class="keyword">int</span> tContig = THTensor_(isContiguous)(t);</span><br><span class="line">  <span class="keyword">int</span> srcContig = THTensor_(isContiguous)(src);</span><br><span class="line">  <span class="keyword">int</span> serial_path = <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (srcSize == r_Size)&#123;</span><br><span class="line">    <span class="keyword">if</span> (r_Contig &amp;&amp; tContig &amp;&amp; srcContig) &#123;</span><br><span class="line">  TH_TENSOR_APPLY3_CONTIG(<span class="keyword">scalar_t</span>, r_, <span class="keyword">scalar_t</span>, t, <span class="keyword">scalar_t</span>, src, THVector_(cadd)(r__data, t_data, src_data, value, r__len););</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      serial_path = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    serial_path = <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (serial_path) &#123;</span><br><span class="line">    TH_TENSOR_APPLY3(</span><br><span class="line">      <span class="keyword">scalar_t</span>, r_, <span class="keyword">scalar_t</span>, t, <span class="keyword">scalar_t</span>, src, </span><br><span class="line">      *r__data = *t_data + value * *src_data;</span><br><span class="line">    );</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个函数实现的功能很简单，就是 <code>*r_ = *t + value * *src</code>，把<code>src</code>的值乘以<code>value</code>然后和<code>t</code>相加，最后赋值给<code>r_</code>，其中<code>r_, t, src</code>都是THTensor。函数首先获取元素个数和是否连续等信息，如果连续的话调用<code>TH_TENSOR_APPLY3_CONTIG</code>来处理，否则调用<code>TH_TENSOR_APPLY3</code>处理。后者太复杂了，我们主要看前者的实现：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> _OPENMP</span></span><br><span class="line"><span class="comment">// 多线程加速</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> TH_TENSOR_APPLY3_CONTIG(TYPE1, TENSOR1, TYPE2, TENSOR2, TYPE3, TENSOR3, CODE) \ </span></span><br><span class="line">&#123; \ </span><br><span class="line">  <span class="keyword">int</span> inOmp = omp_in_parallel(); \</span><br><span class="line">  <span class="keyword">ptrdiff_t</span> TH_TENSOR_size = THTensor_(nElement)(TENSOR1); \ </span><br><span class="line">  # 启动 OpenMP 多线程</span><br><span class="line">  PRAGMA(omp parallel <span class="keyword">if</span> ((TH_TENSOR_size &gt; TH_OMP_OVERHEAD_THRESHOLD) &amp;&amp; (!inOmp))) \</span><br><span class="line">  &#123; \</span><br><span class="line">    <span class="keyword">size_t</span> num_threads = omp_get_num_threads(); \</span><br><span class="line">    <span class="comment">// 获取线程数</span></span><br><span class="line">    <span class="keyword">size_t</span> tid = omp_get_thread_num(); \</span><br><span class="line">    <span class="comment">// 计算开始和结尾</span></span><br><span class="line">    <span class="keyword">ptrdiff_t</span> TH_TENSOR_offset = tid*(TH_TENSOR_size/num_threads);\</span><br><span class="line">    <span class="keyword">ptrdiff_t</span> TH_TENSOR_end =(tid==num_threads<span class="number">-1</span>)?TH_TENSOR_size: \</span><br><span class="line">      TH_TENSOR_offset + TH_TENSOR_size / num_threads; \</span><br><span class="line">    <span class="comment">// 每个线程负责 TH_TENSOR_offset 到 TH_TENSOR_end 之间的数据</span></span><br><span class="line">    <span class="keyword">ptrdiff_t</span> TENSOR1##_len = TH_TENSOR_end - TH_TENSOR_offset; \</span><br><span class="line">    <span class="comment">// 获取Tensor的数据</span></span><br><span class="line">    TYPE1 *TENSOR1##_data = TENSOR1-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;() + TH_TENSOR_offset; \</span><br><span class="line">    TYPE2 *TENSOR2##_data = TENSOR2-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;() + TH_TENSOR_offset; \</span><br><span class="line">    TYPE3 *TENSOR3##_data = TENSOR3-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;() + TH_TENSOR_offset; \</span><br><span class="line">    CODE \</span><br><span class="line">  &#125; \</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line"><span class="comment">// 普通实现</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> TH_TENSOR_APPLY3_CONTIG(TYPE1, TENSOR1, TYPE2, TENSOR2, TYPE3, TENSOR3, CODE) \ </span></span><br><span class="line">&#123; \</span><br><span class="line">  <span class="comment">// 获取Tensor的数据</span></span><br><span class="line">  TYPE1 *TENSOR1##_data = TENSOR1-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;(); \</span><br><span class="line">  TYPE2 *TENSOR2##_data = TENSOR2-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;(); \</span><br><span class="line">  TYPE3 *TENSOR3##_data = TENSOR3-&gt;data&lt;<span class="keyword">scalar_t</span>&gt;(); \</span><br><span class="line">  <span class="keyword">ptrdiff_t</span> TENSOR1##_len = THTensor_(nElement)(TENSOR1); \</span><br><span class="line">  <span class="comment">// 执行传入的代码</span></span><br><span class="line">  CODE \</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure><p>上半部分的实现为OPENMP的多线程加速版，下面是普通版。这个宏接收7个参数：三个Tensor和对应类型，还有要执行的操作。<code>THTensor_(cadd)</code>中传入的操作是<code>THVector_(cadd)(r__data, t_data, src_data, value, r__len);</code>，它的定义为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// THVectorDispatch.cpp</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THVector_</span><span class="params">(cadd)</span><span class="params">(<span class="keyword">scalar_t</span> *z, <span class="keyword">const</span> <span class="keyword">scalar_t</span> *x, <span class="keyword">const</span> <span class="keyword">scalar_t</span> *y, <span class="keyword">const</span> <span class="keyword">scalar_t</span> c, <span class="keyword">const</span> <span class="keyword">ptrdiff_t</span> n)</span> </span>&#123;</span><br><span class="line">  THVector_(cadd_DISPATCHPTR)(z, x, y, c, n);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 动态派发函数指针</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="title">void</span> <span class="params">(*THVector_(cadd_DISPATCHPTR))</span><span class="params">(<span class="keyword">scalar_t</span> *, <span class="keyword">const</span> <span class="keyword">scalar_t</span> *, <span class="keyword">const</span> <span class="keyword">scalar_t</span> *, <span class="keyword">const</span> <span class="keyword">scalar_t</span>, <span class="keyword">const</span> <span class="keyword">ptrdiff_t</span>)</span> </span>= &amp;THVector_(cadd_DEFAULT);</span><br><span class="line"><span class="comment">// 对各种向量化指令的支持</span></span><br><span class="line">static FunctionDescription THVector_(cadd_DISPATCHTABLE)[] = &#123;</span><br><span class="line">  <span class="meta">#<span class="meta-keyword">if</span> defined(__NEON__)</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">if</span> defined(TH_REAL_IS_FLOAT)</span></span><br><span class="line">      FUNCTION_IMPL(THVector_(cadd_NEON), SIMDExtension_NEON),</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">if</span> defined(USE_AVX2)</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">if</span> defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT)</span></span><br><span class="line">      FUNCTION_IMPL(THVector_(cadd_AVX2), SIMDExtension_AVX2),</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">if</span> defined(USE_AVX)</span></span><br><span class="line">    <span class="meta">#<span class="meta-keyword">if</span> defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT)</span></span><br><span class="line">      FUNCTION_IMPL(THVector_(cadd_AVX), SIMDExtension_AVX),</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">  FUNCTION_IMPL(THVector_(cadd_DEFAULT), SIMDExtension_DEFAULT)</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// THVectorDefault.cpp</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">THVector_</span><span class="params">(cadd_DEFAULT)</span><span class="params">(<span class="keyword">scalar_t</span> *z, <span class="keyword">const</span> <span class="keyword">scalar_t</span> *x, <span class="keyword">const</span> <span class="keyword">scalar_t</span> *y, <span class="keyword">const</span> <span class="keyword">scalar_t</span> c, <span class="keyword">const</span> <span class="keyword">ptrdiff_t</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">ptrdiff_t</span> i = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span>(; i&lt;n<span class="number">-4</span>; i+=<span class="number">4</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    z[i] = x[i] + c * y[i];</span><br><span class="line">    z[i+<span class="number">1</span>] = x[i+<span class="number">1</span>] + c * y[i+<span class="number">1</span>];</span><br><span class="line">    z[i+<span class="number">2</span>] = x[i+<span class="number">2</span>] + c * y[i+<span class="number">2</span>];</span><br><span class="line">    z[i+<span class="number">3</span>] = x[i+<span class="number">3</span>] + c * y[i+<span class="number">3</span>];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span>(; i&lt;n; i++)</span><br><span class="line">    z[i] = x[i] + c * y[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里涉及到对SIMD向量化指令的支持和动态派发，可以看到<code>THVector_(cadd)</code>函数里调用的是<code>THVector_(cadd_DISPATCHPTR)</code>，而它是一个函数指针，默认指向<code>THVector_(cadd_DEFAULT)</code>，这个是不支持向量化指令的实现。代码中间部分的数组<code>FunctionDescription THVector_(cadd_DISPATCHTABLE)[]</code>记录各种向量化指令的实现，最后是默认的实现。</p><p>动态派发的实现在<code>TH/vector/simd.h</code>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INIT_DISPATCH_PTR(OP)                                     \ </span></span><br><span class="line">  <span class="keyword">do</span> &#123;                                                            \ </span><br><span class="line">    <span class="keyword">size_t</span> i;                                                     \ </span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; <span class="keyword">sizeof</span>(THVector_(OP ## _DISPATCHTABLE)) /     \ </span><br><span class="line">                         <span class="keyword">sizeof</span>(FunctionDescription); ++i) &#123;      \</span><br><span class="line">      THVector_(OP ## _DISPATCHPTR) =                             \</span><br><span class="line">  <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">decltype</span>(THVector_(OP ## _DISPATCHPTR))&gt; \</span><br><span class="line">  (THVector_(OP ## _DISPATCHTABLE)[i].function);            \</span><br><span class="line">      <span class="keyword">if</span> (THVector_(OP ## _DISPATCHTABLE)[i].supportedSimdExt     \</span><br><span class="line">        &amp; hostSimdExts) &#123;                                       \</span><br><span class="line">        <span class="keyword">break</span>;                                                    \</span><br><span class="line">      &#125;                                                           \</span><br><span class="line">    &#125;                                                             \</span><br><span class="line">  &#125; <span class="keyword">while</span>(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">FunctionDescription</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="keyword">void</span> *function;</span><br><span class="line">  <span class="keyword">uint32_t</span> supportedSimdExt;</span><br><span class="line">&#125; FunctionDescription;</span><br></pre></td></tr></table></figure><p><code>INIT_DISPATCH_PTR</code> 这个宏做的事就是遍历<code>THVector_(cadd_DISPATCHTABLE)</code>数组，循环内把动态派发指针（<code>THVector_(cadd_DISPATCHPTR)</code>）赋给当前数组元素所对应的函数指针，最后判断一下当前架构是否支持该指令集，如果支持就退出循环；如果向量化指令集都不支持的话，最后还会指向默认实现的函数指针。</p><p>つづく</p><table><thead><tr><th style="text-align:left">上一篇：<a href="https://www.52coding.com.cn/2019/05/05/PyTorch0/">简介</a></th><th style="text-align:right">下一篇：<a href="https://www.52coding.com.cn/2019/05/05/PyTorch2/">THC</a></th></tr></thead><tbody><tr><td style="text-align:left"></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PyTorch中Tensor的存储和表示分开，多个THTensor可能共享一个THStorage，每个THTensor可能拥有不同的view（e.g. size, stride）。这样设计的好处是，有时看起来不一样的数据底层是共享的，比如矩阵与矩阵的转置、二维矩阵与二维矩阵变成一维时的矩阵。这部分的主力实现在 &lt;code&gt;pytorch/aten&lt;/code&gt; 文件夹中，这里面既实现了底层的Tensor操作库，也封装了名为 &lt;strong&gt;ATen&lt;/strong&gt; 的 C++11接口。&lt;/p&gt;
    
    </summary>
    
      <category term="博客" scheme="http://www.52coding.com.cn/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="PyTorch" scheme="http://www.52coding.com.cn/tags/PyTorch/"/>
    
      <category term="Tensor" scheme="http://www.52coding.com.cn/tags/Tensor/"/>
    
      <category term="THTensor" scheme="http://www.52coding.com.cn/tags/THTensor/"/>
    
      <category term="THStorage" scheme="http://www.52coding.com.cn/tags/THStorage/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch源码浅析：简介</title>
    <link href="http://www.52coding.com.cn/2019/05/05/PyTorch0/"/>
    <id>http://www.52coding.com.cn/2019/05/05/PyTorch0/</id>
    <published>2019-05-05T07:49:01.000Z</published>
    <updated>2019-05-05T08:09:12.625Z</updated>
    
    <content type="html"><![CDATA[<p>这个系列文章自底向上针对PyTorch核心源码进行浅析，从Tensor库<span class="math inline">\(\rightarrow​\)</span>神经网络<span class="math inline">\(\rightarrow​\)</span>自动微分<span class="math inline">\(\rightarrow​\)</span>Python扩展，一共五篇。</p><a id="more"></a><h2><span id="目录">目录</span></h2><blockquote><h3><span id="1-thtensor"></span></h3><p>PyTorch中Tensor的存储和表示分开，多个THTensor可能共享一个THStorage，每个THTensor可能拥有不同的view（e.g. size, stride）。这样设计的好处是，有时看起来不一样的数据底层是共享的，比如矩阵与矩阵的转置、二维矩阵与二维矩阵变成一维时的矩阵。这部分的主力实现在 <code>pytorch/aten</code> 文件夹中，这里面既实现了底层的Tensor操作库，也封装了名为 <strong>ATen</strong> 的 C++11接口。</p></blockquote><blockquote><h3><span id="2-thc"></span></h3><p>这篇主要看 Torch CUDA 部分，对应源码目录 <code>aten/src/THC</code>，里面包含了许多C++和CUDA代码。这部分实现了操作 THCTensor 和 THCStorage 的接口，不过底层用的数据结构还是 <code>TensorImpl</code> 和 <code>StorageImpl</code>。THC里的接口也是通过C语言范式实现的，但是Apply系列操作不再由宏来实现，而是使用了C++模板。其他的区别还有allocator不同，以及多了 THCState 结构。</p></blockquote><blockquote><h3><span id="3-nn"></span></h3><p>THNN是一个用C语言实现的神经网络模块的库，提供的功能非常底层。它实现了许多基础的神经网络模块，包括线性层，卷积层，Sigmoid等各种激活层，一些基本的loss函数，这些API都声明在<code>THNN/generic/THNN.h</code>中。每个模块都实现了前向传导（forward）和后向传导（backward）的功能。THCUNN则是对应模块的CUDA实现。</p></blockquote><blockquote><h3><span id="4-autograd"></span></h3><p>这篇博客介绍 PyTorch 中自动微分引擎的实现，主要分为三部分：首先简要介绍一下计算图的原理；然后介绍 PyTorch 中与 autograd 的相关数据结构和 <code>backward()</code> 函数的实现，数据结构包括 <code>torch::autograd::Variable</code>, <code>torch::autograd::Function</code> 等；最后讲一下动态建立计算图的实现，这部分代码涉及到动态派发机制，而且都是用脚本生成的，不太容易理解。</p></blockquote><blockquote><h3><span id="5-python扩展"></span></h3><p>这篇是本系列最后一篇博客了，介绍一下前面的C++代码怎么与Python交互，或者说Python里怎么调用C++代码进行高效的计算。首先简单介绍一下预备知识，既Python的C扩展通常怎么写；然后以比较核心的数据结构 Tensor 和 Storage 为例看一下它们怎么转换为Python类型的；最后稍带点儿Python自微分函数的实现。</p></blockquote><h2><span id="源码目录结构">源码目录结构</span></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">pytorch</span><br><span class="line">├── aten<span class="comment"># ATen: C++ Tensor库</span></span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   ├── conda</span><br><span class="line">│   ├── src</span><br><span class="line">│   │   ├── ATen<span class="comment"># C++ bindings</span></span><br><span class="line">│   │   ├── README.md</span><br><span class="line">│   │   ├── TH              <span class="comment"># torch tensor</span></span><br><span class="line">│   │   ├── THC    <span class="comment"># torch cuda</span></span><br><span class="line">│   │   ├── THCUNN <span class="comment"># torch cuda nn</span></span><br><span class="line">│   │   └── THNN<span class="comment"># torch nn</span></span><br><span class="line">│   └── tools</span><br><span class="line">├── c10 <span class="comment"># 这里面也包含一些Tensor实现</span></span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   ├── core</span><br><span class="line">│   ├── cuda</span><br><span class="line">│   ├── hip</span><br><span class="line">│   ├── macros</span><br><span class="line">│   ├── <span class="built_in">test</span></span><br><span class="line">│   └── util</span><br><span class="line">├── caffe2<span class="comment"># caffe2</span></span><br><span class="line">├── tools</span><br><span class="line">│   ├── autograd<span class="comment"># 生成自微分相关函数的工具</span></span><br><span class="line">│   ├── ...</span><br><span class="line">│   └── shared</span><br><span class="line">├── torch    <span class="comment"># Python模块</span></span><br><span class="line">│   ├── autograd</span><br><span class="line">│   ├── csrc<span class="comment"># C++相关源码</span></span><br><span class="line">│   │   ├── autograd<span class="comment"># 自动微分引擎实现</span></span><br><span class="line">│   │   ├── cuda</span><br><span class="line">│   │   ├── distributed</span><br><span class="line">│   │   ├── generic</span><br><span class="line">│   │   ├── jit</span><br><span class="line">│   │   ├── multiprocessing</span><br><span class="line">│   │   ├── nn</span><br><span class="line">│   │   ├── tensor</span><br><span class="line">│   │   ├── utils</span><br><span class="line">│   │   ├── ...</span><br><span class="line">│   │   └── utils.h</span><br><span class="line">│   ├── cuda</span><br><span class="line">│   ├── nn</span><br><span class="line">│   ├── ...</span><br><span class="line">│   ├── storage.py</span><br><span class="line">│   └── tensor.py</span><br><span class="line">├── ...</span><br><span class="line">└── ubsan.supp</span><br></pre></td></tr></table></figure><h2><span id="代码统计">代码统计</span></h2><table><thead><tr class="header"><th style="text-align: center;">语言</th><th style="text-align: center;">文件数</th><th style="text-align: center;">空行</th><th style="text-align: center;">注释</th><th style="text-align: center;">代码</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">C++</td><td style="text-align: center;">464</td><td style="text-align: center;">25230</td><td style="text-align: center;">9855</td><td style="text-align: center;"><strong>330222</strong></td></tr><tr class="even"><td style="text-align: center;">C/C++ Header</td><td style="text-align: center;">1692</td><td style="text-align: center;">40380</td><td style="text-align: center;">51884</td><td style="text-align: center;"><strong>251915</strong></td></tr><tr class="odd"><td style="text-align: center;">Python</td><td style="text-align: center;">776</td><td style="text-align: center;">27277</td><td style="text-align: center;">31616</td><td style="text-align: center;"><strong>117105</strong></td></tr><tr class="even"><td style="text-align: center;">CUDA</td><td style="text-align: center;">307</td><td style="text-align: center;">7126</td><td style="text-align: center;">4140</td><td style="text-align: center;">40207</td></tr><tr class="odd"><td style="text-align: center;">...</td><td style="text-align: center;">...</td><td style="text-align: center;">...</td><td style="text-align: center;">...</td><td style="text-align: center;">...</td></tr><tr class="even"><td style="text-align: center;">总计</td><td style="text-align: center;">3382</td><td style="text-align: center;">104221</td><td style="text-align: center;">101689</td><td style="text-align: center;"><strong>825756</strong></td></tr></tbody></table><p><strong>注</strong>：仅统计了 <code>torch</code> 和 <code>aten</code> 两个核心文件夹。</p><h2><span id="感受">感受</span></h2><p>一开始只是心血来潮觉得这学期反正不是很忙就立了个flag决定学期内把PyTorch源码看一遍，看的过程很受苦，庆幸最终还是坚持下来了，收获也很大。除了理解了PyTorch是如何运行的、输出这五篇博客之外，我对C++的理解也有显著提升，因为PyTorch大部分代码是用C++写的，各种新特性简直刷新了我对这门语言的认识，由此也专门记了一篇<a href="https://www.52coding.com.cn/2019/05/05/C++/">关于C++的笔记</a>。</p><p>简单说一下我的阅读方法。面对这么多的代码和文件，一下子肯定不知所措，尤其是阅读新模块的时候，我首先会尝试找到该模块的说明，通过 <code>README.md</code> 或前人的博客或API文档了解下该模块大概功能和结构，然后整体（粗略）浏览一遍该模块的代码，对每个文件里的代码是做什么的有个大致概念，最后再根据自己的理解选择性地进行精读。</p><p>つづく</p><table><thead><tr class="header"><th style="text-align: left;"></th><th style="text-align: right;">下一篇：<a href="https://www.52coding.com.cn/2019/05/05/PyTorch1/">THTensor</a></th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"></td><td style="text-align: right;"></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这个系列文章自底向上针对PyTorch核心源码进行浅析，从Tensor库&lt;span class=&quot;math inline&quot;&gt;\(\rightarrow​\)&lt;/span&gt;神经网络&lt;span class=&quot;math inline&quot;&gt;\(\rightarrow​\)&lt;/span&gt;自动微分&lt;span class=&quot;math inline&quot;&gt;\(\rightarrow​\)&lt;/span&gt;Python扩展，一共五篇。&lt;/p&gt;
    
    </summary>
    
      <category term="博客" scheme="http://www.52coding.com.cn/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="PyTorch" scheme="http://www.52coding.com.cn/tags/PyTorch/"/>
    
      <category term="Tensor" scheme="http://www.52coding.com.cn/tags/Tensor/"/>
    
      <category term="NN" scheme="http://www.52coding.com.cn/tags/NN/"/>
    
      <category term="Autograd" scheme="http://www.52coding.com.cn/tags/Autograd/"/>
    
  </entry>
  
  <entry>
    <title>C++笔记</title>
    <link href="http://www.52coding.com.cn/2019/05/05/C++/"/>
    <id>http://www.52coding.com.cn/2019/05/05/C++/</id>
    <published>2019-05-05T07:40:01.000Z</published>
    <updated>2019-05-05T07:43:09.037Z</updated>
    
    <content type="html"><![CDATA[<p>C++这门语言真的是博大精深，在阅读PyTorch源码的时候感触尤为深刻，在此记录一些我认为很好的编程实践以及一些C++的“新”特性。</p><a id="more"></a><h3><span id="条件检查">条件检查</span></h3><p>在程序需要判断一些条件才能执行某些操作的时候，我们通常会写如下代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (cond == <span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="comment">// error, or throw exception</span></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">do</span> something ...</span><br></pre></td></tr></table></figure><p>这些<code>if</code>语句可以简洁为一行代码，如使用<code>assert</code>语句，但如果你不希望程序就此终止的话，也可以自定义相应的处理函数或宏，如果需要检查的条件很多的话，这样进行替换就会使代码整洁许多。下面是在<code>pytorch/c10</code>中的条件判断处理宏（仅供参考）：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> AT_ERROR(...) \</span></span><br><span class="line">  <span class="keyword">throw</span> ::c10::Error(&#123;__func__, __FILE__, <span class="keyword">static_cast</span>&lt;<span class="keyword">uint32_t</span>&gt;(__LINE__)&#125;, ::c10::str(__VA_ARGS__))</span><br><span class="line"></span><br><span class="line">#define AT_CHECK(cond, ...)            \</span><br><span class="line">  <span class="keyword">if</span> (!(cond)) &#123;                       \</span><br><span class="line">    AT_ERROR(::c10::str(__VA_ARGS__)); \</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3><span id="特性测试">特性测试</span></h3><p>宏定义的运用在C/C++中至关重要，宏可以用来解决一些兼容性的问题，并且使代码保持统一。如<code>constexpr</code>关键字在不同版本的C++中范围不同，那么已知一个函数可以在C++14及以上的版本中声明<code>constexpr</code>但在之前的版本中不行，该怎么办呢？一个既能兼容不同编译器又能保持代码整洁统一的方法就是使用宏定义，如：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">AT_CPP14_CONSTEXPR <span class="keyword">const</span> T&amp; <span class="title">front</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    AT_CHECK(!empty(), <span class="string">"ArrayRef: attempted to access front() of empty list"</span>);</span><br><span class="line">    <span class="keyword">return</span> Data[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>AT_CPP14_CONSTEXPR</code>的定义为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">if</span> defined(__cpp_constexpr) &amp;&amp; __cpp_constexpr &gt;= 201304</span></span><br><span class="line"><span class="meta">#  <span class="meta-keyword">define</span> AT_CPP14_CONSTEXPR constexpr</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line"><span class="meta">#  <span class="meta-keyword">define</span> AT_CPP14_CONSTEXPR</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure><p>通过<a href="https://zh.cppreference.com/w/cpp/feature_test" target="_blank" rel="noopener">特性测试</a>检查特性的指定版本来判断能否使用该关键字，如果版本大于我们的要求，则把宏定义为该关键字，否则定义为空。</p><h3><span id="使用-nullptr-而不是-null">使用 nullptr 而不是 NULL</span></h3><p>在C++中，宏<code>NULL</code>只是字面量0，而<code>nullptr</code>才是字面量空指针。</p><h3><span id="函数不要太长">函数不要太长</span></h3><p>在PyTorch中，函数都很短，一般不超过30行，尽量把函数变得精简。</p><h3><span id="ampamp">&amp;&amp;</span></h3><p>当 <code>&amp;&amp;</code> 出现在参数列表中或函数返回值处时表示<strong>右值引用</strong>(RValue-Reference)。所谓右值引用就是对右值的引用，而右值就是只能出现在等号右边的值，也可以理解为没有内存地址的值。如数字字面量，<code>a+1</code>(<code>int a;</code>) 等都是右值。下面是一个<code>&amp;&amp;</code>用法的例子：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">(<span class="keyword">int</span>&amp;&amp; a)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//Some magical code...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> b;</span><br><span class="line">    foo(b); <span class="comment">//Error. An rValue reference cannot be pointed to a lValue.</span></span><br><span class="line">    foo(<span class="number">5</span>); <span class="comment">//Compiles with no error.</span></span><br><span class="line">    foo(b+<span class="number">3</span>); <span class="comment">//Compiles with no error.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span>&amp;&amp; c = b; <span class="comment">//Error. An rValue reference cannot be pointed to a lValue.</span></span><br><span class="line">    <span class="keyword">int</span>&amp;&amp; d = <span class="number">5</span>; <span class="comment">//Compiles with no error.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3><span id="default-和-delete">=default 和 = delete</span></h3><p>详见：https://www.ibm.com/developerworks/cn/aix/library/1212_lufang_c11new/index.html</p><p>简单来说，构造函数、析构函数等特殊函数可以用 <code>=default</code> 来让编译器为其提供默认构造函数或默认析构函数。而 <code>=delete</code> 可以禁用某个函数，如：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">X</span>&#123;</span>            </span><br><span class="line">  <span class="keyword">public</span>: </span><br><span class="line">X(); </span><br><span class="line">X(<span class="keyword">const</span> X&amp;) = <span class="keyword">delete</span>;  <span class="comment">// 声明拷贝构造函数为 deleted 函数</span></span><br><span class="line">  <span class="comment">// 声明拷贝赋值操作符为 deleted 函数</span></span><br><span class="line">X&amp; <span class="keyword">operator</span> = (<span class="keyword">const</span> X &amp;) = <span class="keyword">delete</span>; </span><br><span class="line">&#125;; </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123; </span><br><span class="line">X x1; </span><br><span class="line">  X x2=x1;   <span class="comment">// 错误，拷贝构造函数被禁用</span></span><br><span class="line">X x3; </span><br><span class="line">x3=x1;     <span class="comment">// 错误，拷贝赋值操作符被禁用</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3><span id="智能指针">智能指针</span></h3><h4><span id="unique_ptr">Unique_Ptr</span></h4><p>同时只能有一个智能指针对象指向某块内存。</p><p>特性：</p><ol type="1"><li><p>无法进行复制构造与赋值操作</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">unique_ptr</span>&lt;<span class="keyword">int</span>&gt; ap(<span class="keyword">new</span> <span class="keyword">int</span>(<span class="number">88</span> );</span><br><span class="line"><span class="built_in">unique_ptr</span>&lt;<span class="keyword">int</span>&gt; one (ap) ; <span class="comment">// error</span></span><br><span class="line"><span class="built_in">unique_ptr</span>&lt;<span class="keyword">int</span>&gt; two = one; <span class="comment">// error</span></span><br></pre></td></tr></table></figure></li><li><p>可以进行移动构造和移动赋值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">unique_ptr</span>&lt;<span class="keyword">int</span>&gt; GetVal() &#123;</span><br><span class="line"><span class="built_in">unique_ptr</span>&lt;<span class="keyword">int</span>&gt; up(<span class="keyword">new</span> <span class="keyword">int</span>(<span class="number">88</span>));</span><br><span class="line"><span class="keyword">return</span> up;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">unique_ptr</span>&lt;<span class="keyword">int</span>&gt; uPtr = GetVal();   <span class="comment">//ok</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">unique_ptr</span>&lt;<span class="keyword">int</span>&gt; up(<span class="keyword">new</span> <span class="keyword">int</span>(<span class="number">88</span>));</span><br><span class="line"><span class="built_in">unique_ptr</span>&lt;<span class="keyword">int</span>&gt; uPtr2 = <span class="built_in">std</span>:move(up); <span class="comment">// ok</span></span><br></pre></td></tr></table></figure></li></ol><h4><span id="shared_ptr">Shared_Ptr</span></h4><p><code>shared_ptr</code>是自带引用计数的智能指针，每一个`<code>shared_ptr</code>的拷贝都指向相同的内存。每使用他一次，内部的引用计数加1，每析构一次，内部的引用计数减1，减为0时，删除所指向的堆内存。<code>shared_ptr</code>内部的引用计数是安全的，但是对象的读取需要加锁，<a href="https://www.cnblogs.com/jiayayao/p/6128877.html" target="_blank" rel="noopener">阅读更多</a>。</p><h4><span id="weak_ptr">Weak_Ptr</span></h4><p><code>weak_ptr</code> 用来表达临时所有权的概念：当某个对象只有存在时才需要被访问，而且随时可能被他人删除时，可以使用 <code>weak_ptr</code> 来跟踪该对象。需要获得临时所有权时，则将其转换为 <code>shared_ptr</code>，此时如果原来的 <code>shared_ptr</code> 被销毁，则该对象的生命期将被延长至这个临时的 <code>shared_ptr</code> 同样被销毁为止。</p><p><code>weak_ptr</code>是为了配合<code>shared_ptr</code>而引入的一种智能指针，它指向一个由<code>shared_ptr</code>管理的对象而不影响所指对象的生命周期，也就是将一个<code>weak_ptr</code>绑定到一个<code>shared_ptr</code>不会改变<code>shared_ptr</code>的引用计数。不论是否有<code>weak_ptr</code>指向，一旦最后一个指向对象的<code>shared_ptr</code>被销毁，对象就会被释放。从这个角度看，<code>weak_ptr</code>更像是<code>shared_ptr</code>的一个助手而不是智能指针。C++中提供了<code>lock</code>函数来实现访问功能。如果对象存在，<code>lock()</code>函数返回一个指向共享对象的<code>shared_ptr</code>，否则返回一个空<code>shared_ptr</code>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    A() : a(<span class="number">3</span>) &#123; <span class="built_in">cout</span> &lt;&lt; <span class="string">"A Constructor..."</span> &lt;&lt; <span class="built_in">endl</span>; &#125;</span><br><span class="line">    ~A() &#123; <span class="built_in">cout</span> &lt;&lt; <span class="string">"A Destructor..."</span> &lt;&lt; <span class="built_in">endl</span>; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;A&gt; sp(<span class="keyword">new</span> A());</span><br><span class="line">    weak_ptr&lt;A&gt; wp(sp);</span><br><span class="line">    <span class="comment">//sp.reset();</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">shared_ptr</span>&lt;A&gt; pa = wp.lock())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; pa-&gt;a &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"wp指向对象为空"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/Xiejingfa/article/details/50772571" target="_blank" rel="noopener">阅读更多</a>。</p><h3><span id="变参模板">变参模板</span></h3><p>变参模板声明：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span>... <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">f</span>(<span class="title">T</span>... <span class="title">args</span>);</span></span><br></pre></td></tr></table></figure><p>省略号的作用有两个：</p><ol type="1"><li>声明一个参数包 <code>T… args</code>，这个参数包中可以包含0到任意个模板参数；</li><li>在模板定义的右边，可以将参数包展开成一个一个独立的参数。</li></ol><p><strong>基本用法</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span>... <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">f</span>(<span class="title">T</span>... <span class="title">args</span>)</span></span><br><span class="line"><span class="class">&#123;</span>    </span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="keyword">sizeof</span>...(args) &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//打印变参的个数</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">f();        <span class="comment">//0</span></span><br><span class="line">f(<span class="number">1</span>, <span class="number">2</span>);    <span class="comment">//2</span></span><br><span class="line">f(<span class="number">1</span>, <span class="number">2.5</span>, <span class="string">""</span>);    <span class="comment">//3</span></span><br></pre></td></tr></table></figure><p><strong>递归展开</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">T <span class="title">sum</span><span class="params">(T t)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> t;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> ... Types&gt;</span><br><span class="line"><span class="function">T <span class="title">sum</span> <span class="params">(T first, Types ... rest)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> first + sum&lt;T&gt;(rest...);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sum(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>); <span class="comment">//10</span></span><br></pre></td></tr></table></figure><p><strong>初始化列表+逗号表达式展开</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">printarg</span>(<span class="title">T</span> <span class="title">t</span>)</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; t &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> ...<span class="title">Args</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">expand</span>(<span class="title">Args</span>... <span class="title">args</span>)</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">   <span class="keyword">int</span> arr[] = &#123;(printarg(args), <span class="number">0</span>)...&#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">expand(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>);</span><br></pre></td></tr></table></figure><p>这个例子将分别打印出1,2,3,4四个数字。这种展开参数包的方式，不需要通过递归终止函数，是直接在<code>expand</code>函数体中展开的, <code>printarg</code>不是一个递归终止函数，只是一个处理参数包中每一个参数的函数。</p><p><code>expand</code>函数中的逗号表达式：<code>(printarg(args), 0)</code>，先执行<code>printarg(args)</code>，再得到逗号表达式的结果<code>0</code>。同时还用到了C++11的另外一个特性——初始化列表，通过初始化列表来初始化一个变长数组, <code>{(printarg(args), 0)…}</code>将会展开成<code>((printarg(arg1),0), (printarg(arg2),0), (printarg(arg3),0), ...)</code>，最终会创建一个元素值都为<code>0</code>的数组<code>int arr[sizeof...(Args)]</code>。由于是逗号表达式，在创建数组的过程中会先执行逗号表达式前面的部分<code>printarg(args)</code>打印出参数，也就是说在构造int数组的过程中就将参数包展开了，这个数组的目的纯粹是为了在数组构造的过程展开参数包。</p><p><strong>完美转发</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">F</span>, <span class="title">class</span>... <span class="title">Args</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">expand</span>(<span class="title">const</span> <span class="title">F</span>&amp; <span class="title">f</span>, <span class="title">Args</span>&amp;&amp;...<span class="title">args</span>) </span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="built_in">initializer_list</span>&lt;<span class="keyword">int</span>&gt;&#123;(f(<span class="built_in">std</span>::forward&lt;Args&gt;(args)), <span class="number">0</span>)...&#125;;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 范型Lambda表达式</span></span><br><span class="line">expand([](<span class="keyword">auto</span> i)&#123;<span class="built_in">cout</span> &lt;&lt; i &lt;&lt; <span class="built_in">endl</span>;&#125;, <span class="number">1</span>, <span class="number">2.0</span>, ”test”);</span><br></pre></td></tr></table></figure><p><code>std::forward</code>可以保持参数的左值或右值性质，所以叫完美转发。</p><h3><span id="函数返回值类型后置">函数返回值类型后置</span></h3><p>例子：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> ReadyQueue::push(FunctionTask item) -&gt; <span class="keyword">void</span> &#123;</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="built_in">std</span>::lock_guard&lt;<span class="built_in">std</span>::mutex&gt; lock(mutex);</span><br><span class="line">    ++item.base-&gt;outstanding_tasks;</span><br><span class="line">    heap.push(<span class="built_in">std</span>::move(item));</span><br><span class="line">  &#125;</span><br><span class="line">  not_empty.notify_one();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>解析：https://blog.csdn.net/fjb2080/article/details/7527349</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;C++这门语言真的是博大精深，在阅读PyTorch源码的时候感触尤为深刻，在此记录一些我认为很好的编程实践以及一些C++的“新”特性。&lt;/p&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://www.52coding.com.cn/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="C++" scheme="http://www.52coding.com.cn/tags/C/"/>
    
      <category term="template" scheme="http://www.52coding.com.cn/tags/template/"/>
    
      <category term="智能指针" scheme="http://www.52coding.com.cn/tags/%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/"/>
    
  </entry>
  
  <entry>
    <title>Microeconomics - The Costs of Production</title>
    <link href="http://www.52coding.com.cn/2019/04/10/The%20Costs%20of%20Production/"/>
    <id>http://www.52coding.com.cn/2019/04/10/The Costs of Production/</id>
    <published>2019-04-10T03:23:45.000Z</published>
    <updated>2019-04-12T07:10:35.318Z</updated>
    
    <content type="html"><![CDATA[<p>In this chapter, we examine firm behavior in more detail. This topic will give you a better understanding of the decisions behind the supply curve. In addition, it will introduce you to a part of economics called <em>industrial organization</em> — the study of how firms’ decisions about prices and quantities depend on the market conditions they face.</p><a id="more"></a><h2><span id="what-are-costs">WHAT ARE COSTS?</span></h2><p>We begin our discussion of costs at a Cookie factory. By examining some of the issues that the owner faces in his/her business, we can learn some lessons about costs that apply to all firms in an economy.</p><h3><span id="total-revenue-total-cost-and-profit">Total Revenue, Total Cost, and Profit</span></h3><p>The amount that the firm receives for the sale of its output is called its <strong>total revenue</strong>. The amount that the firm pays to buy inputs is called its <strong>total cost</strong>. <strong>Profit</strong> is a firm’s total revenue minus its total cost:</p><p><span class="math display">\[\text{Profit} = \text{Total revenue} - \text{Total cost}\]</span></p><p>To see how a firm goes about maximizing profit, we must consider fully how to measure its total revenue and its total cost. Total revenue is the easy part: It equals the <em>quantity</em> of output the firm produces <em>times</em> the <em>price</em> at which it sells its output. By contrast, the measurement of a firm’s total cost is more subtle.</p><h3><span id="costs-as-opportunity-costs">Costs As Opportunity Costs</span></h3><p><strong>explicit costs</strong>: input costs that require an outlay of money by the firm <strong>implicit costs</strong>: input costs that do not require an outlay of money by the firm</p><p>Imagine that the owner of the cookie factory is skilled with computers and could earn ¥100 per hour working as a programmer. For every hour that she works at her cookie factory, she gives up ¥100 in income, and this forgone income is also part of her costs. The total cost of her business is the sum of the explicit costs and the implicit costs.</p><h3><span id="the-cost-of-capital-as-an-opportunity-cost">The Cost of Capital as An Opportunity Cost</span></h3><p>An important implicit cost of almost every business is the opportunity cost of the financial capital that has been invested in the business. Suppose, for instance, that the owner of the cookie factory used ¥300,000 of her savings to buy her cookie factory from its previous owner. If she had instead left this money deposited in a savings account that pays an interest rate of 5 percent, she would have earned ¥15,000 per year. To own her cookie factory, therefore, Caroline has given up ¥15,000 a year in interest income. This forgone ¥15,000 is one of the <strong>implicit opportunity costs</strong> of Caroline’s business.</p><h3><span id="economic-profit-versus-accounting-profit">Economic Profit Versus Accounting Profit</span></h3><p>Now let’s return to the firm’s objective: <strong>profit</strong>. An economist measures a firm’s <strong>economic profit</strong> as the firm’s total revenue minus all the opportunity costs (explicit and implicit) of producing the goods and services sold. An accountant measures the firm’s <strong>accounting profit</strong> as the firm’s total revenue minus only the firm’s explicit costs as shown in Figure 1.</p><p><img src="/images/costandprod/DraggedImage.jpg"></p><p>Economic profit is an important concept because it is what motivates the firms that supply goods and services. As we will see, <em>a firm making positive economic profit will stay in business</em>. It is covering all its opportunity costs and has some revenue left to reward the firm owners. When a firm is making economic losses, the business owners are failing to earn enough revenue to cover all the costs of production. Unless conditions change, the firm owners will eventually close down the business and exit the industry. To understand business decisions, we need to keep an eye on economic profit.</p><p><strong>Economic profit equals to zero</strong> means your business is running well and you pay yourself the same amount as you get paid somewhere else.</p><h2><span id="production-and-costs">PRODUCTION AND COSTS</span></h2><p><img src="/images/costandprod/DraggedImage-1.jpg"></p><p>Table 1 shows how the quantity of cookies produced per hour at the cookie factory depends on the number of workers. This relationship between the <em>quantity of inputs</em> (workers) and <em>quantity of output</em> (cookies) is called the <strong>production function</strong>.</p><p><img src="/images/costandprod/DraggedImage-2.jpg"></p><p>To take a step toward understanding these decisions, the third column in the table gives the <strong>marginal product</strong> of a worker. Notice that as the number of workers increases, the marginal product declines. The second worker has a marginal product of 40 cookies, the third worker has a marginal product of 30 cookies, and the fourth worker has a marginal product of 20 cookies. This property is called <strong>diminishing marginal product</strong>.</p><p>Diminishing marginal product is also apparent in Figure 2. The production function’s slope tells us the change in the factory’s output of cookies for each additional input of labor. That is, the <strong>slope of the production function</strong> measures the <em>marginal product</em> of a worker. As the number of workers increases, the marginal product declines, and the production function becomes flatter.</p><h3><span id="from-the-production-function-to-the-total-cost-curve">From The Production Function To The Total-Cost Curve</span></h3><p>Our goal in the next is to study firms’ production and pricing decisions. For this purpose, the most important relationship in Table 1 is between quantity produced and total costs. Panel (b) of Figure 2 graphs these two columns of data with the quantity produced on the horizontal axis and total cost on the vertical axis. This graph is called the <strong>total-cost curve</strong>.</p><p>Now compare the total-cost curve in panel (b) with the production function in panel (a). These two curves are opposite sides of the same coin. The total-cost curve gets <em>steeper</em> as the amount produced rises, whereas the production function gets <em>flatter</em> as production rises. These changes in slope occur for the same reason. High production of cookies means that Caroline’s kitchen is crowded with many workers. Because the kitchen is crowded, each additional worker adds less to production, reflecting diminishing marginal product. Therefore, the production function is relatively flat. But now turn this logic around: When the kitchen is crowded, producing an additional cookie requires a lot of additional labor and is thus very costly. Therefore, when the quantity produced is large, the total-cost curve is relatively steep.</p><h2><span id="the-various-measures-of-cost">THE VARIOUS MEASURES OF COST</span></h2><p><img src="/images/costandprod/DraggedImage-3.jpg"></p><p>To see how related measures of cost are derived, we consider the example in Table 2. This table presents cost data on Conrad’s Coffee Shop. Figure 3 plots Conrad’s total-cost curve. Conrad’s total-cost curve becomes steeper as the quantity produced rises, which (as we have discussed) reflects <em>diminishing marginal product</em>.</p><p><img src="/images/costandprod/DraggedImage-4.jpg"></p><h3><span id="fixed-and-variable-costs">Fixed And Variable Costs</span></h3><p><strong>Fixed costs</strong> are costs that do not vary with the <em>quantity of output produced</em>. They are incurred even if the firm produces nothing at all. Conrad’s fixed costs include any rent he pays because this cost is the same regardless of how much coffee he produces.</p><p><strong>Variable costs</strong> are costs that change as the firm alters the quantity of output produced. Conrad’s variable costs include the cost of coffee beans, milk, sugar, and paper cups: The more cups of coffee Conrad makes, the more of these items he needs to buy.</p><p>A firm’s <strong>total cost</strong> is the sum of fixed and variable costs. In Table 2, total cost in the second column equals fixed cost in the third column plus variable cost in the fourth column.</p><h3><span id="average-and-marginal-cost">Average and Marginal Cost</span></h3><p>Total cost divided by the quantity of output is called <strong>average total cost</strong>. Because total cost is the sum of fixed and variable costs, average total cost can be expressed as the sum of average fixed cost and average variable cost. <strong>Average fixed cost</strong> is the fixed cost divided by the quantity of output, and <strong>average variable cost</strong> is the variable cost divided by the quantity of output.</p><p>Although average total cost tells us the cost of the typical unit, it does not tell us how much total cost will change as the firm alters its level of production. <strong>Marginal cost</strong> is the increase in total cost that arises from an extra unit of production. In the table, the marginal cost appears halfway between two rows because it represents the change in total cost as quantity of output increases from one level to another.</p><p>Mathematically:</p><p><span class="math display">\[ATC = TC/Q \\MC = \triangle TC / \triangle Q\]</span></p><h3><span id="cost-curves-and-their-shapes">Cost Curves And Their Shapes</span></h3><p><img src="/images/costandprod/DraggedImage-5.jpg"></p><p>Figure 4 graphs Conrad’s costs using the data from Table 2. The horizontal axis measures the quantity the firm produces, and the vertical axis measures marginal and average costs. The graph shows four curves: average total cost (ATC), average fixed cost (AFC), average variable cost (AVC), and marginal cost (MC).</p><p><strong>Rising Marginal Cost</strong> Conrad’s marginal cost rises with the quantity of out- put produced. This reflects the property of diminishing marginal product.</p><p><strong>U-Shaped Average Total Cost</strong> Average total cost is the sum of average fixed cost and average variable cost. Average fixed cost always declines as output rises because the fixed cost is spread over a larger number of units. Average variable cost typically rises as output increases because of diminishing marginal product. Average total cost reflects the shapes of both average fixed cost and average variable cost.</p><p>The bottom of the U-shape occurs at the quantity that minimizes average total cost. This <em>quantity</em> is sometimes called the <strong>efficient scale</strong> of the firm. At the efficient scale, these two forces are balanced to yield the lowest average total cost.</p><p><strong>The Relationship Between Marginal Cost and Average Total Cost</strong> * Whenever marginal cost is less than average total cost, average total cost is falling. * Whenever marginal cost is greater than average total cost, average total cost is rising.</p><p>To see why, consider an analogy. Average total cost is like your cumulative grade point average. Marginal cost is like the grade in the next course you will take. If your grade in your next course is less than your grade point average, your grade point average will fall. If your grade in your next course is higher than your grade point average, your grade point average will rise. The mathematics of average and marginal costs is exactly the same as the mathematics of average and marginal grades.</p><p>This relationship between average total cost and marginal cost has an important corollary: <strong>The marginal-cost curve crosses the average-total-cost curve at its minimum</strong>. As you will see in the next chapter, <strong>minimum average total cost</strong> plays a key role in the analysis of competitive firms.</p><h3><span id="typical-cost-curves">Typical Cost Curves</span></h3><p><img src="/images/costandprod/DraggedImage-6.jpg"></p><p>Actual firms are usually more complicated than what we talked about before. In many firms, marginal product does not start to fall immediately after the first worker is hired. Depending on the production process, the second or third worker might have a higher marginal product than the first because a team of workers can divide tasks and work more productively than a single worker. Firms exhibiting this pattern would experience increasing marginal product for a while before diminishing marginal product set in.</p><p>Figure 5 shows the cost curves for such a firm, including average total cost (ATC), average fixed cost (AFC), average variable cost (AVC), and marginal cost (MC). Despite these differences from our previous example, the cost curves shown here share the three properties that are most important to remember:</p><ul><li>Marginal cost eventually rises with the quantity of output.</li><li>The average-total-cost curve is U-shaped.</li><li>The marginal-cost curve crosses the average-total-cost curve at the <strong>minimum of average total cost</strong>.</li></ul><h2><span id="conclusion">CONCLUSION</span></h2><p><img src="/images/costandprod/DraggedImage-7.jpg"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this chapter, we examine firm behavior in more detail. This topic will give you a better understanding of the decisions behind the supply curve. In addition, it will introduce you to a part of economics called &lt;em&gt;industrial organization&lt;/em&gt; — the study of how firms’ decisions about prices and quantities depend on the market conditions they face.&lt;/p&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://www.52coding.com.cn/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="微观经济型原理" scheme="http://www.52coding.com.cn/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%9E%8B%E5%8E%9F%E7%90%86/"/>
    
      <category term="Marginal Cost" scheme="http://www.52coding.com.cn/tags/Marginal-Cost/"/>
    
      <category term="Profit" scheme="http://www.52coding.com.cn/tags/Profit/"/>
    
  </entry>
  
  <entry>
    <title>Microeconomics - Consumers, Producers, and the Efficiency of Markets</title>
    <link href="http://www.52coding.com.cn/2019/03/20/Consumers,%20Producers,%20and%20the%20Efficiency%20of%20Markets/"/>
    <id>http://www.52coding.com.cn/2019/03/20/Consumers, Producers, and the Efficiency of Markets/</id>
    <published>2019-03-20T08:17:37.000Z</published>
    <updated>2019-04-12T07:10:01.054Z</updated>
    
    <content type="html"><![CDATA[<p>In this chapter, we take up the topic of <strong>welfare economics</strong>, the study of how the allocation of resources affects economic well-being. This analysis leads to a profound conclusion: The equilibrium of supply and demand in a market maximizes the total benefits received by buyers and sellers.</p><a id="more"></a><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#consumer-surplus">CONSUMER SURPLUS</a><ul><li><a href="#willingness-to-pay">Willingness To Pay</a></li><li><a href="#using-the-demand-curve-to-measure-consumer-surplus">Using The Demand Curve To Measure Consumer Surplus</a></li><li><a href="#how-a-lower-price-raises-consumer-surplus">How A Lower Price Raises Consumer Surplus</a></li><li><a href="#what-does-consumer-surplus-measure">What Does Consumer Surplus Measure?</a></li></ul></li><li><a href="#producer-surplus">PRODUCER SURPLUS</a><ul><li><a href="#cost-and-the-willingness-to-sell">Cost And The Willingness To Sell</a></li><li><a href="#using-the-supply-curve-to-measure-producer-surplus">Using The Supply Curve To Measure Producer Surplus</a></li><li><a href="#how-a-higher-price-raises-producer-surplus">How A Higher Price Raises Producer Surplus</a></li></ul></li><li><a href="#market-efficiency">MARKET EFFICIENCY</a><ul><li><a href="#the-benevolent-social-planner">The Benevolent Social Planner</a></li><li><a href="#evaluating-the-market-equilibrium">Evaluating The Market Equilibrium</a></li></ul></li><li><a href="#conclusion">CONCLUSION</a></li></ul><!-- tocstop --><h2><span id="consumer-surplus">CONSUMER SURPLUS</span></h2><h3><span id="willingness-to-pay">Willingness To Pay</span></h3><p><strong>Willingness to pay</strong> is the maximum amount that a buyer will pay for a good. <strong>Consumer surplus</strong> is the amount a buyer is willing to pay for a good minus the amount the buyer actually pays for it. For example, John is willing to pay ¥100 for an album but pays only ¥80 for it. So John receives <em>consumer surplus</em> of ¥20.</p><h3><span id="using-the-demand-curve-to-measure-consumer-surplus">Using The Demand Curve To Measure Consumer Surplus</span></h3><p>Say John, Paul, George, and Ringo willing to buy an old album. Table 1 shows the maximum price that each of the four possible buyers would pay. The table in Figure 1 shows the demand schedule that corresponds to Table 1.</p><p><img src="/images/surplus/DraggedImage.jpg"> <img src="/images/surplus/DraggedImage-1.jpg"></p><p>Because the demand curve reflects buyers’ willingness to pay, we can also use it to measure consumer surplus. Figure 2 uses the demand curve to compute consumer surplus in our two examples.</p><p><img src="/images/surplus/DraggedImage-2.jpg"></p><p>We can find out that <strong>the area below the demand curve and above the price measures the consumer surplus in a market</strong>. This is true because the height of the demand curve measures the value buyers place on the good, as measured by their willingness to pay for it. The difference between this willingness to pay and the market price is each buyer’s consumer surplus. Thus, the total area below the demand curve and above the price is the sum of the consumer surplus of all buyers in the market for a good or service.</p><h3><span id="how-a-lower-price-raises-consumer-surplus">How A Lower Price Raises Consumer Surplus</span></h3><p><img src="/images/surplus/DraggedImage-3.jpg"></p><p>Figure 3 shows a typical demand curve. In panel (a), consumer surplus at a price of P1 is the area of triangle ABC. Now suppose that the price falls from P1 to P2, as shown in panel (b). The consumer surplus now equals area ADF.</p><p>This increase in consumer surplus is composed of two parts. First, those buyers who were already buying Q1 of the good at the higher price P1 are better off because they now pay less. It equals the area of the rectangle BCDE. Seconds, some new buyers enter the market. The consumer surplus these newcomers receive is the area of the triangle CEF.</p><h3><span id="what-does-consumer-surplus-measure">What Does Consumer Surplus Measure?</span></h3><p>Consumer surplus, the amount that buyers are willing to pay for a good minus the amount they actually pay for it, measures the benefit that buyers reveille from a good as the buyers themselves perceive it. Thus, consumer surplus is a good measure of economic well-being if policymakers want to respect the <strong>preferences of buyers</strong>.</p><h2><span id="producer-surplus">PRODUCER SURPLUS</span></h2><h3><span id="cost-and-the-willingness-to-sell">Cost And The Willingness To Sell</span></h3><p>A seller’s <strong>cost</strong> of doing a work is the value of everything the seller must give up to produce a good. <strong>Producer surplus</strong> is the amount a seller is paid for a good minus the seller’s cost of providing it.</p><h3><span id="using-the-supply-curve-to-measure-producer-surplus">Using The Supply Curve To Measure Producer Surplus</span></h3><p>Say there are four painters, Mary, Frida, Georgia, Grandma, compete for painting a house. Table 2 shows the costs of them. <img src="/images/surplus/DraggedImage-4.jpg"></p><p>Producer surplus is closely related to the supply curve. The table in Figure 4 shows the supply schedule that corresponds to the costs in Table 2. The graph in Figure 4 shows the supply curve that corresponds to this supply schedule.</p><p><img src="/images/surplus/DraggedImage-5.jpg"></p><p>Because the supply curve reflects sellers’ cost, we can use it to measure producer surplus, as shown in Figure 5. <strong>The area below the price and above the supply curve measures the producer surplus in a market</strong>. The logic is straightforward: The height of the supply curve measures sellers’ costs, and the difference between the price and the cost of production is each seller’s producer surplus. Thus, the total area is the sum of the producer surplus of all sellers. <img src="/images/surplus/DraggedImage-6.jpg"></p><h3><span id="how-a-higher-price-raises-producer-surplus">How A Higher Price Raises Producer Surplus</span></h3><p><img src="/images/surplus/DraggedImage-7.jpg"></p><p>Figure 6 shows a typical upward-sloping supply curve that would arise in a market with many sellers. In panel (a), the price is P1, and producer surplus is the area of triangle ABC. Panel (b) shows what happens when the price rises from P1 to P2. Producer surplus now equals area ADF.</p><h2><span id="market-efficiency">MARKET EFFICIENCY</span></h2><h3><span id="the-benevolent-social-planner">The Benevolent Social Planner</span></h3><p>One possible way to measure the economic well-being of a society is the sum of consumer and producer surplus, which we called <strong>total surplus</strong>. Consumer surplus is the benefit that buyers receive from participating in a market, and producer surplus is the benefit that sellers receive. It is therefore natural to use total surplus as a measure of society’s economic well-being.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Total surplus = Value to buyers - Cost to sellers</span><br></pre></td></tr></table></figure><p>Total surplus in a market is the total value to buyers of the goods, as measured by their willingness to pay, minus the total cost to sellers of providing those goods. If an allocation of resources maximizes total surplus, we say that the allocation exhibits <strong>efficiency</strong>.</p><p>In addition to efficiency, the social planner might also care about <strong>equality</strong>— that is, whether the various buyers and sellers in the market have a similar level of economic well-being.</p><h3><span id="evaluating-the-market-equilibrium">Evaluating The Market Equilibrium</span></h3><p><img src="/images/surplus/DraggedImage-8.jpg"></p><p>Figure 7 shows consumer and producer surplus when a market reaches the equilibrium of supply and demand curve. The total area between the supply and demand curves up to the point of equilibrium represents the total surplus in this market. Those buyers who value the good more than the price choose the buy the good; buyers who value it less than the price do not. Similarly, those sellers whose costs are less than the price choose to produce and sell the good.</p><p>These observations lead to three insights about market outcomes: 1. Free markets allocate the supply of goods to the buyers who value them most highly, as measured by their willingness to pay. 2. Free markets allocate the demand for goods to the sellers who can produce them at the least cost. 3. Free markets produce the quantity of goods that maximizes the sum of consumer and producer surplus.</p><p><img src="/images/surplus/DraggedImage-9.jpg"></p><p>Figure 8 illustrates why this is true. At any quantity below the equilibrium level, such as Q1, the value to the marginal buyer exceeds the cost to the marginal seller. As a result, increasing the quantity produced and consumed raises total surplus. This continues to be true until the quantity reaches the equilibrium level. Similar situations happed when the quantity larger than the equilibrium level.</p><p>Therefore, the equilibrium outcome is an <strong>efficient</strong> allocation of resources.</p><h2><span id="conclusion">CONCLUSION</span></h2><p>This chapter introduced the basic tools of welfare economics — consumer and producer surplus — and used them to evaluate the efficiency of free markets. We showed that the forces of supply and demand allocate resources efficiently.</p><p>A word of warning is in order. To conclude that markets are efficient, we made several assumptions about how markets work. First, our analysis assumed that markets are perfectly competitive. In the world, however, competition is sometimes far from perfect. Second, out analysis assumed that the outcome in a market matters only to the buyers and sellers in that market. Yet, in the world, the decisions of buyers and sellers sometimes affect people who are not participants in the market at all. Pollution is the classic example. Such side effects, called <strong>externalities</strong>, cause welfare in a market to depend on more than just the value to the buyers and the cost to the sellers.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this chapter, we take up the topic of &lt;strong&gt;welfare economics&lt;/strong&gt;, the study of how the allocation of resources affects economic well-being. This analysis leads to a profound conclusion: The equilibrium of supply and demand in a market maximizes the total benefits received by buyers and sellers.&lt;/p&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://www.52coding.com.cn/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="微观经济型原理" scheme="http://www.52coding.com.cn/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%9E%8B%E5%8E%9F%E7%90%86/"/>
    
      <category term="Market Efficiency" scheme="http://www.52coding.com.cn/tags/Market-Efficiency/"/>
    
      <category term="Consumer Surplus" scheme="http://www.52coding.com.cn/tags/Consumer-Surplus/"/>
    
      <category term="Producer Surplus" scheme="http://www.52coding.com.cn/tags/Producer-Surplus/"/>
    
  </entry>
  
  <entry>
    <title>Microeconomics - Elasticity and Its Application</title>
    <link href="http://www.52coding.com.cn/2019/02/22/Elasticity%20and%20Its%20Application/"/>
    <id>http://www.52coding.com.cn/2019/02/22/Elasticity and Its Application/</id>
    <published>2019-02-22T08:02:01.000Z</published>
    <updated>2019-04-12T07:10:06.980Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#the-elasticity-of-demand">The Elasticity of Demand</a><ul><li><a href="#the-price-elasticity-of-demand-and-its-determinants">The Price Elasticity of Demand and Its Determinants</a></li><li><a href="#computing-the-price-elasticity-of-demand">Computing the Price Elasticity of Demand</a></li><li><a href="#the-midpoint-method-a-better-way-to-calculate-percentage-changes-and-elasticities">The Midpoint Method: A Better Way To Calculate Percentage Changes and Elasticities</a></li><li><a href="#the-variety-of-demand-curves">The Variety of Demand Curves</a></li><li><a href="#total-revenue-and-the-price-elasticity-of-demand">Total Revenue and The Price Elasticity of Demand</a></li><li><a href="#elasticity-and-total-revenue-along-a-linear-demand-curve">Elasticity and Total Revenue Along A Linear Demand Curve</a></li><li><a href="#other-demand-elasticities">Other Demand Elasticities</a></li></ul></li><li><a href="#the-elasticity-of-supply">The Elasticity Of Supply</a><ul><li><a href="#the-price-elasticity-of-supply-and-its-determinants">The Price Elasticity of Supply And Its Determinants</a></li><li><a href="#computing-the-price-elasticity-of-supply">Computing The Price Elasticity of Supply</a></li><li><a href="#the-vary-of-supply-curves">The Vary Of Supply Curves</a></li></ul></li><li><a href="#three-applications-of-supply-demand-and-elasticity">Three Applications Of Supply, Demand, And Elasticity</a><ul><li><a href="#can-good-news-for-farming-be-bad-news-for-farmers">Can Good News For Farming Be Bad News For Farmers</a></li><li><a href="#why-did-opec-fail-to-keep-the-price-of-oil-high">Why Did OPEC Fail To Keep The Price Of Oil High?</a></li><li><a href="#does-drug-interdiction-increase-or-decrease-drug-related-crime">Does Drug Interdiction Increase Or Decrease Drug-Related Crime?</a></li></ul></li><li><a href="#the-distributional-effects-of-tax">The Distributional Effects of Tax</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="the-elasticity-of-demand">The Elasticity of Demand</span></h2><p>To measure how much consumers respond to changes in economic variables, economists use the concept of <strong>elasticity</strong>. Elasticity is a measure of the responsiveness of quantity demanded or quantity supplied to one of its determinants.</p><h3><span id="the-price-elasticity-of-demand-and-its-determinants">The Price Elasticity of Demand and Its Determinants</span></h3><p>The <strong>price elasticity of demand</strong> measures how much the quantity demanded responds to a change in price. Demand for a good is said to be <em>elastic</em> if the quantity demanded responds substantially to changes in the price. Demand is said to be <em>inelastic</em> if the quantity demanded responds only slightly to changes in the price.</p><p>Based on experience, however, we can state some general rules about what determines the price elasticity of demand.</p><ul><li>Availablity of Close Substitutes</li><li>Necessities versus Luxuries</li><li>Definition of the Market</li><li>Time Horizon</li></ul><h3><span id="computing-the-price-elasticity-of-demand">Computing the Price Elasticity of Demand</span></h3><p><strong>Price elasticity of demand</strong> = Percentage change in quantity demanded / Percentage change in price</p><p>For example, suppose that a 10 percent increase in the price of an ice-cream cone causes the amount of ie cream you buy to fall by 20 percent. We calculate your elasticity of demand as</p><p>Price elasticity of demand = 20 percent / 10 percent = 2.</p><p>In this example, the elasticity is 2, reflecting that the change in the quantity demanded is proportionately twice as large as the change in the price. <em>A larger price elasticity implies a greater responsiveness of quantity demanded to price.</em></p><h3><span id="the-midpoint-method-a-better-way-to-calculate-percentage-changes-and-elasticities">The Midpoint Method: A Better Way To Calculate Percentage Changes and Elasticities</span></h3><p>The elasticity from point A to point B seems different from the elasticity from point B to point A. This difference arises because the percentage changes are calculated from a different base.</p><p>One way to avoid this problem is to use the <strong>midpoint method</strong> for calculating elasticities. The midpoint method computes a percentage change by dividing the midpoint of the initial and final levels. The following formula expresses the midpoint method for calculating the price elasticity of demand between two points, denoted (Q1, P1) and (Q2, P2):</p><p><img src="/images/elastic/DraggedImage.jpg"></p><h3><span id="the-variety-of-demand-curves">The Variety of Demand Curves</span></h3><p>Demand is considered <strong>elastic</strong> when the elasticity is greater than 1. Demand is considered <strong>inelastic</strong> when the elasticity is less than 1. Because the price elasticity of demand measures how much quantity demanded responds to changes in the price, it is closely related to the slope of the demand curve. The <strong>flatter</strong> the demand curve that passes through a given point, the <strong>greater</strong> the price elasticity of demand. The <strong>steeper</strong> the demand curve that passes through a given point, the <strong>smaller</strong> the price elasticity of demand. Figure 1 shows five cases.</p><p><img src="/images/elastic/DraggedImage-1.jpg"></p><h3><span id="total-revenue-and-the-price-elasticity-of-demand">Total Revenue and The Price Elasticity of Demand</span></h3><p><strong>Toal revenue</strong> is the amount paid by buyers and received by sellers of the good. In any market, total revenue is P * Q, the price of the good times the quantity of the good sold. We can show total revenue graphically, as in Figure 2.</p><p><img src="/images/elastic/DraggedImage-2.jpg"></p><p>How does total revenue change as one moves along the demand curve? There are some examples in Figure 3.</p><p><img src="/images/elastic/DraggedImage-3.jpg"></p><p>Although the examples in this figure are extreme, they illustrate some general rules:</p><ul><li>When demand is <strong>inelastic</strong>, price and total revenue move in the <strong>same direction</strong>.</li><li>When demand is <strong>elastic</strong>, price and total revenue move in <strong>opposite directions</strong>.</li><li>If demand is <strong>unit elastic</strong> (a price elasticity exactly equal to 1), total revenue <strong>remains constant</strong> when the price changes.</li></ul><h3><span id="elasticity-and-total-revenue-along-a-linear-demand-curve">Elasticity and Total Revenue Along A Linear Demand Curve</span></h3><p><img src="/images/elastic/DraggedImage-4.jpg"></p><p>Even though the slope of a linear demand curve is constant, the elasticity is not. This is true because the slope is the ratio of <em>changes</em> in the two variables, whereas the elasticity is the ratio of <em>percentage changes</em> in the two variables. At points with a low price and high quantity, the demand curve is inelastic. At points with a high price and low quantity, the demand curve is elastic.</p><p>The linear demand curve illustrates that the price elasticity of demand need not be the same at all points on a demand curve. A constant elasticity is possible, but it is not always the case.</p><h3><span id="other-demand-elasticities">Other Demand Elasticities</span></h3><p><strong>The Income Elasticity of Demand</strong> measures how the quantity demanded changes as consumer income changes.</p><p><img src="/images/elastic/DraggedImage-5.jpg"></p><p>Most of goods are <em>normal goods</em>: Higher income raises the quantity demanded, which have positive income elasticities. A few goods, such as bus rides, are <em>inferior goods</em>: Higher income lowers the quantity demanded, which have negative income elasticities.</p><p><strong>The Cross-Price Elasticity of Demand</strong> measures how the quantity demanded of one good responds to a change in the price of another good.</p><p><img src="/images/elastic/DraggedImage-6.jpg"></p><p>Substitutes are goods that are typically used in place of one another, whose cross-price elasticity is positive. Conversely, complements are goods that are typically used together, whose cross-price elasticity is negative.</p><h2><span id="the-elasticity-of-supply">The Elasticity Of Supply</span></h2><h3><span id="the-price-elasticity-of-supply-and-its-determinants">The Price Elasticity of Supply And Its Determinants</span></h3><p>The <strong>price elasticity of supply</strong> measures how much the quantity supplied responds to changes in the price. Supply of a good is said to be <em>elastic</em> if the quantity supplied responds substantially to changes in the price.</p><p>In most markets, a key determinant of the price elasticity of supply is the <strong>time period</strong> being considered. Supply is usually <em>more elastic in the long run</em> than in the short run.</p><h3><span id="computing-the-price-elasticity-of-supply">Computing The Price Elasticity of Supply</span></h3><p>Economists compute the price elasticity of supply as the percentage change in the quantity supplied divided by the percentage change in the price. That is, <img src="/images/elastic/DraggedImage-7.jpg"></p><h3><span id="the-vary-of-supply-curves">The Vary Of Supply Curves</span></h3><p><img src="/images/elastic/DraggedImage-8.jpg"></p><p><img src="/images/elastic/DraggedImage-9.jpg"></p><h2><span id="three-applications-of-supply-demand-and-elasticity">Three Applications Of Supply, Demand, And Elasticity</span></h2><h3><span id="can-good-news-for-farming-be-bad-news-for-farmers">Can Good News For Farming Be Bad News For Farmers</span></h3><p>The raise of production provided by new farming technology could make farmers worse off. Because the new technic increase the amount of wheat that can be produced on each acre of land, farmers are willing to supply more wheat at any price. In other words, <strong>the supply curve shift to the right</strong> and the demand curve still remain the same, which cause the price of wheat falls.</p><p><img src="/images/elastic/DraggedImage-10.jpg"></p><p>Wheat being an <strong>inelastic good</strong>, the price of wheat falls doesn’t make people buy a lot more wheat. So a decrease in price causes farmers’ total revenue to fall.</p><p>You may wonder why farmers would adopt the new technology. The answer goes to the heart of <strong>how competitive markets work</strong>. Because each farmer is only a small part of the market for wheat, it’s better to use the new technic to produce and sell more wheat at any given price. Yet when all farmers do this, the supply of wheat increases, the price falls, and farmers are worse off.</p><p>It is important to keep in mind that what is good for farmers is not necessarily good for society as a whole. Improvement in farm technology can be bad for farmers because it makes farmers increasingly unnecessary, but it is surely good for consumers who pay less for food.</p><h3><span id="why-did-opec-fail-to-keep-the-price-of-oil-high">Why Did OPEC Fail To Keep The Price Of Oil High?</span></h3><p>The OPEC episode of 1970s and 1980s shows how supply and demand can behave differently in the short run and in the long run. <strong>In the short run</strong>, both the supply and demand for oil are <strong>inelastic</strong>. Supply is inelastic because the quantity of known oil reserves and the capacity for oil extraction cannot be changed quickly. Demand is inelastic because buying habits do not respond immediately to changes in price. Thus, as panel (a) of Figure 8 shows, <em>the short-run supply and demand curves are steep</em>.</p><p><img src="/images/elastic/DraggedImage-11.jpg"></p><p>The situation is very different in the long run. Over long periods of time, producers of oil outside OPEC respond to high prices by increasing oil exploration. Consumers respond with greater conservation. Thus, as panel (b) of Figure 8 shows, <em>the long-run supply and demand curves are more elastic</em>.</p><h3><span id="does-drug-interdiction-increase-or-decrease-drug-related-crime">Does Drug Interdiction Increase Or Decrease Drug-Related Crime?</span></h3><p><img src="/images/elastic/DraggedImage-12.jpg"></p><p>Suppose the government increase the number of federal agents devoted to the war on drugs. When the government stops some drugs from entering the country and arrests more smugglers, it raises the cost of selling drugs and, therefore, reduces the quantity of drugs supplied at any given price. But the <em>demand for drugs is not changed</em>, as panel (a) of Figure 9 shows.</p><p>But in terms of drug-related crime, since the demand for drugs is inelastic, then an increase in price raises total revenue in the drug market. Addicts who already had to steal to support their habits would have an even greater need for quick cash. Thus, <strong>drug interdiction could increase drug-related crime</strong>.</p><p>Rather than trying to reduce the supply of drugs, policymakers might try to reduce the demand by pursuing a policy of <strong>drug education</strong>. Successful drug education has the effects shown in panel (b) of Figure 9. Thus, in contrast to drug interdiction, <strong>drug education can reduce both drug use and drug-related crime</strong>.</p><p>However, the demand for drugs is probably inelastic over short periods, it may be more elastic elastic over longer periods because higher prices would discourage experimentation with drugs among the young and, over time, lead to fewer drug addicts. In this case, <strong>drug interdiction would increase drug-related crime in the short run while decreasing it in the long run</strong>.</p><h2><span id="the-distributional-effects-of-tax">The Distributional Effects of Tax</span></h2><p>Suppose this is the supply and demand curve of the gasoline market of Chicago without tax. The market price will be ¥3 when there is no tax. So buyers pay ¥3 to buy one gallon of gasoline and sellers receive ¥3 for one gallon of gasoline.</p><p><img src="/images/elastic/DraggedImage-13.jpg"></p><p>Suppose the government imposes a tax of ¥0.5 per gallon of gasoline and suppose the buyer will pay the tax. Then the demand curve will move left because there's a tax more. And that is changing the tax everywhere in the curve. So from any point, vertically in the curve, to the other point, we're going to have a distance of 50 cents. Let’s say the new demand curve generate a new equilibrium at price ¥2.75. So, what the buyers end up paying is at ¥2.75, the buyer that goes to the Sellers, plus the 50 cents that goes to the government. So in the end, they're actually paying ¥3.25 for a gallon of gasoline.</p><p><img src="/images/elastic/DraggedImage.png"></p><p>Now the buyers are paying ¥3.25. So they are worse off because they have a higher price by ¥0.25. And the sellers, while they were receiving ¥3 before, now they're only getting ¥2.75. So they are worse off by 25 cents too. Well, it looks like, in this particular situation, the buyers are sharing 25 cents, they're paying of the tax and sellers are putting up with 25 cents of the tax. So, <strong>this tax is equally distributed among the buyers and the sellers</strong>. And it doesn't matter if the sellers are the ones sending this tax to the government.</p><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">No Tax</th><th style="text-align: center;">Tax ¥0.5</th><th style="text-align: center;">Change</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Market Price</td><td style="text-align: center;">¥3.00</td><td style="text-align: center;">¥2.75</td><td style="text-align: center;">¥0.25</td></tr><tr class="even"><td style="text-align: center;">Buyers Pay</td><td style="text-align: center;">¥3.00</td><td style="text-align: center;">¥3.25</td><td style="text-align: center;">¥0.25</td></tr><tr class="odd"><td style="text-align: center;">Sellers Receive</td><td style="text-align: center;">¥3.00</td><td style="text-align: center;">¥2.75</td><td style="text-align: center;">¥0.25</td></tr></tbody></table><p>So what determined the distribution of the tax is which side of the market <em>has the most trouble adjusting to a tax</em>. And in this particular example, we're assuming that both have the same trouble and that is based on the elasticity. And the more inclined the curve is, the more inelastic that side of the market is.</p><p>Let's say the demand curve is like that below and the supply curve is a lot flatter. So this model here makes the assumption that the demand of gasoline is more inelastic than the supply of gasoline.</p><p><img src="/images/elastic/DraggedImage-14.jpg"></p><p>What happened? The market price went down to ¥2.80. The buyers pays ¥2.80 to the sellers, but then they pay 50 cents to the government. So they're actually paying effectively ¥3.30 with this tax. So they're paying 30 cents more than they were paying before per gallon of gasoline. How about the sellers? The sellers were selling it for ¥3 before. Now, they're getting ¥2.80 from the buyers, so they're worse off 20 cents. And it's a whole different story, because now, of the 50 cents, 30 cents are shared by the buyers and only 20 cents by the sellers. So <em>the sellers are not sharing as much of the burden of this tax as the buyers are</em>.</p><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">No Tax</th><th style="text-align: center;">Tax ¥0.5</th><th style="text-align: center;">Change</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Market Price</td><td style="text-align: center;">¥3.00</td><td style="text-align: center;">¥2.80</td><td style="text-align: center;">¥0.20</td></tr><tr class="even"><td style="text-align: center;">Buyers Pay</td><td style="text-align: center;">¥3.00</td><td style="text-align: center;">¥3.30</td><td style="text-align: center;">¥0.30</td></tr><tr class="odd"><td style="text-align: center;">Sellers Receive</td><td style="text-align: center;">¥3.00</td><td style="text-align: center;">¥2.80</td><td style="text-align: center;">¥0.20</td></tr></tbody></table><p>So the seller has a lot more freedom here to adjust to this tax by perhaps charging a higher price to the buyers. The buyers cannot adjust as easily and they will have to put up with most of the burden. So in the end, <strong>the most inelastic side of the market is the one that actually shares most of the burden of the tax</strong> regardless of who send the tax to the government.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#the-elasticity-of-demand&quot;&gt;The Elasticity of Demand&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#the-price-elasticity-of-demand-and-its-determinants&quot;&gt;The Price Elasticity of Demand and Its Determinants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#computing-the-price-elasticity-of-demand&quot;&gt;Computing the Price Elasticity of Demand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#the-midpoint-method-a-better-way-to-calculate-percentage-changes-and-elasticities&quot;&gt;The Midpoint Method: A Better Way To Calculate Percentage Changes and Elasticities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#the-variety-of-demand-curves&quot;&gt;The Variety of Demand Curves&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#total-revenue-and-the-price-elasticity-of-demand&quot;&gt;Total Revenue and The Price Elasticity of Demand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#elasticity-and-total-revenue-along-a-linear-demand-curve&quot;&gt;Elasticity and Total Revenue Along A Linear Demand Curve&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#other-demand-elasticities&quot;&gt;Other Demand Elasticities&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#the-elasticity-of-supply&quot;&gt;The Elasticity Of Supply&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#the-price-elasticity-of-supply-and-its-determinants&quot;&gt;The Price Elasticity of Supply And Its Determinants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#computing-the-price-elasticity-of-supply&quot;&gt;Computing The Price Elasticity of Supply&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#the-vary-of-supply-curves&quot;&gt;The Vary Of Supply Curves&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#three-applications-of-supply-demand-and-elasticity&quot;&gt;Three Applications Of Supply, Demand, And Elasticity&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#can-good-news-for-farming-be-bad-news-for-farmers&quot;&gt;Can Good News For Farming Be Bad News For Farmers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#why-did-opec-fail-to-keep-the-price-of-oil-high&quot;&gt;Why Did OPEC Fail To Keep The Price Of Oil High?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#does-drug-interdiction-increase-or-decrease-drug-related-crime&quot;&gt;Does Drug Interdiction Increase Or Decrease Drug-Related Crime?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#the-distributional-effects-of-tax&quot;&gt;The Distributional Effects of Tax&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://www.52coding.com.cn/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="微观经济型原理" scheme="http://www.52coding.com.cn/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%9E%8B%E5%8E%9F%E7%90%86/"/>
    
      <category term="elasticity" scheme="http://www.52coding.com.cn/tags/elasticity/"/>
    
      <category term="economic" scheme="http://www.52coding.com.cn/tags/economic/"/>
    
  </entry>
  
  <entry>
    <title>Microeconomics - The Market Forces of Supply and Demand</title>
    <link href="http://www.52coding.com.cn/2019/02/11/The%20Market%20Forces%20of%20Supply%20and%20Demand/"/>
    <id>http://www.52coding.com.cn/2019/02/11/The Market Forces of Supply and Demand/</id>
    <published>2019-02-11T10:28:47.000Z</published>
    <updated>2019-04-12T07:11:40.482Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#markets-and-competition">Markets and Competition</a><ul><li><a href="#what-is-a-market">What Is A Market?</a></li><li><a href="#what-is-competition">What is Competition?</a></li></ul></li><li><a href="#demand">Demand</a><ul><li><a href="#the-demand-curve-the-relationship-between-price-and-quantity-demand">The Demand Curve: The Relationship Between Price and Quantity Demand</a></li><li><a href="#market-demand-vs-individual-demand">Market Demand VS. Individual Demand</a></li><li><a href="#shifts-in-the-demand-curve">Shifts In the Demand Curve</a></li></ul></li><li><a href="#supply">Supply</a><ul><li><a href="#the-supply-curve-the-relationship-between-price-and-quantity-supplied">The Supply Curve: The Relationship Between Price and Quantity Supplied</a></li><li><a href="#market-supply-vs-individual-supply">Market Supply VS. Individual Supply</a></li><li><a href="#shifts-in-the-supply-curve">Shifts In The Supply Curve</a></li></ul></li><li><a href="#supply-and-demand-together">Supply And Demand Together</a><ul><li><a href="#equilibrium">Equilibrium</a></li><li><a href="#three-steps-to-analyzing-changes-in-equilibrium">Three Steps To Analyzing Changes In Equilibrium</a></li></ul></li><li><a href="#conclusion-how-prices-allocate-resources">Conclusion: How prices Allocate Resources</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="markets-and-competition">Markets and Competition</span></h2><h3><span id="what-is-a-market">What Is A Market?</span></h3><p>A <strong>market</strong> is a group of buyers and sellers of a particular good or service. Buyers decide the demand for the product while sellers decide the supply of the product.</p><h3><span id="what-is-competition">What is Competition?</span></h3><p>Economists use the term <strong>competitive market</strong> to describe a market in which there are so many buyers and so many sellers that each has a negligible impact on the market price.</p><p>Competition has various degrees, from <strong>perfectly competitive</strong> to <strong>monopoly</strong>.</p><p><em>Perfectly competitive</em> requires two characteristics: 1. the goods offered for sale are all exactly the same 2. the buyers and sellers are so numerous that no single buyer or seller has any influence over the market price.</p><p>However, some marketplace has only one seller, such a seller is called a <em>monopoly</em>.</p><h2><span id="demand">Demand</span></h2><p>We begin our study of markets by examining the behavior of buyers. To focus our thinking, let’s keep in mind a particular good — ice cream.</p><h3><span id="the-demand-curve-the-relationship-between-price-and-quantity-demand">The Demand Curve: The Relationship Between Price and Quantity Demand</span></h3><p>The <strong>quantity demanded</strong> of any good is the amount of the good that buyers are willing and able to purchase. The relationship of price and quantity demanded follows the <strong>law of demand</strong>: Other things equal, when the price of a good rises, the quantity demanded of the good falls, and when the price falls, the quantity demanded rises. A table that shows the relationship between the price of a good and the quantity demanded is called <strong>demand schedule</strong>.</p><p><img src="/images/IMG_BD7E38AD92C1-1.jpg"></p><p>The graph in Figure 1 uses the numbers from the table to illustrate the law of demand. By convention, the quantity of ice cream demanded is on the horizontal axis. The downward-sloping line relating price and quantity demanded is called the <strong>demand curve</strong>.</p><h3><span id="market-demand-vs-individual-demand">Market Demand VS. Individual Demand</span></h3><p>Figure 1 shows the individual demand of ice-cream. However, most of the time, we want to focus on the <strong>market demand</strong> which is the sum of all individual’s demands.</p><p><img src="/images/market/DraggedImage.jpg"></p><p>Figure 2 shows the Catherine’s demand, Nicholas’s demand as well as the market demand. At each price, the total quantity demand is the sum of Catherine’s quantity demand and Nicholas’s quantity demand. The market demand curve shows how the total quantity demanded of a good varies as the price of the good varies, while all the other factors are held constant.</p><h3><span id="shifts-in-the-demand-curve">Shifts In the Demand Curve</span></h3><p><img src="/images/market/DraggedImage-1.jpg"></p><p>Figure 3 illustrates shifts in the demand. There are many variables that can shift the demand curve. Here are the most important.</p><p><strong>Income</strong> If the demand for a good falls when income falls, the good is called a <strong>normal good</strong>. If the demand for a good rises when income falls, the good is called an <strong>inferior good</strong>, such as bus rides. As your income falls, you are less likely to buy a car or take a cab and more likely to ride a bus.</p><p><strong>Prices of Related Goods</strong> When a fall in the price of one good reduces the demand for another good, the two goods are called <strong>substitutes</strong>, such as ice cream and frozen yogurt, hot dogs and hamburgers. When a fall in the price of one good raises the demand for another good, the two goods are called <strong>complements</strong>, such as gasoline and automobiles, computers and software.</p><p><strong>Tastes</strong> If you like ice cream, you buy more of it. Economists examine what happens when tastes change.</p><p><strong>Expectations</strong> Your expectations about the future may affect your demand for a good or service today.</p><p><strong>Number of Buyers</strong> Market demand depends on the number of buyers.</p><p><strong>Summary</strong> <img src="/images/market/DraggedImage-2.jpg"></p><h2><span id="supply">Supply</span></h2><p>We now turn to the other side of the market and examine the behavior of sellers. Once again, to focus our thinking, let’s consider the market for ice cream.</p><h3><span id="the-supply-curve-the-relationship-between-price-and-quantity-supplied">The Supply Curve: The Relationship Between Price and Quantity Supplied</span></h3><p><strong>Quantity supplied</strong> is the amount of a good that sellers are willing and able to sell. When other things equal, the quantity supplied of a good rises when the price of the good rises, which is called <strong>the law of supply</strong>.</p><p><strong>Supply schedule</strong> is a table that shows the relationship between the price of a good and the quantity supplied. The curve relating price and quantity supplied is called the <strong>supply curve</strong>.</p><p><img src="/images/market/DraggedImage-3.jpg"></p><h3><span id="market-supply-vs-individual-supply">Market Supply VS. Individual Supply</span></h3><p>Just as market demand is the sum of the demands of all buyers, market supply is the sum of the supplies of all sellers. As with demand curves, we sum the individual supply curves <em>horizontally</em> to obtain the market supply curves. <img src="/images/market/DraggedImage-4.jpg"></p><h3><span id="shifts-in-the-supply-curve">Shifts In The Supply Curve</span></h3><p><img src="/images/market/DraggedImage-5.jpg"></p><p>Figure 7 illustrate shifts in the supply curve. There are many variables that can shift the supply curve. Here are some of the most important.</p><p><strong>Input Prices</strong> When the price of one or more inputs rises, producing the good is less profitable, and firms supply less the good.</p><p><strong>Technology</strong> By reducing firms’ costs, the advanced technology raised the supply of goods.</p><p><strong>Expectations</strong> The amount of good a firm supplies today may depend on its expectations about the future.</p><p><strong>Number of Sellers</strong> In addition to the preceding factors, which influence the behavior of individual sellers, market supply depends on the number of these sellers.</p><p><strong>Summary</strong></p><p><img src="/images/market/DraggedImage-6.jpg"></p><h2><span id="supply-and-demand-together">Supply And Demand Together</span></h2><h3><span id="equilibrium">Equilibrium</span></h3><p>Figure 8 shows the market supply curve and market demand together. The intersection point is called <strong>equilibrium</strong>. Equilibrium is a situation in which the market price has reached the level at which quantity supplied equals quantity demanded. The price that balances quantity supplied and quantity demanded is called <strong>equilibrium</strong>.</p><p><img src="/images/market/DraggedImage-7.jpg"></p><p>At the equilibrium price, the quantity of the good that buyers are willing and able to buy exactly balances the quantity that sellers are willing and able to sell. The actions of buyers and sellers <strong>naturally move markets toward the equilibrium</strong> of supply and demand.</p><p><strong>Law of supply and demand</strong>: The price of any good adjusts to bring the quantity supplied and the quantity demanded for that good into balance.</p><p><img src="/images/market/DraggedImage-8.jpg"></p><h3><span id="three-steps-to-analyzing-changes-in-equilibrium">Three Steps To Analyzing Changes In Equilibrium</span></h3><ol type="1"><li>Decide whether the event shifts the supply or demand curve (or perhaps both).</li><li>Decide in which direction the curve shifts.</li><li>Use the supply-and-demand diagram to see how the shift changes the equilibrium price and quantity.</li></ol><p>Example 1:</p><p><img src="IMG_A9BD8FC2E549-1.jpeg"></p><p>In the ice-cream example, <strong>supply</strong> (which <em>refers to the position of the supply curve</em>) does not change because the weather does not alter firms’ desire to sell at any given price. Instead, the hot weather alters consumers’ desire to buy at any given price and thereby shifts the demand curve to the right. The increase in demand causes the equilibrium price to rise. When the price rises, the <strong>quantity supplied</strong> rises. This increase in quantity supplied is represented by the <strong>movement along the supply curve</strong>.</p><p>Example 2: Shifts in Both Supply and Demand</p><p><img src="/images/market/DraggedImage-9.jpg"></p><p><strong>Summary</strong></p><p><img src="/images/market/DraggedImage-10.jpg"></p><h2><span id="conclusion-how-prices-allocate-resources">Conclusion: How prices Allocate Resources</span></h2><p>Consider the allocation of beachfront land. Because the amount of this land is limited, not everyone can enjoy the luxury of living by the beach. Who gets this resource? The answer is whoever is <strong>willing and able to pay the price</strong>. The price of beachfront land adjusts until the quantity of land demanded exactly balances the quantity supplied. Thus, in market economies, <strong>prices are the mechanism for rationing scarce resources</strong>.</p><p>Similarly, prices determine who produces each good and how much is produced. If an invisible hand guides market economies, as Adam Smith famously suggested, then the price system is the baton that the invisible hand uses to conduct the economic orchestra.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#markets-and-competition&quot;&gt;Markets and Competition&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#what-is-a-market&quot;&gt;What Is A Market?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#what-is-competition&quot;&gt;What is Competition?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#demand&quot;&gt;Demand&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#the-demand-curve-the-relationship-between-price-and-quantity-demand&quot;&gt;The Demand Curve: The Relationship Between Price and Quantity Demand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#market-demand-vs-individual-demand&quot;&gt;Market Demand VS. Individual Demand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#shifts-in-the-demand-curve&quot;&gt;Shifts In the Demand Curve&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#supply&quot;&gt;Supply&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#the-supply-curve-the-relationship-between-price-and-quantity-supplied&quot;&gt;The Supply Curve: The Relationship Between Price and Quantity Supplied&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#market-supply-vs-individual-supply&quot;&gt;Market Supply VS. Individual Supply&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#shifts-in-the-supply-curve&quot;&gt;Shifts In The Supply Curve&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#supply-and-demand-together&quot;&gt;Supply And Demand Together&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#equilibrium&quot;&gt;Equilibrium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#three-steps-to-analyzing-changes-in-equilibrium&quot;&gt;Three Steps To Analyzing Changes In Equilibrium&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#conclusion-how-prices-allocate-resources&quot;&gt;Conclusion: How prices Allocate Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://www.52coding.com.cn/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="微观经济型原理" scheme="http://www.52coding.com.cn/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%9E%8B%E5%8E%9F%E7%90%86/"/>
    
      <category term="Supply Curve" scheme="http://www.52coding.com.cn/tags/Supply-Curve/"/>
    
      <category term="Demand Curve" scheme="http://www.52coding.com.cn/tags/Demand-Curve/"/>
    
      <category term="Equilibrium" scheme="http://www.52coding.com.cn/tags/Equilibrium/"/>
    
  </entry>
  
  <entry>
    <title>Recognizing Vehicles</title>
    <link href="http://www.52coding.com.cn/2018/12/09/RS%20-%20Recognizing%20Vehicles/"/>
    <id>http://www.52coding.com.cn/2018/12/09/RS - Recognizing Vehicles/</id>
    <published>2018-12-09T11:54:07.000Z</published>
    <updated>2019-04-12T03:28:03.249Z</updated>
    
    <content type="html"><![CDATA[<p>HKUST CSIT5401 Recognition System lecture notes 6. 识别系统复习笔记。</p><p>Vehicle recognition, including detection, tracking and identification, has been a research topic among automotive manufacturers, suppliers and universities for enhancing road safety.</p><p>For example, the <a href="http://www.argo.ce.unipr.it/ARGO/english/index.html" target="_blank" rel="noopener">ARGO</a> project, started in 1996 at the University of Parma and the University of Pavia, Italy, is aimed at developing a system for improving road safety by controlling and supervising the driver activity.</p><a id="more"></a><!-- toc --><ul><li><a href="#lane-detection">Lane Detection</a><ul><li><a href="#canny-edge-detector">Canny edge detector</a></li></ul></li><li><a href="#vehicle-detection-based-on-symmetry">Vehicle Detection based on Symmetry</a></li><li><a href="#visual-saliency-for-detection-and-tracking">Visual Saliency For Detection and Tracking</a></li><li><a href="#application-examples">Application Examples</a></li></ul><!-- tocstop --><h2><span id="lane-detection">Lane Detection</span></h2><p>For vehicle detection and tracking and intelligent transportation systems, lane marking detection is one of the key steps. Lane markings can be detected based on the camera inverse perspective mapping (IPM) and the assumption that the lane markings are represented by almost vertical bright lines of constant width, surrounded by a darker background.</p><p>The lanes can then be detected by using the camera <strong>inverse perspective mapping (IPM)</strong> and the <strong>Canny edge detector</strong>.</p><p><strong>Inverse perspective mapping</strong></p><p><img src="/images/impres.png"></p><p><img src="/images/ipmreason.png"></p><h3><span id="canny-edge-detector">Canny edge detector</span></h3><p><a href="http://en.wikipedia.org/wiki/Canny_edge_detector" target="_blank" rel="noopener">Canny edge detector</a> is one of the most popular detectors of edge pixels. The edge detection process serves to simplify the analysis of images by drastically <strong>reducing the amount of data to be processed</strong>, while at the same time <strong>preserving useful structural information</strong> about object boundaries.</p><p>There are three common criteria relevant to edge detector performance.</p><ul><li>It is important that edges that occur in the image should not be missed and that there be <em>no spurious responses</em>.</li><li>The edge points should be well localized. That is, the distance between the points marked by the detector and the true edge center should be minimized.</li><li>The last requirement is to circumvent the possibility of multiple responses to a single edge.</li></ul><p><img src="/images/cedbalala.jpg"></p><p>In the figure above, (a) is a noisy step edge; (b) is a difference of boxes operator; (c) is the output of filtering by box operator; (d) is the first derivative of Gaussian operator; and (e) is the first derivative of Gaussian applied to the edge.</p><p><img src="/images/cannysd.png"></p><p>The <strong>edge center</strong> is marked as <strong>red circle</strong> at a local maximum in the output of the filter responses. Within the region of the edge, the boxes operator exhibits more local maxima than the Gaussian operator. Therefore, <strong>the Gaussian operator is better</strong> than the boxes operator in this example.</p><p><strong>The three performance criteria</strong></p><ol type="1"><li><p>Good detection → Detection Criterion</p><p>There should be a low probability of failing to mark real edge points, and low probability of falsely marking non-edge points. This is controlled by signal-to-noise ratio: <span class="math display">\[\text{SNR}=\frac{|\int_{-w}^{+w}G(-x)f(x)dx |}{n_0\sqrt{\int_{-w}^{+w}f(x)^2dx}}\]</span></p></li><li><p>Good localization → Localization Criterion</p><p>The points marked as edge points by the operator should be as close as possible to the true edge center. The localization is defined as the reciprocal of <span class="math inline">\(\delta x_0\)</span>: <span class="math display">\[\text{Localization}=\frac{|\int_{-w}^{+w}G(-x)f&#39;(x)dx |}{n_0\sqrt{\int_{-w}^{+w}f&#39;(x)^2dx}}\]</span></p></li><li><p>Only one response to a single edge → Multiple Response Constraint <span class="math display">\[x_{zc}(f)=\pi\left(\frac{\int_{-\infty}^{+\infty}f&#39;(x)^2dx}{\int_{-\infty}^{+\infty}f&#39;&#39;(x)^2dx}\right)^{1/2}\]</span></p></li></ol><p>It is very difficult to find the function <span class="math inline">\(f\)</span> (filter) which maximizes the detection and localization criteria subject to the multiple response constraint. Numerical optimization is therefore used.</p><p>The solution is of the form <span class="math display">\[f(x)=a_1e^{\alpha x}\sin\omega x+a_2e^{\alpha x}\cos\omega x+a_3e^{-\alpha x}\sin\omega x\\+a_4e^{-\alpha x}\cos\omega x+c\]</span> The variables are determined by the non-linear optimization with boundary conditions.</p><p>The numerically estimated optimal edge detector can be approximated by the <strong>first derivative of a Gaussian</strong> G, where <span class="math display">\[G(x)=\exp(-\frac{x^2}{2\sigma^2})\\f(x)=G&#39;(x)=-\frac{x}{\sigma^2}\exp(-\frac{x^2}{2\sigma^2})\]</span> For 2D, the solution proposed by Canny amounts to convolving the initial image with a Gaussian function followed by computation of the derivatives in <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> of the result.</p><p><img src="/images/firstderofgaus.png"></p><p>There are three main steps in the Canny edge detection</p><ol type="1"><li><p>Gradient calculation</p><p>compute <span class="math inline">\(M\)</span>: <span class="math display">\[I_x=\frac{\partial}{\partial x}(I\ast G(x,y))\\I_y=\frac{\partial}{\partial y}(I\ast G(x,y))\\M=\sqrt{I_x^2+I_y^2}\approx |I_x|+|I_y|\]</span></p></li><li><p>Non-maximum suppression</p><p>keep local maximum, set others to zero.</p></li><li><p>Hysteresis thresholding</p><p><img src="/images/hyposisthres.png"></p></li></ol><h2><span id="vehicle-detection-based-on-symmetry">Vehicle Detection based on Symmetry</span></h2><p>Vehicle detection is based on the assumption that the rear or frontal views of the vehicles are generally symmetric; can be characterized by a rectangular bounding box which satisfies specific aspect ratio constraints and is placed in a specific region of the image, e.g., within lanes.</p><p>These features are used to identify vehicles in the image.</p><ol type="1"><li>Area of interest is identified on the basis of road position and perspective constraints.</li><li>This area of interest is searched for possible vertical symmetries. Not only are the gray level symmetries considered, but also vertical and horizontal edge symmetries are considered.</li><li>Step 3: Once the symmetry axis has been detected, the lower part (the two bottom corners) of a rectangular bounding box is detected.</li><li>The top horizontal limit of the vehicle is then searched according to the pre-defined aspect ratio.</li></ol><p><img src="/images/vdbasedonsys.png"></p><p>Vertical and horizontal binary edges can help solve the problems of strong reflection areas in the vehicle images. The analysis of symmetry produces symmetry maps for gray-level intensity, edge (total), horizontal edge, vertical edge and total (combined) symmetry. The symmetry axis can be found from the total symmetry map.</p><p><strong>Bounding box detection</strong></p><p>After detecting the symmetry axis, the width of the symmetrical region is checked for the presence of two corners representing the bottom of the bounding box around the vehicle.</p><p>Once the two corners are detected, the top side of the rectangle box can be detected by searching.</p><p><img src="/images/boundingboxde.png"></p><h2><span id="visual-saliency-for-detection-and-tracking">Visual Saliency For Detection and Tracking</span></h2><p><strong>Visual saliency</strong> refers to the idea that certain parts of a scene are pre-attentively distinctive (pop-out) and create some form of immediate significant visual arousal within the early stages of the <strong>Human Visual System (HVS)</strong>. The figure below is an example.</p><p>In image analysis, an <strong>edge</strong> is a pop-out region (<strong>region of saliency</strong>) since the edge is more visually significant than the other parts of the image.</p><p><img src="/images/vsexamples.png"></p><p>The salient points are literally the points on the object which are almost unique. These points maximize the discrimination between objects. The visual saliency is defined in terms of <strong>local signal complexity</strong>.</p><p><strong>Shannon entropy</strong> of local attributes (called <strong>local entropy</strong>) is <span class="math display">\[H_{D,Rx}(x)=-\sum_{i\in D}P_{D,Rx}(x,d_i)\log_2P_{D,Rx}(x,d_i)\]</span> where <span class="math inline">\(x\)</span> is point location, <span class="math inline">\(Rx\)</span> is local neighborhood at <span class="math inline">\(x\)</span>, <span class="math inline">\(D\)</span> is descriptor (e.g. intensity), and <span class="math inline">\(P_{D,Rx}(x,d_i)\)</span> is histogram value at <span class="math inline">\(x\)</span>.</p><p><img src="/images/entropylsakdjad.jpg"></p><p>Below are sample frames from the processed sequences using a <strong>fixed scale</strong> based on local entropy. Red square boxes represent the most salient icons or parts of the image. The size of the local window or scale and threshold used were selected manually to give the most satisfactory results.</p><p><img src="/images/saliegsdas.png"></p><ul><li>Problem: the scale is fixed and global. For example, the scale is inappropriate for the pedestrians and the road markings in DT sequence.</li><li>Problem: Small salient regions are not picked up. Highly textured regions, e.g., large intensity variation regions, are picked up. For example, trees and bushes in Vicky sequence.</li></ul><p>Therefore, scale is an important and implicit part of the saliency detection problem.</p><p><strong>Scale selection for salient region detection</strong></p><p>Scale is selected based on the scale-space behavior of the saliency of a given feature.</p><p>For each pixel position <span class="math inline">\(x\)</span></p><ul><li><p>For each scale <span class="math inline">\(s\)</span> inside a range between <span class="math inline">\(s_\min\)</span> and <span class="math inline">\(s_\max\)</span> :</p><ul><li><p>Measure the local descriptor values (e.g.,intensity values) within a window of scale <span class="math inline">\(s\)</span>.</p></li><li><p>Estimate the local probability density function (PDF) from this (e.g. using histogram).</p></li><li><p>Calculate the local entropy <span class="math inline">\(H_D\)</span> <span class="math display">\[H_{D}(s,x)=-\sum_{i\in D}P_{D}(s,x,d_i)\log_2P_{D}(s,x,d_i)\]</span></p></li></ul></li><li><p>Select scales for which the entropy is peaked <span class="math display">\[s:\frac{\partial^2H_D(s,x)}{\partial s^2}&lt;0\]</span></p></li><li><p>Detection performance can be further improved if change of histogram is considered.</p></li></ul><p><img src="/images/salenscaleg.png"></p><p><strong>Vehicle Detection by using AdaBoost</strong></p><p>Vehicles can be detected by using <a href="https://www.52coding.com.cn/2018/12/06/RS%20-%20Recognizing%20Faces/#adaboost">AdaBoost</a>.</p><p><img src="/images/vehildabboo.png"></p><p>By using the AdaBoost, vehicles in their lateral view can be detected in real time.</p><p><strong>Vehicle Recognition</strong></p><p>After vehicle detection, the vehicle can be recognition by using PCA and classification methods, e.g., k-NN or Bayesian methods.</p><h2><span id="application-examples">Application Examples</span></h2><p><strong>Vehicle counting in traffic surveillance</strong></p><p><img src="/images/survelleg.png"></p><p><strong>Traffic jam detection and alarming</strong></p><p><img src="/images/trafficjamsdeg.png"></p><p><strong>Abnormal vehicle behavior detection</strong></p><p><img src="/images/abnormaldetec.png"></p><p><strong>Target tracking in night</strong></p><p><img src="/images/targetrackinni.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HKUST CSIT5401 Recognition System lecture notes 6. 识别系统复习笔记。&lt;/p&gt;
&lt;p&gt;Vehicle recognition, including detection, tracking and identification, has been a research topic among automotive manufacturers, suppliers and universities for enhancing road safety.&lt;/p&gt;
&lt;p&gt;For example, the &lt;a href=&quot;http://www.argo.ce.unipr.it/ARGO/english/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ARGO&lt;/a&gt; project, started in 1996 at the University of Parma and the University of Pavia, Italy, is aimed at developing a system for improving road safety by controlling and supervising the driver activity.&lt;/p&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://www.52coding.com.cn/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Recognition System" scheme="http://www.52coding.com.cn/tags/Recognition-System/"/>
    
      <category term="Canny" scheme="http://www.52coding.com.cn/tags/Canny/"/>
    
      <category term="visual saliency" scheme="http://www.52coding.com.cn/tags/visual-saliency/"/>
    
      <category term="lane detection" scheme="http://www.52coding.com.cn/tags/lane-detection/"/>
    
      <category term="vehicle detection" scheme="http://www.52coding.com.cn/tags/vehicle-detection/"/>
    
  </entry>
  
  <entry>
    <title>Recognizing License Plates</title>
    <link href="http://www.52coding.com.cn/2018/12/08/RS%20-%20Recognizing%20License%20Plates/"/>
    <id>http://www.52coding.com.cn/2018/12/08/RS - Recognizing License Plates/</id>
    <published>2018-12-08T11:54:07.000Z</published>
    <updated>2019-04-12T03:28:00.362Z</updated>
    
    <content type="html"><![CDATA[<p>HKUST CSIT5401 Recognition System lecture notes 5. 识别系统复习笔记。</p><p><a href="http://www.licenseplaterecognition.com/" target="_blank" rel="noopener">License Plate Recognition</a> is an image-processing technology which is used to identify vehicles by their license plates. This technology is used in various security and traffic applications, such as the access-control system, toll payment, parking fee payment, etc.</p><a id="more"></a><p>A number of license plate recognition units are installed in different locations and the passing vehicle plate numbers are matched between the points. The average speed and travel time between these points can be calculated and presented in order to monitor traffic loads. Additionally, the average speed may be used to issue a speeding ticket.</p><!-- toc --><ul><li><a href="#automatic-vehicle-identification-system">Automatic Vehicle Identification System</a></li><li><a href="#license-plate-detection">License Plate Detection</a><ul><li><a href="#global-search">Global Search</a></li><li><a href="#partial-image-analysis">Partial Image Analysis</a></li><li><a href="#sliding-concentric-windows">Sliding Concentric Windows</a></li><li><a href="#adaboost">Adaboost</a></li></ul></li><li><a href="#character-segmentation">Character Segmentation</a></li><li><a href="#character-recognition">Character Recognition</a></li></ul><!-- tocstop --><h2><span id="automatic-vehicle-identification-system">Automatic Vehicle Identification System</span></h2><p>The installation and responses of sensors help to frame a front-view/rear-view of a passing vehicle. <strong>Infra-red sensors</strong> are used for vehicle sensing.</p><p><img src="/images/infredsens.png"></p><p><strong>Anisotropic magneto-resistive (AMR) sensors</strong> for automated vehicle sensing.</p><p><img src="/images/amrsense.png"></p><p><strong>License plate recognition</strong> is generally composed of three steps.</p><ol type="1"><li>Location of the license plate region (License Plate Detection)</li><li>Segmentation of the plate characters (Character Segmentation)</li><li>Recognition of the plate characters (Character Recognition)</li></ol><p>The license plate recognition should operate fast enough to make sure that the system does not miss a single object of interest that moves through the scene.</p><p>With the growth of the computer processing power, the latest developments operate within less than 50ms for plate detection and recognition. It enables the processing of more than 20 frames per second for videos.</p><h2><span id="license-plate-detection">License Plate Detection</span></h2><p>There are several methods for detecting license plate in a vehicle image.</p><ul><li>Global search [Comelli-TVT-95$]$</li><li>Partial image analysis (vertical edge density)[Anagnostopoulos-TITS-08$]$</li><li>Sliding concentric windows [Anagnostopoulos-TITS-06$]$</li><li>AdaBoost [Dlagnekov-04$]$</li></ul><h3><span id="global-search">Global Search</span></h3><p>Comelli et al. presented a system called RITA. RITA can recognize automatically the characters written on the license plate placed on the rear-side of motor vehicles. The goal is to read only the Italian license plates and reject all the others. It is assumed that a Italian license plate is rectangular and the plate contains black characters over a white background.</p><p>The license plate detection algorithm is a <strong>global searching</strong> method because the algorithm picks within the vehicle image globally the area presenting the maximum local contrast based on <strong>gradient analysis</strong>. The picked area possibly corresponds to the rectangle that contains the license plate.</p><p>The algorithm selects the area that presents the <strong>maximum local contrast</strong> that (possibly) corresponds to the rectangle that contains the license plate.</p><p><img src="/images/globalsearch.png"></p><h3><span id="partial-image-analysis">Partial Image Analysis</span></h3><p>The vehicle image can be filtered to extract <strong>vertical edges</strong> and scanned with N-row distance. The number of the existing edges along each scan line is recorded.</p><p>If the number of the edges is greater than a threshold value, the presence of a plate can be assumed.</p><p><img src="/images/piasdsa.png"></p><p>Specifically, if the plate is not found in the first scanning processing, then the algorithm is repeated, reducing the threshold for counting edges or adjusting the threshold for finding vertical edges.</p><h3><span id="sliding-concentric-windows">Sliding Concentric Windows</span></h3><p>An adaptive image segmentation technique, called <strong>sliding concentric windows</strong> (SCW), was proposed for license plate detection. The SCW method was developed to describe the local irregularity in the vehicle image.</p><p>The method uses image statistics such as the standard deviation and the mean for finding possible plate locations.</p><p><img src="/images/slidingwc.png"></p><p>In two concentric windows A and B of different sizes (<span class="math inline">\(2X_1\times 2Y_1\)</span> and <span class="math inline">\(2X_2\times2Y_2\)</span> respectively), which scan the vehicle image from left to right and from top to bottom, the mean or the standard deviation is calculated.</p><p>If the ratio of the statistical measurements in the two windows exceeds a threshold set by the user, then the central pixel of the concentric windows is considered to belong to a license plate. <span class="math display">\[I_{output}=\begin{cases}0 &amp; \text{if }\frac{M_B}{M_A}\leq T,\\1 &amp; \text{if }\frac{M_B}{M_A}&gt; T\end{cases}\]</span> where <span class="math inline">\(M\)</span> is the statistical measurement, eigher mean or standard devation.</p><p>The result is a binary image <span class="math inline">\(I_{output}\)</span>, which eliminates all the redundant regions from the original vehicle image.</p><p>The result binary image is used as a <strong>mask for highlighting the license plate</strong> by computing the product between the binary mask and the input vehicle image. The license plate can then be found in the highlighted image based on the binary mask.</p><p><img src="/images/scwimg.png"></p><h3><span id="adaboost">Adaboost</span></h3><p>Adaptive boosting (AdaBoost) was used in conjunction with the rectangle features for training a strong classifier based on weak classifiers.</p><p>For detecting license plates, a total of 100 rectangle features can be applied to sub-regions sized 45(columns) × 15(rows) pixels being scanned as the expected license plate areas in the original vehicle image.</p><p><img src="/images/recpladaboo.png"></p><p>Within the 100 rectangle features for detection, there are 37 variance based features, 40 x-derivative features, 18 y-derivative features, and 5 mean pixel intensity features.</p><p><img src="/images/plrecoadaboo.png"></p><p>When sliding the search window across the vehicle image to be analyzed, several matches can be found. Clustering method can be used to group detected windows that are close to each other and <strong>use the mean window as the detected location</strong>.</p><h2><span id="character-segmentation">Character Segmentation</span></h2><p>In most systems with a subsequent recognition module, the vertical resolution of the plate vary from 20 to 40 pixels. Prior to character recognition, the detected license plates are enhanced for <strong>improving plate image quality</strong>, e.g., image normalization and histogram equalization.</p><p><img src="/images/characseg.png"></p><p>Given the enhanced detected license plate image, the goal is to <strong>segment each character</strong> in the image. A global threshold can be found to segment the detected license plate. <a href="http://en.wikipedia.org/wiki/Otsu&#39;s_method" target="_blank" rel="noopener">Otsu's method</a> is one of widely used methods for image binarization.</p><p><strong>Otsu's method</strong></p><p>The method is designed for finding optimum global threshold for image binarization and is optimum in the sense that it maximizes the between-class variance.</p><p>There are six steps.</p><ol type="1"><li><p>Compute the normalized histogram of the input image. Denote the components of histogram by <span class="math display">\[p_i=\frac{n_i}{MN}, i=0, 1, ..., L-1\]</span> where <span class="math inline">\(L\)</span> is the number of gray levels; <span class="math inline">\(n_i\)</span> is the number of pixels with intensity <span class="math inline">\(i\)</span>; <span class="math inline">\(M\)</span> is the number of rows; and <span class="math inline">\(N\)</span> is the number of columns.</p></li><li><p>Compute the cumulative sums (the probability that a pixel is assigned to class <span class="math inline">\(C_1\)</span>) <span class="math display">\[P_1(k)=\sum_{i=0}^kp_i\]</span> where <span class="math inline">\(k\)</span> is current threshold for thresholding the input image into two classes <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span>.</p></li><li><p>Compute the cumulative means <span class="math display">\[m(k)=\sum_{i=0}^kip_i,\ k=0,1,...,L-1\]</span></p></li><li><p>Compute the global intensity mean <span class="math display">\[m_G=\sum_{i=0}^{L-1}ip_i\]</span> where <span class="math inline">\(L\)</span> is the number of gray levels.</p></li><li><p>Compute the between-class variance <span class="math display">\[\sigma^2_B(k)=\frac{[m_GP_1(k)-m(k)]^2}{P_1(k)[1-P_1(k)]},\ k=0, 1, ..., L-1\]</span></p></li><li><p>Obtain the Otsu threshold, <span class="math inline">\(k^\ast\)</span>, as the value for <span class="math inline">\(k\)</span> for which the value of <strong>between-class variance is maximum</strong>. If the maximum is not unique, obtain <span class="math inline">\(k^\ast\)</span> by averaging the values of <span class="math inline">\(k\)</span> corresponding to the various maxima detected. <span class="math display">\[\sigma^2_B(k^\ast)=\max_{0\leq k\leq L-2}\sigma^2_B(k)\]</span></p></li></ol><p><img src="/images/otsuresukt.png"></p><p>Global thresholding on the entire image may not always produce useful results due to uneven lighting environment.</p><p><img src="/images/charseg.png"></p><p>Characters can be extracted from the license plate image. Each character can then be segmented by using the thresholding method. Instead of dividing the image into regular blocks, the shape (size) of each block is defined adaptively for each character.</p><p><img src="/images/characsegggg.png"></p><p>Projections of binary edge images are performed. Rows of strings are separated based on the horizontal pixel accumulation. Same for columns of characters.</p><p>After the blocks for characters are defined adaptively, the Otsu's method is applied for each blocks adaptively.</p><p><strong>Maximally stable extremal regions</strong></p><p>Characters can be extracted and segmented by thresholding the image with a variable brightness threshold, and using the enumeration of extremal regions which are stable for a large range of the threshold <span class="math inline">\(T\)</span>.</p><p>Extremal regions are connected components of an image binarized at certain threshold. When the threshold <span class="math inline">\(T\)</span> is increasing/decreasing, the behavior of the extremal regions is used for character classification and segmentation.</p><p><a href="http://en.wikipedia.org/wiki/Maximally_stable_extremal_regions" target="_blank" rel="noopener">Maximally stable extremal regions (MSERs)</a> are usually of arbitrary shape. The MSER detector is stable and invariant to affine transformations, which is useful for handling viewpoint changes.</p><p><img src="/images/msersss.png"></p><p><img src="/images/mseralgo.png"></p><p><img src="/images/mserrrr.png"></p><p><img src="/images/mserapp.png"></p><h2><span id="character-recognition">Character Recognition</span></h2><p>After the characters are segmented, the segmented characters will be matched against a set of pre-defined characters, e.g. ten numerals (zero to nine), alphabets, etc.</p><p>The pre-defined characters usually have single font, fixed character size, and are not rotated heavily. Therefore, pattern/template matching is a suitable technique for character recognition. Templates can be generated in advance for the matching tasks.</p><p><img src="/images/charrecogpatt.png"></p><p>The matching process can be done by computing the <strong>normalized cross-correlation</strong> values for all the translational shifts of each character template over the character block (sub-image).</p><p>The normalized cross-correlation is defined as <span class="math display">\[C_{fg}=\frac{\sum_{m=1}^M\sum_{n=1}^N(f(i,j)-\bar{f})(g(i,j)-\bar{g})}{\sqrt{\sum_{m=1}^M\sum_{n=1}^N(f(i,j)-\bar{f})^2(g(i,j)-\bar{g})^2}}\]</span> where <span class="math inline">\(g\)</span> is shifted template and <span class="math inline">\(f\)</span> is character block.</p><p>More advanced techniques, e.g. <a href="http://en.wikipedia.org/wiki/Shape_context" target="_blank" rel="noopener">shape context</a>, can be used for character recognition. ([Belongie-02, Treiber-10])</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HKUST CSIT5401 Recognition System lecture notes 5. 识别系统复习笔记。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.licenseplaterecognition.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;License Plate Recognition&lt;/a&gt; is an image-processing technology which is used to identify vehicles by their license plates. This technology is used in various security and traffic applications, such as the access-control system, toll payment, parking fee payment, etc.&lt;/p&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://www.52coding.com.cn/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Recognition System" scheme="http://www.52coding.com.cn/tags/Recognition-System/"/>
    
      <category term="Adaboost" scheme="http://www.52coding.com.cn/tags/Adaboost/"/>
    
      <category term="Otsu" scheme="http://www.52coding.com.cn/tags/Otsu/"/>
    
      <category term="MSER" scheme="http://www.52coding.com.cn/tags/MSER/"/>
    
      <category term="sliding concentric windows" scheme="http://www.52coding.com.cn/tags/sliding-concentric-windows/"/>
    
  </entry>
  
  <entry>
    <title>Recognizing Fingerprints</title>
    <link href="http://www.52coding.com.cn/2018/12/07/RS%20-%20Recognizing%20Fingerprints/"/>
    <id>http://www.52coding.com.cn/2018/12/07/RS - Recognizing Fingerprints/</id>
    <published>2018-12-07T13:20:07.000Z</published>
    <updated>2019-04-12T03:27:51.914Z</updated>
    
    <content type="html"><![CDATA[<p>HKUST CSIT5401 Recognition System lecture notes 4. 识别系统复习笔记。</p><!-- toc --><ul><li><a href="#fingerprint-image-acquisition-systems">Fingerprint image acquisition systems</a></li><li><a href="#minutiae">Minutiae</a></li><li><a href="#fingerprint-enhancement">Fingerprint Enhancement</a><ul><li><a href="#normalization">Normalization</a></li><li><a href="#orientation-image-estimation">Orientation Image Estimation</a></li><li><a href="#ridge-frequency-estimation">Ridge Frequency Estimation</a></li><li><a href="#region-mask-estimation">Region mask estimation</a></li><li><a href="#gabor-filter">Gabor Filter*</a></li></ul></li><li><a href="#fingerprint-matching">Fingerprint Matching</a></li><li><a href="#fingercode">FingerCode</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="fingerprint-image-acquisition-systems">Fingerprint image acquisition systems</span></h2><p><a href="http://en.wikipedia.org/wiki/Fingerprint" target="_blank" rel="noopener">Fingerprint</a> matching (recognition) is the most popular biometric technique used in automatic personal identification. The main reason for the popularity of fingerprints as a form of identification is that the fingerprint of a person is unique and remains invariant with his or her age.</p><p>There are three kinds of sensing devices typically: <strong>optical sensors</strong>, <strong>solid-state sensors</strong> and <strong>ultrasound sensors</strong>.</p><p><strong>Optical sensors</strong></p><p>The finger touches the top side of a glass prism. The left side of the prism is illustrated through a diffused light. The light entering the prism is reflected at the valleys, and absorbed at the ridges. The lack of reflection allows the ridges to be discriminated from the valleys. The light rays exit from the right side of the prism and are focused through a lens onto an image sensor.</p><p><img src="/images/ridgeandvall.png"></p><p><strong>Solid-state sensors</strong></p><p>All silicon-based sensors consist of an array of pixels, each pixel being a tiny sensor itself. The user directly touches the surface of the silicon. The physical information is converted into <strong>electrical signals</strong> by using the capacitive sensor (other kinds of sensor can also be used, e.g., thermal, electric field and piezoelectric).</p><p>The capacitive sensor is a 2D array of micro-capacitor plates embedded in a chip. The other plate of each micro-capacitor is the finger skin itself.</p><p><img src="/images/ssdfinger.png"></p><p>Small electrical charges are created between the surface of the finger and each of the silicon plates when a finger is placed on the chip. The magnitude of these electrical charges depends on the distance between the fingerprint surface and the capacitance plates.</p><p><strong>Ultrasound sensors</strong></p><p>An ultrasound sensor is based on sending acoustic signals toward the fingertip and capturing the echo signal. The echo signal is used to compute the range image of the fingerprint and, subsequently, the ridge structure itself.</p><p><img src="/images/ultrasound.png"></p><h2><span id="minutiae">Minutiae</span></h2><p>An <strong>automatic fingerprint identification system</strong> (AFIS) consists of various processing stages.</p><p><img src="/images/afis.png"></p><p>In AFIS, the high-level structural features (<strong>ridges and valleys</strong>) are extracted from the fingerprint image for the purpose of representation and matching. The ridges and valleys in a fingerprint alternate, flowing in a local constant direction.</p><p><img src="/images/ridandval.png"></p><p>The ridges (or the valleys) exhibit anomalies of various kinds, such as ridge bifurcations, ridge endings, short ridges, and ridge crossovers. These features are called <a href="http://en.wikipedia.org/wiki/Minutiae" target="_blank" rel="noopener">minutiae</a>.</p><p>In a good quality rolled fingerprint image, there are about 70 to 80 minutia points and in a latent fingerprint the number of minutiae is much less (approximately 20 to 30 minutia points). Commercially available fingerprint identification systems typically use <strong>ridge bifurcations</strong> and <strong>ridge endings</strong> as features.</p><p><img src="/images/bifandending.png"></p><ul><li>A ridge ending is defined as the point where a ridge ends abruptly.</li><li>A ridge bifurcation is defined as the point where a ridge diverges into branch ridges.</li></ul><p>A critical step in fingerprint matching is to automatically and reliably <strong>extract minutiae</strong> from the input fingerprint images, which is a difficult task.</p><p><img src="/images/bifandend2.png"></p><h2><span id="fingerprint-enhancement">Fingerprint Enhancement</span></h2><p>Fingerprint images can be of very poor quality. An enhancement algorithm which can improve the clarity of the ridge structure is therefore necessary.</p><p>The flowchart of the fingerprint enhancement algorithm is shown below.</p><p><img src="/images/imgnorflow.png"></p><h3><span id="normalization">Normalization</span></h3><p>A gray-level fingerprint image <span class="math inline">\(I\)</span> is defined as an <span class="math inline">\(N \times N\)</span> matrix. At the <span class="math inline">\(i\)</span> th row and <span class="math inline">\(j\)</span> th column, the intensity of the pixel is <span class="math inline">\(I(i, j)\)</span>. It is assumed that the fingerprint images are scanned at a resolution of 500 dots per inch (dpi), which is the resolution recommended by FBI.</p><p>The mean and variance of a gray-level fingerprint image are defined as</p><p><img src="/images/imgnorma.png"></p><p>An input fingerprint image is normalized so that it has a pre-specified mean <span class="math inline">\(M_0\)</span> and variance <span class="math inline">\(\text{VAR}_0\)</span>.</p><p>The normalized image <span class="math inline">\(G(i,j)\)</span> is defined as <span class="math display">\[G(i,j)=\begin{cases}M_0+\sqrt{\frac{VAR_0(I(i,j)-M)^2}{VAR}} &amp;\text{if }I(i,j)&gt;M\\M_0-\sqrt{\frac{VAR_0(I(i,j)-M)^2}{VAR}}&amp;\text{otherwise}\\\end{cases}\]</span> <img src="/images/imgnor2.png"></p><h3><span id="orientation-image-estimation">Orientation Image Estimation</span></h3><p>The orientation image (field) is estimated from the normalized input fingerprint image. By viewing a fingerprint image as an oriented image, a <strong>least-mean-square orientation estimation</strong> algorithm is used to estimate the local orientation.</p><p>The main steps are as follows.</p><p>[1] Divide the normalized image <span class="math inline">\(G\)</span> into blocks of size <span class="math inline">\(w \times w\)</span>.</p><p>[2] For each block, compute the gradients <span class="math inline">\(I_x\)</span> and <span class="math inline">\(I_y\)</span> at each pixel <span class="math inline">\((i , j)\)</span>.</p><p>[3] Estimate the local orientation of each block centered at pixel <span class="math inline">\((i, j)\)</span> using the following equations. <span class="math display">\[\theta(i,j)=90^o+\frac{1}{2}\text{atan2}\left(\frac{V_1(i,j)}{V_2(i,j)}\right)\]</span> where <span class="math display">\[V_1(i,j)=\sum_{u=i-\frac{w}2}^{i+\frac{w}2}\sum_{v=j-\frac{w}2}^{j+\frac{w}2}2I_x(u,v)I_y(i,v)\\V_2(i,j)=\sum_{u=i-\frac{w}2}^{i+\frac{w}2}\sum_{v=j-\frac{w}2}^{j+\frac{w}2}(I_x(u,v)^2-I_y(u,v)^2)\\-180^o≤\text{atan2}(x)≤180^o\]</span> [4] Due to the presence of noise, corrupted ridge and valley structures, minutiae, etc. in the input image, the estimated local ridge orientation may not always be correct. The local orientation image can be smoothed by using the low-pass smoothing filter and the concept of continuous vector field.</p><p><img src="/images/orienesti.png"></p><h3><span id="ridge-frequency-estimation">Ridge Frequency Estimation</span></h3><p>The gray levels along ridges and valleys can be modeled as a sinusoidal-shaped wave along a direction normal to the local ridge orientation.</p><p><img src="/images/ridgefreq.jpg"></p><p>Let <span class="math inline">\(G\)</span> be the normalized image and <span class="math inline">\(O\)</span> be the orientation image (field). For estimating the ridge frequency <span class="math inline">\(Ω\)</span> image,</p><ul><li><p>Step 1: divide <span class="math inline">\(G\)</span> into blocks of size <span class="math inline">\(w \times w\)</span>.</p></li><li><p>Step 2: for each block centered at pixel <span class="math inline">\((i, j)\)</span>, compute an oriented window of size <span class="math inline">\(w \times l\)</span> that is defined in the ridge coordinate system <span class="math inline">\((k, d)\)</span>. <img src="/images/ridgesys.png"></p></li><li><p>Step 3: for each block centered at pixel <span class="math inline">\((i, j)\)</span>, compute the x-signature, <span class="math inline">\(X[0], X[1], ..., X[l-1]\)</span>, of the ridges and valleys within the oriented window, where <span class="math display">\[X[k]=\frac{1}w\sum_{d=0}^{w-1}G(u,v)\\u=i+(d-\frac{w}2)\cos O(i,j)+(k+\frac{l}2)\sin O(i,j)\\v=i+(d-\frac{w}2)\sin O(i,j)+(\frac{l}2-k)\cos O(i,j)\]</span> <img src="/images/w2blabla.png"> <img src="/images/orientationdsa.png"></p></li></ul><p>The x-signature forms a discrete sinusoidal-shape wave, which has the same frequency as that of the ridges and valleys in the oriented window. Therefore, the <strong>frequency of ridges and valleys can be estimated from the x-signature</strong>.</p><p>Let <span class="math inline">\(T(i, j)\)</span> be the average number of pixels between two consecutive peaks in the x-signature, then the ridge frequency <span class="math inline">\(Ω(i, j)\)</span> is computed as <span class="math display">\[\Omega(i,j)=\frac{1}{T(i,j)}\]</span></p><h3><span id="region-mask-estimation">Region mask estimation</span></h3><p>A pixel (or a block) in an input fingerprint image can be either in a recoverable region or an unrecoverable region. Classification of pixels into recoverable and unrecoverable categories can be performed based on the assessment of the shape of the wave formed by the local ridges and valleys.</p><p><img src="/images/imgmaskest.png"></p><p>Three features are used to characterize the sinusoidal-shaped wave: amplitude, frequency, and variance.</p><p><img src="/images/imgmaskest2.png"></p><p>Typical fingerprint images where both recoverable and unrecoverable regions were manually labeled can be selected for region mask estimation. The above three features can be computed for each image.</p><p>Using the k-NN and clustering algorithms, each <span class="math inline">\(w \times w\)</span> block in an input fingerprint image can be classified into a recoverable or an unrecoverable block.</p><h3><span id="gabor-filter">Gabor Filter*</span></h3><p>The configurations of parallel ridges and valleys with well-defined frequency and orientation in a fingerprint image provide useful information which helps in removing undesired noise.</p><p>A special (bandpass) filter, namely <strong>Gabor filter</strong>, that is tuned to the corresponding frequency and orientation can efficiently remove the undesired noise and preserve the true ridge and valley structures.</p><p>Gabor filters have both frequency-selective and orientation-selective properties and are used for removing noise and preserving true ridge or valley structures.</p><p>The even-symmetric Gabor filter has the following general form:</p><p><img src="/images/gaborfilter.png"></p><p>The frequency of the filter <span class="math inline">\(f\)</span> is completely determined by the local ridge frequency and the Gabor filter orientation is determined by the local ridge orientation.</p><p><img src="/images/gabororien.png"></p><p>The ridge pixels are assigned a value '1' (white) and the remaining pixels are assigned a value '0' (black) in the resulting binary ridge image.</p><p><img src="/images/imgseg.png"></p><p>Once the ridges are located, <strong>directional smoothing</strong> is applied to smooth the ridges. A 3×7 mask is placed along the orientation field for each window. The mask containing all '1's enables us to count the number of '1's in the mask area.If the count of '1's is more than 25% of the total number of pixels, the ridge point is retained.</p><p><strong>Extracting minutiae</strong></p><p>Locating minutia points in the thinned image is relatively easy.</p><p>A count of the number of 'on' neighbors at a point of interest in a <span class="math inline">\(3\times 3\)</span> window is sufficient for this purpose.</p><ul><li>A ridge end point has only neighbor in the window.</li><li>A ridge bifurcation has at least three neighbors.</li></ul><p>Some post-processing can be performed to further improve detection quality.</p><p><img src="/images/endandbif3.jpg"></p><p><img src="/images/minuextra.png"></p><h2><span id="fingerprint-matching">Fingerprint Matching</span></h2><p>Matching a query fingerprint and a database fingerprint is equivalent to matching their minutia sets. Each database fingerprint minutia, <span class="math inline">\(p\)</span>, is examined to determine whether there is a corresponding query fingerprint minutia, <span class="math inline">\(q\)</span>.</p><p>There are three steps.</p><ol type="1"><li>Registration</li><li>Minutia paring</li><li>Matching score computation</li></ol><p><strong>Registration via Hough Transform</strong></p><p>The input to the registration algorithm consists of two sets of minutia points <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>. <span class="math inline">\(|P|\)</span> and <span class="math inline">\(|Q|\)</span> represent the sizes of point sets <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> respectively. <span class="math display">\[P=\{(p_x^1,p_y^1,\alpha^1),...,(p_x^{|P|},p_y^{|P|},\alpha^{|P|})\}\\Q=\{(q_x^1,q_y^1,\beta^1),...,(q_x^{|Q|},q_y^{|Q|},\beta^{|Q|})\}\]</span> Each minutia has three components: x-coordinate, y-coordinate and orientation of the minutia. Each minutia in <span class="math inline">\(P\)</span> is <strong>rotated, scaled and translated</strong> for matching against a minutia in <span class="math inline">\(Q\)</span>.</p><p>The usual <strong>Hough transform</strong> for line detection can be generalized for point matching.</p><p><img src="/images/regishoughtra.png"></p><p>The transform has maximum value of <span class="math inline">\(A\)</span> means that it can match as much points as possible.</p><p><img src="/images/imgregriscomp.png"></p><p><strong>Minutia pairing and score computation</strong></p><p>After minutia registration, the minutiae need to be paired. Two minutiae are said to be paired or matched if their components <span class="math inline">\((x, y,θ)\)</span> are equal with some tolerance after registration.</p><p><img src="/images/minutiamatch.png"></p><p>The matching algorithm is based on finding the number of paired minutiae between each database fingerprint and the query fingerprint.</p><p>In order to reduce the amount of computation, the matching algorithm takes into account only those minutiae that fall within a common bounding box. The common bounding box is the intersection of the bounding box for query and reference (database) fingerprints. Once the count of matching minutiae is obtained, a matching score is computed. The matching score is used for deciding the degree of match. Finally, a set of top ten scoring reference fingerprints is obtained as a result of matching.</p><h2><span id="fingercode">FingerCode</span></h2><p><em>FingerCode</em> is a new representation for the fingerprints which yields a <strong>relatively short, fixed length code</strong> suitable for matching as well as storage on a smartcard.</p><p>The matching reduces to finding the <strong>Euclidean distance</strong> between these <em>FingerCodes</em> and hence the matching is very fast and the representation is amenable to indexing.</p><p>The FingerCode partitions the region of interest of the given fingerprint image with respect to a reference point. A feature vector is composed of an ordered enumeration of features extracted from the information contained in each sector specified by the tessellation.</p><p><img src="/images/fingercode1.png"></p><p>The feature elements capture the local information by using the <strong>Gabor filterbank</strong>. The ordered enumeration of the tessellation captures the <strong>invariant global relationships</strong> among the local patterns.</p><p>These features capture both the global pattern of ridges and valleys and the local characteristics. Matching is based on the Euclidean distance between the FingerCodes.</p><p>There are four steps for extracting the FingerCode.</p><ol type="1"><li>Determining the reference point for the fingerprint image.</li><li>Partitioning the region around the reference point.</li><li>Filtering the region of interest in eight different directions using a bank of Gabor filters.</li><li>Computing the <strong>average absolute deviation</strong> (AAD) from the mean of gray values in individual sectors in filtered images to define the <em>FingerCode</em> (the feature vector) for matching.</li></ol><p><strong>Determining the reference point</strong></p><p><img src="/images/fingercode2.png"></p><p>Given an input fingerprint image, there are seven steps for finding the reference point.</p><ol type="1"><li><p>Estimate the orientation field <span class="math inline">\(O\)</span> using a window size of <span class="math inline">\(w\times w\)</span>.</p></li><li><p>Smooth the orientation field.</p></li><li><p>Compute the sine component <span class="math inline">\(E\)</span> of the orientation field <span class="math inline">\(O\)</span>.<br><span class="math display">\[E(i,j)=\sin O(i,j)\]</span></p></li><li><p>Initialize <span class="math inline">\(A\)</span>, a label image used to indicate the reference point.</p></li><li><p>For each pixel <span class="math inline">\(E(i, j)\)</span>, integrate pixel intensities in regions <span class="math inline">\(R_I\)</span> and <span class="math inline">\(R_{II}\)</span>, and assign the corresponding pixels in <span class="math inline">\(A\)</span> according to the value of their difference. <span class="math display">\[A(i,j)=\sum_{R_I}E(i,j)-\sum_{R_{II}}E(i,j)\]</span> <img src="/images/fingercode4.png"></p></li><li><p>Find the maximum value in <span class="math inline">\(A\)</span> and assign its coordinate to the reference point.</p></li><li><p>Repeat steps 1-6 by using a window size <span class="math inline">\(w&#39;\times w&#39;\)</span>, where <span class="math inline">\(w&#39; &lt; w\)</span>, and restrict the search for the reference point in step 6 in a local neighborhood of the detected reference point.</p></li></ol><p>The geometry of regions <span class="math inline">\(R_I\)</span> and <span class="math inline">\(R_{II}\)</span> is designed to capture the maximum curvature in concave ridges.</p><p><img src="/images/fingercode3.png"></p><p><strong>Partitioning the region around the reference point</strong></p><p>Given the detected reference point, the input fingerprint image is partitioned into 80 sectors.</p><p><img src="/images/fingercode5.png"></p><p><strong>Filtering the region of interest</strong></p><p>A minutia point can be viewed as an anomaly in locally parallel ridges and it is the information that is captured by using the Gabor filters.</p><p>Before filtering the fingerprint image, <strong>image normalization</strong> is performed separately for each sector with <span class="math inline">\(M_0\)</span> and <span class="math inline">\(VAR_0\)</span>.</p><p>An even symmetric Gabor filter is given <a href="#gabor-filter*">the Gabor filer setction</a>. The filter frequency <span class="math inline">\(f\)</span> can be set to the average ridge frequency. The average ridge frequency is the reciprocal of the average inter-ridge distance, which is around 10 pixels in a 500 dpi fingerprint image. Eight different values (<span class="math inline">\(0^o\)</span>, <span class="math inline">\(22.5^o\)</span>, <span class="math inline">\(45^o\)</span>, <span class="math inline">\(67.5^o\)</span>, <span class="math inline">\(90^o\)</span>, <span class="math inline">\(112.5^o\)</span>, <span class="math inline">\(135^o\)</span> and <span class="math inline">\(157.5^o\)</span>) are used for the direction θ with respect to the x-axis.</p><p>A fingerprint convolved with a <span class="math inline">\(0^o\)</span>-oriented filter accentuates those ridges which are parallel to the x-axis and smoothes the ridges in the other directions. These eight directional-sensitive filters capture <strong>most of the global ridge directionality information</strong> as well as the <strong>local ridge characteristics</strong> present in a fingerprint.</p><p><img src="/images/fingercode6.png"></p><p><strong>Compute AAD</strong></p><p>Let <span class="math inline">\(F_{iθ}(x, y)\)</span> be the θ-direction filtered image for sector <span class="math inline">\(S_i\)</span>. The feature value <span class="math inline">\(V_{iθ}\)</span> is the average absolute deviation (AAD) from the mean which is defined as <span class="math display">\[V_{i\theta}=\frac{1}{n_i}\sum_{(x,y)\in S_i}|F_{i\theta}(x,y)-P_{i\theta}|\]</span> where <span class="math inline">\(P_{i\theta}=\frac{1}{n_i}\sum_{(x,y)\in S_i}F_{i\theta}(x,y)\)</span>; <span class="math inline">\(n_i\)</span> is the number of pixels in <span class="math inline">\(S_i\)</span>.</p><p><img src="/images/fingercode7.png"></p><p>The average absolute deviation of each sector in each of the eight filtered images defines the components of the feature vector. Fingerprint matching is based on finding the <strong>Euclidean distance</strong> between the corresponding <em>FingerCodes</em>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HKUST CSIT5401 Recognition System lecture notes 4. 识别系统复习笔记。&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#fingerprint-image-acquisition-systems&quot;&gt;Fingerprint image acquisition systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#minutiae&quot;&gt;Minutiae&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#fingerprint-enhancement&quot;&gt;Fingerprint Enhancement&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#normalization&quot;&gt;Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#orientation-image-estimation&quot;&gt;Orientation Image Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#ridge-frequency-estimation&quot;&gt;Ridge Frequency Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#region-mask-estimation&quot;&gt;Region mask estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#gabor-filter&quot;&gt;Gabor Filter*&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#fingerprint-matching&quot;&gt;Fingerprint Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#fingercode&quot;&gt;FingerCode&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://www.52coding.com.cn/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Recognition System" scheme="http://www.52coding.com.cn/tags/Recognition-System/"/>
    
      <category term="fingerprints" scheme="http://www.52coding.com.cn/tags/fingerprints/"/>
    
      <category term="fingercode" scheme="http://www.52coding.com.cn/tags/fingercode/"/>
    
      <category term="hough transform" scheme="http://www.52coding.com.cn/tags/hough-transform/"/>
    
      <category term="minutiae" scheme="http://www.52coding.com.cn/tags/minutiae/"/>
    
  </entry>
  
  <entry>
    <title>Recognizing Faces</title>
    <link href="http://www.52coding.com.cn/2018/12/06/RS%20-%20Recognizing%20Faces/"/>
    <id>http://www.52coding.com.cn/2018/12/06/RS - Recognizing Faces/</id>
    <published>2018-12-06T14:03:09.000Z</published>
    <updated>2019-04-12T03:27:49.612Z</updated>
    
    <content type="html"><![CDATA[<p>HKUST CSIT5401 Recognition System lecture notes 3. 识别系统复习笔记。</p><!-- toc --><ul><li><a href="#histogram-equalization">Histogram Equalization</a></li><li><a href="#image-pyramid-and-neural-networks">Image Pyramid and Neural Networks</a></li><li><a href="#integral-image">Integral Image</a></li><li><a href="#adaboost">Adaboost</a></li><li><a href="#face-recognition-pca">Face Recognition (PCA)</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="histogram-equalization">Histogram Equalization</span></h2><p><a href="http://en.wikipedia.org/wiki/Face_detection" target="_blank" rel="noopener">Face detection</a> is the first step in automated face recognition. Its reliability has a major influence on the performance and usability of the entire face recognition system.</p><p>Due to lighting or shadow, intensity can vary significantly in an image. Normalization of pixel intensity helps correct variations in imaging parameters in cameras as well as changes in illumination conditions. One widely used technique is <a href="http://en.wikipedia.org/wiki/Histogram_equalization" target="_blank" rel="noopener">histogram equalization</a>, which is based on image histogram. It helps reduce extreme illumination.</p><p><strong>Image histogram</strong></p><p>It is assumed that there is a digital image with <span class="math inline">\(L\)</span> gray levels <span class="math inline">\(r_k\)</span>. The probability of occurrence of gray level <span class="math inline">\(r_k\)</span> is given by <span class="math display">\[p_r(r_k)=\frac{n_k}{N}\]</span> where <span class="math inline">\(n_k\)</span> is number of pixels with gray level <span class="math inline">\(r_k\)</span>; <span class="math inline">\(N\)</span> is total number of pixels in an image; <span class="math inline">\(k = 0,1,2,...,L-1\)</span>.</p><p><img src="/images/imagehis.png"></p><p>We want an image with equally many pixels at every gray level, or the output intensity approx follows <strong>uniform distribution</strong>.</p><p>That is, a flat histogram, where each gray level, <span class="math inline">\(r_k\)</span>, appears equal number of times, i.e., <span class="math inline">\(N/L\)</span> times.</p><p><img src="/images/imgequ.png"></p><p>Assume that variable <span class="math inline">\(r\)</span> has been normalized between <span class="math inline">\([0,1]\)</span>. The intensity transformation is <span class="math inline">\(s = T(r)\)</span>, such that</p><ul><li><span class="math inline">\(T(r)\)</span> is single-valued and non-decreasing in the interval <span class="math inline">\(0≤r≤1\)</span>.</li><li><span class="math inline">\(0≤T(r)≤1\)</span> for <span class="math inline">\(0≤r≤1\)</span>.</li></ul><p><strong>Histogram equalization transform</strong></p><p>The intensity transformation is the cumulative distribution function (CDF) of <span class="math inline">\(r\)</span>, which is represented by <span class="math display">\[s=T(r)=\int_0^rp_r(w)dw\]</span> The discrete implementation is given by <span class="math display">\[s_k=T(r_k)=\sum_{j=0}^k\frac{n_j}{N}=\sum_{j=0}^kp_r(r_j)\]</span> where <span class="math inline">\(s_k\)</span> is the <strong>output intensity</strong>; <span class="math inline">\(r_k\)</span> is the input intensity; <span class="math inline">\(n_j\)</span> is the number of pixels with gray level <span class="math inline">\(r_j\)</span>.</p><p>Below are some examples:</p><p><img src="/images/hisequeg.png"></p><p><img src="/images/hisequeg2.png"></p><p>Histogram equalization can significantly improve image appearance</p><ul><li>Automatic</li><li>User doesn’t have to perform windowing</li></ul><p>Nice pre-processing step before face detection</p><ul><li>Account for different lighting conditions</li><li>Account for different camera/device properties</li></ul><p>There are two methods for <strong>face detection</strong>:</p><ol type="1"><li>Method using image pyramid and neural networks [Rowley-Baluja-Kanade-98]</li><li>Method using integral image and AdaBoost learning [Viola-Jones-04]</li></ol><h2><span id="image-pyramid-and-neural-networks">Image Pyramid and Neural Networks</span></h2><p>With the neural networks, a classifier may be trained directly using preprocessed and normalized face and nonface training subwindows.</p><p><a href="http://www.cs.cmu.edu/~har/" target="_blank" rel="noopener">Rowley et al</a> use the preprocessed 20x20 subwindows as the input to a neural network. The final decision is made to classify the 20x20 subwindow into face and nonface. The architecture is shown below.</p><p><img src="/images/facepy.png"></p><p>Instead of upright, frontal faces, a <strong>router network</strong> can be trained to process each input window so that orientation can be estimated. Once the orientation is estimated, the input window can be prepared for detector neural network.</p><p><img src="/images/router.png"></p><p><strong>Rowley et al.</strong> proposed two neural networks, as presented in the previous slides. The first one is the router network which is trained to estimate the orientation of an assumed face in the 20x20 sub-window. The second one is the normal frontal, upright face detector. However, it only handles <strong>in-plane rotation</strong>.</p><p><strong>Huang et al.</strong> proposed a multi-view face tree structure for handling both in-plane and <strong>out-of-plane rotations</strong>. Every node corresponds to a strong classifier.</p><p><img src="/images/hung.png"></p><h2><span id="integral-image">Integral Image</span></h2><p><strong>Method using integral image and AdaBoost learning</strong></p><p>The <a href="http://en.wikipedia.org/wiki/Summed_area_table" target="_blank" rel="noopener">integral image</a> <span class="math inline">\(ii(x, y)\)</span> at location <span class="math inline">\((x, y)\)</span> contains the <strong>sum of the pixel intensity values above and to the left</strong> of the location <span class="math inline">\((x, y)\)</span>, inclusive.</p><p>The <span class="math inline">\(ii\)</span> is defined as <span class="math display">\[ii(x,y)=\sum_{x&#39;≤x,y&#39;≤y}i(x&#39;,y&#39;)\]</span> where <span class="math inline">\(ii(x,y)\)</span> is the integral image and <span class="math inline">\(i(x,y)\)</span> is the original input image.</p><p><img src="/images/integralimg.png"></p><p>Using the following pair of recurrences: <span class="math display">\[s(x, y)=s(x, y-1)+i(x,y)\\ii(x,y)=ii(x-1, y)+s(x,y)\]</span> where <span class="math inline">\(s(x,y)\)</span> is the cumulative row sum, <span class="math inline">\(s(x, -1) = 0\)</span>, and <span class="math inline">\(ii(-1, y)=0\)</span>, the integral image can be computed in one pass over the original image.</p><p>Using the integral image, any rectangular sum can be computed in four array references.</p><p><img src="/images/inteeg.png"></p><p><strong>Rectangle features</strong></p><p>The features for face detection are Haar-like functions. There are three kinds of features.</p><p>[1] Two-rectangle feature: The difference between the sum of the pixels within two rectangular regions.</p><p><img src="/images/recfea1.png"></p><p>[2] Three-rectangle feature: The feature is the sum within two outside rectangles subtracted from the sum in a center rectangle.</p><p><img src="/images/recfea2.png"></p><p>[3] Four-rectangle feature: The difference between diagonal pairs of rectangles.</p><p><img src="/images/recfea3.png"></p><p>The rectangle features are sensitive to the presence of edges, bars/lines, and other simple image structures in different scales and at different locations.</p><p>Given that the base resolution of the detector is 24 x 24 pixels, the exhaustive set of rectangle features is quite large, 160,000.</p><p>Given a feature set and a training set of positive and negative images, a classification function must be learned to classify a pattern into either face or non-face.</p><h2><span id="adaboost">Adaboost</span></h2><p>In this work, the classifier is designed based on the assumption that a very small number of features can be combined to form an effective classifier.</p><p>The <a href="http://en.wikipedia.org/wiki/AdaBoost" target="_blank" rel="noopener">AdaBoost</a> learning algorithm is used to boost the classification performance of a simple learning algorithm. The simple learning algorithm is applied to all rectangle features.</p><p>It does this by <strong>combining a collection of weak classification functions</strong> (weak classifiers with relatively high classification error) to form a stronger classifier. The final strong classifier takes the form of <strong>a weighted combination of weak classifiers followed by a threshold</strong>.</p><p>Weak classifier <span class="math inline">\(h_t\)</span> (each classifier compute one rectangle feature): <span class="math display">\[h_t(\vec{x})=\begin{cases}1\ \text{if }\vec{x}\text{ represents a face image }(f_t(\vec{x})&gt;\text{Threshold})\\-1\ \text{otherwise}\end{cases}\\f_t(\vec{x})=\sum_{white} x-\sum_{black} x\]</span> The strong classifier is <span class="math display">\[H(\vec{x})=\text{sgn}\left(\sum_{t=1}^T\alpha_th_t(\vec{x})\right)\]</span> where <span class="math inline">\(\alpha_t\)</span> is weight; and <span class="math inline">\(\text{sgn}(x)\)</span> is sign function: <span class="math display">\[\text{sgn}(x)=\begin{cases} -1,  &amp; \mbox{if }x≤0 \\1, &amp; \mbox{if }x&gt;0\end{cases}\]</span> <strong>Algorithm</strong></p><p>Given example images and classifications <span class="math inline">\((\vec{x}_i, y_i), i = 1, 2,..., N\)</span>, where <span class="math inline">\(N\)</span> is the total number of images.</p><p>Start with equal weights on each image <span class="math inline">\(\vec{x}_i\)</span>.</p><p>For <span class="math inline">\(t=1, ..., T\)</span>:</p><ul><li><p>Normalize all weights <span class="math inline">\(w_i = \frac{w_i}{\sum_{j=1}^Nw_j}\)</span> such that <span class="math inline">\(\sum_{i=1}^Nw_i=1\)</span>.</p></li><li><p>Select the weak classifier <span class="math inline">\(h_k\)</span> with minimum error: <span class="math display">\[e_k=\sum_{i=1}^Nw_i\left(\frac{1-h_k(\vec{x}_i)y_i}{2}\right)\]</span> where <span class="math inline">\(0≤e_k≤1\)</span>.</p></li><li><p>Set weight for selected weak classifier <span class="math display">\[\alpha_t=\frac{1}{2}\ln\left(\frac{1-e_k}{e_k}\right)\]</span></p></li><li><p>Reweight the examples (boosting) <span class="math display">\[w_i=w_i\exp(-\alpha_iy_ih_k(\vec{x}_i))\]</span></p></li></ul><p>For the last step, if the weak classifier classify example <span class="math inline">\(i\)</span> correctly, i.e. <span class="math inline">\(h_k(\vec{x}_i)=y_i\)</span>, then the example weight <span class="math inline">\(w_i=w_ie^{-\alpha_t}\)</span> will decrease; if the weak classifier classify example <span class="math inline">\(i\)</span> wrongly, the weight <span class="math inline">\(w_i=w_i^{\alpha_t}\)</span> will increase.</p><p>Values of <span class="math inline">\(T\)</span> can be 200 for <span class="math inline">\(N=10^8\)</span> images and 180,000 filters. Given the above strong classifier, a new image can classified as either face or non-face.</p><h2><span id="face-recognition-pca">Face Recognition (PCA)</span></h2><p>Images of faces often belong to a manifold of intrinsically low dimension. For example, if there are three 3x1 images (see below), then each image has three intensity values. If each intensity value is viewed as a coordinate in a 3D space, then each image can be viewed as a point in a 3D space.</p><p><img src="/images/imgspace.png"></p><p>To represent these points effectively, the number of dimensions can be reduced from three to one. It is the concept of <a href="http://en.wikipedia.org/wiki/Dimension_reduction" target="_blank" rel="noopener">dimensionality reduction</a>.</p><p><a href="http://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="noopener">Principal component analysis</a> (PCA) is a method for performing dimensionality reduction of high dimensional face images.</p><p><strong>Eigenfaces</strong></p><p>Let us consider a set of <span class="math inline">\(N\)</span> sample images (image vectors) with <span class="math inline">\(m\times n\)</span> dimensions:</p><p><img src="/images/eigenface1.png"></p><p>Each image is represented by a 1D vector with dimensions <span class="math inline">\((m\times n) \times 1\)</span>. The <strong>mean image vector</strong> is given by <span class="math display">\[\vec{x}=\frac{1}{N}\sum_{i=1}^N\begin{bmatrix}x_{i,1}      \\\vdots \\x_{i,mn}\end{bmatrix}\]</span> The <strong>scatter matrix</strong> is given by <span class="math display">\[\vec{S}=[\vec{x_1}-\bar{x}\ \ \vec{x_2}-\bar{x}\ \dots\ \vec{x_N}-\bar{x}]\begin{bmatrix}(\vec{x_1}-\bar{x})^T     \\(\vec{x_2}-\bar{x})^T\\\vdots \\(\vec{x_N}-\bar{x})^T\end{bmatrix}\]</span> The corresponding <span class="math inline">\(t\)</span> eigenvectors with non-zero eigenvalues <span class="math inline">\(\lambda_i\)</span> are <span class="math display">\[\vec{e}_1\ \ \vec{e}_2\ \ \dots\ \ \vec{e}_t\]</span> where <span class="math inline">\(\lambda_1≥\lambda_2≥...≥\lambda_t\)</span>.</p><p>Then the origin image vector can be approximated by <span class="math display">\[\vec{x}_j\approx\bar{x}+\sum_{i=1}^tg_{ji}\vec{e}_i\]</span> where <span class="math inline">\(g_{ji}=(\vec{x}_j-\bar{x})\cdot\vec{e}_i\)</span>.</p><p><img src="/images/egface.png"></p><p>Since the eigenvectors <span class="math inline">\(e\)</span> have the same dimension as the image vectors, the eigenvectors are referred as <a href="http://en.wikipedia.org/wiki/Eigenface" target="_blank" rel="noopener">Eigenfaces</a>. The value of <span class="math inline">\(t\)</span> is usually much smaller than the value of <span class="math inline">\(mn\)</span>. Therefore, the number of dimensions can be reduced significantly.</p><p>For each image <span class="math inline">\(\vec{x}_i\)</span>, the dimension reduced representation is <span class="math display">\[(g_{i1}, g_{i2}, ..., g_{it})\]</span> To detect if the new image <span class="math inline">\(\vec{x}\)</span> with <span class="math inline">\(t\)</span> coefficients <span class="math inline">\((g_1, g_2, ..., g_t)\)</span> is a face: <span class="math display">\[||\vec{x}-(\bar{x}+g_1\vec{e}_1+g_2\vec{e}_2+...+g_t\vec{e}_t)||&lt;\text{Threshold}\]</span> If it is a face, find the closest labeled face based on the nearest neighbor in the <span class="math inline">\(t\)</span>-dimensional space.</p><p><strong>Near-infrared images for face recognition</strong></p><p>Most current face recognition systems are based on face images captured in the visible light spectrum. The infrared imaging system is able to produce face images of good condition regardless of visible lights in the environment.</p><p><img src="/images/infared.png"></p><p><img src="/images/infeared2.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HKUST CSIT5401 Recognition System lecture notes 3. 识别系统复习笔记。&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#histogram-equalization&quot;&gt;Histogram Equalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#image-pyramid-and-neural-networks&quot;&gt;Image Pyramid and Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#integral-image&quot;&gt;Integral Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#adaboost&quot;&gt;Adaboost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#face-recognition-pca&quot;&gt;Face Recognition (PCA)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://www.52coding.com.cn/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="PCA" scheme="http://www.52coding.com.cn/tags/PCA/"/>
    
      <category term="Recognition System" scheme="http://www.52coding.com.cn/tags/Recognition-System/"/>
    
      <category term="Face Recognition" scheme="http://www.52coding.com.cn/tags/Face-Recognition/"/>
    
      <category term="histogram equalization" scheme="http://www.52coding.com.cn/tags/histogram-equalization/"/>
    
      <category term="Adaboost" scheme="http://www.52coding.com.cn/tags/Adaboost/"/>
    
  </entry>
  
  <entry>
    <title>Recognizing Irises</title>
    <link href="http://www.52coding.com.cn/2018/12/05/RS%20-%20Recognizing%20Irises/"/>
    <id>http://www.52coding.com.cn/2018/12/05/RS - Recognizing Irises/</id>
    <published>2018-12-05T13:56:09.000Z</published>
    <updated>2019-04-12T03:27:57.922Z</updated>
    
    <content type="html"><![CDATA[<p>HKUST CSIT5401 Recognition System lecture notes 2. 识别系统复习笔记。</p><!-- toc --><ul><li><a href="#introduction">Introduction</a></li><li><a href="#image-acquisition-systems">Image Acquisition Systems</a></li><li><a href="#iris-localization">Iris localization</a></li><li><a href="#pattern-matching">Pattern Matching</a><ul><li><a href="#alignment-registration">Alignment (Registration)</a></li><li><a href="#representation">Representation</a></li><li><a href="#goodness-of-match">Goodness of Match</a></li><li><a href="#decision-fld">Decision (FLD)</a></li></ul></li><li><a href="#hough-transform">Hough Transform</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="introduction">Introduction</span></h2><p>Face recognition and iris recognition are non-invasive method for verification and identification of people. In particular, the spatial patterns that are apparent in the human iris are highly distinctive to an individual.</p><p><img src="/images/iries.png"></p><p><strong>Schematic diagram of iris recognition</strong></p><p><img src="/images/iries_recog.png"></p><h2><span id="image-acquisition-systems">Image Acquisition Systems</span></h2><p>One of the major challenges of automated iris recognition is to capture a high-quality image of the iris while remaining non-invasive to the human operator.</p><p>There are three concerns while acquiring iris images:</p><ul><li>To support recognition, it is desirable to acquire images of the iris with sufficient resolution and sharpness.</li><li>It is important to have good contrast in the interior iris pattern without resorting to a level of illumination that annoys the operator, i.e., adequate intensity of source constrained by operator comfort with brightness.</li><li>These images must be well framed (i.e., centered) without unduly constraining the operator.</li></ul><p><strong>The Daugman system</strong></p><p>The Daugman system captures images with the iris diameter typically between 100 and 200 pixels from a distance of 15-46cm.</p><p>The system makes use of an LED-based point light source in conjunction with a standard video. By carefully positioning of the point source below the operator, reflections of the light source off eyeglasses can be avoided in the imaged iris.</p><p><img src="/images/daugman.png"></p><p>The Daugman system provides the operator with live video feedback via a tiny liquidcrystal display placed in line with the camera's optics via a beam splitter. This allows the operator to see what the camera is capturing and to adjust his position accordingly.</p><p><strong>The Wildes system</strong></p><p>The Wildes system images the iris with approximately 256 pixels across the diameter from 20cm. The system makes use of a diffused source and polarization in conjunction with a low-light level camera.</p><p>The use of matched <strong>circular polarizer</strong> at the light source and camera essentially eliminates the specular reflection of the light source.</p><p><img src="/images/wildes.png"></p><p>The coupling of a low light level camera with a diffused illumination allows for a level of illumination that is entirely unobjectionable to human operators.</p><p>The relative sizes and positions of the square contours are chosen so that when the eye is in an appropriate position, the squares overlap and appear as one to the operator.</p><h2><span id="iris-localization">Iris localization</span></h2><p><img src="/images/iries_loc.png"></p><p>Image acquisition will capture the iris as part of a larger image that also contains data derived from the immediately surrounding eye region. For example, eyelashes, upper eyelid, lower eyelid and sclera. Therefore, prior to performing iris pattern matching, it is important to <strong>localize</strong> that portion of the acquired image that corresponds to an iris.</p><p><strong>The Wildes system</strong> makes use of the <strong>first derivatives</strong> of image intensity to signal the location of edges that correspond to the borders of the iris.</p><ul><li>Step 1: The image intensity information is converted into binary edge-map.</li><li>Step 2: The edge points vote to particular contour parameter values.</li></ul><p><strong>Step 1</strong></p><p>The edge map is recovered via <strong>gradient-based edge detection</strong>. This operation consists of thresholding the magnitude of the image intensity gradient magnitude. <span class="math inline">\(I\)</span> is the intensity and (x, y) are the image coordinates. <span class="math display">\[\text{Gradient magnitude }|\triangledown G(x, y)\ast I(x, y)|\\\text{2D Gaussian function } G(x, y)=\frac{1}{2\pi\sigma^2}\exp(\frac{(x-x_0)^2+(y-y_0)^2}{2\sigma^2})\]</span> <img src="/images/iris_edge.png"></p><p><strong>Step 2</strong></p><p>The voting procedure is realized via the <a href="#hough-transform">Hough transform</a>. For circular limbic or pupillary boundaries and a set of recovered edge points, a Hough transform is defined as follows.</p><p>Edge points <span class="math inline">\((x_j, y_j)\)</span> for <span class="math inline">\(j = 1, ..., n\)</span>: <span class="math display">\[H(x_c, y_c, r)=\sum_{j=1}^nh(x_j,y_j,x_c,y_c,r)\]</span> where <span class="math display">\[h(x_j, y_j, x_c, y_c, r)=\begin{cases}1, \text{ if }\ g(x_j, y_j, x_c, y_c, r)=0\\0, \text{ otherwise}\end{cases}\\g(x_j, y_j, x_c, y_c, r)=(x_j-x_c)^2+(y_j-y_c)^2-r^2\]</span> For every parameter triple <span class="math inline">\((x_c, y_c, r)\)</span> that represents a circle through the edge point <span class="math inline">\((x_j, y_j)\)</span>, <span class="math display">\[g(x_j, y_j, x_c, y_c, r)=0\]</span> <img src="/images/wildescir.png"></p><p>The parameter triple that <strong>maximizes</strong> the Hough space <span class="math inline">\(H\)</span> is common to the largest number of edge points and is a reasonable choice to represent the contour of interest.</p><hr><p>The limbus and pupil are modeled with <strong>circular contour models</strong>.</p><p><strong>The Daugman system</strong> fits the <strong>circular contours</strong> via gradient ascent on the parameters so as to maximize <span class="math display">\[\left|\frac{\partial}{\partial r}G(r)\ast\oint_{x_c,y_c,r}\frac{I(x,y)}{2\pi r}ds\right|\]</span> where <span class="math inline">\(G(r)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(r-r_0)^2}{2\sigma^2})\)</span>; <span class="math inline">\(r_0\)</span> is the center.</p><p>The first part of the equation is to perform Gaussian smoothing; while the second part is computing the average intensity along the circle.</p><p><img src="/images/IMG_66247EE67551-1.jpg"></p><p>In order to incorporate directional tuning of the image derivative, the arc of integration <span class="math inline">\(ds\)</span> is restricted to the left and right quadrants (i.e., near vertical edges) when fitting the <em>limbic boundary</em>.</p><p>This arc is considered over a fuller range when fitting the <em>pupillary boundary</em>.</p><h2><span id="pattern-matching">Pattern Matching</span></h2><p>Having localized the region of an acquired image that corresponds to the iris, the final task is to decide if this pattern matches a previously stored iris pattern.</p><p>There are four steps:</p><ol type="1"><li>Alignment: bringing the newly acquired iris pattern into spatial alignment with a candidate data base entry.</li><li>Representation: choosing a representation of the aligned iris patterns that makes their distinctive patterns apparent.</li><li>Goodness of Match: evaluating the goodness of match between the newly acquired and data base representations.</li><li>Decision: deciding if the newly acquired data and the data base entry were derived from the same iris based on the goodness of match.</li></ol><h3><span id="alignment-registration">Alignment (Registration)</span></h3><p>To make a detailed comparison between two images, it is advantageous to establish a precise correspondence (or matching) between characteristic structures across the pair.</p><p>Both systems (Daugman and Wildes systems) compensate for image shift, scaling and rotation.</p><p><strong>The Daugman system for alignment</strong></p><p>The Daugman system uses <strong>radial scaling</strong> to compensate for overall size as well as a simple model pupil variation based on <strong>linear stretching</strong>.</p><p>The system maps the Cartesian image coordinates <span class="math inline">\((x, y)\)</span> to dimensionless polar image coordinates <span class="math inline">\((r, θ)\)</span> according to <span class="math display">\[x(r,\theta)=(1-r)x_p(0,\theta)+rx_l(1,\theta)\\y(r,\theta)=(1-r)y_p(0,\theta)+ry_l(1,\theta)\]</span> <img src="/images/daugalign.png"></p><p><img src="/images/daugali.png"></p><p><strong>The Wildes system for alignment</strong></p><p>The Wildes system uses an <strong>image-registration</strong> technique to compensate for both scaling and rotation.</p><p>This approach geometrically warps a newly acquired image <span class="math inline">\(I_a (x, y)\)</span> into alignment with a selected data base image <span class="math inline">\(I_d (x, y)\)</span> according to a mapping function <span class="math inline">\((u(x, y), v(x, y))\)</span> such that for all <span class="math inline">\((x, y)\)</span>, the image intensity value at <span class="math inline">\((x, y) – (u(x, y), v(x, y))\)</span> is close to that at <span class="math inline">\((x, y)\)</span> at <span class="math inline">\(I_d\)</span>.</p><p>The mapping function is taken to minimize <span class="math display">\[\int_x\int_y\left(I_d(x,y)-I_a(x-u, y-v)\right)^2dxdy\]</span> under the constrains to capture similarity transformation of image coordinates <span class="math inline">\((x,y)\)</span> to <span class="math inline">\((x&#39;=x-u, y&#39;=y-v)\)</span>.</p><p><img src="/images/imgreg.jpg"></p><p><strong>Translation</strong> <span class="math display">\[\vec{x}&#39;=\vec{x}+\vec{d}\]</span> <img src="/images/imgtrans.png"></p><p><strong>Rotation</strong> <span class="math display">\[\vec{x}&#39;=R_\theta\vec{x}\\R_\theta=\begin{pmatrix}\cos\theta &amp; -\sin\theta \\\sin\theta &amp; \cos\theta\end{pmatrix}\]</span> <img src="/images/imgrot.png"></p><p><strong>Rotation + Translation</strong> <span class="math display">\[\vec{x}&#39;=R\vec{x}+\vec{d}\]</span> <strong>Scaling + Translation</strong> <span class="math display">\[\vec{x}&#39;=S\vec{x}+\vec{d}\]</span> <img src="/images/scatra.png"></p><p><strong>Shearing</strong> <span class="math display">\[\vec{x}&#39;=K\vec{x}\\K=\begin{bmatrix}1      &amp; k_{xy}     \\k_{yx}     &amp; 1\end{bmatrix}\]</span> <img src="/images/shearing.png"></p><p><strong>Affine</strong>: translation + rotation + scaling + shearing <span class="math display">\[\vec{x}&#39;=R_\theta S K\vec{x}+\vec{d}\]</span> Example: <span class="math display">\[\begin{bmatrix}x&#39; \\y&#39;\end{bmatrix}=\begin{bmatrix}\cos\theta &amp; -\sin\theta \\\sin\theta &amp; \cos\theta\end{bmatrix}\begin{bmatrix}s_x &amp; 0 \\0 &amp; s_y\end{bmatrix}\begin{bmatrix}1 &amp; k_{xy} \\k_{yx} &amp; 1\end{bmatrix}\begin{bmatrix}x \\y\end{bmatrix}+\begin{bmatrix}d_x \\d_y\end{bmatrix}\]</span></p><h3><span id="representation">Representation</span></h3><p>To represent the iris image for matching, both the Daugman and Wildes systems capture the multiscale information extracted from the image.</p><p>The Wildes system makes use of the Laplacian of Gaussian filters to construct a <strong>Laplacian pyramid</strong>.</p><p>The Laplacian of Gaussian (LoG) filter is given by <span class="math display">\[-\frac{1}{\pi\sigma^4}\left(1-\frac{\rho^2}{2\sigma^2}\right)\exp(-\frac{\rho^2}{2\sigma^2})\]</span> where <span class="math inline">\(\rho\)</span> is radial distance of a point from the filter's center; <span class="math inline">\(\sigma\)</span> is standard deviation.</p><p>A Laplacian pyramid is formed by collecting the LoG filtered images.</p><p><img src="/images/logpra.png"></p><h3><span id="goodness-of-match">Goodness of Match</span></h3><p>The Wildes system uses the <strong>normalized correlation</strong> between the acquired representation and data base representation. In discrete form, the normalized correlation can be defined as follows.</p><p>Let <span class="math inline">\(p_1[i, j]\)</span> and <span class="math inline">\(p_2[i, j]\)</span> be two image arrays of size <span class="math inline">\(n \times m\)</span>.</p><p><img src="/images/gom.png"></p><p>The normal correlation is <span class="math display">\[NC=\frac{\sum_{i=1}^n\sum_{j=1}^m(p_1[i,j]-\mu_1)(p_2[i,j]-\mu_2)}{nm\sigma_1\sigma_2}\]</span> <img src="/images/gompy.png"></p><h3><span id="decision-fld">Decision (FLD)</span></h3><p>The Wildes system combines four estimated normalized correlation values into a single <strong>accept/reject</strong> judgment.</p><p>In this application, the concept of <a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis" target="_blank" rel="noopener">Fisher's linear discriminant</a> is used for making binary decision. A <strong>weight vector</strong> is found such that <strong>the variance within a class of iris data is minimized</strong> while <strong>the variance between different classes of iris data is maximized</strong> for the transformed samples.</p><p>In iris recognition application, usually there are two classes: <strong>Authentic class (A)</strong> and <strong>Imposter class (I)</strong>.</p><p>To make a binary decision on a line, all points are projected onto the weight vector (or samples are transformed by using the weight vector).</p><p><img src="/images/fld1.png"></p><p>In iris recognition using the Wildes system, all samples are 4-dimensional vectors. Let there be n 4-dimensional samples.</p><p><img src="/images/fld2.png"></p><p>The total within class variance is <span class="math display">\[\vec{S}_w=\vec{S}_i+\vec{S}_a\]</span> Between class variance is <span class="math display">\[\vec{S}_b=(\vec{\mu}_a-\vec{\mu}_i)(\vec{\mu}_a-\vec{\mu}_i)^T\]</span> If all samples are transformed, the ratio of between class variance to total within class variance is <span class="math display">\[\frac{\vec{w}^T\vec{S}_b\vec{w}}{\vec{w}^T\vec{S}_w\vec{w}}\]</span> The ratio is maximized when <span class="math display">\[\vec{w}=\vec{S}_w^{-1}(\vec{\mu}_a-\vec{\mu}_i)\]</span> And the separation point for decision making is <span class="math display">\[\frac{1}{2}\vec{w}^T(\vec{\mu}_a+\vec{\mu}_i)\]</span> Therefore, values above this point will be taken as derived from class <span class="math inline">\(A\)</span>; values below this point will be taken as derived from class <span class="math inline">\(I\)</span>.</p><p><img src="/images/fld3.png"></p><h2><span id="hough-transform">Hough Transform</span></h2><p><strong>Detecting Lines</strong></p><p>Idea: if two edge points <span class="math inline">\((x_i, y_i)\)</span> and <span class="math inline">\((x_j, y_j)\)</span> lie on the same straight line, then they should have the same values of slope and y-intercepts on the xy-plane.</p><p><img src="/images/houghline.png"></p><p><strong>[1]</strong> For a point <span class="math inline">\((x_i,y_i)\)</span>, we set up a straight line equation: <span class="math display">\[y_i=ax_i+b\Leftrightarrow b=(-x_i)a+y_i\]</span> where <span class="math inline">\(a\)</span> = slope, <span class="math inline">\(b\)</span> = y-intercept, <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> are known and fixed.</p><p><strong>[2]</strong> We subdivide the a axis into <span class="math inline">\(K\)</span> increments between <span class="math inline">\([a_\min,a_\max]\)</span>. For each increment of <span class="math inline">\(a\)</span>, we evaluate the value of <span class="math inline">\(b\)</span>.</p><p><strong>[3]</strong> A relationship between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> can be plotted in a parameter space, i.e., ab-plane.</p><p><strong>[4]</strong> We partition the parameter space into a number of bins (accumulator cells), and increment the corresponding bin <span class="math inline">\(A(a,b)\)</span> by 1 (<span class="math inline">\(b\)</span> is rounded into the nearest integer).</p><p><img src="/images/houghbin.png"></p><p><strong>[5]</strong> For another point <span class="math inline">\((x_j,y_j)\)</span>, we set up another straight line equation: <span class="math display">\[y_j=ax_j+b\Leftrightarrow b=(-x_j)a+y_j\]</span> <strong>[6]</strong> Similarly, we subdivide the a axis into K increments between <span class="math inline">\([a_\min,a_\max]\)</span>. For each increment of <span class="math inline">\(a\)</span>, we evaluate the value of <span class="math inline">\(b\)</span>. We plot the relationship between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> in the same parameter space, and update bin values in the discrete parameter space.</p><p><strong>[7]</strong> The bin <span class="math inline">\(A(a,b)\)</span> having the highest count corresponds to the straight line passing through the points <span class="math inline">\((x_i,y_i)\)</span> and <span class="math inline">\((x_j,y_j)\)</span>.</p><p><img src="/images/hough2.png"></p><p><strong>[8]</strong> The same procedure can be applied to all points. The bin <span class="math inline">\(A(a,b)\)</span> having the <strong>highest count</strong> corresponds to the straight line passing through (or passing near) the largest number of points.</p><p>Problem: Values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> run from negative infinity to positive infinity. We need infinite number of bins!</p><p>Solution: use normal representation of a line: <span class="math display">\[x\cos(\theta)+y\sin(\theta)=\rho\]</span> <img src="/images/houghnormal.png"></p><p><span class="math inline">\(\theta\)</span> runs from <span class="math inline">\(–90^o\)</span> to <span class="math inline">\(90^o\)</span>. <span class="math inline">\(\rho\)</span> runs from <span class="math inline">\(-\sqrt{2}D\)</span> to <span class="math inline">\(\sqrt{2}D\)</span>, where <span class="math inline">\(D\)</span> is the distance between corners in the image (length and width).</p><p><strong>Circle Hough Transform (CHT)</strong></p><p>The Hough transform can be used to determine the parameters of a circle when a number of points that fall on the perimeter are known. A circle with radius <span class="math inline">\(R\)</span> and center <span class="math inline">\((a, b)\)</span> can be described with the parametric equations: <span class="math display">\[x=a+R\cos(\theta)\\y=b+R\sin(\theta)\]</span> When the angle <span class="math inline">\(θ\)</span> sweeps through the full 360 degree range the points <span class="math inline">\((x, y)\)</span> trace the perimeter of a circle.</p><p>If the circles in an image are of <strong>known radius</strong> <span class="math inline">\(R\)</span>, then the search can be reduced to 2D. The objective is to find the <span class="math inline">\((a, b)\)</span> coordinates of the centers.</p><p><img src="/images/cht1.png"></p><p>If the <strong>radius is not known</strong>, then the locus of points in parameter space will fall on the surface of a <strong>cone</strong>. Each point <span class="math inline">\((x, y)\)</span> on the perimeter of a circle will produce a cone surface in parameter space. The triplet <span class="math inline">\((a, b, R)\)</span> will correspond to the accumulation cell where the largest number of cone surfaces intersect.</p><p><img src="/images/cone.png">The drawing above illustrates the generation of a conical surface in parameter space for one <span class="math inline">\((x, y)\)</span> point. A circle with a different radius will be constructed at each level, <span class="math inline">\(r\)</span>.</p><p>The search for circles with unknown radius can be conducted by using a three dimensional accumulation matrix.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HKUST CSIT5401 Recognition System lecture notes 2. 识别系统复习笔记。&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#image-acquisition-systems&quot;&gt;Image Acquisition Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#iris-localization&quot;&gt;Iris localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#pattern-matching&quot;&gt;Pattern Matching&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#alignment-registration&quot;&gt;Alignment (Registration)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#representation&quot;&gt;Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#goodness-of-match&quot;&gt;Goodness of Match&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#decision-fld&quot;&gt;Decision (FLD)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#hough-transform&quot;&gt;Hough Transform&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://www.52coding.com.cn/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Recognition System" scheme="http://www.52coding.com.cn/tags/Recognition-System/"/>
    
      <category term="Iris" scheme="http://www.52coding.com.cn/tags/Iris/"/>
    
      <category term="FLD" scheme="http://www.52coding.com.cn/tags/FLD/"/>
    
      <category term="Daugman" scheme="http://www.52coding.com.cn/tags/Daugman/"/>
    
      <category term="Wildes" scheme="http://www.52coding.com.cn/tags/Wildes/"/>
    
      <category term="Hough Transform" scheme="http://www.52coding.com.cn/tags/Hough-Transform/"/>
    
  </entry>
  
  <entry>
    <title>Recognizing Image Features and Patterns</title>
    <link href="http://www.52coding.com.cn/2018/12/04/RS%20-%20Recognizing%20Image%20Features%20and%20Patterns/"/>
    <id>http://www.52coding.com.cn/2018/12/04/RS - Recognizing Image Features and Patterns/</id>
    <published>2018-12-04T13:56:09.000Z</published>
    <updated>2019-04-12T03:27:54.554Z</updated>
    
    <content type="html"><![CDATA[<p>HKUST CSIT5401 Recognition System lecture notes 1. 识别系统复习笔记。</p><!-- toc --><ul><li><a href="#laplacian-point-detector">Laplacian Point Detector</a></li><li><a href="#line-detector">Line Detector</a></li><li><a href="#edge-detectors">Edge Detectors</a><ul><li><a href="#gradient-operator">Gradient Operator</a></li><li><a href="#marr-hildreth-edge-detector">Marr-Hildreth Edge Detector</a></li></ul></li><li><a href="#scale-invariant-feature-transform-sift">Scale Invariant Feature Transform (SIFT)</a></li><li><a href="#harris-corner-detector">Harris Corner Detector</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="laplacian-point-detector">Laplacian Point Detector</span></h2><p>There are three different types of intensity discontinuities in a digital image:</p><ul><li>Point (Isolated Point)</li><li>Line</li><li>Edge (Ideal, Ramp and Roof)</li></ul><p>Intensity discountinuity is detected based on the mask response <span class="math inline">\(R\)</span> within a pre-defined window, e.g. <span class="math inline">\(3\times3\)</span>: <span class="math display">\[R=\sum_{i=1}^9w_iz_i\]</span> where <span class="math inline">\(w_i\)</span> represent weights within a pre-defined window; <span class="math inline">\(z_i\)</span> represent intensity values.</p><p>If <span class="math inline">\(|R|≥T\)</span> , then a point has been detected. This point is the location on which the mask is <strong>centred</strong>, where <span class="math inline">\(T\)</span> is a non-negative threshold.</p><p>The mask below is the <strong>Laplacian mask</strong> for detecting point. Sum of all weights is zero to make sure that there is no response at a flat region (constant intensity region).</p><table><thead><tr class="header"><th style="text-align: center;">1</th><th style="text-align: center;">1</th><th style="text-align: center;">1</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">1</td><td style="text-align: center;">-8</td><td style="text-align: center;">1</td></tr><tr class="even"><td style="text-align: center;">1</td><td style="text-align: center;">1</td><td style="text-align: center;">1</td></tr></tbody></table><p>The Laplacian is given as (<span class="math inline">\(f\)</span> is input image): <span class="math display">\[\triangledown^2f(x,y)=\frac{\partial^2f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}\]</span> where <span class="math display">\[\frac{\partial^2f}{\partial x^2}=f(x+1,y)-2f(x,y)+f(x-1,y)\\\frac{\partial^2 f}{\partial y^2}=f(x,y+1)-2f(x,y)+f(x,y-1)\]</span> The discrete implementation of the Laplacian operator is given as: <span class="math display">\[\triangledown^2f(x,y)=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)\]</span> Below are several Laplacian masks:</p><p><img src="/images/pd.png"></p><h2><span id="line-detector">Line Detector</span></h2><p>A line is detected when more than one aligned, connected points are detected or, the <strong>response</strong> of line mask <strong>is greater than some threshold</strong>. The below are line masks for detecting lines (1 pixel thick) in 4 different specific directions:</p><p><img src="/images/ld.png"></p><p>If we want to detect a line in a specified direction, then we should use the mask associated with that direction and threshold its output responses.</p><p>If 4 line masks are used, then the final response is equal to the <strong>largest</strong> response among the masks: <span class="math display">\[R = \max\left(|R_{horizontal}|, |R_{45}|, |R_{vertical}|, |R_{-45}|\right)\]</span> Example code (Matlab):</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">f = imread(<span class="string">'xxx.png'</span>); <span class="comment">% read image</span></span><br><span class="line">w = [<span class="number">2</span> <span class="number">-1</span> <span class="number">-1</span>; <span class="number">-1</span> <span class="number">2</span> <span class="number">-1</span>; <span class="number">-1</span> <span class="number">-1</span> <span class="number">2</span>]; <span class="comment">% mask</span></span><br><span class="line">g = <span class="built_in">abs</span>(imfilter(double(f),w)); <span class="comment">% mask responses</span></span><br></pre></td></tr></table></figure><h2><span id="edge-detectors">Edge Detectors</span></h2><p>Edge is the boundary of regions. The boundary has meaningful discontinuities in grey intensity level. There are three types of edges: ideal edge (left), ramp edge (middle) and roof edge (right).</p><p><img src="/images/3t.png"></p><p>For an <strong>ideal edge</strong> (step edge, left), an edge is a collection of connected pixels on the region boundary. Ideal edges can occur over the distance of 1 pixel.</p><p><strong>Roof edges</strong> (right) are models of lines through a region, with the base (width) of a roof edge being determined by the thickness and sharpness of the line. In the limit, when its base is 1 pixel wide, a roof edge becomes a 1 pixel thick line running through a region in an image. Roof edges can represent thin features, e.g., roads, line drawings, etc.</p><p>For a <strong>ramp edge</strong> (middle):</p><ul><li>edge point is any point contained in the ramp</li><li>edge length is determined by the length of the ramp</li><li>the slope of the ramp is inversely proportional to the degree of blurring in the edge</li><li>the <strong>first derivative</strong> of the intensity profile is positive at the points of transition into and out of the ramp (we move from left to right)</li><li>the <strong>second derivative</strong> of the intensity profile is positive at the transition associated with the dark side of the edge, and negative at the transition associated with the light side of the edge</li></ul><p><img src="/images/ramp.png"></p><p>The <strong>magnitude of the first derivative</strong> can be used to detect the presence of an edge. The <strong>sign of the second derivative</strong> can be used to determine whether an edge pixel lies on the dark or light side of an edge. The <strong>zero-crossing property</strong> of the second derivative is very useful for <strong>locating</strong> the centres of thick edge.</p><p>However, fairly little noise can have a significant impact on the first and second derivatives used for edge detection in images. <strong>Image smoothing</strong> is commonly used prior to the edge detection so that the estimations of the two derivatives can be more accurate.</p><h3><span id="gradient-operator">Gradient Operator</span></h3><p>The computation of the gradient of an image is based on obtaining the partial derivatives <span class="math inline">\(G_x = \partial f/\partial x\)</span> and <span class="math inline">\(G_y = \partial f/\partial y\)</span> at every pixel location (x,y). The gradient direction and gradient magnitude are <span class="math display">\[\tan^{-1}\left(\frac{G_y}{G_x}\right)\\|\triangledown f|\approx|G_x|+|G_y|\]</span> <img src="/images/z.png"></p><p>The are several ways to approximate the partial derivatives:</p><ul><li>Roberts cross-gradient operators<ul><li><span class="math inline">\(G_x = (z_9-z_5)\)</span></li><li><span class="math inline">\(G_y=(z_8-z_6)\)</span></li></ul></li><li>Prewitt operators<ul><li><span class="math inline">\(G_x=(z_7+z_8+z_9)-(z_1+z_2+z_3)\)</span></li><li><span class="math inline">\(G_y=(z_3+z_6+z_9)-(z_1+z_4+z_7)\)</span></li></ul></li><li>Sobel operators<ul><li><span class="math inline">\(G_x=(z_7+2z_8+z_9)-(z_1+2z_2+z_3)\)</span></li><li><span class="math inline">\(G_y=(z_3+2z_6+z_9)-(z_1+2z_4+z_7)\)</span></li></ul></li></ul><p><img src="/images/ed.png"></p><p>Below are diagonal edge masks for detecting discontinuities in the <strong>diagonal directions</strong>.</p><p><img src="/images/diag.png"></p><h3><span id="marr-hildreth-edge-detector">Marr-Hildreth Edge Detector</span></h3><p>The Laplacian of an image f(x,y) at location (x,y) is defined as <span class="math display">\[\triangledown^2f(x,y)=\frac{\partial^2f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}\]</span> There are two approximations: <span class="math display">\[\triangledown^2f=4z_5-(z_2+z_4+z_6+z_8)\\\triangledown^2f=8z_5-(z_1+z_2+z_3+z_4+z_6+z_7+z_8+z_9)\]</span> The Laplacian generally is <strong>not</strong> used in its original form for edge detection (based on zero-crossing property) because it is <strong>unacceptably sensitive to noise</strong>. We smooth the image by using a Gaussian blurring function <span class="math inline">\(G(r)\)</span>, where <span class="math inline">\(r\)</span> = radius, before we apply the Laplacian operator: <span class="math display">\[G(r)=\exp(\frac{-r^2}{2\sigma^2})\]</span> The <strong>Laplacian of a Gaussian (LoG)</strong> operator is defined by <span class="math display">\[\text{LoG}(f)=\triangledown^2(f\ast G)=f\ast(\triangledown^2G)\]</span> Using the LoG, the location of edges can be detected reliably based on the zero-crossing values because image noise level is reduced by the Gaussian function.</p><p><strong>Marr-Hildreth algorithm</strong></p><ol type="1"><li><p>Filter the input with an n-by-n Gaussian blurring filter <span class="math inline">\(G(r)\)</span>.</p></li><li><p>Compute the Laplacian of the image resulting from Step 1 using one of the following 3-by-3 masks.</p><p><img src="/images/mha.png"></p></li><li><p>Find the <strong>zero crossings</strong> of the image from Step 2.</p><ol type="1"><li>A zero crossing at a pixel implies that the signs of at least two of its opposing neighboring pixels must be different.</li><li>There are four cases to test: left/right, up/down, and the two diagonals.</li><li>For testing, the signs of the two opposing neighboring pixels must be different and their absolute Laplacian values must be larger than or equal to some threshold.</li><li>If yes, we call the current pixel a zero-crossing pixel.</li></ol></li></ol><p><img src="/images/mhed.png"></p><h2><span id="scale-invariant-feature-transform-sift">Scale Invariant Feature Transform (SIFT)</span></h2><p>SIFT is useful for finding distinctive patches (<strong>keypoints</strong>) in images and transforming keypoints (locations) into <strong>features vectors</strong> for recognition tasks.</p><p>SIFT consists of four steps:</p><ol type="1"><li>Scale-space extrema detection</li><li>Keypoint localization</li><li>Orientation assignment</li><li>Keypoint descriptor</li></ol><p>SIFT feature is</p><ul><li>invariant to image rotation and scale</li><li>partially invariant to change in illumination and 3D camera viewpoint</li></ul><p>The method is a cascade (one step followed by the other step) filtering approach, in which more computationally expensive operations are applied only at locations that pass an initial test.</p><p><strong>[1] Scale-space extrema detection</strong></p><p>Candidate locations are identified by searching for stable features across all scales in the scale space. The <strong>scale space</strong> of an image is defined as <span class="math display">\[L(x,y,\sigma)=G(x,y,\sigma)\ast I(x,y)\\G(x,y,\sigma)=\frac{1}{2\pi\sigma^2}\exp(-\frac{x^2+y^2}{2\sigma^2})\]</span> , where <span class="math inline">\(\ast\)</span> is the convolution operator (filtering operation), the variable-scale Gaussian is <span class="math inline">\(G(x, y, \sigma)\)</span> and the image is <span class="math inline">\(I(s, y)\)</span>.</p><p>The difference between two nearby scales separated by a constant multiplicative factor <span class="math inline">\(k\)</span> is <span class="math display">\[\begin{align}D(x,y,\sigma)&amp;=L(x,y,k\sigma)-L(x,y,\sigma)\\&amp;=\left(G(x,y,k\sigma)-G(x,y,\sigma)\right)\ast I(x,y)\end{align}\]</span> The DoG function provides a close and efficient approximation to the scale-normalized Laplacian of Gaussian (LoG).</p><p><img src="/images/dog.jpg"></p><p>Local <strong>extrema</strong> (maxima and minima) of <span class="math inline">\(D( x, y, σ)\)</span> are detected by comparing the values of its 26 neighbors, including 9 from the above image, 9 from bottom image and 8 from the current image.It is selected only if the center pixel is larger than <strong>all</strong> of these neighbors (26 neighbors) or smaller than all of them.</p><p><img src="/images/extra.png"></p><p><img src="/images/exdet.png"></p><p><strong>[2] Keypoint localization</strong></p><p>Once a keypoint candidate has been found by comparing a pixel to its neighbors, the next step is to determine whether the keypoint is selected based on <strong>local contrast and localization along edge</strong>. Therefore, the keypoints will be rejected if these points have low contrast.</p><p>All extrema are discarded if <span class="math inline">\(|D(x)|&lt;0.03\)</span> (minimum contrast), where <span class="math inline">\(x\)</span> represents a relative image position with maximum value of <span class="math inline">\(D\)</span>. The keypoints will be rejected if these points are poorly localized along an edge (determined based on the <strong>ratio of principle curvatures</strong>).</p><p><img src="/images/kl.png"></p><p><strong>[3] Orientation assignment</strong></p><p>Each corresponding keypoint in <span class="math inline">\(L\)</span> is assigned a dominant orientation. The keypoint descriptor can be represented relative to this orientation and therefore achieve invariance to image rotation.</p><p>The gradient magnitude and orientation are <span class="math display">\[m(x,y)=\sqrt{(L(x+1,y)-L(x-1,y))^2+(L(x,y+1)-L(x,y-1))^2}\\\theta(x,y)=\tan^{-1}\left(\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}\right)\]</span> An <strong>orientation histogram</strong> is formed from the gradient orientations within a region around the keypoint. The orientation histogram has 36 bins covering the 360 degree range of orientations, 10 degrees per bin. Each sample added to the histogram is weighted by its gradient magnitude and by a Gaussian-weighted circular window (with σ that is 1.5 times that of the scale of the keypoint, and it is related to effective window size). Peak in the orientation histogram corresponds to dominant direction of the local gradients.</p><p><img src="/images/IMG_2E4DF1ED416A-1.jpg"></p><p><strong>[4] Descriptor for local image region</strong></p><p><img src="/images/kd.png"></p><p>For each detected keypoint, a local image descriptor is computed. It is partially invariant to change in illumination and 3D viewpoint.</p><p>In order to achieve orientation invariance, the coordinates and the gradient orientations are rotated relative to the keypoint orientation. A Gaussian weighting function with σ equal to one half the width of the descriptor window is used to assign a weight to the magnitude of each sample point.</p><p>Each subregion generates an orientation histogram with 8 orientation bins. Therefore, for each keypoint, if <span class="math inline">\(2\times2\)</span> descriptor is used, a feature vector can be formed with <span class="math inline">\(2\times2\times8 = 32\)</span> elements; if <span class="math inline">\(4\times4\)</span> descriptor is used, there will be <span class="math inline">\(4\times4\times8 = 128\)</span> elements in a feature vector. The <strong>optimal</strong> setting is 4x4 subregions and 8 orientation bins (the above picture).</p><p><strong>Feature matching</strong></p><p><img src="/images/IMG_043BB8EF0160-1.jpg"></p><p>Matlab Implementation</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% num = match(image1, image2)</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% This function reads two images, finds their SIFT features, and</span></span><br><span class="line"><span class="comment">%   displays lines connecting the matched keypoints.  A match is accepted</span></span><br><span class="line"><span class="comment">%   only if its distance is less than distRatio times the distance to the</span></span><br><span class="line"><span class="comment">%   second closest match.</span></span><br><span class="line"><span class="comment">% It returns the number of matches displayed.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% Example: match('scene.pgm','book.pgm');</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">num</span> = <span class="title">match</span><span class="params">(image1, image2)</span></span></span><br><span class="line"><span class="comment">% Find SIFT keypoints for each image</span></span><br><span class="line">[im1, des1, loc1] = sift(image1);</span><br><span class="line">[im2, des2, loc2] = sift(image2);</span><br><span class="line"><span class="comment">% For efficiency in Matlab, it is cheaper to compute dot products between</span></span><br><span class="line"><span class="comment">%  unit vectors rather than Euclidean distances.  Note that the ratio of</span></span><br><span class="line"><span class="comment">%  angles (acos of dot products of unit vectors) is a close approximation</span></span><br><span class="line"><span class="comment">%  to the ratio of Euclidean distances for small angles.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% distRatio: Only keep matches in which the ratio of vector angles from the</span></span><br><span class="line"><span class="comment">%   nearest to second nearest neighbor is less than distRatio.</span></span><br><span class="line">distRatio = <span class="number">0.6</span>;</span><br><span class="line"><span class="comment">% For each descriptor in the first image, select its match to second image.</span></span><br><span class="line">des2t = des2';</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">size</span>(des1,<span class="number">1</span>)</span><br><span class="line">    dotprods = des1(<span class="built_in">i</span>,:) * des2t; <span class="comment">% compute orientation of des1[i] and des2[j] for all j</span></span><br><span class="line">    [vals, indx] = <span class="built_in">sort</span>(<span class="built_in">acos</span>(dotprods)); <span class="comment">% Take inverse cosine and sort results</span></span><br><span class="line">    <span class="comment">% Check if nearest neighbor has angle less than distRatio times 2nd.</span></span><br><span class="line">    <span class="keyword">if</span> (vals(<span class="number">1</span>) &lt; distRatio * vals(<span class="number">2</span>))</span><br><span class="line">      match(<span class="built_in">i</span>) = indx(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      match(<span class="built_in">i</span>) = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p><img src="/images/IMG_DAD55EC9C383-1.jpg"></p><h2><span id="harris-corner-detector">Harris Corner Detector</span></h2><p><strong>Basic Idea</strong></p><p>We should easily recognize the point by looking at intensity values within a small window. hifting the window in any direction should yield a large change in appearance.</p><p><img src="/images/cornersad1.png"></p><p>Harris corner detector gives a mathematical approach for determining which case holds.</p><p>Change of intensity for the shift <span class="math inline">\([u, v]\)</span>: <span class="math display">\[E(u,v)=\sum_{x,y}w(x,y)[I(x+u,y+v)-I(x,y)]\]</span> where <span class="math inline">\(w(x,y)\)</span> is window function.</p><p>For nearly constant patches, this will be near 0. For very distinctive patches, this will be larger. Hence... we want patches where <span class="math inline">\(E(u,v)\)</span> is LARGE.</p><p><img src="/images/harriasequ.png"></p><p>For small shifts <span class="math inline">\([u, v]\)</span>, we have a bilinear approximation: <span class="math display">\[E(u,v)\approx [u,v]M[u,v]^T\]</span> where <span class="math inline">\(M\)</span> is a 2x2 matrix computed from image derivatives: <span class="math display">\[M=\sum_{x,y}w(x,y)\begin{bmatrix}I_x^2      &amp; I_xI_y      \\I_xI_y      &amp; I_y^2\end{bmatrix}\]</span> Treat gradient vectors as a set of <span class="math inline">\((dx,dy)\)</span> points with a center of mass defined as being at <span class="math inline">\((0,0)\)</span>. Fit an ellipse to that set of points via scatter matrix.</p><p><img src="/images/harrianaly.png"></p><p><img src="/images/harrrieclips.png"></p><p>Measure of corner response: <span class="math display">\[R=Det(M)-k(Trace(M))^2\]</span> where <span class="math display">\[Det(M)=\lambda_1\lambda_2\\Trace(M)=\lambda_1+\lambda_2\]</span> According to <span class="math inline">\(R\)</span>, we can detect corner:</p><p><img src="/images/harrieres.png"></p><p><span class="math inline">\(R\)</span> depends only on eigenvalues of <span class="math inline">\(M\)</span>. As we can see from figure above,</p><ul><li><span class="math inline">\(R\)</span> is large for a corner.</li><li><span class="math inline">\(R\)</span> is negative with large magnitude for an edge</li><li><span class="math inline">\(|R|\)</span> is small for a flat region</li></ul><p><strong>Harris Corner Detection Algorithm</strong></p><ol type="1"><li><p>Compute <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> derivatives of image: <span class="math display">\[I_x=G_\sigma^x\ast I\\I_y=G_\sigma^y\ast I\]</span></p></li><li><p>Compute products of derivatives at every pixel: <span class="math display">\[I_x^2=I_x\cdot I_x\\I_y^2=I_y\cdot I_y\\I_{xy}=I_x\cdot I_y\]</span></p></li><li><p>Compute the sums of the products of derrivatives at each pixel: <span class="math display">\[S_x^2=G_\sigma&#39;\ast I_x^2\\S_y^2=G_\sigma&#39;\ast I_y^2\\S_{xy}=G_\sigma&#39;\ast I_{xy}\]</span></p></li><li><p>Define at each pixel <span class="math inline">\((x,y)\)</span> the matrix: <span class="math display">\[M(x,y)=\begin{bmatrix}S_x^2(x,y)      &amp; S_{xy}(x,y)      \\S_{xy}(x,y)     &amp; S_y^2(x,y)\end{bmatrix}\]</span></p></li><li><p>Compute the response of the detector at each pixel: <span class="math display">\[R=Det(M)-k(Trace(M))^2\]</span></p></li><li><p>Threshold of value of <span class="math inline">\(R\)</span>. Compute nonmax suppression.</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HKUST CSIT5401 Recognition System lecture notes 1. 识别系统复习笔记。&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#laplacian-point-detector&quot;&gt;Laplacian Point Detector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#line-detector&quot;&gt;Line Detector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#edge-detectors&quot;&gt;Edge Detectors&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#gradient-operator&quot;&gt;Gradient Operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#marr-hildreth-edge-detector&quot;&gt;Marr-Hildreth Edge Detector&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#scale-invariant-feature-transform-sift&quot;&gt;Scale Invariant Feature Transform (SIFT)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#harris-corner-detector&quot;&gt;Harris Corner Detector&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://www.52coding.com.cn/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Recognition System" scheme="http://www.52coding.com.cn/tags/Recognition-System/"/>
    
      <category term="Prewitt" scheme="http://www.52coding.com.cn/tags/Prewitt/"/>
    
      <category term="Sobel" scheme="http://www.52coding.com.cn/tags/Sobel/"/>
    
      <category term="Marr-Hildreth Edge Detector" scheme="http://www.52coding.com.cn/tags/Marr-Hildreth-Edge-Detector/"/>
    
      <category term="SIFT" scheme="http://www.52coding.com.cn/tags/SIFT/"/>
    
  </entry>
  
  <entry>
    <title>RL - Deep Deterministic Policy Gradient (DDPG)</title>
    <link href="http://www.52coding.com.cn/2018/11/30/RL%20-%20Deep%20Deterministic%20Policy%20Gradient/"/>
    <id>http://www.52coding.com.cn/2018/11/30/RL - Deep Deterministic Policy Gradient/</id>
    <published>2018-11-30T05:53:09.000Z</published>
    <updated>2019-04-12T03:26:48.527Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1509.02971" target="_blank" rel="noopener">Deep Deterministic Policy Gradient (DDPG)</a> 是由 DeepMind 的 Lillicrap 等人于2015年提出的算法，发表在ICLR 2016上。DDPG 是基于 <a href="http://proceedings.mlr.press/v32/silver14.pdf" target="_blank" rel="noopener">DPG</a> 算法的改进，可以看作是 Actor-critic 和 <a href="https://www.52coding.com.cn/2018/11/16/RL%20-%20DQN%20and%20A3C/">DQN</a> 的结合，它同时学习一个 Q-function 和一个策略（policy）：用 Q-learning 的方法学习 Q-function，然后用 Q-function 更新策略。</p><a id="more"></a><h2><span id="dpg">DPG</span></h2><p><a href="http://proceedings.mlr.press/v32/silver14.pdf" target="_blank" rel="noopener">Deterministic Policy Gradient (DPG)</a> 是把策略梯度（policy gradient）算法扩展到确定性策略（deterministic policy）上。事实上，DPG 被证明是随机策略梯度（stochastic policy gradient）的一种特殊情况。</p><blockquote><p>随机策略梯度： <span class="math display">\[\triangledown_\theta J(\pi_\theta)=\mathbb{E}_{s\sim\rho^\pi, a\sim\pi_\theta}[\triangledown_\theta\log\pi_\theta(a|s)Q^\pi(s,a)]\]</span> 其中，<span class="math inline">\(\pi_\theta\)</span> 是由参数为 <span class="math inline">\(\theta\)</span> 的函数近似的策略，<span class="math inline">\(\rho^\pi\)</span> 为策略 <span class="math inline">\(\pi_\theta\)</span> 的状态分布（state distribution）。</p></blockquote><p>大部分 model-free 的增强学习算法属于泛化的<a href="https://www.52coding.com.cn/2017/12/07/RL%20-%20Planning%20by%20Dynamic%20Programming/">策略迭代（policy iteration）</a>算法，一般分为两步：策略评估（policy evaluation） 和策略改进（ policy improvement）。策略评估通常使用 <a href="https://www.52coding.com.cn/2017/12/16/RL%20-%20Model-Free%20Prediction/#monte-carlo-learning">Monte-Carlo evaluation</a> 或 <a href="https://www.52coding.com.cn/2017/12/16/RL%20-%20Model-Free%20Prediction/#temporal-difference-learning">temporal difference learning</a> 来近似 <span class="math inline">\(Q^\pi(s, a)\)</span>。策略改进则通常通过最大化评估的 action-value 来得到：<span class="math inline">\(\mu^{k+1}(s) = \arg\max_aQ^{\mu^k}(s, a)\)</span>。</p><p>然而，在连续的动作空间（continuous action spaces）里这种最大化却是不可行的。在离散的动作空间里，我们可以为每个action计算相应 Q-value 然后进行比较；但是在连续的动作空间中，我们不可能把每个action的 Q-value 都计算出来再比较，而通过对 Q-function 求导求的方式计算最大值开销又很大。所以，取而代之的是单独用一个函数近似策略，然后<strong>用 Q-function 的梯度来改进该策略</strong>。具体来说，对于每个访问过的状态 <span class="math inline">\(s\)</span>，策略函数的参数 <span class="math inline">\(\theta^{k+1}\)</span> 根据 <span class="math inline">\(\triangledown_\theta Q^{\mu_k}(s, \mu_\theta(s))\)</span> 来更新： <span class="math display">\[\begin{align}\theta^{k+1}&amp;=\theta^k+\alpha\mathbb{E}_{s\sim\rho^{\mu^k}}[\triangledown_\theta Q^{\mu^k}(s, \mu_\theta(s))]\\&amp;= \theta^k + \alpha\mathbb{E}_{s\sim\rho^{\mu^k}}[\triangledown_\theta \mu_\theta(s)\triangledown_aQ^{\mu^k}(s, a)|_{a=\mu_\theta(s)}]\end{align}\]</span></p><p>可以证明，上述更新也属于策略梯度算法，这就是DPG算法的策略更新公式。</p><h2><span id="ddpg">DDPG</span></h2><p>DDPG改进了 Q-function 的学习方式，而策略端的更新方式和DPG相同，即如式(1)所示。在 DPG 中，Q-function 是通过 Q-learning 的方式来学习的，而当使用神经网络来近似 Q-function 的时候会导致训练不稳定，DDPG 应用了 <a href="https://www.52coding.com.cn/2018/11/16/RL%20-%20DQN%20and%20A3C/#deep-q-network">DQN</a> 中的两个trick来解决不稳定的问题，也就是<strong>经验池（replay buffer）</strong>和<strong>目标网络（target network）</strong>。</p><p>具体来说，经验池 <span class="math inline">\(D\)</span> 每次探索都会存储元组 <span class="math inline">\((s, a, r, s&#39;, d)\)</span>，其中 <span class="math inline">\(d\)</span> 为一个布尔变量，如果 <code>d == True</code>，就表明 <span class="math inline">\(s&#39;\)</span> 是终止状态（terminal state）。 每次更新时都会从经验池随机采样一批数据进行更新。经验池的大小是一个需要微调的超参数：如果经验池过小的话，会导致对池内数据过拟合；如果经验池存储所有数据的话，又会放慢学习的速度。</p><p>目标网络是指用单独的网络参数来生成目标（q-target），设策略函数的参数为 <span class="math inline">\(\theta\)</span>，Q-function 的参数为 <span class="math inline">\(\phi\)</span>，则对应的目标网络参数为 <span class="math inline">\(\theta_{tag}\)</span> 和 <span class="math inline">\(\phi_{tag}\)</span>，生成的目标为： <span class="math display">\[r + \gamma(1-d)Q_{\phi_{tag}}(s&#39;, \mu_{\theta_{tag}}(s&#39;))\]</span> 所以 Q-value 端的更新公式为： <span class="math display">\[\triangledown_\phi \mathbb{E}_{s,a,r,s&#39;,d\sim D}\left[\left(Q(s,a)-(r + \gamma(1-d)Q_{\phi_{tag}}(s&#39;, \mu_{\theta_{tag}}(s&#39;)))\right)^2\right]\]</span> 与DQN不同的是，DDPG中的目标网络使用“软更新”的方式，即目标网络并不是隔一定时间后与主网络同步，而是朝着主网络缓慢移动： <span class="math display">\[\theta_{tag}\leftarrow\tau\theta+(1-\tau)\theta_{tag}\\\phi_{tag}\leftarrow\tau\phi+(1-\tau)\phi_{tag}\]</span> 其中，<span class="math inline">\(\tau \in (0, 1)\)</span> 是一个超参数，通常取值接近 1。</p><p>为了增加探索能力，训练时在选择每个动作的时候都会加上随机噪声 <span class="math inline">\(\mathcal{N}\)</span>，论文作者建议使用时间相关的 <a href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process" target="_blank" rel="noopener">OU噪声</a>，但是最近的研究结果表明使用不相关的、zero-mean的高斯噪声效果会更好。同时为了取得更高质量的训练数据，噪声可以随着训练过程逐步减小。另外，在评测时应去掉噪声。</p><p><strong>DDPG算法</strong></p><p><img src="/images/ddpg_algo.svg"></p><h2><span id="总结">总结</span></h2><p><strong>特点</strong></p><ul><li>off-policy算法</li><li>只能用于连续的动作空间</li><li>可以看作是把DQN应用到连续动作空间</li></ul><h2><span id="references">References</span></h2><p>[1] http://proceedings.mlr.press/v32/silver14.pdf</p><p>[2] https://arxiv.org/abs/1509.02971</p><p>[3] https://spinningup.openai.com/en/latest/algorithms/ddpg.html</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1509.02971&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Deep Deterministic Policy Gradient (DDPG)&lt;/a&gt; 是由 DeepMind 的 Lillicrap 等人于2015年提出的算法，发表在ICLR 2016上。DDPG 是基于 &lt;a href=&quot;http://proceedings.mlr.press/v32/silver14.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DPG&lt;/a&gt; 算法的改进，可以看作是 Actor-critic 和 &lt;a href=&quot;https://www.52coding.com.cn/2018/11/16/RL%20-%20DQN%20and%20A3C/&quot;&gt;DQN&lt;/a&gt; 的结合，它同时学习一个 Q-function 和一个策略（policy）：用 Q-learning 的方法学习 Q-function，然后用 Q-function 更新策略。&lt;/p&gt;
    
    </summary>
    
      <category term="博客" scheme="http://www.52coding.com.cn/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy Gradient" scheme="http://www.52coding.com.cn/tags/Policy-Gradient/"/>
    
      <category term="DPG" scheme="http://www.52coding.com.cn/tags/DPG/"/>
    
      <category term="DDPG" scheme="http://www.52coding.com.cn/tags/DDPG/"/>
    
      <category term="DQN" scheme="http://www.52coding.com.cn/tags/DQN/"/>
    
  </entry>
  
  <entry>
    <title>RL - Proximal Policy Optimization (PPO)</title>
    <link href="http://www.52coding.com.cn/2018/11/25/RL%20-%20PPO/"/>
    <id>http://www.52coding.com.cn/2018/11/25/RL - PPO/</id>
    <published>2018-11-25T14:00:09.000Z</published>
    <updated>2019-04-12T03:27:35.929Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">Proximal Policy Optimization (PPO, PPO-Clip, PPO-Penalty)</a> 是由<a href="https://www.52coding.com.cn/2018/11/22/RL%20-%20TRPO/">TRPO</a>的作者Schulman等人于2017年提出的策略梯度类算法。PPO算法的思路和TRPO一致，都是想在优化时采取尽可能大的步幅但又不能太大以至于产生崩坏。相比于比TRPO，PPO实现起来更简单，泛化能力更强，可以使用随机梯度下降（SGD）进行优化。</p><a id="more"></a><h2><span id="背景">背景</span></h2><p>PPO的背景与<a href="https://www.52coding.com.cn/2018/11/22/RL%20-%20TRPO/#背景">TRPO的背景</a>一致，最终TRPO推导出如下的带约束优化问题： <span class="math display">\[\max_{\theta}\mathbb{E}_t[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t]\\\text{subject to }\mathbb{E}_t[\text{KL}[\pi_{\theta_{old}}(\cdot|s_t), \pi_\theta(\cdot|s_t)]]\]</span> 令 <span class="math inline">\(r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\)</span> 为新旧策略的概率比（易知 <span class="math inline">\(r_t(\theta_{old}) = 1\)</span>）。TRPO最大化的替代目标（surrogate objective）可以写为如下形式： <span class="math display">\[L^{CPI}(\theta)=\mathbb{E}_t[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t]=\mathbb{E}_t[r_t(\theta)A_t]\]</span> 如果不加约束的话，直接优化该目标会产生巨大的更新，导致更新不稳定甚至崩溃。所以需要考虑一种惩罚方法，使 <span class="math inline">\(r_t(\theta)\)</span> 接近 <span class="math inline">\(1\)</span>。</p><h2><span id="ppo-clip">PPO-Clip</span></h2><p>PPO-Clip的目标函数为： <span class="math display">\[L^{CLIP}(\theta)=\mathbb{E}_t[\min(r_t(\theta)A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t)]\]</span> 其中 <span class="math inline">\(\epsilon\)</span> 为超参数控制截断率，取值通常比较小（0.2左右）。</p><p>上述目标函数的第一项与 <span class="math inline">\(L^{CPI}\)</span> 一致，第二项则是为了限制更新幅度（施加惩罚），控制 <span class="math inline">\(r_t(\theta) \in [1-\epsilon, 1+\epsilon]\)</span>。可见 <span class="math inline">\(L^{CLIP}(\theta)\)</span> 是 <span class="math inline">\(L^{CPI}(\theta)\)</span> 的一个下界。</p><p><img src="/images/lclip.png"></p><p>上图显示了 <span class="math inline">\(\text{clip}\)</span> 函数的工作方式：</p><ul><li>当 <span class="math inline">\(A &gt; 0\)</span> 时，如果想使目标函数取得更大的值，就需要增大 <span class="math inline">\(\pi_\theta(a_t|s_t)\)</span> 的值，也就是增大 <span class="math inline">\(r_t(\theta)\)</span> 。但是式中的 <span class="math inline">\(\min\)</span> 函数限制了 <span class="math inline">\(r_t(\theta)\)</span> 最大取到 <span class="math inline">\(1+\epsilon\)</span>，所以新策略再远离旧策略（<span class="math inline">\(r_t(\theta)\)</span> 继续增大）并不会带来更多地好处。</li><li>当 <span class="math inline">\(A &lt; 0\)</span> 时，如果想使目标函数取得更大的值，就需要减小 <span class="math inline">\(\pi_\theta(a_t|s_t)\)</span> 的值，也就是减小 <span class="math inline">\(r_t(\theta)\)</span> 。但是式中的 <span class="math inline">\(\min\)</span> 函数限制了 <span class="math inline">\(r_t(\theta)\)</span> 最小取到 <span class="math inline">\(1-\epsilon\)</span>，所以新策略再远离旧策略（<span class="math inline">\(r_t(\theta)\)</span> 继续减小）并不会带来更多地好处。</li></ul><p>在实现中，目标函数通常使用更简单的形式： <span class="math display">\[L^{CLIP}(s, a, \theta_k, \theta)=\min(\frac{\pi_\theta(a|s)}{\pi_{\theta_{k}}(a|s)}A^{\pi_{\theta_k}}(s, a), g(\epsilon, A^{\pi_{\theta_k}}(s, a)))\]</span> 其中， <span class="math display">\[g(\epsilon, A^{\pi_{\theta_k}}(s, a))=\begin{cases} (1+\epsilon)A,  &amp; \mbox{if }A ≥0 \\(1-\epsilon)A, &amp; \mbox{if }A&lt;0\end{cases}\]</span></p><blockquote><p>注：即便对 <span class="math inline">\(r_t(\theta)\)</span> 加上截断，新策略仍染有可能偏离旧策略很远，不过有很多trick来处理这个问题。其中一个特别简单的处理方式就是：如果新策略和旧策略的平均KL距离大于某个阈值，则停止进行更新（<strong>early stopping</strong>）。</p></blockquote><p>相比于TRPO，由于没有了KL约束，PPO可以用SGD来进行优化，实现简单很多。</p><p><strong>PPO-Clip算法</strong></p><p><img src="/images/ppo_algo.svg"></p><h2><span id="ppo-penalty">PPO-Penalty</span></h2><p>这种方法使用KL距离作为惩罚项，关键在于求出能够自适应多种任务的惩罚因子 <span class="math inline">\(\beta\)</span>。算法的逻辑为，在每次策略进行更新时：</p><ul><li><p>使用SGD优化目标函数： <span class="math display">\[L^{KLPEN}(\theta)=\mathbb{E}_t\left[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t-\beta\cdot\text{KL}[\pi_{old}(a_t|s_t), \pi_\theta(a_t|s_t)]\right]\]</span></p></li><li><p>计算 <span class="math inline">\(d = \mathbb{E}\left[\text{KL}[\pi_{\theta_{old}}(\cdot|s_t), \pi_\theta(\cdot|s_t)]\right]\)</span></p><ul><li>如果 <span class="math inline">\(d &lt; d_{targ}/1.5\)</span>，则 <span class="math inline">\(\beta \leftarrow \beta/2\)</span></li><li>如果 <span class="math inline">\(d &gt; d_{targ} \times 1.5\)</span>，则 <span class="math inline">\(\beta \leftarrow\beta \times 2\)</span></li></ul><p>其中，<span class="math inline">\(d_{targ}\)</span> 为超参数。</p></li></ul><blockquote><p>注：PPO-Penalty 没有 PPO-Clip 效果好。</p></blockquote><h2><span id="实验和总结">实验和总结</span></h2><p><strong>特点</strong></p><ul><li>训练稳定</li><li>通过限制 <span class="math inline">\(r_t(\theta)\)</span> 来找到尽可能大的并且合理的步长</li><li>on-policy 算法</li><li>可用于离散和连续的动作空间</li><li>相比于TRPO，PPO实现简单，效果更好</li></ul><p><strong>实验性能</strong></p><p>在大部分 MuJoCo 环境中强于其他策略梯度类算法，在Atari环境中，表现仅次于ACER，但是学习速度优于ACER。</p><p><img src="/images/ppo_mujoco.png"></p><p><img src="/images/ppo_atari.png"></p><h3><span id="代码">代码</span></h3><p>自己也实现了一下PPO-Clip算法，代码在<a href="https://github.com/NeymarL/Pacman-RL/blob/master/src/ppo.py" target="_blank" rel="noopener">这里</a>。下图显示了在 OpenAI <a href="https://gym.openai.com/" target="_blank" rel="noopener">Gym</a> 上的 <code>MsPacman-ram-v0</code> 环境上的测试结果：</p><p><img src="/images/ppo_pacman.png"></p><p><img src="/images/sample1.gif"></p><p><img src="/images/sample2.gif"></p><h2><span id="references">References</span></h2><p>[1] https://arxiv.org/abs/1707.06347</p><p>[2] https://spinningup.openai.com/en/latest/algorithms/ppo.html</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Proximal Policy Optimization (PPO, PPO-Clip, PPO-Penalty)&lt;/a&gt; 是由&lt;a href=&quot;https://www.52coding.com.cn/2018/11/22/RL%20-%20TRPO/&quot;&gt;TRPO&lt;/a&gt;的作者Schulman等人于2017年提出的策略梯度类算法。PPO算法的思路和TRPO一致，都是想在优化时采取尽可能大的步幅但又不能太大以至于产生崩坏。相比于比TRPO，PPO实现起来更简单，泛化能力更强，可以使用随机梯度下降（SGD）进行优化。&lt;/p&gt;
    
    </summary>
    
      <category term="博客" scheme="http://www.52coding.com.cn/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy Gradient" scheme="http://www.52coding.com.cn/tags/Policy-Gradient/"/>
    
      <category term="PPO" scheme="http://www.52coding.com.cn/tags/PPO/"/>
    
  </entry>
  
  <entry>
    <title>RL - Trust Region Policy Optimization (TRPO)</title>
    <link href="http://www.52coding.com.cn/2018/11/22/RL%20-%20TRPO/"/>
    <id>http://www.52coding.com.cn/2018/11/22/RL - TRPO/</id>
    <published>2018-11-22T09:10:09.000Z</published>
    <updated>2019-04-12T03:27:40.560Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="noopener">Trust Region Policy Optimization (TRPO)</a>算法是由伯克利大学的Schulman等人于2015年提出的策略梯度（Policy Gradients）算法。TRPO通过最大化新策略相对于当前策略的优势来保证每次更新都是单调递增的（稳定），同时找到尽可能大的更新步幅。算法推导出的最终结果是在KL约束下最大化替代优势函数。</p><a id="more"></a><h2><span id="背景">背景</span></h2><p>考虑经典的 MDP<span class="math inline">\(&lt;S, A, P, r, \rho_0, \gamma&gt;\)</span>，其中 <span class="math inline">\(S\)</span> 是所有状态（state）的集合，<span class="math inline">\(A\)</span> 是所有动作（action）的集合，<span class="math inline">\(P: S\times A\times S \rightarrow \mathbb{R}\)</span> 是转移概率分布，<span class="math inline">\(r\)</span> 是奖励（reward）函数，<span class="math inline">\(\rho_0\)</span> 是初始状态（<span class="math inline">\(s_0\)</span>）分布，<span class="math inline">\(\gamma\)</span> 是折扣因子（ discount factor）。</p><p>定义 <span class="math inline">\(\pi\)</span> 为一个随机策略：<span class="math inline">\(\pi: S\times A\rightarrow [0, 1]\)</span>，定义 <span class="math inline">\(\eta(\pi)\)</span> 来衡量策略 <span class="math inline">\(\pi\)</span> 的好坏： <span class="math display">\[\eta(\pi)=\mathbb{E}_{s_0, a_0, ...\sim\pi}[\sum_{t=0}^\infty\gamma^tr(s_t)]\]</span> 接着定义 state-action value function <span class="math inline">\(Q_\pi\)</span>, value function <span class="math inline">\(V_\pi\)</span>, 优势函数（advantage function）<span class="math inline">\(A_\pi\)</span>: <span class="math display">\[Q_\pi(s_t, a_t) = \mathbb{E}_{s_{t+1}, a_{t+1}, ...\sim\pi}[\sum_{l=0}^\infty\gamma^lr_{s_{t+l}}]\]</span></p><p><span class="math display">\[V_\pi(s_t) =\mathbb{E}_{a_t, s_{t+1}, ...\sim\pi}[\sum_{l=0}^\infty\gamma^lr_{s_{t+l}}]\]</span></p><p><span class="math display">\[A_\pi(s, a) = Q_\pi(s, a) - V_\pi(s)\]</span></p><p>然后可以通过下式来衡量策略 <span class="math inline">\(\tilde{\pi}\)</span> 相对于策略 <span class="math inline">\(\pi\)</span> 的优势（证明详见论文）： <span class="math display">\[\begin{align}\eta(\tilde{\pi})&amp;=\eta(\pi)+\mathbb{E}_{s_0, a_0, ...\sim\color{red}{\tilde{\pi}}}[\sum_{t=0}^\infty\gamma^tA_\pi(s_t,a_t)]\\&amp;= \eta(\pi)+\sum_s\color{red}{\rho_\tilde{\pi}(s)}\sum_a\tilde{\pi}(a|s)A_\pi(s, a)\end{align}\]</span> 其中 <span class="math inline">\(\rho_\pi\)</span> 为策略 <span class="math inline">\(\pi\)</span> 的折扣访问频率（discounted visitation frequency）： <span class="math display">\[\rho_\pi(s) = P(s_0=s)+\gamma P(s_1=s) + \gamma^2 P(s_2=s)+...\]</span> 通过上式可知，只要每个状态 <span class="math inline">\(s\)</span> 的期望优势非负，即 <span class="math inline">\(\sum_a\tilde{\pi}(a|s)A_\pi(s, a)&gt;0\)</span>，就可以保证更新是单调非递减的，这其实就是经典的<a href="https://www.52coding.com.cn/2017/12/07/RL%20-%20Planning%20by%20Dynamic%20Programming/">策略迭代（policy iteration）</a>的更新方式。然而，由于 <span class="math inline">\(\rho_\tilde{\pi}(s)\)</span> 的存在，导致直接优化上式很困难，所以引入一个<strong>替代优势</strong>（surrogate advantage）： <span class="math display">\[\begin{align}L_\pi(\tilde{\pi})&amp;=\eta(\pi)+\sum_s\color{red}{\rho_\pi(s)}\sum_a\tilde{\pi}(a|s)A_\pi(s, a)\\\end{align}\]</span> 经过一系列推导，可以得到策略 <span class="math inline">\(\tilde{\pi}\)</span> 的优势下界： <span class="math display">\[\eta(\tilde{\pi})≥L_\pi(\tilde{\pi})-C\cdot D_{KL}^\max(\pi, \tilde{\pi})\]</span> 其中，<span class="math inline">\(C=\frac{4\epsilon\gamma}{(1-\gamma)^2}\)</span>，<span class="math inline">\(D_{KL}^\max\)</span> 是最大的KL散度。</p><p>这里相当于对优化目标 <span class="math inline">\(L_\pi(\tilde{\pi})\)</span> 进行了惩罚，惩罚因子为 <span class="math inline">\(C\)</span>，惩罚项为KL散度，目的是限制新旧策略的差距。通过最大化上述公式，就能最大化新策略 <span class="math inline">\(\tilde{\pi}\)</span> 所得到的奖励，同时又不会离旧策略 <span class="math inline">\(\pi\)</span> 太远（导致对当前数据过拟合），算法如下：</p><p><img src="/images/IMG_1925FD469BD9-1.jpeg"></p><h2><span id="trpo">TRPO</span></h2><p>由于Deep RL都是使用参数为 <span class="math inline">\(\theta\)</span> 的神经网络来拟合策略 <span class="math inline">\(\pi_\theta\)</span>，为了使公式更简洁，把算法1中公式的 <span class="math inline">\(\pi\)</span> 替换成 <span class="math inline">\(\theta\)</span>: <span class="math display">\[\theta = \arg\max_{\theta}[L(\theta_{old}, \theta)-C\cdot D_{KL}^\max(\theta_{old}|| \theta)]\]</span> 其中， <span class="math display">\[\begin{align}L(\theta_{old}, \theta) &amp;= \sum_s\rho_{\theta_{old}}(s)\sum_a\pi_\theta(a|s)A_{\theta_{old}}(s,a) \\&amp;= \mathbb{E}_{s,a\sim\pi_{\theta_{old}}}[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)}A_{\theta_{old}}(s,a)]\end{align}\]</span> TRPO是算法1的近似，区别在于：TRPO没有使用惩罚项 <span class="math inline">\(C\)</span>，而是使用 KL散度约束（i.e. trust region constraint）： <span class="math display">\[\theta = \arg\max_\theta L(\theta_{old}, \theta)\\\text{ s.t. }\bar{D}_{KL}(\theta||\theta_{old})≤\delta\]</span> 其中，<span class="math inline">\(\bar{D}_{KL}\)</span> 是平均KL散度： <span class="math display">\[\bar{D}_{KL}(\theta||\theta_{old})=\mathbb{E}_{s\sim\pi_{\theta_{old}}}[D_{KL}(\pi_{\theta}(\cdot|s)||\pi_{\theta_{old}}(\cdot|s))]\]</span> 然而上面的带约束优化也并非容易，所以TRPO对上式进行了一些近似，对目标函数和约束进行泰勒展开可以得到： <span class="math display">\[L(\theta_{old}, \theta) \approx g^T(\theta-\theta_{old})\]</span></p><p><span class="math display">\[\bar{D}_{KL}(\theta||\theta_{old})\approx \frac{1}{2}(\theta-\theta_{old})^TH(\theta-\theta_{old})\]</span></p><p>其中，<span class="math inline">\(g\)</span> 是替代函数的梯度在 <span class="math inline">\(\theta=\theta_{old}\)</span> 处的值，凑巧的是，它和策略梯度的值正好相等：<span class="math inline">\(g = \triangledown_\theta J(\pi_\theta)|_{\theta_{old}}\)</span>；<span class="math inline">\(H\)</span> 是对于 <span class="math inline">\(\theta\)</span> 的海森矩阵（Hessian matrix）。</p><p>于是得到如下的近似优化问题： <span class="math display">\[\theta_{k+1}=\arg\max_\theta g^T(\theta-\theta_k)\\\text{s.t. }\frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k)≤\delta\]</span> 通过拉格朗日法求解上述约束优化问题得： <span class="math display">\[\theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g\]</span> 这个就是 <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf" target="_blank" rel="noopener">Natural Policy Gradient</a> 的更新公式。不过，由于泰勒近似引入了误差，上式的解可能不满足 KL 约束，所以 TRPO 增加了一个线性搜索（backtracking line search）： <span class="math display">\[\theta_{k+1}=\theta_k+\alpha^j\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g\]</span> 其中，<span class="math inline">\(\alpha\in(0,1)\)</span> 是回溯系数（backtracking coefficient），<span class="math inline">\(j\)</span> 是最小的非负整数使得 <span class="math inline">\(\pi_{\theta_{k+1}}\)</span> 满足 KL 约束并且产生<strong>正</strong>的替代优势，这样就可以保证训练进步是单调的。</p><p>最后，计算和存储 <span class="math inline">\(H^{-1}\)</span> 的开销是很大的，尤其是神经网络的参数动不动就几M。TRPO 使用<a href="https://www.wikiwand.com/en/Conjugate_gradient_method" target="_blank" rel="noopener">共轭梯度法（conjugate gradient）</a>来解 <span class="math inline">\(Hx = g\)</span>，这样就不用直接计算和存储 <span class="math inline">\(H\)</span>。</p><p>最终的更新公式为： <span class="math display">\[\theta_{k+1}=\theta_k+\alpha^j\sqrt{\frac{2\delta}{\hat{x}^TH\hat{x}}}\hat{x}\]</span> 其中， <span class="math display">\[\begin{align}\hat{x}&amp;\approx H^{-1}g &amp; \text{(using conjugate gradient)}\end{align}\]</span></p><p><span class="math display">\[H\hat{x} = \triangledown_\theta((\triangledown_\theta\bar{D}_{KL}(\theta||\theta_k))^T\hat{x})\]</span></p><p><strong>TRPO算法</strong></p><p><img src="/images/trpo.svg"></p><h2><span id="performance">Performance</span></h2><p><strong>TRPO的一些特点</strong></p><ul><li>保证每次更新在当前训练数据上都是进步的，训练过程更加稳定</li><li>通过满足KL约束来找尽可能大的步长</li><li>on-policy 算法</li><li>可用于离散和连续的动作空间</li><li>算法较为复杂</li></ul><p><strong>实验性能</strong></p><p>在模拟机器人走路、游泳等任务中取得了在当时不错的效果；在通过视频输入玩Atari游戏的任务中表现不如DQN等方法。</p><p><img src="/images/trpo_per.jpg"></p><p>下图是我在 OpenAI <a href="https://gym.openai.com/" target="_blank" rel="noopener">Gym</a> 的 <code>Walker2d-v2</code> 和 <code>MsPacman-ram-v0</code> 中测试的结果。</p><p><img src="/images/walker.png"></p><p><img src="/images/pacman.png"></p><h2><span id="references">References</span></h2><p>[1] https://arxiv.org/abs/1502.05477</p><p>[2] https://spinningup.openai.com/en/latest/algorithms/trpo.html</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1502.05477&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Trust Region Policy Optimization (TRPO)&lt;/a&gt;算法是由伯克利大学的Schulman等人于2015年提出的策略梯度（Policy Gradients）算法。TRPO通过最大化新策略相对于当前策略的优势来保证每次更新都是单调递增的（稳定），同时找到尽可能大的更新步幅。算法推导出的最终结果是在KL约束下最大化替代优势函数。&lt;/p&gt;
    
    </summary>
    
      <category term="博客" scheme="http://www.52coding.com.cn/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy Gradient" scheme="http://www.52coding.com.cn/tags/Policy-Gradient/"/>
    
      <category term="TRPO" scheme="http://www.52coding.com.cn/tags/TRPO/"/>
    
  </entry>
  
</feed>
