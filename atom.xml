<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>NIUHE</title>
  
  <subtitle>日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.52coding.com.cn/"/>
  <updated>2018-12-07T13:22:02.079Z</updated>
  <id>http://www.52coding.com.cn/</id>
  
  <author>
    <name>NIUHE</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Recognizing Fingerprints</title>
    <link href="http://www.52coding.com.cn/2018/12/07/RS%20-%20Recognizing%20Fingerprints/"/>
    <id>http://www.52coding.com.cn/2018/12/07/RS - Recognizing Fingerprints/</id>
    <published>2018-12-07T13:20:07.000Z</published>
    <updated>2018-12-07T13:22:02.079Z</updated>
    
    <content type="html"><![CDATA[<p>HKUST CSIT5401 Recognition System lecture notes 4. 识别系统复习笔记。</p><!-- toc --><ul><li><a href="#fingerprint-image-acquisition-systems">Fingerprint image acquisition systems</a></li><li><a href="#minutiae">Minutiae</a></li><li><a href="#fingerprint-enhancement">Fingerprint Enhancement</a><ul><li><a href="#normalization">Normalization</a></li><li><a href="#orientation-image-estimation">Orientation Image Estimation</a></li><li><a href="#ridge-frequency-estimation">Ridge Frequency Estimation</a></li><li><a href="#region-mask-estimation">Region mask estimation</a></li><li><a href="#gabor-filter">Gabor Filter*</a></li></ul></li><li><a href="#fingerprint-matching">Fingerprint Matching</a></li><li><a href="#fingercode">FingerCode</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="fingerprint-image-acquisition-systems">Fingerprint image acquisition systems</span></h2><p><a href="http://en.wikipedia.org/wiki/Fingerprint" target="_blank" rel="noopener">Fingerprint</a> matching (recognition) is the most popular biometric technique used in automatic personal identification. The main reason for the popularity of fingerprints as a form of identification is that the fingerprint of a person is unique and remains invariant with his or her age.</p><p>There are three kinds of sensing devices typically: <strong>optical sensors</strong>, <strong>solid-state sensors</strong> and <strong>ultrasound sensors</strong>.</p><p><strong>Optical sensors</strong></p><p>The finger touches the top side of a glass prism. The left side of the prism is illustrated through a diffused light. The light entering the prism is reflected at the valleys, and absorbed at the ridges. The lack of reflection allows the ridges to be discriminated from the valleys. The light rays exit from the right side of the prism and are focused through a lens onto an image sensor.</p><p><img src="/images/ridgeandvall.png"></p><p><strong>Solid-state sensors</strong></p><p>All silicon-based sensors consist of an array of pixels, each pixel being a tiny sensor itself. The user directly touches the surface of the silicon. The physical information is converted into <strong>electrical signals</strong> by using the capacitive sensor (other kinds of sensor can also be used, e.g., thermal, electric field and piezoelectric).</p><p>The capacitive sensor is a 2D array of micro-capacitor plates embedded in a chip. The other plate of each micro-capacitor is the finger skin itself.</p><p><img src="/images/ssdfinger.png"></p><p>Small electrical charges are created between the surface of the finger and each of the silicon plates when a finger is placed on the chip. The magnitude of these electrical charges depends on the distance between the fingerprint surface and the capacitance plates.</p><p><strong>Ultrasound sensors</strong></p><p>An ultrasound sensor is based on sending acoustic signals toward the fingertip and capturing the echo signal. The echo signal is used to compute the range image of the fingerprint and, subsequently, the ridge structure itself.</p><p><img src="/images/ultrasound.png"></p><h2><span id="minutiae">Minutiae</span></h2><p>An <strong>automatic fingerprint identification system</strong> (AFIS) consists of various processing stages.</p><p><img src="/images/afis.png"></p><p>In AFIS, the high-level structural features (<strong>ridges and valleys</strong>) are extracted from the fingerprint image for the purpose of representation and matching. The ridges and valleys in a fingerprint alternate, flowing in a local constant direction.</p><p><img src="/images/ridandval.png"></p><p>The ridges (or the valleys) exhibit anomalies of various kinds, such as ridge bifurcations, ridge endings, short ridges, and ridge crossovers. These features are called <a href="http://en.wikipedia.org/wiki/Minutiae" target="_blank" rel="noopener">minutiae</a>.</p><p>In a good quality rolled fingerprint image, there are about 70 to 80 minutia points and in a latent fingerprint the number of minutiae is much less (approximately 20 to 30 minutia points). Commercially available fingerprint identification systems typically use <strong>ridge bifurcations</strong> and <strong>ridge endings</strong> as features.</p><p><img src="/images/bifandending.png"></p><ul><li>A ridge ending is defined as the point where a ridge ends abruptly.</li><li>A ridge bifurcation is defined as the point where a ridge diverges into branch ridges.</li></ul><p>A critical step in fingerprint matching is to automatically and reliably <strong>extract minutiae</strong> from the input fingerprint images, which is a difficult task.</p><p><img src="/images/bifandend2.png"></p><h2><span id="fingerprint-enhancement">Fingerprint Enhancement</span></h2><p>Fingerprint images can be of very poor quality. An enhancement algorithm which can improve the clarity of the ridge structure is therefore necessary.</p><p>The flowchart of the fingerprint enhancement algorithm is shown below.</p><p><img src="/images/imgnorflow.png"></p><h3><span id="normalization">Normalization</span></h3><p>A gray-level fingerprint image <span class="math inline">\(I\)</span> is defined as an <span class="math inline">\(N \times N\)</span> matrix. At the <span class="math inline">\(i\)</span> th row and <span class="math inline">\(j\)</span> th column, the intensity of the pixel is <span class="math inline">\(I(i, j)\)</span>. It is assumed that the fingerprint images are scanned at a resolution of 500 dots per inch (dpi), which is the resolution recommended by FBI.</p><p>The mean and variance of a gray-level fingerprint image are defined as</p><p><img src="/images/imgnorma.png"></p><p>An input fingerprint image is normalized so that it has a pre-specified mean <span class="math inline">\(M_0\)</span> and variance <span class="math inline">\(\text{VAR}_0\)</span>.</p><p>The normalized image <span class="math inline">\(G(i,j)\)</span> is defined as <span class="math display">\[G(i,j)=\begin{cases}M_0+\sqrt{\frac{VAR_0(I(i,j)-M)^2}{VAR}} &amp;\text{if }I(i,j)&gt;M\\M_0-\sqrt{\frac{VAR_0(I(i,j)-M)^2}{VAR}}&amp;\text{otherwise}\\\end{cases}\]</span> <img src="/images/imgnor2.png"></p><h3><span id="orientation-image-estimation">Orientation Image Estimation</span></h3><p>The orientation image (field) is estimated from the normalized input fingerprint image. By viewing a fingerprint image as an oriented image, a <strong>least-mean-square orientation estimation</strong> algorithm is used to estimate the local orientation.</p><p>The main steps are as follows.</p><p>[1] Divide the normalized image <span class="math inline">\(G\)</span> into blocks of size <span class="math inline">\(w \times w\)</span>.</p><p>[2] For each block, compute the gradients <span class="math inline">\(I_x\)</span> and <span class="math inline">\(I_y\)</span> at each pixel <span class="math inline">\((i , j)\)</span>.</p><p>[3] Estimate the local orientation of each block centered at pixel <span class="math inline">\((i, j)\)</span> using the following equations. <span class="math display">\[\theta(i,j)=90^o+\frac{1}{2}\text{atan2}\left(\frac{V_1(i,j)}{V_2(i,j)}\right)\]</span> where <span class="math display">\[V_1(i,j)=\sum_{u=i-\frac{w}2}^{i+\frac{w}2}\sum_{v=j-\frac{w}2}^{j+\frac{w}2}2I_x(u,v)I_y(i,v)\\V_2(i,j)=\sum_{u=i-\frac{w}2}^{i+\frac{w}2}\sum_{v=j-\frac{w}2}^{j+\frac{w}2}(I_x(u,v)^2-I_y(u,v)^2)\\-180^o≤\text{atan2}(x)≤180^o\]</span> [4] Due to the presence of noise, corrupted ridge and valley structures, minutiae, etc. in the input image, the estimated local ridge orientation may not always be correct. The local orientation image can be smoothed by using the low-pass smoothing filter and the concept of continuous vector field.</p><p><img src="/images/orienesti.png"></p><h3><span id="ridge-frequency-estimation">Ridge Frequency Estimation</span></h3><p>The gray levels along ridges and valleys can be modeled as a sinusoidal-shaped wave along a direction normal to the local ridge orientation.</p><p><img src="/images/ridgefreq.jpg"></p><p>Let <span class="math inline">\(G\)</span> be the normalized image and <span class="math inline">\(O\)</span> be the orientation image (field). For estimating the ridge frequency <span class="math inline">\(Ω\)</span> image,</p><ul><li><p>Step 1: divide <span class="math inline">\(G\)</span> into blocks of size <span class="math inline">\(w \times w\)</span>.</p></li><li><p>Step 2: for each block centered at pixel <span class="math inline">\((i, j)\)</span>, compute an oriented window of size <span class="math inline">\(w \times l\)</span> that is defined in the ridge coordinate system <span class="math inline">\((k, d)\)</span>.</p></li><li><p>Step 3: for each block centered at pixel <span class="math inline">\((i, j)\)</span>, compute the x-signature, <span class="math inline">\(X[0], X[1], ..., X[l-1]\)</span>, of the ridges and valleys within the oriented window, where <span class="math display">\[X[k]=\frac{1}w\sum_{d=0}^{w-1}G(u,v)\\u=i+(d-\frac{w}2)\cos O(i,j)+(k+\frac{l}2)\sin O(i,j)\\v=i+(d-\frac{w}2)\sin O(i,j)+(\frac{l}2-k)\cos O(i,j)\]</span></p></li></ul><p>The x-signature forms a discrete sinusoidal-shape wave, which has the same frequency as that of the ridges and valleys in the oriented window. Therefore, the <strong>frequency of ridges and valleys can be estimated from the x-signature</strong>.</p><p>Let <span class="math inline">\(T(i, j)\)</span> be the average number of pixels between two consecutive peaks in the x-signature, then the ridge frequency <span class="math inline">\(Ω(i, j)\)</span> is computed as <span class="math display">\[\Omega(i,j)=\frac{1}{T(i,j)}\]</span></p><h3><span id="region-mask-estimation">Region mask estimation</span></h3><p>A pixel (or a block) in an input fingerprint image can be either in a recoverable region or an unrecoverable region. Classification of pixels into recoverable and unrecoverable categories can be performed based on the assessment of the shape of the wave formed by the local ridges and valleys.</p><p><img src="/images/imgmaskest.png"></p><p>Three features are used to characterize the sinusoidal-shaped wave: amplitude, frequency, and variance.</p><p><img src="/images/imgmaskest2.png"></p><p>Typical fingerprint images where both recoverable and unrecoverable regions were manually labeled can be selected for region mask estimation. The above three features can be computed for each image.</p><p>Using the k-NN and clustering algorithms, each <span class="math inline">\(w \times w\)</span> block in an input fingerprint image can be classified into a recoverable or an unrecoverable block.</p><h3><span id="gabor-filter">Gabor Filter*</span></h3><p>The configurations of parallel ridges and valleys with well-defined frequency and orientation in a fingerprint image provide useful information which helps in removing undesired noise.</p><p>A special (bandpass) filter, namely <strong>Gabor filter</strong>, that is tuned to the corresponding frequency and orientation can efficiently remove the undesired noise and preserve the true ridge and valley structures.</p><p>Gabor filters have both frequency-selective and orientation-selective properties and are used for removing noise and preserving true ridge or valley structures.</p><p>The even-symmetric Gabor filter has the following general form:</p><p><img src="/images/gaborfilter.png"></p><p>The frequency of the filter <span class="math inline">\(f\)</span> is completely determined by the local ridge frequency and the Gabor filter orientation is determined by the local ridge orientation.</p><p><img src="/images/gabororien.png"></p><p>The ridge pixels are assigned a value '1' (white) and the remaining pixels are assigned a value '0' (black) in the resulting binary ridge image.</p><p><img src="/images/imgseg.png"></p><p>Once the ridges are located, <strong>directional smoothing</strong> is applied to smooth the ridges. A 3×7 mask is placed along the orientation field for each window. The mask containing all '1's enables us to count the number of '1's in the mask area.If the count of '1's is more than 25% of the total number of pixels, the ridge point is retained.</p><p><strong>Extracting minutiae</strong></p><p>Locating minutia points in the thinned image is relatively easy.</p><p>A count of the number of 'on' neighbors at a point of interest in a <span class="math inline">\(3\times 3\)</span> window is sufficient for this purpose.</p><ul><li>A ridge end point has only neighbor in the window.</li><li>A ridge bifurcation has at least three neighbors.</li></ul><p>Some post-processing can be performed to further improve detection quality.</p><p><img src="/images/endandbif3.jpg"></p><p><img src="/images/minuextra.png"></p><h2><span id="fingerprint-matching">Fingerprint Matching</span></h2><p>Matching a query fingerprint and a database fingerprint is equivalent to matching their minutia sets. Each database fingerprint minutia, <span class="math inline">\(p\)</span>, is examined to determine whether there is a corresponding query fingerprint minutia, <span class="math inline">\(q\)</span>.</p><p>There are three steps.</p><ol type="1"><li>Registration</li><li>Minutia paring</li><li>Matching score computation</li></ol><p><strong>Registration via Hough Transform</strong></p><p>The input to the registration algorithm consists of two sets of minutia points <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>. <span class="math inline">\(|P|\)</span> and <span class="math inline">\(|Q|\)</span> represent the sizes of point sets <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> respectively. <span class="math display">\[P=\{(p_x^1,p_y^1,\alpha^1),...,(p_x^{|P|},p_y^{|P|},\alpha^{|P|})\}\\Q=\{(q_x^1,q_y^1,\beta^1),...,(q_x^{|Q|},q_y^{|Q|},\beta^{|Q|})\}\]</span> Each minutia has three components: x-coordinate, y-coordinate and orientation of the minutia. Each minutia in <span class="math inline">\(P\)</span> is <strong>rotated, scaled and translated</strong> for matching against a minutia in <span class="math inline">\(Q\)</span>.</p><p>The usual <strong>Hough transform</strong> for line detection can be generalized for point matching.</p><p><img src="/images/regishoughtra.png"></p><p>The transform has maximum value of <span class="math inline">\(A\)</span> means that it can match as much points as possible.</p><p><img src="/images/imgregriscomp.png"></p><p><strong>Minutia pairing and score computation</strong></p><p>After minutia registration, the minutiae need to be paired. Two minutiae are said to be paired or matched if their components <span class="math inline">\((x, y,θ)\)</span> are equal with some tolerance after registration.</p><p><img src="/images/minutiamatch.png"></p><p>The matching algorithm is based on finding the number of paired minutiae between each database fingerprint and the query fingerprint.</p><p>In order to reduce the amount of computation, the matching algorithm takes into account only those minutiae that fall within a common bounding box. The common bounding box is the intersection of the bounding box for query and reference (database) fingerprints. Once the count of matching minutiae is obtained, a matching score is computed. The matching score is used for deciding the degree of match. Finally, a set of top ten scoring reference fingerprints is obtained as a result of matching.</p><h2><span id="fingercode">FingerCode</span></h2><p><em>FingerCode</em> is a new representation for the fingerprints which yields a <strong>relatively short, fixed length code</strong> suitable for matching as well as storage on a smartcard.</p><p>The matching reduces to finding the <strong>Euclidean distance</strong> between these <em>FingerCodes</em> and hence the matching is very fast and the representation is amenable to indexing.</p><p>The FingerCode partitions the region of interest of the given fingerprint image with respect to a reference point. A feature vector is composed of an ordered enumeration of features extracted from the information contained in each sector specified by the tessellation.</p><p><img src="/images/fingercode1.png"></p><p>The feature elements capture the local information by using the <strong>Gabor filterbank</strong>. The ordered enumeration of the tessellation captures the <strong>invariant global relationships</strong> among the local patterns.</p><p>These features capture both the global pattern of ridges and valleys and the local characteristics. Matching is based on the Euclidean distance between the FingerCodes.</p><p>There are four steps for extracting the FingerCode.</p><ol type="1"><li>Determining the reference point for the fingerprint image.</li><li>Partitioning the region around the reference point.</li><li>Filtering the region of interest in eight different directions using a bank of Gabor filters.</li><li>Computing the <strong>average absolute deviation</strong> (AAD) from the mean of gray values in individual sectors in filtered images to define the <em>FingerCode</em> (the feature vector) for matching.</li></ol><p><strong>Determining the reference point</strong></p><p><img src="/images/fingercode2.png"></p><p>Given an input fingerprint image, there are seven steps for finding the reference point.</p><ol type="1"><li><p>Estimate the orientation field <span class="math inline">\(O\)</span> using a window size of <span class="math inline">\(w\times w\)</span>.</p></li><li><p>Smooth the orientation field.</p></li><li><p>Compute the sine component <span class="math inline">\(E\)</span> of the orientation field <span class="math inline">\(O\)</span>.<br><span class="math display">\[E(i,j)=\sin O(i,j)\]</span></p></li><li><p>Initialize <span class="math inline">\(A\)</span>, a label image used to indicate the reference point.</p></li><li><p>For each pixel <span class="math inline">\(E(i, j)\)</span>, integrate pixel intensities in regions <span class="math inline">\(R_I\)</span> and <span class="math inline">\(R_{II}\)</span>, and assign the corresponding pixels in <span class="math inline">\(A\)</span> according to the value of their difference. <span class="math display">\[A(i,j)=\sum_{R_I}E(i,j)-\sum_{R_{II}}E(i,j)\]</span> <img src="/images/fingercode4.png"></p></li><li><p>Find the maximum value in <span class="math inline">\(A\)</span> and assign its coordinate to the reference point.</p></li><li><p>Repeat steps 1-6 by using a window size <span class="math inline">\(w&#39;\times w&#39;\)</span>, where <span class="math inline">\(w&#39; &lt; w\)</span>, and restrict the search for the reference point in step 6 in a local neighborhood of the detected reference point.</p></li></ol><p>The geometry of regions <span class="math inline">\(R_I\)</span> and <span class="math inline">\(R_{II}\)</span> is designed to capture the maximum curvature in concave ridges.</p><p><img src="/images/fingercode3.png"></p><p><strong>Partitioning the region around the reference point</strong></p><p>Given the detected reference point, the input fingerprint image is partitioned into 80 sectors.</p><p><img src="/images/fingercode5.png"></p><p><strong>Filtering the region of interest</strong></p><p>A minutia point can be viewed as an anomaly in locally parallel ridges and it is the information that is captured by using the Gabor filters.</p><p>Before filtering the fingerprint image, <strong>image normalization</strong> is performed separately for each sector with <span class="math inline">\(M_0\)</span> and <span class="math inline">\(VAR_0\)</span>.</p><p>An even symmetric Gabor filter is given <a href="#gabor-filter*">the Gabor filer setction</a>. The filter frequency <span class="math inline">\(f\)</span> can be set to the average ridge frequency. The average ridge frequency is the reciprocal of the average inter-ridge distance, which is around 10 pixels in a 500 dpi fingerprint image. Eight different values (<span class="math inline">\(0^o\)</span>, <span class="math inline">\(22.5^o\)</span>, <span class="math inline">\(45^o\)</span>, <span class="math inline">\(67.5^o\)</span>, <span class="math inline">\(90^o\)</span>, <span class="math inline">\(112.5^o\)</span>, <span class="math inline">\(135^o\)</span> and <span class="math inline">\(157.5^o\)</span>) are used for the direction θ with respect to the x-axis.</p><p>A fingerprint convolved with a <span class="math inline">\(0^o\)</span>-oriented filter accentuates those ridges which are parallel to the x-axis and smoothes the ridges in the other directions. These eight directional-sensitive filters capture <strong>most of the global ridge directionality information</strong> as well as the <strong>local ridge characteristics</strong> present in a fingerprint.</p><p><img src="/images/fingercode6.png"></p><p><strong>Compute AAD</strong></p><p>Let <span class="math inline">\(F_{iθ}(x, y)\)</span> be the θ-direction filtered image for sector <span class="math inline">\(S_i\)</span>. The feature value <span class="math inline">\(V_{iθ}\)</span> is the average absolute deviation (AAD) from the mean which is defined as <span class="math display">\[V_{i\theta}=\frac{1}{n_i}\sum_{(x,y)\in S_i}|F_{i\theta}(x,y)-P_{i\theta}|\]</span> where <span class="math inline">\(P_{i\theta}=\frac{1}{n_i}\sum_{(x,y)\in S_i}F_{i\theta}(x,y)\)</span>; <span class="math inline">\(n_i\)</span> is the number of pixels in <span class="math inline">\(S_i\)</span>.</p><p><img src="/images/fingercode7.png"></p><p>The average absolute deviation of each sector in each of the eight filtered images defines the components of the feature vector. Fingerprint matching is based on finding the <strong>Euclidean distance</strong> between the corresponding <em>FingerCodes</em>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HKUST CSIT5401 Recognition System lecture notes 4. 识别系统复习笔记。&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#fingerprint-image-acquisition-systems&quot;&gt;Fingerprint image acquisition systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#minutiae&quot;&gt;Minutiae&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#fingerprint-enhancement&quot;&gt;Fingerprint Enhancement&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#normalization&quot;&gt;Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#orientation-image-estimation&quot;&gt;Orientation Image Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#ridge-frequency-estimation&quot;&gt;Ridge Frequency Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#region-mask-estimation&quot;&gt;Region mask estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#gabor-filter&quot;&gt;Gabor Filter*&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#fingerprint-matching&quot;&gt;Fingerprint Matching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#fingercode&quot;&gt;FingerCode&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Recognition System" scheme="http://www.52coding.com.cn/tags/Recognition-System/"/>
    
      <category term="fingerprints" scheme="http://www.52coding.com.cn/tags/fingerprints/"/>
    
      <category term="fingercode" scheme="http://www.52coding.com.cn/tags/fingercode/"/>
    
      <category term="hough transform" scheme="http://www.52coding.com.cn/tags/hough-transform/"/>
    
      <category term="minutiae" scheme="http://www.52coding.com.cn/tags/minutiae/"/>
    
  </entry>
  
  <entry>
    <title>Recognizing Faces</title>
    <link href="http://www.52coding.com.cn/2018/12/06/RS%20-%20Recognizing%20Faces/"/>
    <id>http://www.52coding.com.cn/2018/12/06/RS - Recognizing Faces/</id>
    <published>2018-12-06T14:03:09.000Z</published>
    <updated>2018-12-07T13:21:27.734Z</updated>
    
    <content type="html"><![CDATA[<p>HKUST CSIT5401 Recognition System lecture notes 3. 识别系统复习笔记。</p><!-- toc --><ul><li><a href="#histogram-equalization">Histogram Equalization</a></li><li><a href="#image-pyramid-and-neural-networks">Image Pyramid and Neural Networks</a></li><li><a href="#integral-image">Integral Image</a></li><li><a href="#adaboost">Adaboost</a></li><li><a href="#face-recognition-pca">Face Recognition (PCA)</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="histogram-equalization">Histogram Equalization</span></h2><p><a href="http://en.wikipedia.org/wiki/Face_detection" target="_blank" rel="noopener">Face detection</a> is the first step in automated face recognition. Its reliability has a major influence on the performance and usability of the entire face recognition system.</p><p>Due to lighting or shadow, intensity can vary significantly in an image. Normalization of pixel intensity helps correct variations in imaging parameters in cameras as well as changes in illumination conditions. One widely used technique is <a href="http://en.wikipedia.org/wiki/Histogram_equalization" target="_blank" rel="noopener">histogram equalization</a>, which is based on image histogram. It helps reduce extreme illumination.</p><p><strong>Image histogram</strong></p><p>It is assumed that there is a digital image with <span class="math inline">\(L\)</span> gray levels <span class="math inline">\(r_k\)</span>. The probability of occurrence of gray level <span class="math inline">\(r_k\)</span> is given by <span class="math display">\[p_r(r_k)=\frac{n_k}{N}\]</span> where <span class="math inline">\(n_k\)</span> is number of pixels with gray level <span class="math inline">\(r_k\)</span>; <span class="math inline">\(N\)</span> is total number of pixels in an image; <span class="math inline">\(k = 0,1,2,...,L-1\)</span>.</p><p><img src="/images/imagehis.png"></p><p>We want an image with equally many pixels at every gray level, or the output intensity approx follows <strong>uniform distribution</strong>.</p><p>That is, a flat histogram, where each gray level, <span class="math inline">\(r_k\)</span>, appears equal number of times, i.e., <span class="math inline">\(N/L\)</span> times.</p><p><img src="/images/imgequ.png"></p><p>Assume that variable <span class="math inline">\(r\)</span> has been normalized between <span class="math inline">\([0,1]\)</span>. The intensity transformation is <span class="math inline">\(s = T(r)\)</span>, such that</p><ul><li><span class="math inline">\(T(r)\)</span> is single-valued and non-decreasing in the interval <span class="math inline">\(0≤r≤1\)</span>.</li><li><span class="math inline">\(0≤T(r)≤1\)</span> for <span class="math inline">\(0≤r≤1\)</span>.</li></ul><p><strong>Histogram equalization transform</strong></p><p>The intensity transformation is the cumulative distribution function (CDF) of <span class="math inline">\(r\)</span>, which is represented by <span class="math display">\[s=T(r)=\int_0^rp_r(w)dw\]</span> The discrete implementation is given by <span class="math display">\[s_k=T(r_k)=\sum_{j=0}^k\frac{n_j}{N}=\sum_{j=0}^kp_r(r_j)\]</span> where <span class="math inline">\(s_k\)</span> is the <strong>output intensity</strong>; <span class="math inline">\(r_k\)</span> is the input intensity; <span class="math inline">\(n_j\)</span> is the number of pixels with gray level <span class="math inline">\(r_j\)</span>.</p><p>Below are some examples:</p><p><img src="/images/hisequeg.png"></p><p><img src="/images/hisequeg2.png"></p><p>Histogram equalization can significantly improve image appearance</p><ul><li>Automatic</li><li>User doesn’t have to perform windowing</li></ul><p>Nice pre-processing step before face detection</p><ul><li>Account for different lighting conditions</li><li>Account for different camera/device properties</li></ul><p>There are two methods for <strong>face detection</strong>:</p><ol type="1"><li>Method using image pyramid and neural networks [Rowley-Baluja-Kanade-98]</li><li>Method using integral image and AdaBoost learning [Viola-Jones-04]</li></ol><h2><span id="image-pyramid-and-neural-networks">Image Pyramid and Neural Networks</span></h2><p>With the neural networks, a classifier may be trained directly using preprocessed and normalized face and nonface training subwindows.</p><p><a href="http://www.cs.cmu.edu/~har/" target="_blank" rel="noopener">Rowley et al</a> use the preprocessed 20x20 subwindows as the input to a neural network. The final decision is made to classify the 20x20 subwindow into face and nonface. The architecture is shown below.</p><p><img src="/images/facepy.png"></p><p>Instead of upright, frontal faces, a <strong>router network</strong> can be trained to process each input window so that orientation can be estimated. Once the orientation is estimated, the input window can be prepared for detector neural network.</p><p><img src="/images/router.png"></p><p><strong>Rowley et al.</strong> proposed two neural networks, as presented in the previous slides. The first one is the router network which is trained to estimate the orientation of an assumed face in the 20x20 sub-window. The second one is the normal frontal, upright face detector. However, it only handles <strong>in-plane rotation</strong>.</p><p><strong>Huang et al.</strong> proposed a multi-view face tree structure for handling both in-plane and <strong>out-of-plane rotations</strong>. Every node corresponds to a strong classifier.</p><p><img src="/images/hung.png"></p><h2><span id="integral-image">Integral Image</span></h2><p><strong>Method using integral image and AdaBoost learning</strong></p><p>The <a href="http://en.wikipedia.org/wiki/Summed_area_table" target="_blank" rel="noopener">integral image</a> <span class="math inline">\(ii(x, y)\)</span> at location <span class="math inline">\((x, y)\)</span> contains the <strong>sum of the pixel intensity values above and to the left</strong> of the location <span class="math inline">\((x, y)\)</span>, inclusive.</p><p>The <span class="math inline">\(ii\)</span> is defined as <span class="math display">\[ii(x,y)=\sum_{x&#39;≤x,y&#39;≤y}i(x&#39;,y&#39;)\]</span> where <span class="math inline">\(ii(x,y)\)</span> is the integral image and <span class="math inline">\(i(x,y)\)</span> is the original input image.</p><p><img src="/images/integralimg.png"></p><p>Using the following pair of recurrences: <span class="math display">\[s(x, y)=s(x, y-1)+i(x,y)\\ii(x,y)=ii(x-1, y)+s(x,y)\]</span> where <span class="math inline">\(s(x,y)\)</span> is the cumulative row sum, <span class="math inline">\(s(x, -1) = 0\)</span>, and <span class="math inline">\(ii(-1, y)=0\)</span>, the integral image can be computed in one pass over the original image.</p><p>Using the integral image, any rectangular sum can be computed in four array references.</p><p><img src="/images/inteeg.png"></p><p><strong>Rectangle features</strong></p><p>The features for face detection are Haar-like functions. There are three kinds of features.</p><p>[1] Two-rectangle feature: The difference between the sum of the pixels within two rectangular regions.</p><p><img src="/images/recfea1.png"></p><p>[2] Three-rectangle feature: The feature is the sum within two outside rectangles subtracted from the sum in a center rectangle.</p><p><img src="/images/recfea2.png"></p><p>[3] Four-rectangle feature: The difference between diagonal pairs of rectangles.</p><p><img src="/images/recfea3.png"></p><p>The rectangle features are sensitive to the presence of edges, bars/lines, and other simple image structures in different scales and at different locations.</p><p>Given that the base resolution of the detector is 24 x 24 pixels, the exhaustive set of rectangle features is quite large, 160,000.</p><p>Given a feature set and a training set of positive and negative images, a classification function must be learned to classify a pattern into either face or non-face.</p><h2><span id="adaboost">Adaboost</span></h2><p>In this work, the classifier is designed based on the assumption that a very small number of features can be combined to form an effective classifier.</p><p>The <a href="http://en.wikipedia.org/wiki/AdaBoost" target="_blank" rel="noopener">AdaBoost</a> learning algorithm is used to boost the classification performance of a simple learning algorithm. The simple learning algorithm is applied to all rectangle features.</p><p>It does this by <strong>combining a collection of weak classification functions</strong> (weak classifiers with relatively high classification error) to form a stronger classifier. The final strong classifier takes the form of <strong>a weighted combination of weak classifiers followed by a threshold</strong>.</p><p>Weak classifier <span class="math inline">\(h_t\)</span> (each classifier compute one rectangle feature): <span class="math display">\[h_t(\vec{x})=\begin{cases}1\ \text{if }\vec{x}\text{ represents a face image }(f_t(\vec{x})&gt;\text{Threshold})\\-1\ \text{otherwise}\end{cases}\\f_t(\vec{x})=\sum_{white} x-\sum_{black} x\]</span> The strong classifier is <span class="math display">\[H(\vec{x})=\text{sgn}\left(\sum_{t=1}^T\alpha_th_t(\vec{x})\right)\]</span> where <span class="math inline">\(\alpha_t\)</span> is weight; and <span class="math inline">\(\text{sgn}(x)\)</span> is sign function: <span class="math display">\[\text{sgn}(x)=\begin{cases} -1,  &amp; \mbox{if }x≤0 \\1, &amp; \mbox{if }x&gt;0\end{cases}\]</span> <strong>Algorithm</strong></p><p>Given example images and classifications <span class="math inline">\((\vec{x}_i, y_i), i = 1, 2,..., N\)</span>, where <span class="math inline">\(N\)</span> is the total number of images.</p><p>Start with equal weights on each image <span class="math inline">\(\vec{x}_i\)</span>.</p><p>For <span class="math inline">\(t=1, ..., T\)</span>:</p><ul><li><p>Normalize all weights <span class="math inline">\(w_i = \frac{w_i}{\sum_{j=1}^Nw_j}\)</span> such that <span class="math inline">\(\sum_{i=1}^Nw_i=1\)</span>.</p></li><li><p>Select the weak classifier <span class="math inline">\(h_k\)</span> with minimum error: <span class="math display">\[e_k=\sum_{i=1}^Nw_i\left(\frac{1-h_k(\vec{x}_i)y_i}{2}\right)\]</span> where <span class="math inline">\(0≤e_k≤1\)</span>.</p></li><li><p>Set weight for selected weak classifier <span class="math display">\[\alpha_t=\frac{1}{2}\ln\left(\frac{1-e_k}{e_k}\right)\]</span></p></li><li><p>Reweight the examples (boosting) <span class="math display">\[w_i=w_i\exp(-\alpha_iy_ih_k(\vec{x}_i))\]</span></p></li></ul><p>For the last step, if the weak classifier classify example <span class="math inline">\(i\)</span> correctly, i.e. <span class="math inline">\(h_k(\vec{x}_i)=y_i\)</span>, then the example weight <span class="math inline">\(w_i=w_ie^{-\alpha_t}\)</span> will decrease; if the weak classifier classify example <span class="math inline">\(i\)</span> wrongly, the weight <span class="math inline">\(w_i=w_i^{\alpha_t}\)</span> will increase.</p><p>Values of <span class="math inline">\(T\)</span> can be 200 for <span class="math inline">\(N=10^8\)</span> images and 180,000 filters. Given the above strong classifier, a new image can classified as either face or non-face.</p><h2><span id="face-recognition-pca">Face Recognition (PCA)</span></h2><p>Images of faces often belong to a manifold of intrinsically low dimension. For example, if there are three 3x1 images (see below), then each image has three intensity values. If each intensity value is viewed as a coordinate in a 3D space, then each image can be viewed as a point in a 3D space.</p><p><img src="/images/imgspace.png"></p><p>To represent these points effectively, the number of dimensions can be reduced from three to one. It is the concept of <a href="http://en.wikipedia.org/wiki/Dimension_reduction" target="_blank" rel="noopener">dimensionality reduction</a>.</p><p><a href="http://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="noopener">Principal component analysis</a> (PCA) is a method for performing dimensionality reduction of high dimensional face images.</p><p><strong>Eigenfaces</strong></p><p>Let us consider a set of <span class="math inline">\(N\)</span> sample images (image vectors) with <span class="math inline">\(m\times n\)</span> dimensions:</p><p><img src="/images/eigenface1.png"></p><p>Each image is represented by a 1D vector with dimensions <span class="math inline">\((m\times n) \times 1\)</span>. The <strong>mean image vector</strong> is given by <span class="math display">\[\vec{x}=\frac{1}{N}\sum_{i=1}^N\begin{bmatrix}x_{i,1}      \\\vdots \\x_{i,mn}\end{bmatrix}\]</span> The <strong>scatter matrix</strong> is given by <span class="math display">\[\vec{S}=[\vec{x_1}-\bar{x}\ \ \vec{x_2}-\bar{x}\ \dots\ \vec{x_N}-\bar{x}]\begin{bmatrix}(\vec{x_1}-\bar{x})^T     \\(\vec{x_2}-\bar{x})^T\\\vdots \\(\vec{x_N}-\bar{x})^T\end{bmatrix}\]</span> The corresponding <span class="math inline">\(t\)</span> eigenvectors with non-zero eigenvalues <span class="math inline">\(\lambda_i\)</span> are <span class="math display">\[\vec{e}_1\ \ \vec{e}_2\ \ \dots\ \ \vec{e}_t\]</span> where <span class="math inline">\(\lambda_1≥\lambda_2≥...≥\lambda_t\)</span>.</p><p>Then the origin image vector can be approximated by <span class="math display">\[\vec{x}_j\approx\bar{x}+\sum_{i=1}^tg_{ji}\vec{e}_i\]</span> where <span class="math inline">\(g_{ji}=(\vec{x}_j-\bar{x})\cdot\vec{e}_i\)</span>.</p><p><img src="/images/egface.png"></p><p>Since the eigenvectors <span class="math inline">\(e\)</span> have the same dimension as the image vectors, the eigenvectors are referred as <a href="http://en.wikipedia.org/wiki/Eigenface" target="_blank" rel="noopener">Eigenfaces</a>. The value of <span class="math inline">\(t\)</span> is usually much smaller than the value of <span class="math inline">\(mn\)</span>. Therefore, the number of dimensions can be reduced significantly.</p><p>For each image <span class="math inline">\(\vec{x}_i\)</span>, the dimension reduced representation is <span class="math display">\[(g_{i1}, g_{i2}, ..., g_{it})\]</span> To detect if the new image <span class="math inline">\(\vec{x}\)</span> with <span class="math inline">\(t\)</span> coefficients <span class="math inline">\((g_1, g_2, ..., g_t)\)</span> is a face: <span class="math display">\[||\vec{x}-(\bar{x}+g_1\vec{e}_1+g_2\vec{e}_2+...+g_t\vec{e}_t)||&lt;\text{Threshold}\]</span> If it is a face, find the closest labeled face based on the nearest neighbor in the <span class="math inline">\(t\)</span>-dimensional space.</p><p><strong>Near-infrared images for face recognition</strong></p><p>Most current face recognition systems are based on face images captured in the visible light spectrum. The infrared imaging system is able to produce face images of good condition regardless of visible lights in the environment.</p><p><img src="/images/infared.png"></p><p><img src="/images/infeared2.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HKUST CSIT5401 Recognition System lecture notes 3. 识别系统复习笔记。&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#histogram-equalization&quot;&gt;Histogram Equalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#image-pyramid-and-neural-networks&quot;&gt;Image Pyramid and Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#integral-image&quot;&gt;Integral Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#adaboost&quot;&gt;Adaboost&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#face-recognition-pca&quot;&gt;Face Recognition (PCA)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="PCA" scheme="http://www.52coding.com.cn/tags/PCA/"/>
    
      <category term="Recognition System" scheme="http://www.52coding.com.cn/tags/Recognition-System/"/>
    
      <category term="Face Recognition" scheme="http://www.52coding.com.cn/tags/Face-Recognition/"/>
    
      <category term="histogram equalization" scheme="http://www.52coding.com.cn/tags/histogram-equalization/"/>
    
      <category term="Adaboost" scheme="http://www.52coding.com.cn/tags/Adaboost/"/>
    
  </entry>
  
  <entry>
    <title>Recognizing Irises</title>
    <link href="http://www.52coding.com.cn/2018/12/05/RS%20-%20Recognizing%20Irises/"/>
    <id>http://www.52coding.com.cn/2018/12/05/RS - Recognizing Irises/</id>
    <published>2018-12-05T13:56:09.000Z</published>
    <updated>2018-12-07T13:21:47.965Z</updated>
    
    <content type="html"><![CDATA[<p>HKUST CSIT5401 Recognition System lecture notes 2. 识别系统复习笔记。</p><!-- toc --><ul><li><a href="#introduction">Introduction</a></li><li><a href="#image-acquisition-systems">Image Acquisition Systems</a></li><li><a href="#iris-localization">Iris localization</a></li><li><a href="#pattern-matching">Pattern Matching</a><ul><li><a href="#alignment-registration">Alignment (Registration)</a></li><li><a href="#representation">Representation</a></li><li><a href="#goodness-of-match">Goodness of Match</a></li><li><a href="#decision-fld">Decision (FLD)</a></li></ul></li><li><a href="#hough-transform">Hough Transform</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="introduction">Introduction</span></h2><p>Face recognition and iris recognition are non-invasive method for verification and identification of people. In particular, the spatial patterns that are apparent in the human iris are highly distinctive to an individual.</p><p><img src="/images/iries.png"></p><p><strong>Schematic diagram of iris recognition</strong></p><p><img src="/images/iries_recog.png"></p><h2><span id="image-acquisition-systems">Image Acquisition Systems</span></h2><p>One of the major challenges of automated iris recognition is to capture a high-quality image of the iris while remaining non-invasive to the human operator.</p><p>There are three concerns while acquiring iris images:</p><ul><li>To support recognition, it is desirable to acquire images of the iris with sufficient resolution and sharpness.</li><li>It is important to have good contrast in the interior iris pattern without resorting to a level of illumination that annoys the operator, i.e., adequate intensity of source constrained by operator comfort with brightness.</li><li>These images must be well framed (i.e., centered) without unduly constraining the operator.</li></ul><p><strong>The Daugman system</strong></p><p>The Daugman system captures images with the iris diameter typically between 100 and 200 pixels from a distance of 15-46cm.</p><p>The system makes use of an LED-based point light source in conjunction with a standard video. By carefully positioning of the point source below the operator, reflections of the light source off eyeglasses can be avoided in the imaged iris.</p><p><img src="/images/daugman.png"></p><p>The Daugman system provides the operator with live video feedback via a tiny liquidcrystal display placed in line with the camera's optics via a beam splitter. This allows the operator to see what the camera is capturing and to adjust his position accordingly.</p><p><strong>The Wildes system</strong></p><p>The Wildes system images the iris with approximately 256 pixels across the diameter from 20cm. The system makes use of a diffused source and polarization in conjunction with a low-light level camera.</p><p>The use of matched <strong>circular polarizer</strong> at the light source and camera essentially eliminates the specular reflection of the light source.</p><p><img src="/images/wildes.png"></p><p>The coupling of a low light level camera with a diffused illumination allows for a level of illumination that is entirely unobjectionable to human operators.</p><p>The relative sizes and positions of the square contours are chosen so that when the eye is in an appropriate position, the squares overlap and appear as one to the operator.</p><h2><span id="iris-localization">Iris localization</span></h2><p><img src="/images/iries_loc.png"></p><p>Image acquisition will capture the iris as part of a larger image that also contains data derived from the immediately surrounding eye region. For example, eyelashes, upper eyelid, lower eyelid and sclera. Therefore, prior to performing iris pattern matching, it is important to <strong>localize</strong> that portion of the acquired image that corresponds to an iris.</p><p><strong>The Wildes system</strong> makes use of the <strong>first derivatives</strong> of image intensity to signal the location of edges that correspond to the borders of the iris.</p><ul><li>Step 1: The image intensity information is converted into binary edge-map.</li><li>Step 2: The edge points vote to particular contour parameter values.</li></ul><p><strong>Step 1</strong></p><p>The edge map is recovered via <strong>gradient-based edge detection</strong>. This operation consists of thresholding the magnitude of the image intensity gradient magnitude. <span class="math inline">\(I\)</span> is the intensity and (x, y) are the image coordinates. <span class="math display">\[\text{Gradient magnitude }|\triangledown G(x, y)\ast I(x, y)|\\\text{2D Gaussian function } G(x, y)=\frac{1}{2\pi\sigma^2}\exp(\frac{(x-x_0)^2+(y-y_0)^2}{2\sigma^2})\]</span> <img src="/images/iris_edge.png"></p><p><strong>Step 2</strong></p><p>The voting procedure is realized via the <a href="#hough-transform">Hough transform</a>. For circular limbic or pupillary boundaries and a set of recovered edge points, a Hough transform is defined as follows.</p><p>Edge points <span class="math inline">\((x_j, y_j)\)</span> for <span class="math inline">\(j = 1, ..., n\)</span>: <span class="math display">\[H(x_c, y_c, r)=\sum_{j=1}^nh(x_j,y_j,x_c,y_c,r)\]</span> where <span class="math display">\[h(x_j, y_j, x_c, y_c, r)=\begin{cases}1, \text{ if }\ g(x_j, y_j, x_c, y_c, r)=0\\0, \text{ otherwise}\end{cases}\\g(x_j, y_j, x_c, y_c, r)=(x_j-x_c)^2+(y_j-y_c)^2-r^2\]</span> For every parameter triple <span class="math inline">\((x_c, y_c, r)\)</span> that represents a circle through the edge point <span class="math inline">\((x_j, y_j)\)</span>, <span class="math display">\[g(x_j, y_j, x_c, y_c, r)=0\]</span> <img src="/images/wildescir.png"></p><p>The parameter triple that <strong>maximizes</strong> the Hough space <span class="math inline">\(H\)</span> is common to the largest number of edge points and is a reasonable choice to represent the contour of interest.</p><hr><p>The limbus and pupil are modeled with <strong>circular contour models</strong>.</p><p><strong>The Daugman system</strong> fits the <strong>circular contours</strong> via gradient ascent on the parameters so as to maximize <span class="math display">\[\left|\frac{\partial}{\partial r}G(r)\ast\oint_{x_c,y_c,r}\frac{I(x,y)}{2\pi r}ds\right|\]</span> where <span class="math inline">\(G(r)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(r-r_0)^2}{2\sigma^2})\)</span>; <span class="math inline">\(r_0\)</span> is the center.</p><p>The first part of the equation is to perform Gaussian smoothing; while the second part is computing the average intensity along the circle.</p><p><img src="/images/IMG_66247EE67551-1.jpg"></p><p>In order to incorporate directional tuning of the image derivative, the arc of integration <span class="math inline">\(ds\)</span> is restricted to the left and right quadrants (i.e., near vertical edges) when fitting the <em>limbic boundary</em>.</p><p>This arc is considered over a fuller range when fitting the <em>pupillary boundary</em>.</p><h2><span id="pattern-matching">Pattern Matching</span></h2><p>Having localized the region of an acquired image that corresponds to the iris, the final task is to decide if this pattern matches a previously stored iris pattern.</p><p>There are four steps:</p><ol type="1"><li>Alignment: bringing the newly acquired iris pattern into spatial alignment with a candidate data base entry.</li><li>Representation: choosing a representation of the aligned iris patterns that makes their distinctive patterns apparent.</li><li>Goodness of Match: evaluating the goodness of match between the newly acquired and data base representations.</li><li>Decision: deciding if the newly acquired data and the data base entry were derived from the same iris based on the goodness of match.</li></ol><h3><span id="alignment-registration">Alignment (Registration)</span></h3><p>To make a detailed comparison between two images, it is advantageous to establish a precise correspondence (or matching) between characteristic structures across the pair.</p><p>Both systems (Daugman and Wildes systems) compensate for image shift, scaling and rotation.</p><p><strong>The Daugman system for alignment</strong></p><p>The Daugman system uses <strong>radial scaling</strong> to compensate for overall size as well as a simple model pupil variation based on <strong>linear stretching</strong>.</p><p>The system maps the Cartesian image coordinates <span class="math inline">\((x, y)\)</span> to dimensionless polar image coordinates <span class="math inline">\((r, θ)\)</span> according to <span class="math display">\[x(r,\theta)=(1-r)x_p(0,\theta)+rx_l(1,\theta)\\y(r,\theta)=(1-r)y_p(0,\theta)+ry_l(1,\theta)\]</span> <img src="/images/daugalign.png"></p><p><img src="/images/daugali.png"></p><p><strong>The Wildes system for alignment</strong></p><p>The Wildes system uses an <strong>image-registration</strong> technique to compensate for both scaling and rotation.</p><p>This approach geometrically warps a newly acquired image <span class="math inline">\(I_a (x, y)\)</span> into alignment with a selected data base image <span class="math inline">\(I_d (x, y)\)</span> according to a mapping function <span class="math inline">\((u(x, y), v(x, y))\)</span> such that for all <span class="math inline">\((x, y)\)</span>, the image intensity value at <span class="math inline">\((x, y) – (u(x, y), v(x, y))\)</span> is close to that at <span class="math inline">\((x, y)\)</span> at <span class="math inline">\(I_d\)</span>.</p><p>The mapping function is taken to minimize <span class="math display">\[\int_x\int_y\left(I_d(x,y)-I_a(x-u, y-v)\right)^2dxdy\]</span> under the constrains to capture similarity transformation of image coordinates <span class="math inline">\((x,y)\)</span> to <span class="math inline">\((x&#39;=x-u, y&#39;=y-v)\)</span>.</p><p><img src="/images/imgreg.jpg"></p><p><strong>Translation</strong> <span class="math display">\[\vec{x}&#39;=\vec{x}+\vec{d}\]</span> <img src="/images/imgtrans.png"></p><p><strong>Rotation</strong> <span class="math display">\[\vec{x}&#39;=R_\theta\vec{x}\\R_\theta=\begin{pmatrix}\cos\theta &amp; -\sin\theta \\\sin\theta &amp; \cos\theta\end{pmatrix}\]</span> <img src="/images/imgrot.png"></p><p><strong>Rotation + Translation</strong> <span class="math display">\[\vec{x}&#39;=R\vec{x}+\vec{d}\]</span> <strong>Scaling + Translation</strong> <span class="math display">\[\vec{x}&#39;=S\vec{x}+\vec{d}\]</span> <img src="/images/scatra.png"></p><p><strong>Shearing</strong> <span class="math display">\[\vec{x}&#39;=K\vec{x}\\K=\begin{bmatrix}1      &amp; k_{xy}     \\k_{yx}     &amp; 1\end{bmatrix}\]</span> <img src="/images/shearing.png"></p><p><strong>Affine</strong>: translation + rotation + scaling + shearing <span class="math display">\[\vec{x}&#39;=R_\theta S K\vec{x}+\vec{d}\]</span> Example: <span class="math display">\[\begin{bmatrix}x&#39; \\y&#39;\end{bmatrix}=\begin{bmatrix}\cos\theta &amp; -\sin\theta \\\sin\theta &amp; \cos\theta\end{bmatrix}\begin{bmatrix}s_x &amp; 0 \\0 &amp; s_y\end{bmatrix}\begin{bmatrix}1 &amp; k_{xy} \\k_{yx} &amp; 1\end{bmatrix}\begin{bmatrix}x \\y\end{bmatrix}+\begin{bmatrix}d_x \\d_y\end{bmatrix}\]</span></p><h3><span id="representation">Representation</span></h3><p>To represent the iris image for matching, both the Daugman and Wildes systems capture the multiscale information extracted from the image.</p><p>The Wildes system makes use of the Laplacian of Gaussian filters to construct a <strong>Laplacian pyramid</strong>.</p><p>The Laplacian of Gaussian (LoG) filter is given by <span class="math display">\[-\frac{1}{\pi\sigma^4}\left(1-\frac{\rho^2}{2\sigma^2}\right)\exp(-\frac{\rho^2}{2\sigma^2})\]</span> where <span class="math inline">\(\rho\)</span> is radial distance of a point from the filter's center; <span class="math inline">\(\sigma\)</span> is standard deviation.</p><p>A Laplacian pyramid is formed by collecting the LoG filtered images.</p><p><img src="/images/logpra.png"></p><h3><span id="goodness-of-match">Goodness of Match</span></h3><p>The Wildes system uses the <strong>normalized correlation</strong> between the acquired representation and data base representation. In discrete form, the normalized correlation can be defined as follows.</p><p>Let <span class="math inline">\(p_1[i, j]\)</span> and <span class="math inline">\(p_2[i, j]\)</span> be two image arrays of size <span class="math inline">\(n \times m\)</span>.</p><p><img src="/images/gom.png"></p><p>The normal correlation is <span class="math display">\[NC=\frac{\sum_{i=1}^n\sum_{j=1}^m(p_1[i,j]-\mu_1)(p_2[i,j]-\mu_2)}{nm\sigma_1\sigma_2}\]</span> <img src="/images/gompy.png"></p><h3><span id="decision-fld">Decision (FLD)</span></h3><p>The Wildes system combines four estimated normalized correlation values into a single <strong>accept/reject</strong> judgment.</p><p>In this application, the concept of <a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis" target="_blank" rel="noopener">Fisher's linear discriminant</a> is used for making binary decision. A <strong>weight vector</strong> is found such that <strong>the variance within a class of iris data is minimized</strong> while <strong>the variance between different classes of iris data is maximized</strong> for the transformed samples.</p><p>In iris recognition application, usually there are two classes: <strong>Authentic class (A)</strong> and <strong>Imposter class (I)</strong>.</p><p>To make a binary decision on a line, all points are projected onto the weight vector (or samples are transformed by using the weight vector).</p><p><img src="/images/fld1.png"></p><p>In iris recognition using the Wildes system, all samples are 4-dimensional vectors. Let there be n 4-dimensional samples.</p><p><img src="/images/fld2.png"></p><p>The total within class variance is <span class="math display">\[\vec{S}_w=\vec{S}_i+\vec{S}_a\]</span> Between class variance is <span class="math display">\[\vec{S}_b=(\vec{\mu}_a-\vec{\mu}_i)(\vec{\mu}_a-\vec{\mu}_i)^T\]</span> If all samples are transformed, the ratio of between class variance to total within class variance is <span class="math display">\[\frac{\vec{w}^T\vec{S}_b\vec{w}}{\vec{w}^T\vec{S}_w\vec{w}}\]</span> The ratio is maximized when <span class="math display">\[\vec{w}=\vec{S}_w^{-1}(\vec{\mu}_a-\vec{\mu}_i)\]</span> And the separation point for decision making is <span class="math display">\[\frac{1}{2}\vec{w}^T(\vec{\mu}_a+\vec{\mu}_i)\]</span> Therefore, values above this point will be taken as derived from class <span class="math inline">\(A\)</span>; values below this point will be taken as derived from class <span class="math inline">\(I\)</span>.</p><p><img src="/images/fld3.png"></p><h2><span id="hough-transform">Hough Transform</span></h2><p><strong>Detecting Lines</strong></p><p>Idea: if two edge points <span class="math inline">\((x_i, y_i)\)</span> and <span class="math inline">\((x_j, y_j)\)</span> lie on the same straight line, then they should have the same values of slope and y-intercepts on the xy-plane.</p><p><img src="/images/houghline.png"></p><p><strong>[1]</strong> For a point <span class="math inline">\((x_i,y_i)\)</span>, we set up a straight line equation: <span class="math display">\[y_i=ax_i+b\Leftrightarrow b=(-x_i)a+y_i\]</span> where <span class="math inline">\(a\)</span> = slope, <span class="math inline">\(b\)</span> = y-intercept, <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> are known and fixed.</p><p><strong>[2]</strong> We subdivide the a axis into <span class="math inline">\(K\)</span> increments between <span class="math inline">\([a_\min,a_\max]\)</span>. For each increment of <span class="math inline">\(a\)</span>, we evaluate the value of <span class="math inline">\(b\)</span>.</p><p><strong>[3]</strong> A relationship between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> can be plotted in a parameter space, i.e., ab-plane.</p><p><strong>[4]</strong> We partition the parameter space into a number of bins (accumulator cells), and increment the corresponding bin <span class="math inline">\(A(a,b)\)</span> by 1 (<span class="math inline">\(b\)</span> is rounded into the nearest integer).</p><p><img src="/images/houghbin.png"></p><p><strong>[5]</strong> For another point <span class="math inline">\((x_j,y_j)\)</span>, we set up another straight line equation: <span class="math display">\[y_j=ax_j+b\Leftrightarrow b=(-x_j)a+y_j\]</span> <strong>[6]</strong> Similarly, we subdivide the a axis into K increments between <span class="math inline">\([a_\min,a_\max]\)</span>. For each increment of <span class="math inline">\(a\)</span>, we evaluate the value of <span class="math inline">\(b\)</span>. We plot the relationship between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> in the same parameter space, and update bin values in the discrete parameter space.</p><p><strong>[7]</strong> The bin <span class="math inline">\(A(a,b)\)</span> having the highest count corresponds to the straight line passing through the points <span class="math inline">\((x_i,y_i)\)</span> and <span class="math inline">\((x_j,y_j)\)</span>.</p><p><img src="/images/hough2.png"></p><p><strong>[8]</strong> The same procedure can be applied to all points. The bin <span class="math inline">\(A(a,b)\)</span> having the <strong>highest count</strong> corresponds to the straight line passing through (or passing near) the largest number of points.</p><p>Problem: Values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> run from negative infinity to positive infinity. We need infinite number of bins!</p><p>Solution: use normal representation of a line: <span class="math display">\[x\cos(\theta)+y\sin(\theta)=\rho\]</span> <img src="/images/houghnormal.png"></p><p><span class="math inline">\(\theta\)</span> runs from <span class="math inline">\(–90^o\)</span> to <span class="math inline">\(90^o\)</span>. <span class="math inline">\(\rho\)</span> runs from <span class="math inline">\(-\sqrt{2}D\)</span> to <span class="math inline">\(\sqrt{2}D\)</span>, where <span class="math inline">\(D\)</span> is the distance between corners in the image (length and width).</p><p><strong>Circle Hough Transform (CHT)</strong></p><p>The Hough transform can be used to determine the parameters of a circle when a number of points that fall on the perimeter are known. A circle with radius <span class="math inline">\(R\)</span> and center <span class="math inline">\((a, b)\)</span> can be described with the parametric equations: <span class="math display">\[x=a+R\cos(\theta)\\y=b+R\sin(\theta)\]</span> When the angle <span class="math inline">\(θ\)</span> sweeps through the full 360 degree range the points <span class="math inline">\((x, y)\)</span> trace the perimeter of a circle.</p><p>If the circles in an image are of <strong>known radius</strong> <span class="math inline">\(R\)</span>, then the search can be reduced to 2D. The objective is to find the <span class="math inline">\((a, b)\)</span> coordinates of the centers.</p><p><img src="/images/cht1.png"></p><p>If the <strong>radius is not known</strong>, then the locus of points in parameter space will fall on the surface of a <strong>cone</strong>. Each point <span class="math inline">\((x, y)\)</span> on the perimeter of a circle will produce a cone surface in parameter space. The triplet <span class="math inline">\((a, b, R)\)</span> will correspond to the accumulation cell where the largest number of cone surfaces intersect.</p><p><img src="/images/cone.png">The drawing above illustrates the generation of a conical surface in parameter space for one <span class="math inline">\((x, y)\)</span> point. A circle with a different radius will be constructed at each level, <span class="math inline">\(r\)</span>.</p><p>The search for circles with unknown radius can be conducted by using a three dimensional accumulation matrix.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HKUST CSIT5401 Recognition System lecture notes 2. 识别系统复习笔记。&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#image-acquisition-systems&quot;&gt;Image Acquisition Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#iris-localization&quot;&gt;Iris localization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#pattern-matching&quot;&gt;Pattern Matching&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#alignment-registration&quot;&gt;Alignment (Registration)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#representation&quot;&gt;Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#goodness-of-match&quot;&gt;Goodness of Match&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#decision-fld&quot;&gt;Decision (FLD)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#hough-transform&quot;&gt;Hough Transform&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Recognition System" scheme="http://www.52coding.com.cn/tags/Recognition-System/"/>
    
      <category term="Iris" scheme="http://www.52coding.com.cn/tags/Iris/"/>
    
      <category term="FLD" scheme="http://www.52coding.com.cn/tags/FLD/"/>
    
      <category term="Daugman" scheme="http://www.52coding.com.cn/tags/Daugman/"/>
    
      <category term="Wildes" scheme="http://www.52coding.com.cn/tags/Wildes/"/>
    
      <category term="Hough Transform" scheme="http://www.52coding.com.cn/tags/Hough-Transform/"/>
    
  </entry>
  
  <entry>
    <title>Recognizing Image Features and Patterns</title>
    <link href="http://www.52coding.com.cn/2018/12/04/RS%20-%20Recognizing%20Image%20Features%20and%20Patterns/"/>
    <id>http://www.52coding.com.cn/2018/12/04/RS - Recognizing Image Features and Patterns/</id>
    <published>2018-12-04T13:56:09.000Z</published>
    <updated>2018-12-07T13:21:34.175Z</updated>
    
    <content type="html"><![CDATA[<p>HKUST CSIT5401 Recognition System lecture notes 1. 识别系统复习笔记。</p><!-- toc --><ul><li><a href="#laplacian-point-detector">Laplacian Point Detector</a></li><li><a href="#line-detector">Line Detector</a></li><li><a href="#edge-detectors">Edge Detectors</a><ul><li><a href="#gradient-operator">Gradient Operator</a></li><li><a href="#marr-hildreth-edge-detector">Marr-Hildreth Edge Detector</a></li></ul></li><li><a href="#scale-invariant-feature-transform-sift">Scale Invariant Feature Transform (SIFT)</a></li><li><a href="#visual-saliency-and-local-entropy">Visual Saliency and Local Entropy</a></li><li><a href="#corner-detector">Corner Detector</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="laplacian-point-detector">Laplacian Point Detector</span></h2><p>There are three different types of intensity discontinuities in a digital image:</p><ul><li>Point (Isolated Point)</li><li>Line</li><li>Edge (Ideal, Ramp and Roof)</li></ul><p>Intensity discountinuity is detected based on the mask response <span class="math inline">\(R\)</span> within a pre-defined window, e.g. <span class="math inline">\(3\times3\)</span>: <span class="math display">\[R=\sum_{i=1}^9w_iz_i\]</span> where <span class="math inline">\(w_i\)</span> represent weights within a pre-defined window; <span class="math inline">\(z_i\)</span> represent intensity values.</p><p>If <span class="math inline">\(|R|≥T\)</span> , then a point has been detected. This point is the location on which the mask is <strong>centred</strong>, where <span class="math inline">\(T\)</span> is a non-negative threshold.</p><p>The mask below is the <strong>Laplacian mask</strong> for detecting point. Sum of all weights is zero to make sure that there is no response at a flat region (constant intensity region).</p><table><thead><tr class="header"><th style="text-align: center;">1</th><th style="text-align: center;">1</th><th style="text-align: center;">1</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">1</td><td style="text-align: center;">-8</td><td style="text-align: center;">1</td></tr><tr class="even"><td style="text-align: center;">1</td><td style="text-align: center;">1</td><td style="text-align: center;">1</td></tr></tbody></table><p>The Laplacian is given as (<span class="math inline">\(f\)</span> is input image): <span class="math display">\[\triangledown^2f(x,y)=\frac{\partial^2f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}\]</span> where <span class="math display">\[\frac{\partial^2f}{\partial x^2}=f(x+1,y)-2f(x,y)+f(x-1,y)\\\frac{\partial^2 f}{\partial y^2}=f(x,y+1)-2f(x,y)+f(x,y-1)\]</span> The discrete implementation of the Laplacian operator is given as: <span class="math display">\[\triangledown^2f(x,y)=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)\]</span> Below are several Laplacian masks:</p><p><img src="/images/pd.png"></p><h2><span id="line-detector">Line Detector</span></h2><p>A line is detected when more than one aligned, connected points are detected or, the <strong>response</strong> of line mask <strong>is greater than some threshold</strong>. The below are line masks for detecting lines (1 pixel thick) in 4 different specific directions:</p><p><img src="/images/ld.png"></p><p>If we want to detect a line in a specified direction, then we should use the mask associated with that direction and threshold its output responses.</p><p>If 4 line masks are used, then the final response is equal to the <strong>largest</strong> response among the masks: <span class="math display">\[R = \max\left(|R_{horizontal}|, |R_{45}|, |R_{vertical}|, |R_{-45}|\right)\]</span> Example code (Matlab):</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">f = imread(<span class="string">'xxx.png'</span>); <span class="comment">% read image</span></span><br><span class="line">w = [<span class="number">2</span> <span class="number">-1</span> <span class="number">-1</span>; <span class="number">-1</span> <span class="number">2</span> <span class="number">-1</span>; <span class="number">-1</span> <span class="number">-1</span> <span class="number">2</span>]; <span class="comment">% mask</span></span><br><span class="line">g = <span class="built_in">abs</span>(imfilter(double(f),w)); <span class="comment">% mask responses</span></span><br></pre></td></tr></table></figure><h2><span id="edge-detectors">Edge Detectors</span></h2><p>Edge is the boundary of regions. The boundary has meaningful discontinuities in grey intensity level. There are three types of edges: ideal edge (left), ramp edge (middle) and roof edge (right).</p><p><img src="/images/3t.png"></p><p>For an <strong>ideal edge</strong> (step edge, left), an edge is a collection of connected pixels on the region boundary. Ideal edges can occur over the distance of 1 pixel.</p><p><strong>Roof edges</strong> (right) are models of lines through a region, with the base (width) of a roof edge being determined by the thickness and sharpness of the line. In the limit, when its base is 1 pixel wide, a roof edge becomes a 1 pixel thick line running through a region in an image. Roof edges can represent thin features, e.g., roads, line drawings, etc.</p><p>For a <strong>ramp edge</strong> (middle):</p><ul><li>edge point is any point contained in the ramp</li><li>edge length is determined by the length of the ramp</li><li>the slope of the ramp is inversely proportional to the degree of blurring in the edge</li><li>the <strong>first derivative</strong> of the intensity profile is positive at the points of transition into and out of the ramp (we move from left to right)</li><li>the <strong>second derivative</strong> of the intensity profile is positive at the transition associated with the dark side of the edge, and negative at the transition associated with the light side of the edge</li></ul><p><img src="/images/ramp.png"></p><p>The <strong>magnitude of the first derivative</strong> can be used to detect the presence of an edge. The <strong>sign of the second derivative</strong> can be used to determine whether an edge pixel lies on the dark or light side of an edge. The <strong>zero-crossing property</strong> of the second derivative is very useful for <strong>locating</strong> the centres of thick edge.</p><p>However, fairly little noise can have a significant impact on the first and second derivatives used for edge detection in images. <strong>Image smoothing</strong> is commonly used prior to the edge detection so that the estimations of the two derivatives can be more accurate.</p><h3><span id="gradient-operator">Gradient Operator</span></h3><p>The computation of the gradient of an image is based on obtaining the partial derivatives <span class="math inline">\(G_x = \partial f/\partial x\)</span> and <span class="math inline">\(G_y = \partial f/\partial y\)</span> at every pixel location (x,y). The gradient direction and gradient magnitude are <span class="math display">\[\tan^{-1}\left(\frac{G_y}{G_x}\right)\\|\triangledown f|\approx|G_x|+|G_y|\]</span> <img src="/images/z.png"></p><p>The are several ways to approximate the partial derivatives:</p><ul><li>Roberts cross-gradient operators<ul><li><span class="math inline">\(G_x = (z_9-z_5)\)</span></li><li><span class="math inline">\(G_y=(z_8-z_6)\)</span></li></ul></li><li>Prewitt operators<ul><li><span class="math inline">\(G_x=(z_7+z_8+z_9)-(z_1+z_2+z_3)\)</span></li><li><span class="math inline">\(G_y=(z_3+z_6+z_9)-(z_1+z_4+z_7)\)</span></li></ul></li><li>Sobel operators<ul><li><span class="math inline">\(G_x=(z_7+2z_8+z_9)-(z_1+2z_2+z_3)\)</span></li><li><span class="math inline">\(G_y=(z_3+2z_6+z_9)-(z_1+2z_4+z_7)\)</span></li></ul></li></ul><p><img src="/images/ed.png"></p><p>Below are diagonal edge masks for detecting discontinuities in the <strong>diagonal directions</strong>.</p><p><img src="/images/diag.png"></p><h3><span id="marr-hildreth-edge-detector">Marr-Hildreth Edge Detector</span></h3><p>The Laplacian of an image f(x,y) at location (x,y) is defined as <span class="math display">\[\triangledown^2f(x,y)=\frac{\partial^2f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}\]</span> There are two approximations: <span class="math display">\[\triangledown^2f=4z_5-(z_2+z_4+z_6+z_8)\\\triangledown^2f=8z_5-(z_1+z_2+z_3+z_4+z_6+z_7+z_8+z_9)\]</span> The Laplacian generally is <strong>not</strong> used in its original form for edge detection (based on zero-crossing property) because it is <strong>unacceptably sensitive to noise</strong>. We smooth the image by using a Gaussian blurring function <span class="math inline">\(G(r)\)</span>, where <span class="math inline">\(r\)</span> = radius, before we apply the Laplacian operator: <span class="math display">\[G(r)=\exp(\frac{-r^2}{2\sigma^2})\]</span> The <strong>Laplacian of a Gaussian (LoG)</strong> operator is defined by <span class="math display">\[\text{LoG}(f)=\triangledown^2(f\ast G)=f\ast(\triangledown^2G)\]</span> Using the LoG, the location of edges can be detected reliably based on the zero-crossing values because image noise level is reduced by the Gaussian function.</p><p><strong>Marr-Hildreth algorithm</strong></p><ol type="1"><li><p>Filter the input with an n-by-n Gaussian blurring filter <span class="math inline">\(G(r)\)</span>.</p></li><li><p>Compute the Laplacian of the image resulting from Step 1 using one of the following 3-by-3 masks.</p><p><img src="/images/mha.png"></p></li><li><p>Find the <strong>zero crossings</strong> of the image from Step 2.</p><ol type="1"><li>A zero crossing at a pixel implies that the signs of at least two of its opposing neighboring pixels must be different.</li><li>There are four cases to test: left/right, up/down, and the two diagonals.</li><li>For testing, the signs of the two opposing neighboring pixels must be different and their absolute Laplacian values must be larger than or equal to some threshold.</li><li>If yes, we call the current pixel a zero-crossing pixel.</li></ol></li></ol><p><img src="/images/mhed.png"></p><h2><span id="scale-invariant-feature-transform-sift">Scale Invariant Feature Transform (SIFT)</span></h2><p>SIFT is useful for finding distinctive patches (<strong>keypoints</strong>) in images and transforming keypoints (locations) into <strong>features vectors</strong> for recognition tasks.</p><p>SIFT consists of four steps:</p><ol type="1"><li>Scale-space extrema detection</li><li>Keypoint localization</li><li>Orientation assignment</li><li>Keypoint descriptor</li></ol><p>SIFT feature is</p><ul><li>invariant to image rotation and scale</li><li>partially invariant to change in illumination and 3D camera viewpoint</li></ul><p>The method is a cascade (one step followed by the other step) filtering approach, in which more computationally expensive operations are applied only at locations that pass an initial test.</p><p><strong>[1] Scale-space extrema detection</strong></p><p>Candidate locations are identified by searching for stable features across all scales in the scale space. The <strong>scale space</strong> of an image is defined as <span class="math display">\[L(x,y,\sigma)=G(x,y,\sigma)\ast I(x,y)\\G(x,y,\sigma)=\frac{1}{2\pi\sigma^2}\exp(-\frac{x^2+y^2}{2\sigma^2})\]</span> , where <span class="math inline">\(\ast\)</span> is the convolution operator (filtering operation), the variable-scale Gaussian is <span class="math inline">\(G(x, y, \sigma)\)</span> and the image is <span class="math inline">\(I(s, y)\)</span>.</p><p>The difference between two nearby scales separated by a constant multiplicative factor <span class="math inline">\(k\)</span> is <span class="math display">\[\begin{align}D(x,y,\sigma)&amp;=L(x,y,k\sigma)-L(x,y,\sigma)\\&amp;=\left(G(x,y,k\sigma)-G(x,y,\sigma)\right)\ast I(x,y)\end{align}\]</span> The DoG function provides a close and efficient approximation to the scale-normalized Laplacian of Gaussian (LoG).</p><p><img src="/images/dog.jpg"></p><p>Local <strong>extrema</strong> (maxima and minima) of <span class="math inline">\(D( x, y, σ)\)</span> are detected by comparing the values of its 26 neighbors, including 9 from the above image, 9 from bottom image and 8 from the current image.It is selected only if the center pixel is larger than <strong>all</strong> of these neighbors (26 neighbors) or smaller than all of them.</p><p><img src="/images/extra.png"></p><p><img src="/images/exdet.png"></p><p><strong>[2] Keypoint localization</strong></p><p>Once a keypoint candidate has been found by comparing a pixel to its neighbors, the next step is to determine whether the keypoint is selected based on <strong>local contrast and localization along edge</strong>. Therefore, the keypoints will be rejected if these points have low contrast.</p><p>All extrema are discarded if <span class="math inline">\(|D(x)|&lt;0.03\)</span> (minimum contrast), where <span class="math inline">\(x\)</span> represents a relative image position with maximum value of <span class="math inline">\(D\)</span>. The keypoints will be rejected if these points are poorly localized along an edge (determined based on the <strong>ratio of principle curvatures</strong>).</p><p><img src="/images/kl.png"></p><p><strong>[3] Orientation assignment</strong></p><p>Each corresponding keypoint in <span class="math inline">\(L\)</span> is assigned a dominant orientation. The keypoint descriptor can be represented relative to this orientation and therefore achieve invariance to image rotation.</p><p>The gradient magnitude and orientation are <span class="math display">\[m(x,y)=\sqrt{(L(x+1,y)-L(x-1,y))^2+(L(x,y+1)-L(x,y-1))^2}\\\theta(x,y)=\tan^{-1}\left(\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}\right)\]</span> An <strong>orientation histogram</strong> is formed from the gradient orientations within a region around the keypoint. The orientation histogram has 36 bins covering the 360 degree range of orientations, 10 degrees per bin. Each sample added to the histogram is weighted by its gradient magnitude and by a Gaussian-weighted circular window (with σ that is 1.5 times that of the scale of the keypoint, and it is related to effective window size). Peak in the orientation histogram corresponds to dominant direction of the local gradients.</p><p><img src="/images/IMG_2E4DF1ED416A-1.jpg"></p><p><strong>[4] Descriptor for local image region</strong></p><p><img src="/images/kd.png"></p><p>For each detected keypoint, a local image descriptor is computed. It is partially invariant to change in illumination and 3D viewpoint.</p><p>In order to achieve orientation invariance, the coordinates and the gradient orientations are rotated relative to the keypoint orientation. A Gaussian weighting function with σ equal to one half the width of the descriptor window is used to assign a weight to the magnitude of each sample point.</p><p>Each subregion generates an orientation histogram with 8 orientation bins. Therefore, for each keypoint, if <span class="math inline">\(2\times2\)</span> descriptor is used, a feature vector can be formed with <span class="math inline">\(2\times2\times8 = 32\)</span> elements; if <span class="math inline">\(4\times4\)</span> descriptor is used, there will be <span class="math inline">\(4\times4\times8 = 128\)</span> elements in a feature vector. The <strong>optimal</strong> setting is 4x4 subregions and 8 orientation bins (the above picture).</p><p><strong>Feature matching</strong></p><p><img src="/images/IMG_043BB8EF0160-1.jpg"></p><p>Matlab Implementation</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% num = match(image1, image2)</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% This function reads two images, finds their SIFT features, and</span></span><br><span class="line"><span class="comment">%   displays lines connecting the matched keypoints.  A match is accepted</span></span><br><span class="line"><span class="comment">%   only if its distance is less than distRatio times the distance to the</span></span><br><span class="line"><span class="comment">%   second closest match.</span></span><br><span class="line"><span class="comment">% It returns the number of matches displayed.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% Example: match('scene.pgm','book.pgm');</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">num</span> = <span class="title">match</span><span class="params">(image1, image2)</span></span></span><br><span class="line"><span class="comment">% Find SIFT keypoints for each image</span></span><br><span class="line">[im1, des1, loc1] = sift(image1);</span><br><span class="line">[im2, des2, loc2] = sift(image2);</span><br><span class="line"><span class="comment">% For efficiency in Matlab, it is cheaper to compute dot products between</span></span><br><span class="line"><span class="comment">%  unit vectors rather than Euclidean distances.  Note that the ratio of</span></span><br><span class="line"><span class="comment">%  angles (acos of dot products of unit vectors) is a close approximation</span></span><br><span class="line"><span class="comment">%  to the ratio of Euclidean distances for small angles.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% distRatio: Only keep matches in which the ratio of vector angles from the</span></span><br><span class="line"><span class="comment">%   nearest to second nearest neighbor is less than distRatio.</span></span><br><span class="line">distRatio = <span class="number">0.6</span>;</span><br><span class="line"><span class="comment">% For each descriptor in the first image, select its match to second image.</span></span><br><span class="line">des2t = des2';</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : <span class="built_in">size</span>(des1,<span class="number">1</span>)</span><br><span class="line">    dotprods = des1(<span class="built_in">i</span>,:) * des2t; <span class="comment">% compute orientation of des1[i] and des2[j] for all j</span></span><br><span class="line">    [vals, indx] = <span class="built_in">sort</span>(<span class="built_in">acos</span>(dotprods)); <span class="comment">% Take inverse cosine and sort results</span></span><br><span class="line">    <span class="comment">% Check if nearest neighbor has angle less than distRatio times 2nd.</span></span><br><span class="line">    <span class="keyword">if</span> (vals(<span class="number">1</span>) &lt; distRatio * vals(<span class="number">2</span>))</span><br><span class="line">      match(<span class="built_in">i</span>) = indx(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      match(<span class="built_in">i</span>) = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p><img src="/images/IMG_DAD55EC9C383-1.jpg"></p><h2><span id="visual-saliency-and-local-entropy">Visual Saliency and Local Entropy</span></h2><p>To be continued...</p><h2><span id="corner-detector">Corner Detector</span></h2><p>To be continued...</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HKUST CSIT5401 Recognition System lecture notes 1. 识别系统复习笔记。&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#laplacian-point-detector&quot;&gt;Laplacian Point Detector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#line-detector&quot;&gt;Line Detector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#edge-detectors&quot;&gt;Edge Detectors&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#gradient-operator&quot;&gt;Gradient Operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#marr-hildreth-edge-detector&quot;&gt;Marr-Hildreth Edge Detector&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#scale-invariant-feature-transform-sift&quot;&gt;Scale Invariant Feature Transform (SIFT)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#visual-saliency-and-local-entropy&quot;&gt;Visual Saliency and Local Entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#corner-detector&quot;&gt;Corner Detector&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Recognition System" scheme="http://www.52coding.com.cn/tags/Recognition-System/"/>
    
      <category term="Prewitt" scheme="http://www.52coding.com.cn/tags/Prewitt/"/>
    
      <category term="Sobel" scheme="http://www.52coding.com.cn/tags/Sobel/"/>
    
      <category term="Marr-Hildreth Edge Detector" scheme="http://www.52coding.com.cn/tags/Marr-Hildreth-Edge-Detector/"/>
    
      <category term="SIFT" scheme="http://www.52coding.com.cn/tags/SIFT/"/>
    
  </entry>
  
  <entry>
    <title>RL - Deep Deterministic Policy Gradient (DDPG)</title>
    <link href="http://www.52coding.com.cn/2018/11/30/RL%20-%20Deep%20Deterministic%20Policy%20Gradient/"/>
    <id>http://www.52coding.com.cn/2018/11/30/RL - Deep Deterministic Policy Gradient/</id>
    <published>2018-11-30T05:53:09.000Z</published>
    <updated>2018-11-30T06:03:01.740Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1509.02971" target="_blank" rel="noopener">Deep Deterministic Policy Gradient (DDPG)</a> 是由 DeepMind 的 Lillicrap 等人于2015年提出的算法，发表在ICLR 2016上。DDPG 是基于 <a href="http://proceedings.mlr.press/v32/silver14.pdf" target="_blank" rel="noopener">DPG</a> 算法的改进，可以看作是 Actor-critic 和 <a href="https://www.52coding.com.cn/2018/11/16/RL%20-%20DQN%20and%20A3C/">DQN</a> 的结合，它同时学习一个 Q-function 和一个策略（policy）：用 Q-learning 的方法学习 Q-function，然后用 Q-function 更新策略。</p><a id="more"></a><h2><span id="dpg">DPG</span></h2><p><a href="http://proceedings.mlr.press/v32/silver14.pdf" target="_blank" rel="noopener">Deterministic Policy Gradient (DPG)</a> 是把策略梯度（policy gradient）算法扩展到确定性策略（deterministic policy）上。事实上，DPG 被证明是随机策略梯度（stochastic policy gradient）的一种特殊情况。</p><blockquote><p>随机策略梯度： <span class="math display">\[\triangledown_\theta J(\pi_\theta)=\mathbb{E}_{s\sim\rho^\pi, a\sim\pi_\theta}[\triangledown_\theta\log\pi_\theta(a|s)Q^\pi(s,a)]\]</span> 其中，<span class="math inline">\(\pi_\theta\)</span> 是由参数为 <span class="math inline">\(\theta\)</span> 的函数近似的策略，<span class="math inline">\(\rho^\pi\)</span> 为策略 <span class="math inline">\(\pi_\theta\)</span> 的状态分布（state distribution）。</p></blockquote><p>大部分 model-free 的增强学习算法属于泛化的<a href="https://www.52coding.com.cn/2017/12/07/RL%20-%20Planning%20by%20Dynamic%20Programming/">策略迭代（policy iteration）</a>算法，一般分为两步：策略评估（policy evaluation） 和策略改进（ policy improvement）。策略评估通常使用 <a href="https://www.52coding.com.cn/2017/12/16/RL%20-%20Model-Free%20Prediction/#monte-carlo-learning">Monte-Carlo evaluation</a> 或 <a href="https://www.52coding.com.cn/2017/12/16/RL%20-%20Model-Free%20Prediction/#temporal-difference-learning">temporal difference learning</a> 来近似 <span class="math inline">\(Q^\pi(s, a)\)</span>。策略改进则通常通过最大化评估的 action-value 来得到：<span class="math inline">\(\mu^{k+1}(s) = \arg\max_aQ^{\mu^k}(s, a)\)</span>。</p><p>然而，在连续的动作空间（continuous action spaces）里这种最大化却是不可行的。在离散的动作空间里，我们可以为每个action计算相应 Q-value 然后进行比较；但是在连续的动作空间中，我们不可能把每个action的 Q-value 都计算出来再比较，而通过对 Q-function 求导求的方式计算最大值开销又很大。所以，取而代之的是单独用一个函数近似策略，然后<strong>用 Q-function 的梯度来改进该策略</strong>。具体来说，对于每个访问过的状态 <span class="math inline">\(s\)</span>，策略函数的参数 <span class="math inline">\(\theta^{k+1}\)</span> 根据 <span class="math inline">\(\triangledown_\theta Q^{\mu_k}(s, \mu_\theta(s))\)</span> 来更新： <span class="math display">\[\begin{align}\theta^{k+1}&amp;=\theta^k+\alpha\mathbb{E}_{s\sim\rho^{\mu^k}}[\triangledown_\theta Q^{\mu^k}(s, \mu_\theta(s))]\\&amp;= \theta^k + \alpha\mathbb{E}_{s\sim\rho^{\mu^k}}[\triangledown_\theta \mu_\theta(s)\triangledown_aQ^{\mu^k}(s, a)|_{a=\mu_\theta(s)}]\end{align}\]</span></p><p>可以证明，上述更新也属于策略梯度算法，这就是DPG算法的策略更新公式。</p><h2><span id="ddpg">DDPG</span></h2><p>DDPG改进了 Q-function 的学习方式，而策略端的更新方式和DPG相同，即如式(1)所示。在 DPG 中，Q-function 是通过 Q-learning 的方式来学习的，而当使用神经网络来近似 Q-function 的时候会导致训练不稳定，DDPG 应用了 <a href="https://www.52coding.com.cn/2018/11/16/RL%20-%20DQN%20and%20A3C/#deep-q-network">DQN</a> 中的两个trick来解决不稳定的问题，也就是<strong>经验池（replay buffer）</strong>和<strong>目标网络（target network）</strong>。</p><p>具体来说，经验池 <span class="math inline">\(D\)</span> 每次探索都会存储元组 <span class="math inline">\((s, a, r, s&#39;, d)\)</span>，其中 <span class="math inline">\(d\)</span> 为一个布尔变量，如果 <code>d == True</code>，就表明 <span class="math inline">\(s&#39;\)</span> 是终止状态（terminal state）。 每次更新时都会从经验池随机采样一批数据进行更新。经验池的大小是一个需要微调的超参数：如果经验池过小的话，会导致对池内数据过拟合；如果经验池存储所有数据的话，又会放慢学习的速度。</p><p>目标网络是指用单独的网络参数来生成目标（q-target），设策略函数的参数为 <span class="math inline">\(\theta\)</span>，Q-function 的参数为 <span class="math inline">\(\phi\)</span>，则对应的目标网络参数为 <span class="math inline">\(\theta_{tag}\)</span> 和 <span class="math inline">\(\phi_{tag}\)</span>，生成的目标为： <span class="math display">\[r + \gamma(1-d)Q_{\phi_{tag}}(s&#39;, \mu_{\theta_{tag}}(s&#39;))\]</span> 所以 Q-value 端的更新公式为： <span class="math display">\[\triangledown_\phi \mathbb{E}_{s,a,r,s&#39;,d\sim D}\left[\left(Q(s,a)-(r + \gamma(1-d)Q_{\phi_{tag}}(s&#39;, \mu_{\theta_{tag}}(s&#39;)))\right)^2\right]\]</span> 与DQN不同的是，DDPG中的目标网络使用“软更新”的方式，即目标网络并不是隔一定时间后与主网络同步，而是朝着主网络缓慢移动： <span class="math display">\[\theta_{tag}\leftarrow\tau\theta+(1-\tau)\theta_{tag}\\\phi_{tag}\leftarrow\tau\phi+(1-\tau)\phi_{tag}\]</span> 其中，<span class="math inline">\(\tau \in (0, 1)\)</span> 是一个超参数，通常取值接近 1。</p><p>为了增加探索能力，训练时在选择每个动作的时候都会加上随机噪声 <span class="math inline">\(\mathcal{N}\)</span>，论文作者建议使用时间相关的 <a href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process" target="_blank" rel="noopener">OU噪声</a>，但是最近的研究结果表明使用不相关的、zero-mean的高斯噪声效果会更好。同时为了取得更高质量的训练数据，噪声可以随着训练过程逐步减小。另外，在评测时应去掉噪声。</p><p><strong>DDPG算法</strong></p><p><img src="/images/ddpg_algo.svg"></p><h2><span id="总结">总结</span></h2><p><strong>特点</strong></p><ul><li>off-policy算法</li><li>只能用于连续的动作空间</li><li>可以看作是把DQN应用到连续动作空间</li></ul><h2><span id="references">References</span></h2><p>[1] http://proceedings.mlr.press/v32/silver14.pdf</p><p>[2] https://arxiv.org/abs/1509.02971</p><p>[3] https://spinningup.openai.com/en/latest/algorithms/ddpg.html</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1509.02971&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Deep Deterministic Policy Gradient (DDPG)&lt;/a&gt; 是由 DeepMind 的 Lillicrap 等人于2015年提出的算法，发表在ICLR 2016上。DDPG 是基于 &lt;a href=&quot;http://proceedings.mlr.press/v32/silver14.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DPG&lt;/a&gt; 算法的改进，可以看作是 Actor-critic 和 &lt;a href=&quot;https://www.52coding.com.cn/2018/11/16/RL%20-%20DQN%20and%20A3C/&quot;&gt;DQN&lt;/a&gt; 的结合，它同时学习一个 Q-function 和一个策略（policy）：用 Q-learning 的方法学习 Q-function，然后用 Q-function 更新策略。&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="DQN" scheme="http://www.52coding.com.cn/tags/DQN/"/>
    
      <category term="Policy Gradient" scheme="http://www.52coding.com.cn/tags/Policy-Gradient/"/>
    
      <category term="DPG" scheme="http://www.52coding.com.cn/tags/DPG/"/>
    
      <category term="DDPG" scheme="http://www.52coding.com.cn/tags/DDPG/"/>
    
  </entry>
  
  <entry>
    <title>RL - Proximal Policy Optimization (PPO)</title>
    <link href="http://www.52coding.com.cn/2018/11/25/RL%20-%20PPO/"/>
    <id>http://www.52coding.com.cn/2018/11/25/RL - PPO/</id>
    <published>2018-11-25T14:00:09.000Z</published>
    <updated>2018-12-03T07:45:03.755Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">Proximal Policy Optimization (PPO, PPO-Clip, PPO-Penalty)</a> 是由<a href="https://www.52coding.com.cn/2018/11/22/RL%20-%20TRPO/">TRPO</a>的作者Schulman等人于2017年提出的策略梯度类算法。PPO算法的思路和TRPO一致，都是想在优化时采取尽可能大的步幅但又不能太大以至于产生崩坏。相比于比TRPO，PPO实现起来更简单，泛化能力更强，可以使用随机梯度下降（SGD）进行优化。</p><a id="more"></a><h2><span id="背景">背景</span></h2><p>PPO的背景与<a href="https://www.52coding.com.cn/2018/11/22/RL%20-%20TRPO/#背景">TRPO的背景</a>一致，最终TRPO推导出如下的带约束优化问题： <span class="math display">\[\max_{\theta}\mathbb{E}_t[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t]\\\text{subject to }\mathbb{E}_t[\text{KL}[\pi_{\theta_{old}}(\cdot|s_t), \pi_\theta(\cdot|s_t)]]\]</span> 令 <span class="math inline">\(r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\)</span> 为新旧策略的概率比（易知 <span class="math inline">\(r_t(\theta_{old}) = 1\)</span>）。TRPO最大化的替代目标（surrogate objective）可以写为如下形式： <span class="math display">\[L^{CPI}(\theta)=\mathbb{E}_t[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t]=\mathbb{E}_t[r_t(\theta)A_t]\]</span> 如果不加约束的话，直接优化该目标会产生巨大的更新，导致更新不稳定甚至崩溃。所以需要考虑一种惩罚方法，使 <span class="math inline">\(r_t(\theta)\)</span> 接近 <span class="math inline">\(1\)</span>。</p><h2><span id="ppo-clip">PPO-Clip</span></h2><p>PPO-Clip的目标函数为： <span class="math display">\[L^{CLIP}(\theta)=\mathbb{E}_t[\min(r_t(\theta)A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t)]\]</span> 其中 <span class="math inline">\(\epsilon\)</span> 为超参数控制截断率，取值通常比较小（0.2左右）。</p><p>上述目标函数的第一项与 <span class="math inline">\(L^{CPI}\)</span> 一致，第二项则是为了限制更新幅度（施加惩罚），控制 <span class="math inline">\(r_t(\theta) \in [1-\epsilon, 1+\epsilon]\)</span>。可见 <span class="math inline">\(L^{CLIP}(\theta)\)</span> 是 <span class="math inline">\(L^{CPI}(\theta)\)</span> 的一个下界。</p><p><img src="/images/lclip.png"></p><p>上图显示了 <span class="math inline">\(\text{clip}\)</span> 函数的工作方式：</p><ul><li>当 <span class="math inline">\(A &gt; 0\)</span> 时，如果想使目标函数取得更大的值，就需要增大 <span class="math inline">\(\pi_\theta(a_t|s_t)\)</span> 的值，也就是增大 <span class="math inline">\(r_t(\theta)\)</span> 。但是式中的 <span class="math inline">\(\min\)</span> 函数限制了 <span class="math inline">\(r_t(\theta)\)</span> 最大取到 <span class="math inline">\(1+\epsilon\)</span>，所以新策略再远离旧策略（<span class="math inline">\(r_t(\theta)\)</span> 继续增大）并不会带来更多地好处。</li><li>当 <span class="math inline">\(A &lt; 0\)</span> 时，如果想使目标函数取得更大的值，就需要减小 <span class="math inline">\(\pi_\theta(a_t|s_t)\)</span> 的值，也就是减小 <span class="math inline">\(r_t(\theta)\)</span> 。但是式中的 <span class="math inline">\(\min\)</span> 函数限制了 <span class="math inline">\(r_t(\theta)\)</span> 最小取到 <span class="math inline">\(1-\epsilon\)</span>，所以新策略再远离旧策略（<span class="math inline">\(r_t(\theta)\)</span> 继续减小）并不会带来更多地好处。</li></ul><p>在实现中，目标函数通常使用更简单的形式： <span class="math display">\[L^{CLIP}(s, a, \theta_k, \theta)=\min(\frac{\pi_\theta(a|s)}{\pi_{\theta_{k}}(a|s)}A^{\pi_{\theta_k}}(s, a), g(\epsilon, A^{\pi_{\theta_k}}(s, a)))\]</span> 其中， <span class="math display">\[g(\epsilon, A^{\pi_{\theta_k}}(s, a))=\begin{cases} (1+\epsilon)A,  &amp; \mbox{if }A ≥0 \\(1-\epsilon)A, &amp; \mbox{if }A&lt;0\end{cases}\]</span></p><blockquote><p>注：即便对 <span class="math inline">\(r_t(\theta)\)</span> 加上截断，新策略仍染有可能偏离旧策略很远，不过有很多trick来处理这个问题。其中一个特别简单的处理方式就是：如果新策略和旧策略的平均KL距离大于某个阈值，则停止进行更新（<strong>early stopping</strong>）。</p></blockquote><p>相比于TRPO，由于没有了KL约束，PPO可以用SGD来进行优化，实现简单很多。</p><p><strong>PPO-Clip算法</strong></p><p><img src="/images/ppo_algo.svg"></p><h2><span id="ppo-penalty">PPO-Penalty</span></h2><p>这种方法使用KL距离作为惩罚项，关键在于求出能够自适应多种任务的惩罚因子 <span class="math inline">\(\beta\)</span>。算法的逻辑为，在每次策略进行更新时：</p><ul><li><p>使用SGD优化目标函数： <span class="math display">\[L^{KLPEN}(\theta)=\mathbb{E}_t\left[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}A_t-\beta\cdot\text{KL}[\pi_{old}(a_t|s_t), \pi_\theta(a_t|s_t)]\right]\]</span></p></li><li><p>计算 <span class="math inline">\(d = \mathbb{E}\left[\text{KL}[\pi_{\theta_{old}}(\cdot|s_t), \pi_\theta(\cdot|s_t)]\right]\)</span></p><ul><li>如果 <span class="math inline">\(d &lt; d_{targ}/1.5\)</span>，则 <span class="math inline">\(\beta \leftarrow \beta/2\)</span></li><li>如果 <span class="math inline">\(d &gt; d_{targ} \times 1.5\)</span>，则 <span class="math inline">\(\beta \leftarrow\beta \times 2\)</span></li></ul><p>其中，<span class="math inline">\(d_{targ}\)</span> 为超参数。</p></li></ul><blockquote><p>注：PPO-Penalty 没有 PPO-Clip 效果好。</p></blockquote><h2><span id="实验和总结">实验和总结</span></h2><p><strong>特点</strong></p><ul><li>训练稳定</li><li>通过限制 <span class="math inline">\(r_t(\theta)\)</span> 来找到尽可能大的并且合理的步长</li><li>on-policy 算法</li><li>可用于离散和连续的动作空间</li><li>相比于TRPO，PPO实现简单，效果更好</li></ul><p><strong>实验性能</strong></p><p>在大部分 MuJoCo 环境中强于其他策略梯度类算法，在Atari环境中，表现仅次于ACER，但是学习速度优于ACER。</p><p><img src="/images/ppo_mujoco.png"></p><p><img src="/images/ppo_atari.png"></p><h3><span id="代码">代码</span></h3><p>自己也实现了一下PPO-Clip算法，代码在<a href="https://github.com/NeymarL/Pacman-RL/blob/master/src/ppo.py" target="_blank" rel="noopener">这里</a>。下图显示了在 OpenAI <a href="https://gym.openai.com/" target="_blank" rel="noopener">Gym</a> 上的 <code>MsPacman-ram-v0</code> 环境上的测试结果：</p><p><img src="/images/ppo_pacman.png"></p><p><img src="/images/sample1.gif"></p><p><img src="/images/sample2.gif"></p><h2><span id="references">References</span></h2><p>[1] https://arxiv.org/abs/1707.06347</p><p>[2] https://spinningup.openai.com/en/latest/algorithms/ppo.html</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Proximal Policy Optimization (PPO, PPO-Clip, PPO-Penalty)&lt;/a&gt; 是由&lt;a href=&quot;https://www.52coding.com.cn/2018/11/22/RL%20-%20TRPO/&quot;&gt;TRPO&lt;/a&gt;的作者Schulman等人于2017年提出的策略梯度类算法。PPO算法的思路和TRPO一致，都是想在优化时采取尽可能大的步幅但又不能太大以至于产生崩坏。相比于比TRPO，PPO实现起来更简单，泛化能力更强，可以使用随机梯度下降（SGD）进行优化。&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy Gradient" scheme="http://www.52coding.com.cn/tags/Policy-Gradient/"/>
    
      <category term="PPO" scheme="http://www.52coding.com.cn/tags/PPO/"/>
    
  </entry>
  
  <entry>
    <title>RL - Trust Region Policy Optimization (TRPO)</title>
    <link href="http://www.52coding.com.cn/2018/11/22/RL%20-%20TRPO/"/>
    <id>http://www.52coding.com.cn/2018/11/22/RL - TRPO/</id>
    <published>2018-11-22T09:10:09.000Z</published>
    <updated>2018-11-26T05:23:49.419Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="noopener">Trust Region Policy Optimization (TRPO)</a>算法是由伯克利大学的Schulman等人于2015年提出的策略梯度（Policy Gradients）算法。TRPO通过最大化新策略相对于当前策略的优势来保证每次更新都是单调递增的（稳定），同时找到尽可能大的更新步幅。算法推导出的最终结果是在KL约束下最大化替代优势函数。</p><a id="more"></a><h2><span id="背景">背景</span></h2><p>考虑经典的 MDP<span class="math inline">\(&lt;S, A, P, r, \rho_0, \gamma&gt;\)</span>，其中 <span class="math inline">\(S\)</span> 是所有状态（state）的集合，<span class="math inline">\(A\)</span> 是所有动作（action）的集合，<span class="math inline">\(P: S\times A\times S \rightarrow \mathbb{R}\)</span> 是转移概率分布，<span class="math inline">\(r\)</span> 是奖励（reward）函数，<span class="math inline">\(\rho_0\)</span> 是初始状态（<span class="math inline">\(s_0\)</span>）分布，<span class="math inline">\(\gamma\)</span> 是折扣因子（ discount factor）。</p><p>定义 <span class="math inline">\(\pi\)</span> 为一个随机策略：<span class="math inline">\(\pi: S\times A\rightarrow [0, 1]\)</span>，定义 <span class="math inline">\(\eta(\pi)\)</span> 来衡量策略 <span class="math inline">\(\pi\)</span> 的好坏： <span class="math display">\[\eta(\pi)=\mathbb{E}_{s_0, a_0, ...\sim\pi}[\sum_{t=0}^\infty\gamma^tr(s_t)]\]</span> 接着定义 state-action value function <span class="math inline">\(Q_\pi\)</span>, value function <span class="math inline">\(V_\pi\)</span>, 优势函数（advantage function）<span class="math inline">\(A_\pi\)</span>: <span class="math display">\[Q_\pi(s_t, a_t) = \mathbb{E}_{s_{t+1}, a_{t+1}, ...\sim\pi}[\sum_{l=0}^\infty\gamma^lr_{s_{t+l}}]\]</span></p><p><span class="math display">\[V_\pi(s_t) =\mathbb{E}_{a_t, s_{t+1}, ...\sim\pi}[\sum_{l=0}^\infty\gamma^lr_{s_{t+l}}]\]</span></p><p><span class="math display">\[A_\pi(s, a) = Q_\pi(s, a) - V_\pi(s)\]</span></p><p>然后可以通过下式来衡量策略 <span class="math inline">\(\tilde{\pi}\)</span> 相对于策略 <span class="math inline">\(\pi\)</span> 的优势（证明详见论文）： <span class="math display">\[\begin{align}\eta(\tilde{\pi})&amp;=\eta(\pi)+\mathbb{E}_{s_0, a_0, ...\sim\color{red}{\tilde{\pi}}}[\sum_{t=0}^\infty\gamma^tA_\pi(s_t,a_t)]\\&amp;= \eta(\pi)+\sum_s\color{red}{\rho_\tilde{\pi}(s)}\sum_a\tilde{\pi}(a|s)A_\pi(s, a)\end{align}\]</span> 其中 <span class="math inline">\(\rho_\pi\)</span> 为策略 <span class="math inline">\(\pi\)</span> 的折扣访问频率（discounted visitation frequency）： <span class="math display">\[\rho_\pi(s) = P(s_0=s)+\gamma P(s_1=s) + \gamma^2 P(s_2=s)+...\]</span> 通过上式可知，只要每个状态 <span class="math inline">\(s\)</span> 的期望优势非负，即 <span class="math inline">\(\sum_a\tilde{\pi}(a|s)A_\pi(s, a)&gt;0\)</span>，就可以保证更新是单调非递减的，这其实就是经典的<a href="https://www.52coding.com.cn/2017/12/07/RL%20-%20Planning%20by%20Dynamic%20Programming/">策略迭代（policy iteration）</a>的更新方式。然而，由于 <span class="math inline">\(\rho_\tilde{\pi}(s)\)</span> 的存在，导致直接优化上式很困难，所以引入一个<strong>替代优势</strong>（surrogate advantage）： <span class="math display">\[\begin{align}L_\pi(\tilde{\pi})&amp;=\eta(\pi)+\sum_s\color{red}{\rho_\pi(s)}\sum_a\tilde{\pi}(a|s)A_\pi(s, a)\\\end{align}\]</span> 经过一系列推导，可以得到策略 <span class="math inline">\(\tilde{\pi}\)</span> 的优势下界： <span class="math display">\[\eta(\tilde{\pi})≥L_\pi(\tilde{\pi})-C\cdot D_{KL}^\max(\pi, \tilde{\pi})\]</span> 其中，<span class="math inline">\(C=\frac{4\epsilon\gamma}{(1-\gamma)^2}\)</span>，<span class="math inline">\(D_{KL}^\max\)</span> 是最大的KL散度。</p><p>这里相当于对优化目标 <span class="math inline">\(L_\pi(\tilde{\pi})\)</span> 进行了惩罚，惩罚因子为 <span class="math inline">\(C\)</span>，惩罚项为KL散度，目的是限制新旧策略的差距。通过最大化上述公式，就能最大化新策略 <span class="math inline">\(\tilde{\pi}\)</span> 所得到的奖励，同时又不会离旧策略 <span class="math inline">\(\pi\)</span> 太远（导致对当前数据过拟合），算法如下：</p><p><img src="/images/IMG_1925FD469BD9-1.jpeg"></p><h2><span id="trpo">TRPO</span></h2><p>由于Deep RL都是使用参数为 <span class="math inline">\(\theta\)</span> 的神经网络来拟合策略 <span class="math inline">\(\pi_\theta\)</span>，为了使公式更简洁，把算法1中公式的 <span class="math inline">\(\pi\)</span> 替换成 <span class="math inline">\(\theta\)</span>: <span class="math display">\[\theta = \arg\max_{\theta}[L(\theta_{old}, \theta)-C\cdot D_{KL}^\max(\theta_{old}|| \theta)]\]</span> 其中， <span class="math display">\[\begin{align}L(\theta_{old}, \theta) &amp;= \sum_s\rho_{\theta_{old}}(s)\sum_a\pi_\theta(a|s)A_{\theta_{old}}(s,a) \\&amp;= \mathbb{E}_{s,a\sim\pi_{\theta_{old}}}[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)}A_{\theta_{old}}(s,a)]\end{align}\]</span> TRPO是算法1的近似，区别在于：TRPO没有使用惩罚项 <span class="math inline">\(C\)</span>，而是使用 KL散度约束（i.e. trust region constraint）： <span class="math display">\[\theta = \arg\max_\theta L(\theta_{old}, \theta)\\\text{ s.t. }\bar{D}_{KL}(\theta||\theta_{old})≤\delta\]</span> 其中，<span class="math inline">\(\bar{D}_{KL}\)</span> 是平均KL散度： <span class="math display">\[\bar{D}_{KL}(\theta||\theta_{old})=\mathbb{E}_{s\sim\pi_{\theta_{old}}}[D_{KL}(\pi_{\theta}(\cdot|s)||\pi_{\theta_{old}}(\cdot|s))]\]</span> 然而上面的带约束优化也并非容易，所以TRPO对上式进行了一些近似，对目标函数和约束进行泰勒展开可以得到： <span class="math display">\[L(\theta_{old}, \theta) \approx g^T(\theta-\theta_{old})\]</span></p><p><span class="math display">\[\bar{D}_{KL}(\theta||\theta_{old})\approx \frac{1}{2}(\theta-\theta_{old})^TH(\theta-\theta_{old})\]</span></p><p>其中，<span class="math inline">\(g\)</span> 是替代函数的梯度在 <span class="math inline">\(\theta=\theta_{old}\)</span> 处的值，凑巧的是，它和策略梯度的值正好相等：<span class="math inline">\(g = \triangledown_\theta J(\pi_\theta)|_{\theta_{old}}\)</span>；<span class="math inline">\(H\)</span> 是对于 <span class="math inline">\(\theta\)</span> 的海森矩阵（Hessian matrix）。</p><p>于是得到如下的近似优化问题： <span class="math display">\[\theta_{k+1}=\arg\max_\theta g^T(\theta-\theta_k)\\\text{s.t. }\frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k)≤\delta\]</span> 通过拉格朗日法求解上述约束优化问题得： <span class="math display">\[\theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g\]</span> 这个就是 <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf" target="_blank" rel="noopener">Natural Policy Gradient</a> 的更新公式。不过，由于泰勒近似引入了误差，上式的解可能不满足 KL 约束，所以 TRPO 增加了一个线性搜索（backtracking line search）： <span class="math display">\[\theta_{k+1}=\theta_k+\alpha^j\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g\]</span> 其中，<span class="math inline">\(\alpha\in(0,1)\)</span> 是回溯系数（backtracking coefficient），<span class="math inline">\(j\)</span> 是最小的非负整数使得 <span class="math inline">\(\pi_{\theta_{k+1}}\)</span> 满足 KL 约束并且产生<strong>正</strong>的替代优势，这样就可以保证训练进步是单调的。</p><p>最后，计算和存储 <span class="math inline">\(H^{-1}\)</span> 的开销是很大的，尤其是神经网络的参数动不动就几M。TRPO 使用<a href="https://www.wikiwand.com/en/Conjugate_gradient_method" target="_blank" rel="noopener">共轭梯度法（conjugate gradient）</a>来解 <span class="math inline">\(Hx = g\)</span>，这样就不用直接计算和存储 <span class="math inline">\(H\)</span>。</p><p>最终的更新公式为： <span class="math display">\[\theta_{k+1}=\theta_k+\alpha^j\sqrt{\frac{2\delta}{\hat{x}^TH\hat{x}}}\hat{x}\]</span> 其中， <span class="math display">\[\begin{align}\hat{x}&amp;\approx H^{-1}g &amp; \text{(using conjugate gradient)}\end{align}\]</span></p><p><span class="math display">\[H\hat{x} = \triangledown_\theta((\triangledown_\theta\bar{D}_{KL}(\theta||\theta_k))^T\hat{x})\]</span></p><p><strong>TRPO算法</strong></p><p><img src="/images/trpo.svg"></p><h2><span id="performance">Performance</span></h2><p><strong>TRPO的一些特点</strong></p><ul><li>保证每次更新在当前训练数据上都是进步的，训练过程更加稳定</li><li>通过满足KL约束来找尽可能大的步长</li><li>on-policy 算法</li><li>可用于离散和连续的动作空间</li><li>算法较为复杂</li></ul><p><strong>实验性能</strong></p><p>在模拟机器人走路、游泳等任务中取得了在当时不错的效果；在通过视频输入玩Atari游戏的任务中表现不如DQN等方法。</p><p><img src="/images/trpo_per.jpg"></p><p>下图是我在 OpenAI <a href="https://gym.openai.com/" target="_blank" rel="noopener">Gym</a> 的 <code>Walker2d-v2</code> 和 <code>MsPacman-ram-v0</code> 中测试的结果。</p><p><img src="/images/walker.png"></p><p><img src="/images/pacman.png"></p><h2><span id="references">References</span></h2><p>[1] https://arxiv.org/abs/1502.05477</p><p>[2] https://spinningup.openai.com/en/latest/algorithms/trpo.html</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1502.05477&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Trust Region Policy Optimization (TRPO)&lt;/a&gt;算法是由伯克利大学的Schulman等人于2015年提出的策略梯度（Policy Gradients）算法。TRPO通过最大化新策略相对于当前策略的优势来保证每次更新都是单调递增的（稳定），同时找到尽可能大的更新步幅。算法推导出的最终结果是在KL约束下最大化替代优势函数。&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy Gradient" scheme="http://www.52coding.com.cn/tags/Policy-Gradient/"/>
    
      <category term="TRPO" scheme="http://www.52coding.com.cn/tags/TRPO/"/>
    
  </entry>
  
  <entry>
    <title>RL - DQN &amp; A3C &amp; GAE</title>
    <link href="http://www.52coding.com.cn/2018/11/16/RL%20-%20DQN%20and%20A3C/"/>
    <id>http://www.52coding.com.cn/2018/11/16/RL - DQN and A3C/</id>
    <published>2018-11-16T06:24:32.000Z</published>
    <updated>2018-11-23T06:19:03.882Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="deep-q-network">Deep Q-Network</span></h2><p>Deep Q-Network (<a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" target="_blank" rel="noopener">DQN</a>) 是由DeepMind的Mnih等人于2013年提出的算法，该算法成功把深度学习应用到了RL领域，并（一定程度上）解决了训练不稳定的问题，在玩Atari游戏中取得了非常好的结果。</p><p>文章指出使用非线性函数拟合 Q-value 的RL算法不稳定主要因为：</p><ol type="1"><li>同一个观测序列中的数据相关性较大</li><li>当 Q-value 发生了很小的改变，可能导致整个策略（policy）发生较大变化，从而导致 Q-value 和目标 <span class="math inline">\(r + \gamma * \max_{a&#39;}Q(s&#39; ,a&#39;)\)</span> 的差距不稳定</li></ol><a id="more"></a><p>DQN使用了两个trick来解决上述问题：</p><ul><li>Experience replay<ul><li>使用经验池缓存数据，每次训练时从经验池里sample数据，从而降低训练数据之间的相关性</li></ul></li><li>Two Q networks<ul><li>一个网络用来生成 Q-target，另一个网络进行探索；每隔一定时间两个网络进行同步</li><li>这样使得 Q-target 相对稳定</li></ul></li></ul><p><strong>整体算法</strong></p><p><img src="/images/dqn.jpg"></p><hr><h2><span id="asynchronous-actor-critic">Asynchronous Actor Critic</span></h2><p>Asynchronous Actor Critic (<a href="https://arxiv.org/abs/1602.01783" target="_blank" rel="noopener">A3C</a>) 也是由DeepMind的Mnih等人提出的算法，于2016年发表在ICML上。不同于DQN的是，A3C属于策略梯度（Policy Gradient）类算法，而DQN是基于value的；相同的是，A3C也在Atari游戏上取得了非常好的结果（强于DQN）。</p><p>使用上述经验池有以下问题：</p><ol type="1"><li>使用更多的内存和计算资源</li><li>只能使用 <strong>off-policy</strong> 的RL算法（学习 old policy 产生的数据）</li></ol><p>为了使用 on-policy 算法，文章提出了使用异步学习代替经验池的方法，同时也能保持算法的稳定性，其中使用最广泛的是A3C算法，它具有以下特点：</p><ul><li>并行地使用多个 agent 在各自的环境里探索，每个 agent 在同一时刻探索的内容各不相同，从而降低了数据相关性</li><li>在CPU上训练更加高效</li></ul><p><strong>整体算法</strong></p><ol type="1"><li>同步线程专属网络（<span class="math inline">\(\theta&#39;, \theta_v&#39;\)</span>）和全局网络（<span class="math inline">\(\theta, theta_v\)</span>）</li><li>每个 agent 使用线程专属网络各自进行探索</li><li>根据线程专属网络计算梯度：<span class="math inline">\(d\theta, d\theta_v\)</span></li><li>使用 <span class="math inline">\(d\theta, d\theta_v\)</span> 更新全局网络（<span class="math inline">\(\theta, theta_v\)</span>）</li><li>回到第一步</li></ol><p><img src="/images/IMG_25EB14880DD1-1.jpeg"></p><p><strong>其他细节</strong></p><ul><li><strong>主线程向子线程传参数，子线程向主线程传梯度</strong></li><li>agent 和 critic 共用一个网络，输出分为两头</li><li>增加了熵正则化（鼓励探索）<ul><li><span class="math inline">\(\triangledown_{\theta&#39;}\log\pi(a_t|s_t;\theta&#39;)(R_t-V(s_t;\theta_v))+\beta\triangledown_{\theta&#39;}H(\pi(s_t; \theta&#39;))\)</span></li><li><span class="math inline">\(H(X) = E[-\log P(X)]\)</span></li></ul></li><li>代码参考：https://github.com/NeymarL/Pacman-RL/blob/master/src/a3c.py<ul><li><strong>注</strong>：计算 policy loss 中的 advantage 的时候不能保留其梯度，否则 policy 的梯度会流入 value network 中，产生bug</li></ul></li></ul><hr><h2><span id="generalized-advantage-estimator">Generalized Advantage Estimator</span></h2><p>Generalized Advantage Estimator (<a href="https://arxiv.org/abs/1506.02438" target="_blank" rel="noopener">GAE</a>) 是由伯克利大学的Schulman等人于2016年提出的一种新的估计优势函数（Advantage function）的方法。</p><p>我们的目标是定义优势函数 <span class="math inline">\(A^\pi(s_t, a_t)\)</span> 使其用来计算策略梯度： <span class="math display">\[\hat{g}=\mathbb{E}_{s_0, a_0...\sim\pi_\theta}[\sum_{t=0}^\infty A^\pi_t(s_t,a_t)\triangledown_\theta\pi_\theta(a_t|s_t)]\]</span> 优势函数的定义为： <span class="math display">\[A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)\]</span> 我们使用 <span class="math inline">\(V\)</span> 来近似价值函数（value function），那么 TD(0) error <span class="math inline">\(\delta_t^V = r_t +\gamma V(s_{t+1}-V(s_t))\)</span> 就是优势函数的一个估计，并且如果 <span class="math inline">\(V = V^\pi\)</span>，则 <span class="math inline">\(\delta_t^V\)</span> 是 <span class="math inline">\(A^\pi\)</span> 的一个无偏估计： <span class="math display">\[\begin{align}\mathbb{E}_{s_{t+1}}[\delta_t^{V^\pi}]&amp;=\mathbb{E}_{s_{t+1}}[r_t+\gamma V^\pi(s_{t+1})-V^\pi(s_t)]\\&amp;= \mathbb{E}_{s_{t+1}}[Q^\pi(s_t,a_t)-V^\pi(s_t)]\\&amp;= A^\pi(s_t,a_t)\end{align}\]</span> 只要 <span class="math inline">\(V\)</span> 是近似的，TD(0) error就是优势函数的一个有偏估计，那么TD(<span class="math inline">\(\lambda\)</span>) error又如何呢？</p><p>顺着这个思路，我们可以多往后看几步： <span class="math display">\[\begin{array}{lcl}\hat{A}_t^{(1)}&amp;:=\delta_t^V&amp;=-V(s_t)+r_t+\gamma V(s_{t+1})\\\hat{A}_t^{(2)}&amp;:=\delta_t^V + \gamma\delta_{t+1}^V&amp;=-V(s_t)+r_t+\gamma r_{t+1}+\gamma^2 V(s_{t+2})\\\hat{A}_t^{(3)}&amp;:=\delta_t^V + \gamma\delta_{t+1}^V+\gamma^2\delta_{t+2}^V &amp;=-V(s_t)+r_t+\gamma r_{t+1}+\gamma^2 r_{t+2} +\gamma^2 V(s_{t+3}) \\\end{array}\]</span></p><p><span class="math display">\[\hat{A}_t^{k}:=\sum_{l=0}^{k-1}\gamma^l\delta_{t+l}^V=-V(s_t)+r_t+\gamma r_{t+1}+...+\gamma^{k-1}r_{t+k-1}+\gamma^kV(s_{t+k})\]</span></p><p>可以看到，虽然 <span class="math inline">\(\hat{A}_t^{(k)}\)</span> 依旧是有偏估计，但是偏差随着 <span class="math inline">\(k\)</span> 的增大在逐渐减小，因为 <span class="math inline">\(\gamma^kV(s_{t+k})\)</span> 这一项衰减的越来越厉害。当 <span class="math inline">\(k\rightarrow\infty\)</span> 时： <span class="math display">\[\hat{A}_t^{(\infty)}=\sum_{l=0}^\infty\gamma^l\delta_{t+l}^V=-V(s_t)+\sum_{l=0}^\infty\gamma^lr_{t+l}\]</span> 可以看到就是 return 减去 baseline。</p><p><span class="math inline">\(\text{GAE}(\lambda)\)</span> 的定义为这些 <span class="math inline">\(k\)</span> 步估计的指数平均，即 TD(<span class="math inline">\(\lambda\)</span>) error： <span class="math display">\[\begin{align}\text{GAE}_t(\lambda)&amp;:=(1-\lambda)(\hat{A}_t^{(1)}+\lambda\hat{A}_t^{(2)}+\lambda^2\hat{A}_t^{(3)}+...)\\&amp;=(1-\lambda)(\delta_t^V+\lambda(\delta_t^V-\gamma\delta_{t+1}^V)+...)\\&amp;=\sum_{l=0}^\infty(\gamma\lambda)^l\delta_{t+l}^V\end{align}\]</span> 代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rews : rewards, vals: values, lam: lambda</span></span><br><span class="line">deltas = rews[:<span class="number">-1</span>] + gamma * vals[<span class="number">1</span>:] - vals[:<span class="number">-1</span>]</span><br><span class="line">adv_buf = discount_cumsum(deltas, gamma * lam)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discount_cumsum</span><span class="params">(x, discount)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    magic from rllab for computing discounted cumulative sums of vectors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    input: </span></span><br><span class="line"><span class="string">        vector x, </span></span><br><span class="line"><span class="string">        [x0, </span></span><br><span class="line"><span class="string">         x1, </span></span><br><span class="line"><span class="string">         x2]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    output:</span></span><br><span class="line"><span class="string">        [x0 + discount * x1 + discount^2 * x2,  </span></span><br><span class="line"><span class="string">         x1 + discount * x2,</span></span><br><span class="line"><span class="string">         x2]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> scipy.signal.lfilter([<span class="number">1</span>], [<span class="number">1</span>, float(-discount)], x[::<span class="number">-1</span>], axis=<span class="number">0</span>)[::<span class="number">-1</span>]</span><br></pre></td></tr></table></figure><hr><h2><span id="其他">其他</span></h2><h3><span id="batch-normalization">Batch-Normalization</span></h3><p>解决网络层数变多梯度<strong>消失</strong>/爆炸问题</p><ul><li>梯度截断</li><li>初始化</li><li>RELU</li></ul><p>对每层神经元处理结果进行归一化，但又不能破坏上一层提取的特征（变换重构，引入了可学习参数<span class="math inline">\(\gamma, \beta\)</span>）</p><figure><img src="/images/bn.png" alt="bn"><figcaption>bn</figcaption></figure><p>Inference时 <span class="math inline">\(\mu_B\)</span> 和 <span class="math inline">\(\sigma^2_B\)</span> 固定。</p><p>为什么不用白化？</p><ul><li>在模型训练过程中进行白化操作会带来过高的计算代价和运算时间</li></ul><p>在BN中，是通过将activation规范为均值和方差一致的手段使得原本会减小的activation的scale变大。 <span class="math display">\[\frac{\partial h_l}{\partial h_{l-1}} = \frac{\partial BN(w_l h_{l-1})}{\partial h_{l-1}} = \frac{\partial \alpha w_l h_{l-1}}{\partial h_{l-1}}\]</span> 其中 <span class="math inline">\(\alpha\)</span> 指缩放。可以看到此时反向传播乘以的数不再和 <span class="math inline">\(w\)</span> 的尺度相关，也就是说尽管我们在更新过程中改变 <span class="math inline">\(w\)</span> 的值，但是反向传播的梯度却不受影响。</p><h3><span id="activation-layers">Activation Layers</span></h3><h4><span id="relu">ReLU</span></h4><figure><img src="/images/relu.png" alt="relu"><figcaption>relu</figcaption></figure><p>整流线性单元易于优化，因为它们和线性单元非常类似。线性单元和整流线性单元的唯一区别在于整流线性单元在其一半的定义域上输出为零。这使得只要整流线性单元处于激活状态，它的导数都能保持较大。它的梯度不仅大而且一致。整流操作的二阶导数几乎处处为 0，并且在整流线性单元处于激活状态时，它的一阶导数处处为 1。这意味着相比于引入二阶效应的激活函数来说，它的梯度方向对于学习来说更加有用。</p><p>ReLU 的过程更接近生物神经元的作用过程</p><p><strong>Leaky ReLU</strong></p><p>ReLU 及其扩展都是基于一个原则，那就是如果它们的行为更接近线性，那么模型更容易优化。 <span class="math display">\[g(z; \alpha) = \max(0, z) + \alpha \min(0, z)\]</span> <span class="math inline">\(\alpha\)</span> 为固定值或可学习参数。</p><h4><span id="sigmoid-amp-tanh">Sigmoid &amp; Tanh</span></h4><p><img src="/images/sigmoid.png" alt="sigmoid"> <span class="math display">\[g(z) = \frac{1}{1 + e^{-z}}\]</span></p><ul><li>sigmoid 常作为输出单元用来预测二值型变量取值为 1 的概率</li><li>sigmoid 函数在输入取绝对值非常大的正值或负值时会出现<strong>饱和</strong>（saturate）现象，在图像上表现为开始变得很平，此时函数会对输入的微小改变会变得不敏感。仅当输入接近 0 时才会变得敏感。从而使得学习变困难。</li><li>如果要使用 sigmoid 作为激活函数时（浅层网络），tanh 通常要比 sigmoid 函数表现更好。</li></ul><h3><span id="bagging">Bagging</span></h3><p>思想：多个模型平均效果好于单个模型</p><p><strong>Bagging（bootstrap aggregating）</strong>是通过结合几个模型降低泛化误差的技术 (Breiman, 1994)。</p><p>具体来说，Bagging 涉及构造 k 个<strong>不同的数据集</strong>。每个数据集从原始数据集中<strong>重复采样</strong>构成，和原始数据集具有<strong>相同数量</strong>的样例。这意味着，每个数据集以高概率缺少一些来自原始数据集的例子，还包含若干重复的例子（更具体的，如果采样所得的训练集与原始数据集大小相同，那所得数据集中大概有原始数据集 <strong>2/3</strong> 的实例）</p><figure><img src="/images/bagging.png" alt="bagging"><figcaption>bagging</figcaption></figure><p>第一个分类器学到上面的圆圈就会认为数字是8，第二个分类器检测到下面的圈就会认为数字是8，把两个结合起来就知道只有当上下都有圈（置信概率最大）的时候数字才是8。</p><h3><span id="dropout">Dropout</span></h3><p>简单来说，Dropout (Srivastava et al., 2014) 通过<strong>参数共享</strong>提供了一种廉价的 <strong>Bagging</strong> 集成近似，能够训练和评估<strong>指数级数量</strong>的神经网络。</p><figure><img src="/images/dropout.png" alt="dropout"><figcaption>dropout</figcaption></figure><p><strong>Dropout与Bagging的不同点</strong>：</p><ul><li>Bagging 为串行策略；Dropout 为并行策略</li><li>在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型<strong>共享参数</strong>，其中每个模型继承父神经网络参数的不同子集。</li><li>在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;deep-q-network&quot;&gt;Deep Q-Network&lt;/h2&gt;
&lt;p&gt;Deep Q-Network (&lt;a href=&quot;https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DQN&lt;/a&gt;) 是由DeepMind的Mnih等人于2013年提出的算法，该算法成功把深度学习应用到了RL领域，并（一定程度上）解决了训练不稳定的问题，在玩Atari游戏中取得了非常好的结果。&lt;/p&gt;
&lt;p&gt;文章指出使用非线性函数拟合 Q-value 的RL算法不稳定主要因为：&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;同一个观测序列中的数据相关性较大&lt;/li&gt;
&lt;li&gt;当 Q-value 发生了很小的改变，可能导致整个策略（policy）发生较大变化，从而导致 Q-value 和目标 &lt;span class=&quot;math inline&quot;&gt;\(r + \gamma * \max_{a&amp;#39;}Q(s&amp;#39; ,a&amp;#39;)\)&lt;/span&gt; 的差距不稳定&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="Q-learning" scheme="http://www.52coding.com.cn/tags/Q-learning/"/>
    
      <category term="DQN" scheme="http://www.52coding.com.cn/tags/DQN/"/>
    
      <category term="A3C" scheme="http://www.52coding.com.cn/tags/A3C/"/>
    
      <category term="GAE" scheme="http://www.52coding.com.cn/tags/GAE/"/>
    
  </entry>
  
  <entry>
    <title>中国象棋Zero技术详解</title>
    <link href="http://www.52coding.com.cn/2018/11/07/CCZero/"/>
    <id>http://www.52coding.com.cn/2018/11/07/CCZero/</id>
    <published>2018-11-07T09:27:47.000Z</published>
    <updated>2018-11-24T05:27:14.761Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://cczero.org/" target="_blank" rel="noopener">中国象棋Zero（CCZero）</a>是一个开源项目，把<a href="https://arxiv.org/abs/1712.01815" target="_blank" rel="noopener">AlphaZero</a>的算法应用到了中国象棋上，旨在借助广大象棋爱好者之力一起训练出一个可以打败旋风名手的“象棋之神”。因为种种原因吧，这个目标到目前（2018/11/07）为止未能实现，或者说还差得远，而跑谱的人也越来越少了，很可能坚持不了多久了。</p><p>虽然未能实现目标，但在技术上还是有一定意义的，<a href="https://github.com/NeymarL/ChineseChess-AlphaZero" target="_blank" rel="noopener">GitHub</a>上也时不时有人询问技术细节，在此总结一下，记录一些坑以后不要再踩。</p><a id="more"></a><h2><span id="模块">模块</span></h2><p>程序主要分为三大模块（每个模块对应一个目录）：</p><ul><li><code>agents</code>：核心模块，决定如何下棋<ul><li><code>model.py</code>：神经网络模型</li><li><code>player.py</code>：MCTS，输出走法</li><li><code>api.py</code>：供外界调用model</li></ul></li><li><code>envrionment</code>：象棋规则<ul><li>训练（跑谱）使用<code>static_env.py</code>，速度快一些</li><li>用自带GUI下棋时使用的是<code>env.py</code>, <code>chessboard.py</code>这些，可以输出PNG格式的棋谱</li></ul></li><li><code>worker</code>：把agent和envrionment串联起来的脚本<ul><li><code>self_play.py</code>：自我博弈</li><li><code>compute_elo.py</code>：评测并上传结果到官网</li><li><code>optimize.py</code>：训练棋谱</li><li><code>_windows</code>后缀表示是在Windows平台上运行的相应功能，之所以分开是因为两个多进程的启动方式不同，导致代码结构也要发生一些变化，详见<a href="#自我博弈">自我博弈</a>。</li></ul></li></ul><h3><span id="神经网络模型">神经网络模型</span></h3><p><strong>网络输入</strong>：<span class="math inline">\(14\times10\times9\)</span></p><ul><li><span class="math inline">\(10 \times 9\)</span> 是中国象棋棋盘的大小</li><li><span class="math inline">\(14\)</span> 是所有棋子种类（红/黑算不同种类）</li><li>整体的输入就是14个棋盘堆叠在一起，每个棋盘表示一种棋子的位置：棋子所在的位置为1，其余位置为0。</li></ul><p><strong>网络输出</strong></p><ul><li>策略头（policy head）输出：<span class="math inline">\(2086\)</span><ul><li><span class="math inline">\(2086\)</span> 是行动空间的大小。行动空间就是说根据中国象棋的规则，任意棋子在任意位置的走法集合。</li></ul></li><li>价值头（value head）输出：<span class="math inline">\(1\)</span><ul><li>价值头输出一个标量衡量当前局势 <span class="math inline">\(v\in[-1, 1]\)</span>：当 <span class="math inline">\(v\)</span> 接近1时，局势大好；接近0为均势；接近-1为败势。</li></ul></li></ul><p>附：棋子编号表</p><table><thead><tr class="header"><th>棋子</th><th>编号</th></tr></thead><tbody><tr class="odd"><td>兵/卒</td><td>0</td></tr><tr class="even"><td>炮</td><td>1</td></tr><tr class="odd"><td>车</td><td>2</td></tr><tr class="even"><td>马</td><td>3</td></tr><tr class="odd"><td>相/象</td><td>4</td></tr><tr class="even"><td>仕/士</td><td>5</td></tr><tr class="odd"><td>帅/将</td><td>6</td></tr></tbody></table><p><strong>网络结构</strong></p><p>网络主体是 ResNet，输出部分分出两个头，分别输出 policy 和 value。现在的架构是中间有10个残叉块（Residual Block），每个块里面有两个CNN：卷积核大小为 <span class="math inline">\(3 \times 3\)</span>，过滤器个数为192。</p><h3><span id="蒙特卡洛树搜索">蒙特卡洛树搜索</span></h3><p><img src="/images/mcts0.png"></p><p>搜索树中的每个节点都包含所有合法移动 a ∈ A(s) 的边(s，a)。 每条边存储一组统计数据， <span class="math display">\[\{N(s,a) ,W(s,a) ,Q(s,a) ,P(s,a)\}\]</span> 其中 <span class="math inline">\(N(s,a)\)</span> 是访问计数，<span class="math inline">\(W(s,a)\)</span> 是总动作价值，<span class="math inline">\(Q(s,a)\)</span> 是平均动作价值，<span class="math inline">\(P(s,a)\)</span> 是选择该边的先验概率。 多个模拟在单独的搜索线程上并行执行。</p><ol type="1"><li><p>选择</p><p>每个模拟的第一个 in-tree 阶段开始于搜索树的根节点 <span class="math inline">\(s_0\)</span>，并且在模拟时刻 L 处到达叶节点 <span class="math inline">\(s_L\)</span> 时结束。在每个这些时刻 <span class="math inline">\(t &lt; L\)</span> 处，根据搜索树中的统计量选择一个移动: <span class="math inline">\(a_t = \arg\max_a(Q(s_t,a) + U(s_t,a))\)</span>，其中 <span class="math inline">\(U(s_t,a)\)</span> 使用PUCT算法的变体得到 <span class="math display">\[U(s,a)=c_{puct}P(s,a)\frac{\sqrt{\sum_bN(s,b)}}{1+N(s,a)}\]</span> 其中 <span class="math inline">\(c_{puct}​\)</span> 是一个决定探索程度的常数; 这种搜索控制策略最初倾向于具有高先验概率和低访问次数的行为，但后期倾向于具有高动作价值的行为。</p></li><li><p>扩展和评估</p><p>叶子结点 <span class="math inline">\(s_L\)</span> 被加入到等待评估队列进行评估: <span class="math inline">\((d_i(p),v)=f_\theta(d_i(s_L))\)</span>，其中 <span class="math inline">\(d_i\)</span>是旋转或反射操作。神经网络一次评估队列里的 8 个结点;搜索进程直到评估完毕才能继续工作。每个叶子结点和每条边 <span class="math inline">\((s_L,a)\)</span> 的统计值被初始化为 <span class="math inline">\(\{N(s_L,a) = 0,W(s_L,a) = 0,Q(s_L,a) =0, P(s_L, a) = p_a\}\)</span>，然后价值 v 开始回溯。</p></li><li><p>回溯</p><p>每条边的统计值延路径反向更新：访问计数递增 <span class="math inline">\(N(s_t,𝑎_t) = N(s_t,𝑎_t) +1\)</span>，移动价值更新为平均值 <span class="math inline">\(W(s_t,a_t)=W(s_t,a_t)+v\)</span>, <span class="math inline">\(Q(s_t,a_t)=\frac{W(s_t,a_t)}{N(s_t,a_t)}\)</span>。</p></li><li><p>下棋</p><p>在搜索结束时，AlphaGo Zero 在根位置 <span class="math inline">\(s_0\)</span> 选择移动 a，与其指数访问计数成比例，<span class="math inline">\(\pi(a|s_0) = \frac{N(s_0,a)^{1/\tau}}{\sum_bN(s,b)^{1/\tau}}\)</span>，其中 <span class="math inline">\(τ\)</span> 是控制探索水平的参数。搜索树可以在后面的时刻重用：与所选择的移动对应的子节点成为新的根节点; 在这个节点下面的子树被保留以及它的所有统计数据，而树的其余部分被丢弃。</p></li></ol><h4><span id="实现细节">实现细节</span></h4><p><strong>在选择的过程中，发现当前state在history中出现过（形成循环）怎么办？</strong></p><ul><li>根据比赛规则：闲着循环3次判和；违规（长捉、长将等）判负；对方违规判胜。</li></ul><p><strong>Virtual Loss</strong></p><ul><li><p>多线程搜索时，当某一线程选择了某个action时，为了鼓励其他线程选择其他action，应该降低该action的价值（施加virtual loss）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.tree[state].sum_n += <span class="number">1</span></span><br><span class="line">action_state = self.tree[state].a[sel_action]</span><br><span class="line">action_state.n += virtual_loss</span><br><span class="line">action_state.w -= virtual_loss</span><br><span class="line">action_state.q = action_state.w / action_state.n</span><br></pre></td></tr></table></figure></li><li><p>在回溯时，更新value要考虑到virtual loss的影响</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">node = self.tree[state]</span><br><span class="line">action_state = node.a[action]</span><br><span class="line">action_state.n += <span class="number">1</span> - virtual_loss</span><br><span class="line">action_state.w += v + virtual_loss</span><br><span class="line">action_state.q = action_state.w / action_state.n</span><br></pre></td></tr></table></figure></li></ul><p><strong>state表示</strong></p><p>虽然对于神经网络来说state就是<span class="math inline">\(14\times10\times9\)</span>的tensor，但是对于搜索树来说，显然不能用它来表示每个局面。</p><p>在初始版本中，象棋环境（<code>environment/chessboard.py</code>）里是用数组来表示棋盘的，所以在搜索中也使用相应的数组表示state，这样做虽然没什么问题，但是在搜索的过程中需要大量的深拷贝操作（因为需要回溯），增加了许多开销。</p><p>后来版本进行了改进，使用<a href="https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation" target="_blank" rel="noopener">FEN string</a>作为state的表示，降低了拷贝操作的开销；同时也优化了象棋环境（<code>environment/static_env.py</code>），可以直接对FEN进行操作，无需记录复杂的数组。</p><blockquote><p><strong>Forsyth–Edwards Notation</strong> (<strong>FEN</strong>) is a standard <a href="https://www.wikiwand.com/en/Chess_notation" target="_blank" rel="noopener">notation</a> for describing a particular board position of a <a href="https://www.wikiwand.com/en/Chess" target="_blank" rel="noopener">chess</a> game. The purpose of FEN is to provide all the necessary information to restart a game from a particular position.</p></blockquote><h3><span id="自我博弈">自我博弈</span></h3><p>为了提高CPU/GPU利用率，这里使用了多进程，每个进程各自进行自我博弈。Python的多进程有三个实现方式：<code>fork</code>, <code>spawn</code>, <code>forkserver</code>。</p><blockquote><p>On Windows only <code>'spawn'</code> is available. On Unix <code>'fork'</code> and <code>'spawn'</code> are always supported, with <code>'fork'</code> being the default.</p></blockquote><p>由于我自己在macOS/Linux上开发和测试，所以首先实现的是基于<code>fork</code>的多进程，而当我在程序加了<code>mp.set_start_method('spawn')</code>的时候，程序就跑不了了，会报pickling error（貌似是因为传给子进程的参数里不能出现queue的数据结构），于是只能换种方式实现来绕过这个问题。</p><h2><span id="分布式">分布式</span></h2><p>起初我是没打算做成分布式的，实现完上面说述模块之后我用实验室的K80进行训练，练了几天之后发现进步并不明显，几乎还是随机下，很弱智，这是我才意识到即使把它训练到一个业余玩家的水平也需要巨大的算力。</p><p><img src="/images/issueouashd.png"></p><p>后来有一天有人在GitHub上提了一个issue说你可以把它做成分布式的，像LeelaZero那样，我们可以帮你一起训练。<a href="https://zero.sjeng.org/" target="_blank" rel="noopener">LeelaZero</a>是国外一个开发者复现AlphaGo论文搞的围棋AI，因为DeepMind并没有公开程序或代码，所以他想训练出一个公开的围棋之神，然后就邀请网友帮他一起训练，具体的方法就是：网友们在自己的机器上进行自我博弈，然后把博弈的棋谱上传到他的服务器上，然后他攒够一定棋谱之后进行训练神经网络，训练好之后分发新的权重。</p><p>在国内也有很多人帮他训练（跑谱），给我提issue的那个人也是帮LeelaZero训练中的一员。当时正好程序写完了没什么事做，每天就只能等训练结果，然后就决定尝试一下这个模式。因为之前有过Web开发的经验，所以服务器很快就搭好了，测试基本没问题之后就开始运行。</p><p><strong>架构</strong></p><p><img src="/images/architecture.png"></p><p>在维护这个项目正常运行的过程中遇到很多<strong>坑</strong>，程序也做了很多改进：</p><ol type="1"><li>首先是帮忙跑谱的大多都是象棋爱好者，并非开发者，所以我要把Python代码打包成exe文件分发给他们一键执行，最终使用<a href="https://www.pyinstaller.org/" target="_blank" rel="noopener">PyInstaller</a>打包成功，这其中遇到了很多坑：<ul><li>卸载cytoolz；pandas的版本必须为0.20.3</li><li>代码里加上<code>mp.freeze_support()</code>，否则多进程不会正常工作</li></ul></li><li>服务器带宽有限，客户端下载权重太慢，解决办法：把权重放到云存储服务中，如腾讯云/七牛云的对象存储服务。</li><li>中国象棋棋规的完善。并不是说基础的马走日象走田这种规则，而是像长将、长捉等这种比赛规则，这个算是坑最大的一个，直到现在规则还存在少许问题。</li><li>部分支持了UCI协议。这样就可以使用其他的象棋界面加载这个引擎，并且能和其他引擎对弈。</li><li>因为“同行竞争”，我的服务器在今年暑假期间我的服务器经常遭受DDos攻击，由于买不起腾讯云的高防服务，只能尝试其他办法，包括配置弹性IP、配置防火墙、Cloudfare CDN等，但都不好用。最终把服务转移到<a href="https://www.ovh.com/" target="_blank" rel="noopener">OVH</a>提供的VPS上才解决了问题（OVH提供免费的DDos防护）。</li></ol><hr><h2><span id="alphazero-and-exit">AlphaZero and ExIt</span></h2><p><a href="https://arxiv.org/abs/1705.08439" target="_blank" rel="noopener">Expert Iteration（ExIt）</a>是一种模仿学习（Imitation Learning, IL）算法，普通的 IL 算法中，徒弟模仿专家的策略只能提高自己的策略，专家是不会有任何提高的，而 ExIt 算法就是想让师傅教徒弟的时候自己也有提高。</p><p><strong>ExIt 算法</strong> 师傅根据徒弟的策略进行前向搜索（例如MCTS，alpha-beta，贪心搜索等），得出比徒弟更好的策略，然后徒弟再学习师傅的策略，如此循环，随着徒弟的增强，师傅也会越来越强。</p><p><img src="/images/exit.png"></p><p>可见，AlphaZero也属于 ExIt 算法，师傅为 MCTS，徒弟就是神经网络。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://cczero.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;中国象棋Zero（CCZero）&lt;/a&gt;是一个开源项目，把&lt;a href=&quot;https://arxiv.org/abs/1712.01815&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AlphaZero&lt;/a&gt;的算法应用到了中国象棋上，旨在借助广大象棋爱好者之力一起训练出一个可以打败旋风名手的“象棋之神”。因为种种原因吧，这个目标到目前（2018/11/07）为止未能实现，或者说还差得远，而跑谱的人也越来越少了，很可能坚持不了多久了。&lt;/p&gt;
&lt;p&gt;虽然未能实现目标，但在技术上还是有一定意义的，&lt;a href=&quot;https://github.com/NeymarL/ChineseChess-AlphaZero&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;上也时不时有人询问技术细节，在此总结一下，记录一些坑以后不要再踩。&lt;/p&gt;
    
    </summary>
    
      <category term="踩坑现场" scheme="http://www.52coding.com.cn/categories/%E8%B8%A9%E5%9D%91%E7%8E%B0%E5%9C%BA/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="AlphaZero" scheme="http://www.52coding.com.cn/tags/AlphaZero/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CCZero" scheme="http://www.52coding.com.cn/tags/CCZero/"/>
    
      <category term="MCTS" scheme="http://www.52coding.com.cn/tags/MCTS/"/>
    
      <category term="中国象棋" scheme="http://www.52coding.com.cn/tags/%E4%B8%AD%E5%9B%BD%E8%B1%A1%E6%A3%8B/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 搭建博客踩坑记录</title>
    <link href="http://www.52coding.com.cn/2018/11/06/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E8%AE%B0%E5%BD%95/"/>
    <id>http://www.52coding.com.cn/2018/11/06/博客迁移记录/</id>
    <published>2018-11-06T02:41:47.000Z</published>
    <updated>2018-11-26T02:17:57.092Z</updated>
    
    <content type="html"><![CDATA[<p>博客迁移这个事早就想做了，但到现在才有时间和精力来完成。以前太年轻，写的博客系统并不方便维护，迁移的动力主要有以下几个：</p><ol type="1"><li>原博客更新、维护较麻烦。以前的博客是用<a href="https://www.52coding.com.cn/2015/12/21/%E8%AE%B0%E5%BD%95%EF%BC%9A%E7%94%A8PHP%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95web%E6%A1%86%E6%9E%B6/">PHP</a>写的，之前的写作方式是用Markdown写好导出HTML，再修改HTML代码使得静态资源（图片等）加载正确，这就使得修改博客很麻烦；更换主题也很麻烦，博客的主题和Markdown的主题通常会有冲突，所以想换个样式就要改半天CSS。</li><li>觉得UI有些难看，想要简洁一些；</li><li>安全问题。</li></ol><p>现在的解决方案是<a href="https://pages.github.com/" target="_blank" rel="noopener">Github Pages</a> + <a href="https://hexo.io/zh-cn/index.html" target="_blank" rel="noopener">Hexo</a>，主题选的是<a href="https://github.com/CodeDaraW/Hacker" target="_blank" rel="noopener">Hacker</a>，迁移了两天终于搞完了，在此简单记录一下遇到的坑。</p><a id="more"></a><h3><span id="数学公式渲染">数学公式渲染</span></h3><p>由于这款主题并不是原生支持数学公式的，所以要添加些代码来使其支持Mathjax，参考http://searene.me/2016/10/01/Let-hexo-support-mathjax/。</p><p>首先在主题的<code>layout</code>目录下新建<code>mathjax.ejs</code>，文件内容如下：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">%</span> <span class="attr">if</span> (<span class="attr">theme.mathjax.enable</span>)&#123; %&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/x-mathjax-config"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="undefined">  MathJax.Hub.Config(&#123;</span></span><br><span class="line"><span class="undefined">      tex2jax: &#123;</span></span><br><span class="line"><span class="undefined">        inlineMath: [ ['$','$'], ["\\(","\\)"] ],</span></span><br><span class="line"><span class="undefined">        processEscapes: true</span></span><br><span class="line"><span class="undefined">      &#125;</span></span><br><span class="line"><span class="undefined">    &#125;);</span></span><br><span class="line"><span class="undefined">  </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/x-mathjax-config"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="undefined">  MathJax.Hub.Config(&#123;</span></span><br><span class="line"><span class="undefined">        tex2jax: &#123;</span></span><br><span class="line"><span class="undefined">          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']</span></span><br><span class="line"><span class="undefined">        &#125;</span></span><br><span class="line"><span class="undefined">      &#125;);</span></span><br><span class="line"><span class="undefined">  </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/x-mathjax-config"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="undefined">  MathJax.Hub.Queue(function() &#123;</span></span><br><span class="line"><span class="undefined">          var all = MathJax.Hub.getAllJax(), i;</span></span><br><span class="line"><span class="xml">          for(i=0; i <span class="tag">&lt; <span class="attr">all.length</span>; <span class="attr">i</span> += <span class="string">1)</span> &#123;</span></span></span><br><span class="line"><span class="undefined">              all[i].SourceElement().parentNode.className += ' has-jax';</span></span><br><span class="line"><span class="undefined">          &#125;</span></span><br><span class="line"><span class="undefined">      &#125;);</span></span><br><span class="line"><span class="undefined">  </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">%</span> &#125; %&gt;</span></span><br></pre></td></tr></table></figure><p><strong>坑1</strong>：之前在网上查到的代码给的MathJax.js的链接多是过期的，如<code>https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML</code>，然后就被坑了。</p><p>然后在<code>layout.ejs</code>中加上<code>&lt;%- partial('mathjax') %&gt;</code>，文件整体内容为：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE HTML&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">%-</span> <span class="attr">partial</span>('<span class="attr">components</span>/<span class="attr">head</span>') %&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"blog"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"content"</span>&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;<span class="name">%-</span> <span class="attr">partial</span>('<span class="attr">components</span>/<span class="attr">header</span>') %&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;<span class="name">main</span> <span class="attr">class</span>=<span class="string">"site-main posts-loop"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">%-</span> <span class="attr">body</span> %&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">main</span>&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;<span class="name">%-</span> <span class="attr">partial</span>('<span class="attr">components</span>/<span class="attr">footer</span>') %&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">%-</span> <span class="attr">partial</span>('<span class="attr">components</span>/<span class="attr">googleanalytics</span>') %&gt;</span></span><br><span class="line">      <span class="comment">&lt;!-- 新加的 --&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">%-</span> <span class="attr">partial</span>('<span class="attr">mathjax</span>') %&gt;</span> </span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>最后在主题的<code>_config.yml</code>中加上： <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mathjax:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><p>如果没有安装MathJax插件的话需要安装一下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-math --save</span><br></pre></td></tr></table></figure><p>重新生成一下应该就可以渲染数学公式了。</p><p>不过还有些问题，就是你写在数学公式里的下划线(<code>_</code>)、反斜杠(<code>\</code>)、和星号(<code>*</code>)会被当作普通Markdown来处理，比如把下划线(<code>_</code>)和星号(<code>*</code>)替换成<code>&lt;em&gt;</code>标签等导致公式渲染错误。</p><p>解决方案来自https://zhuanlan.zhihu.com/p/33857596，打开<code>nodes_modules/marked/lib/marked.js</code>:</p><ol type="1"><li><p>找到下面的代码:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([\\`*&#123;&#125;\[\]()# +\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure><p>改为：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*&#123;&#125;\[\]()# +\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure></li><li><p>找到em的符号:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em: <span class="regexp">/^\b((?:[^]|_)+?)\b|^*((?:**|[\s\S])+?)*(?!*)/</span>,</span><br></pre></td></tr></table></figure><p>改为：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em:<span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure></li></ol><p>这样就去掉了<code>_</code>的斜体含义，在公式里使用<code>_</code>就没有问题了，不过要使用<code>*</code>的话要用<code>\ast</code>替代。</p><h3><span id="评论">评论</span></h3><p>Hacker这款主题支持两种评论方式，分别是<a href="https://github.com/imsun/gitment" target="_blank" rel="noopener">Gitment</a>和<a href="https://disqus.com/" target="_blank" rel="noopener">Disqus</a>。一开始试了试Gitment，配置好之后发现不能用，其原因是有一个服务过期了而作者也弃坑了没人管，我也懒得折腾就转向了Disqus，注册了之后就可以直接使用，十分方便，<strong>但是</strong>国内要翻墙才能访问。</p><p>最后换成了<a href="https://www.livere.com/" target="_blank" rel="noopener">来必力</a>评论，国内可以正常访问，虽然主题没有内置支持，但是操作很简单，只需把安装代码放到 <code>layout/components/comment.ejs</code> 里即可。</p><h3><span id="搜索">搜索</span></h3><p>主题内置不支持搜索，需要自己动手，丰衣足食。</p><p>首先安装生成搜索内容的插件： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-search --save</span><br></pre></td></tr></table></figure></p><p>然后在<code>_config.yml</code>进行如下配置： <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">search:</span></span><br><span class="line"><span class="attr">  path:</span> <span class="string">search.xml</span></span><br><span class="line"><span class="attr">  field:</span> <span class="string">post</span></span><br><span class="line"><span class="attr">  content:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><p>配置项具体含义参考<a href="https://www.npmjs.com/package/hexo-generator-search" target="_blank" rel="noopener">这里</a>，这个网站还说了应该怎么用这个插件来在博客中支持搜索并且给了demo，分为三步：</p><ol type="1"><li><a href="https://github.com/wzpan/hexo-theme-freemind/blob/master/layout/_widget/search.ejs#L8" target="_blank" rel="noopener">创建搜索框</a></li><li>编写<a href="https://github.com/wzpan/hexo-theme-freemind/blob/master/source/js/search.js" target="_blank" rel="noopener">搜索脚本</a></li><li>在 Hexo 主题中把两部分<a href="https://github.com/wzpan/hexo-theme-freemind/blob/master/layout/_partial/after_footer.ejs#L22" target="_blank" rel="noopener">结合起来</a></li></ol><h4><span id="创建搜索框">创建搜索框</span></h4><p>我的打算是把“搜索”放在顶部，和“主页”、“目录”等一排，是一个单独的页面。所以要先创建新界面：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page &quot;search&quot;</span><br></pre></td></tr></table></figure><p>修改 <code>search</code> 目录下的 <code>index.md</code>：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 搜索</span><br><span class="line">date: 2018-11-23 14:42:39</span><br><span class="line">layout: "search"</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>在主题的 <code>_config.yml</code> 中加上：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">menu:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="string">搜索:</span> <span class="string">/search</span></span><br></pre></td></tr></table></figure><p>在主题的 <code>layout</code> 目录下创建 <code>search.ejs</code>：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">article</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">h2</span> <span class="attr">class</span>=<span class="string">"article-title &lt;% if (page.category)&#123; %&gt; category&lt;% &#125; else if (page.category)&#123; %&gt; category&lt;% &#125; %&gt;"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">%=</span> <span class="attr">page.title</span> || <span class="attr">config.title</span> %&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"site_search"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"form-group"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">id</span>=<span class="string">"local-search-input"</span> <span class="attr">name</span>=<span class="string">"q"</span> <span class="attr">results</span>=<span class="string">"0"</span> <span class="attr">placeholder</span>=<span class="string">"输入关键词"</span> <span class="attr">class</span>=<span class="string">"st-search-input st-default-search-input form-control"</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"archive"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"local-search-result"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">article</span>&gt;</span></span><br></pre></td></tr></table></figure><p>重新build就应该可以看到“搜索”标签和搜索框了。</p><h4><span id="编写搜索脚本">编写搜索脚本</span></h4><p>在主题目录下创建<code>source/js/search.js</code>，源码照 https://github.com/wzpan/hexo-theme-freemind/blob/master/source/js/search.js 稍作修改。同时也下载jquery到<code>js</code>文件夹。</p><p>然后在 <code>layout/components/head.ejs</code>中添加：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"&lt;%- config.root %&gt;js/search.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"&lt;%- config.root %&gt;js/jquery.min.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><h4><span id="结合">结合</span></h4><p>在 <code>layout/search.ejs</code>里添加</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=<span class="string">"text/javascript"</span>&gt;</span><br><span class="line">    <span class="keyword">var</span> search_path = <span class="string">"&lt;%= config.search.path %&gt;"</span>;</span><br><span class="line">    <span class="keyword">if</span> (search_path.length == <span class="number">0</span>) &#123;</span><br><span class="line">        search_path = <span class="string">"search.xml"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">var</span> path = <span class="string">"&lt;%= config.root %&gt;"</span> + search_path;</span><br><span class="line">    searchFunc(path, <span class="string">'local-search-input'</span>, <span class="string">'local-search-result'</span>);</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure><p>这样就完全实现了搜索功能！</p><h3><span id="其他">其他</span></h3><p><strong>分割线</strong></p><p>Hexo中写作不能使用<code>___</code>（三个下划线）来实现分割线，用了的话会generate失败，而且提示的错误很迷，曾经困扰了我很久。如果要用分割线的话需要四个下划线。</p><p><strong>修改网站Icon</strong></p><p>在主题中找到<code>head.ejs</code>文件，其中有一行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;link href=&quot;&lt;%- config.root %&gt;favicon.ico&quot; rel=&quot;icon&quot;&gt;;</span><br></pre></td></tr></table></figure><p>按理来说只要往根目录（<code>source</code>）下放一个<code>favicon.ico</code>的文件即可。</p><p>可是我的就不行，不知道什么原因，把文件名换了就可以了，所以我改成了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;link href=&quot;&lt;%- config.root %&gt;icon.png&quot; type=&quot;image/png&quot; rel=&quot;icon&quot;&gt;</span><br></pre></td></tr></table></figure><p>然后往根目录下放一个<code>icon.png</code>，解决。</p><p><strong>分享</strong></p><p>分享接口使用<a href="https://github.com/overtrue/share.js" target="_blank" rel="noopener">Share.js</a>，只需引入相应的css和js文件，照文档使用即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;博客迁移这个事早就想做了，但到现在才有时间和精力来完成。以前太年轻，写的博客系统并不方便维护，迁移的动力主要有以下几个：&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;原博客更新、维护较麻烦。以前的博客是用&lt;a href=&quot;https://www.52coding.com.cn/2015/12/21/%E8%AE%B0%E5%BD%95%EF%BC%9A%E7%94%A8PHP%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95web%E6%A1%86%E6%9E%B6/&quot;&gt;PHP&lt;/a&gt;写的，之前的写作方式是用Markdown写好导出HTML，再修改HTML代码使得静态资源（图片等）加载正确，这就使得修改博客很麻烦；更换主题也很麻烦，博客的主题和Markdown的主题通常会有冲突，所以想换个样式就要改半天CSS。&lt;/li&gt;
&lt;li&gt;觉得UI有些难看，想要简洁一些；&lt;/li&gt;
&lt;li&gt;安全问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;现在的解决方案是&lt;a href=&quot;https://pages.github.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github Pages&lt;/a&gt; + &lt;a href=&quot;https://hexo.io/zh-cn/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;，主题选的是&lt;a href=&quot;https://github.com/CodeDaraW/Hacker&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hacker&lt;/a&gt;，迁移了两天终于搞完了，在此简单记录一下遇到的坑。&lt;/p&gt;
    
    </summary>
    
      <category term="踩坑现场" scheme="http://www.52coding.com.cn/categories/%E8%B8%A9%E5%9D%91%E7%8E%B0%E5%9C%BA/"/>
    
    
      <category term="Mathjax" scheme="http://www.52coding.com.cn/tags/Mathjax/"/>
    
      <category term="Hexo" scheme="http://www.52coding.com.cn/tags/Hexo/"/>
    
      <category term="Disqus" scheme="http://www.52coding.com.cn/tags/Disqus/"/>
    
      <category term="Gitment" scheme="http://www.52coding.com.cn/tags/Gitment/"/>
    
      <category term="Github Pages" scheme="http://www.52coding.com.cn/tags/Github-Pages/"/>
    
  </entry>
  
  <entry>
    <title>Interdependence and the Gains from Trade</title>
    <link href="http://www.52coding.com.cn/2018/11/03/Interdependence%20and%20the%20Gains%20from%20Trade/"/>
    <id>http://www.52coding.com.cn/2018/11/03/Interdependence and the Gains from Trade/</id>
    <published>2018-11-03T12:10:47.000Z</published>
    <updated>2018-11-06T07:10:39.499Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#a-parable-for-the-modern-economy">A Parable for the Modern Economy</a><ul><li><a href="#production-possibilities">Production Possibilities</a></li><li><a href="#specialization-and-trade">Specialization and Trade</a></li></ul></li><li><a href="#comparative-advance-the-driving-force-of-specialization">Comparative Advance: The Driving Force of Specialization</a><ul><li><a href="#absolute-advantage">Absolute Advantage</a></li><li><a href="#opportunity-cost-and-comparative-advantage">Opportunity Cost and Comparative Advantage</a></li><li><a href="#comparative-advantage-and-trade">Comparative Advantage and Trade</a></li><li><a href="#the-price-and-the-trade">The Price and The Trade</a></li></ul></li><li><a href="#applications-of-comparative-advantage">Applications of Comparative Advantage</a><ul><li><a href="#should-tiger-woods-mow-his-own-lawn">Should Tiger Woods Mow His Own Lawn?</a></li><li><a href="#should-the-united-states-trade-with-other-countries">Should the United States Trade With Other Countries?</a></li></ul></li><li><a href="#summary">Summary</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="a-parable-for-the-modern-economy">A Parable for the Modern Economy</span></h2><h3><span id="production-possibilities">Production Possibilities</span></h3><p>The graph shows the various mixes of output that an economy can produce and illustrate that people face trade-offs.</p><p><img src="/images/IMG_CBD807794C2B-1.png"> If the farmer and rancher do not trade, the production possibilities frontier is also the consumption possibilities frontier. However, the frontier shows trade-offs but do not show what they will choose to do. So let’s suppose the choose the combinations identified in the graph by points A and B.</p><h3><span id="specialization-and-trade">Specialization and Trade</span></h3><p><img src="/images/IMG_FA9EF67DD902-1.png"> The farmer and rancher can both benefit because trade allows each of them to specialize in doing what they do best. The farmer will spend more time growing potatoes and less time raising cattle. The rancher will spend more time raising cattle and less time growing potatoes. As a result of specialization and trade, each of them can consume more meat and more potatoes without working any more hours.</p><h2><span id="comparative-advance-the-driving-force-of-specialization">Comparative Advance: The Driving Force of Specialization</span></h2><p>The puzzle is why the rancher does better in both fields, he still gain from trade? To solve this puzzle, we should answer first who has a lower cost to produce potatoes?</p><h3><span id="absolute-advantage">Absolute Advantage</span></h3><p>We can measure the cost through <em>absolute advantage</em> which is the ability to produce a good using fewer inputs than another producer. In our example, the only input is time. Because the rancher need fewer time to produce both items, he has the absolute advantage in producing both goods. Base on this the rancher has a lower cost to produce potatoes.</p><h3><span id="opportunity-cost-and-comparative-advantage">Opportunity Cost and Comparative Advantage</span></h3><p>Recall the <em>opportunity cost</em> is whatever must be given up to obtain some item. In our example, the opportunity cost of the rancher to produce 1 ounce potatoes is 1/2 ounce meat and the opportunity cost of him to produce 1 ounce meat is 2 ounce potatoes. Similarly, we can compute the opportunity cost for farmer which summarize in table 1. <img src="/images/IMG_857CB49E31D3-1.png"></p><p>We can also measure the cost through <em>comparative advantage</em>, which is the ability to produce a good at a lower opportunity cost than another producer. Through table 1 we can find out that farmer has comparative advantage in producing potatoes and rancher has comparative advantage in producing meat. That’s why the rancher can gain from trade.</p><p>Although it is possible for one person to have an absolute advantage in both goods, it is impossible for one to have a comparative advantage in both goods. Because the opportunity cost of one good is inverse of the opportunity cost of the other.</p><h3><span id="comparative-advantage-and-trade">Comparative Advantage and Trade</span></h3><p>The gains from specialization and trade based on comparative advantage. By specialization in what he has a comparative advantage, the total production of the society raises which means increase the size of economic pie.</p><p>Also, the price of the goods should lower than their opportunity cost of producing it. For example, the farmer exchange 15 ounce potatoes for 5 ounce meat. The price of meat for the farmer is 3 ounce potatoes which is lower than his opportunity cost of producing meat (4 ounce potatoes). Similarly, for the rancher, the price of 1 ounce potatoes is 1/3 ounce meat which is also lower than his opportunity cost of producing potatoes (1/2 ounce meat).</p><p>Conclude: <strong>Trade can benefit everyone in society because it allows people to specialize in activities in which they have a comparative advantage</strong>.</p><h3><span id="the-price-and-the-trade">The Price and The Trade</span></h3><p><strong>For both parties to gain from trade, the price at which they trade must lie between the two opportunity costs</strong>.</p><h2><span id="applications-of-comparative-advantage">Applications of Comparative Advantage</span></h2><h3><span id="should-tiger-woods-mow-his-own-lawn">Should Tiger Woods Mow His Own Lawn?</span></h3><p>Say Tiger Woods can mow his own lawn in 2 hours while, Forrest, the boy next door, can mow the lawn in 4 hours. Should Tiger Woods mow his own lawn?</p><p>Clearly, Tiger Woods has an absolute advantage in mowing the lawn but he has a higher opportunity cost in doing it because he could spend 2 hours filming a commercial advertisement earning $10000 while Forrest can only earn $20 in 4 hours. So Tiger should hire Forrest to mow the lawn and both of them will better off as long as the payment is between $20 and $10000.</p><h3><span id="should-the-united-states-trade-with-other-countries">Should the United States Trade With Other Countries?</span></h3><p>International trade can make some individuals worse off, even as it makes the country as a whole better off. When the US exports food and imports cars, the impact on an American farmer is not the same as the impact on an American autoworker. Yet, international trade is not like war, in which some countries win and others lose. <em>Trade allows all countries to achieve greater prosperity</em>.</p><h2><span id="summary">Summary</span></h2><ul><li>Each person consumes goods and services produced by many other people both in the United States and around the world. Interdependence and trade are desirable because they allow everyone to enjoy a greater <strong>quantity and variety</strong> of goods and services.</li><li>There are two ways to compare the ability of two people in producing a good. The person who can produce the good with the smaller quantity of inputs is said to have an <em>absolute advantage</em> in producing the good. The person who has the smaller <em>opportunity cost</em> of producing the good is said to have a <em>comparative advantage</em>. The gains from trade are based on comparative advantage, not absolute advantage.</li><li>Trade makes everyone better off because it allows people to specialize in those activities in which they have a comparative advantage.</li><li>The principle of comparative advantage applies to countries as well as to people. Economists use the principle of comparative advantage to advocate free trade among countries.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#a-parable-for-the-modern-economy&quot;&gt;A Parable for the Modern Economy&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#production-possibilities&quot;&gt;Production Possibilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#specialization-and-trade&quot;&gt;Specialization and Trade&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#comparative-advance-the-driving-force-of-specialization&quot;&gt;Comparative Advance: The Driving Force of Specialization&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#absolute-advantage&quot;&gt;Absolute Advantage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#opportunity-cost-and-comparative-advantage&quot;&gt;Opportunity Cost and Comparative Advantage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#comparative-advantage-and-trade&quot;&gt;Comparative Advantage and Trade&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#the-price-and-the-trade&quot;&gt;The Price and The Trade&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#applications-of-comparative-advantage&quot;&gt;Applications of Comparative Advantage&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#should-tiger-woods-mow-his-own-lawn&quot;&gt;Should Tiger Woods Mow His Own Lawn?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#should-the-united-states-trade-with-other-countries&quot;&gt;Should the United States Trade With Other Countries?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://www.52coding.com.cn/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="微观经济型原理" scheme="http://www.52coding.com.cn/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%9E%8B%E5%8E%9F%E7%90%86/"/>
    
      <category term="trade" scheme="http://www.52coding.com.cn/tags/trade/"/>
    
      <category term="comparative advantage" scheme="http://www.52coding.com.cn/tags/comparative-advantage/"/>
    
  </entry>
  
  <entry>
    <title>Unity学习笔记</title>
    <link href="http://www.52coding.com.cn/2018/11/01/Unity%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://www.52coding.com.cn/2018/11/01/Unity学习笔记/</id>
    <published>2018-11-01T02:41:47.000Z</published>
    <updated>2018-11-06T04:01:05.679Z</updated>
    
    <content type="html"><![CDATA[<p><strong>记录一些小功能的实现</strong></p><!-- toc --><ul><li><a href="#实现相机跟随">实现相机跟随</a></li><li><a href="#拖动图标在场景生成物体">拖动图标在场景生成物体</a></li><li><a href="#技能冷却效果">技能冷却效果</a></li><li><a href="#鼠标点击选中场景中的物体">鼠标点击选中场景中的物体</a></li><li><a href="#2d人物朝左朝右">2D人物朝左朝右</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="实现相机跟随">实现相机跟随</span></h2><ul><li>方法一<ul><li>把相机设置为目标的Child</li></ul></li><li>方法二<ul><li>设置好距目标的距离和角度，根据数学关系计算出相机位置</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">float distance = 15;// 距离</span><br><span class="line">float rot = 0;// 横向角度</span><br><span class="line">GameObject target;// 目标物体</span><br><span class="line">float roll = 30f * Mathf.PI * 2 / 360; // 纵向角度</span><br><span class="line"></span><br><span class="line">void LateUpdate () &#123;</span><br><span class="line">Vector3 targetPos = target.transform.position;</span><br><span class="line">Vector3 cameraPos;</span><br><span class="line">float d = distance * Mathf.Cos (roll);</span><br><span class="line">float height = distance * Mathf.Sin (roll);</span><br><span class="line">cameraPos.x = targetPos.x + d * Mathf.Cos (rot);</span><br><span class="line">cameraPos.z = targetPos.z + d * Mathf.Sin (rot);</span><br><span class="line">cameraPos.y = targetPos.y + height;</span><br><span class="line">Camera.main.transform.position = cameraPos;</span><br><span class="line">Camera.main.transform.LookAt (target.transform);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>相机随鼠标旋转</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">void Rotate()</span><br><span class="line">&#123;</span><br><span class="line"> float w = Input.GetAxis (&quot;Mouse X&quot;) * rotSpeed;</span><br><span class="line">rot -= w;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void Roll()</span><br><span class="line">&#123;</span><br><span class="line">float w = Input.GetAxis (&quot;Mouse Y&quot;) * rollSpeed * 0.5f;</span><br><span class="line">roll -= w;</span><br><span class="line">if (roll &gt; maxRoll) &#123;</span><br><span class="line">roll = maxRoll;</span><br><span class="line">&#125;</span><br><span class="line">if (roll &lt; minRoll) &#123;</span><br><span class="line">roll = minRoll;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void LateUpdate () &#123;</span><br><span class="line">Rotate();</span><br><span class="line">  Roll();</span><br><span class="line">....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="拖动图标在场景生成物体">拖动图标在场景生成物体</span></h2><p><strong>拖动UI</strong></p><p>新建<code>Drag</code>类，继承<code>IBeginDragHandler, IDragHandler, IEndDragHandler</code>，实现拖动UI功能有三个接口：</p><ul><li><code>public void OnBeginDrag (PointerEventData eventData)</code></li><li><code>public void OnDrag (PointerEventData eventData)</code></li><li><code>public void OnEndDrag (PointerEventData eventData)</code></li></ul><p>在<code>Drag</code>类里实现这三个接口即可实现想要的拖动效果，最后不用忘了把<code>Drag</code>脚本添加到想要被拖动的UI物体上。</p><p><strong>在场景中生成物体</strong></p><p>要实现这个功能:</p><ul><li>首先在<code>OnBeginDrag</code>中生成新的<code>GameObject</code>；</li><li>然后在<code>OnDrag</code>中，根据鼠标在场景里的位置调整<code>GameObject</code>的位置，再检测<code>GameObject</code>的collider有无和其他物体碰撞；</li><li>最后在<code>OnEndDrag</code>中，如果<code>GameObject</code>的最终位置合法，则不再移动；否则销毁物体，生成失败。</li></ul><h2><span id="技能冷却效果">技能冷却效果</span></h2><p><strong>定时器</strong></p><p>实现冷却效果计时器必不可少，实现方法也很简单，只需两个变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bool timerStarted = false;</span><br><span class="line">float remain = 10f;</span><br></pre></td></tr></table></figure><p>然后在<code>Update</code>中作如下更新：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">void Update ()</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">    if (timerStarted) &#123;</span><br><span class="line">remain -= Time.deltaTime;</span><br><span class="line">        if (remain &lt;= 0) &#123;</span><br><span class="line">            CloseTimer();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>UI效果</strong></p><p>Button的层次如下：</p><ul><li><p><code>Button</code></p><ul><li><p><code>Image</code>：按钮显示的图标</p></li><li><p><code>Mask</code>：可以用按钮的默认背景；调整颜色和透明度；ImageType为filled；通过调整Fill Amount来实现转动效果</p><p><img src="/images/Screen%20Shot%202018-10-30%20at%204.09.00%20PM.png"></p></li><li><p><code>CD Text</code>：显示剩余冷却时间</p></li></ul></li></ul><p><strong>Note</strong>：在开始冷却的同时，应把设置<code>btn.interactable = false;</code>，否则按钮可以在冷却过程中再次被点击。</p><p>这里Button的<code>OnClick</code>绑定了两个函数，分别给<code>CharacterController</code>实现技能效果，和给<code>UIManager</code>实现UI动效：</p><p><img src="/images/btnclick.png"></p><h2><span id="鼠标点击选中场景中的物体">鼠标点击选中场景中的物体</span></h2><p>思路：从点击位置向场景发射射线，检测是否击中物体</p><p>实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">void MousePick () &#123;</span><br><span class="line">    if (Input.GetMouseButtonUp (0)) &#123;</span><br><span class="line">        // 发射射线</span><br><span class="line">        Ray myRay = Camera.main.ScreenPointToRay (Input.mousePosition);</span><br><span class="line">        // 选择想被选中的layer</span><br><span class="line">        int layerMask = LayerMask.GetMask (&quot;Building&quot;);</span><br><span class="line">        // 检测碰撞</span><br><span class="line">        RaycastHit2D hit = Physics2D.Raycast (new Vector2 (myRay.origin.x, myRay.origin.y),</span><br><span class="line">            Vector2.down, Mathf.Infinity, layerMask);</span><br><span class="line">        if (hit.collider) &#123;</span><br><span class="line">            // 检测到碰撞，选中该物体</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="2d人物朝左朝右">2D人物朝左朝右</span></h2><p>思路：如果原sprite朝右，那么只要把transform的<code>scale.x</code>变成<code>-1</code>就是朝左了。</p><p><img src="/images/facingside.png"></p><p>实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">void LateUpdate () &#123;</span><br><span class="line">    Vector3 localScale = _transform.localScale;</span><br><span class="line"></span><br><span class="line">    if (_vx &gt; 0) &#123; // moving right so face right</span><br><span class="line">        _facingRight = true;</span><br><span class="line">    &#125; else if (_vx &lt; 0) &#123; // moving left so face left</span><br><span class="line">        _facingRight = false;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // check to see if scale x is right for the player</span><br><span class="line">    // if not, multiple by -1 which is an easy way to flip a sprite</span><br><span class="line">    if ((_facingRight) &amp;&amp; (localScale.x &lt; 0) || </span><br><span class="line">        ((localScale.x &gt; 0)) &#123;</span><br><span class="line">        localScale.x *= -1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // update the scale</span><br><span class="line">    _transform.localScale = localScale;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>未完待续</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;记录一些小功能的实现&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#实现相机跟随&quot;&gt;实现相机跟随&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#拖动图标在场景生成物体&quot;&gt;拖动图标在场景生成物体&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#技能冷却效果&quot;&gt;技能冷却效果&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#鼠标点击选中场景中的物体&quot;&gt;鼠标点击选中场景中的物体&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#2d人物朝左朝右&quot;&gt;2D人物朝左朝右&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="踩坑现场" scheme="http://www.52coding.com.cn/categories/%E8%B8%A9%E5%9D%91%E7%8E%B0%E5%9C%BA/"/>
    
    
      <category term="Unity" scheme="http://www.52coding.com.cn/tags/Unity/"/>
    
      <category term="C#" scheme="http://www.52coding.com.cn/tags/C/"/>
    
      <category term="Game Dev" scheme="http://www.52coding.com.cn/tags/Game-Dev/"/>
    
  </entry>
  
  <entry>
    <title>Thinking Like an Economist</title>
    <link href="http://www.52coding.com.cn/2018/10/03/Thinking%20Like%20an%20Economist/"/>
    <id>http://www.52coding.com.cn/2018/10/03/Thinking Like an Economist/</id>
    <published>2018-10-03T12:10:47.000Z</published>
    <updated>2018-11-06T06:39:17.839Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#the-economist-as-scientist">The Economist as Scientist</a><ul><li><a href="#the-scientific-method-observation-theory-and-more-observation">The Scientific Method: Observation, Theory, and More Observation</a></li><li><a href="#the-role-of-assumptions">The Role of Assumptions</a></li><li><a href="#economic-models">Economic Models</a></li><li><a href="#our-first-model-the-circular-flow-diagram">Our First Model: The Circular-Flow Diagram</a></li><li><a href="#our-second-model-the-production-possibilities-frontier">Our Second Model: The Production Possibilities Frontier</a></li><li><a href="#microeconomics-and-macroeconomics">Microeconomics and Macroeconomics</a></li></ul></li><li><a href="#the-economist-as-policy-adviser">The Economist as Policy Adviser</a><ul><li><a href="#positive-versus-normative-analysis">Positive versus Normative Analysis</a></li><li><a href="#economists-in-washington">Economists in Washington</a></li><li><a href="#why-economists-advice-is-not-always-followed">Why Economists’ Advice Is Not Always Followed</a></li></ul></li><li><a href="#why-economists-disagree">Why Economists Disagree</a><ul><li><a href="#differences-in-scientific-judgments">Differences in Scientific Judgments</a></li><li><a href="#difference-in-values">Difference in Values</a></li><li><a href="#perception-versus-reality">Perception versus Reality</a></li></ul></li></ul><!-- tocstop --><a id="more"></a><h2><span id="the-economist-as-scientist">The Economist as Scientist</span></h2><h3><span id="the-scientific-method-observation-theory-and-more-observation">The Scientific Method: Observation, Theory, and More Observation</span></h3><p>Invention an economic theory is just like in other scientific fields, which is <em>observation, summary to a theory and then collect data to test it</em>. However, it is often <em>difficult or impossible</em> for economists to <em>collect data</em> because you cannot change policies just for experiments. <strong>Therefore, economists often pay attention to the natural experiments offered by history</strong> which can not only give insight into the economy of the past, but also allow to illustrate and evaluate economic theories of the present.</p><h3><span id="the-role-of-assumptions">The Role of Assumptions</span></h3><p><strong>Make assumptions can simplify the question.</strong> e.g. once we understood the international trade in the simplified imaginary world, we are in a better position to understand international trader in the more complex world.</p><p>Also, the art in scientific thinking is <strong>deciding which assumptions to make</strong>. e.g. study short-run effect or long-run effect make different assumptions.</p><h3><span id="economic-models">Economic Models</span></h3><p>Economic models composed with graphs and equations which omit a lot of details to emphases the essence of economy. Each economic models make assumptions to simplify reality so as to improve our understanding of it.</p><h3><span id="our-first-model-the-circular-flow-diagram">Our First Model: The Circular-Flow Diagram</span></h3><p><img src="/images/IMG_9A38B6F35EC5-1.jpeg.jpg"></p><p>e.g. money in your wallet -&gt; buy coffee in markets for goods and services (local Starbucks) -&gt; revenue of the company -&gt; pay rental or wage -&gt; someone’s wallet</p><p>Because of its simplicity, this circular-flow diagram is useful to keep in mind when thinking about <strong>how the pieces of the economy fit together</strong>.</p><h3><span id="our-second-model-the-production-possibilities-frontier">Our Second Model: The Production Possibilities Frontier</span></h3><p><img src="/images/IMG_2122EF5B828A-1.jpeg.jpg"> The production possibilities frontier shows the <em>efficiency</em> of the society. Because resources are <em>scarce</em>, not every conceivable outcome is feasible. Points <strong>on</strong> the production possibilities frontier represent <em>efficient levels</em> of production.</p><p>It also reveals <em>trade-off</em> and <em>opportunity costs</em>: if produce more computers, means have to produce less cars. The <em>opportunity cost</em> is measured by the <strong>slope</strong> of the production possibilities frontier, which means point F’s opportunity cost of a car is lower and point E’ opportunity cost of producing a car is higher. That’s because when at point E, the society has let all of car engineers to produce cars. Producing one more car means moving some of the best computer technicians out of the computer industry and making them autoworkers.</p><p>The production possibilities frontier also change with time, which shows <em>economic growth</em>. <img src="/images/IMG_0EA219852402-1.jpeg.jpg"></p><h3><span id="microeconomics-and-macroeconomics">Microeconomics and Macroeconomics</span></h3><p>Economics is studied on various levels, which is traditionally divided into two broad subfields:</p><ul><li><strong>Microeconomics</strong> is the study of how households and firms make decisions and how they interact in specific markets.</li><li><strong>Macroeconomics</strong> is the study of economy-wide phenomena, including inflation, unemployment, and economic growth.</li></ul><h2><span id="the-economist-as-policy-adviser">The Economist as Policy Adviser</span></h2><h3><span id="positive-versus-normative-analysis">Positive versus Normative Analysis</span></h3><p><strong>positive statements</strong>: claims that attempt to describe the world as it is <strong>normative statements</strong>: claims that attempt to prescribe how the world should be</p><p>Normative statements comes from positive statements as well as value judgements. Deciding what is good or bad policy is not just a matter of science. It also involves our views on ethics, religion, and political philosophy.</p><h3><span id="economists-in-washington">Economists in Washington</span></h3><p>Economists in Whitehouse also face trade-offs. The influence of economists on policy goes beyond their role as advisers: Their research and writings often affect policy indirectly.</p><h3><span id="why-economists-advice-is-not-always-followed">Why Economists’ Advice Is Not Always Followed</span></h3><p>Economists offer crucial input into the policy process, but their advice is only one ingredient of a complex recipe.</p><h2><span id="why-economists-disagree">Why Economists Disagree</span></h2><h3><span id="differences-in-scientific-judgments">Differences in Scientific Judgments</span></h3><p><strong>Economic is a young science and there is still much to be learned.</strong> They disagree because they have different hunches about the validity of alternative theories or about the size of important parameters that measure how economic variables are related.</p><h3><span id="difference-in-values">Difference in Values</span></h3><p>Economists give conflicting advice sometimes because they have different values.</p><h3><span id="perception-versus-reality">Perception versus Reality</span></h3><p>Why do policies such as rent control persist if the experts are united in their opposition? It may be that the realities of the <strong>political process</strong> stand as immovable obstacles. But it also may be that economists have <strong>not yet convinced</strong> enough of the public that these policies are undesirable.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#the-economist-as-scientist&quot;&gt;The Economist as Scientist&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#the-scientific-method-observation-theory-and-more-observation&quot;&gt;The Scientific Method: Observation, Theory, and More Observation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#the-role-of-assumptions&quot;&gt;The Role of Assumptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#economic-models&quot;&gt;Economic Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#our-first-model-the-circular-flow-diagram&quot;&gt;Our First Model: The Circular-Flow Diagram&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#our-second-model-the-production-possibilities-frontier&quot;&gt;Our Second Model: The Production Possibilities Frontier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#microeconomics-and-macroeconomics&quot;&gt;Microeconomics and Macroeconomics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#the-economist-as-policy-adviser&quot;&gt;The Economist as Policy Adviser&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#positive-versus-normative-analysis&quot;&gt;Positive versus Normative Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#economists-in-washington&quot;&gt;Economists in Washington&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#why-economists-advice-is-not-always-followed&quot;&gt;Why Economists’ Advice Is Not Always Followed&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#why-economists-disagree&quot;&gt;Why Economists Disagree&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#differences-in-scientific-judgments&quot;&gt;Differences in Scientific Judgments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#difference-in-values&quot;&gt;Difference in Values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#perception-versus-reality&quot;&gt;Perception versus Reality&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://www.52coding.com.cn/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="微观经济型原理" scheme="http://www.52coding.com.cn/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%9E%8B%E5%8E%9F%E7%90%86/"/>
    
      <category term="Production Possibilities Frontier" scheme="http://www.52coding.com.cn/tags/Production-Possibilities-Frontier/"/>
    
  </entry>
  
  <entry>
    <title>Ten Principles of Economics</title>
    <link href="http://www.52coding.com.cn/2018/09/16/Chapter%201-%20Ten%20Principles%20of%20Economics/"/>
    <id>http://www.52coding.com.cn/2018/09/16/Chapter 1- Ten Principles of Economics/</id>
    <published>2018-09-16T02:41:47.000Z</published>
    <updated>2018-11-06T06:23:18.037Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#how-people-make-decisions">How People Make Decisions</a><ul><li><a href="#principle-1-people-face-trade-offs">Principle 1: People Face Trade-offs</a></li><li><a href="#principle-2-the-cost-of-something-is-what-you-give-up-to-get-it">Principle 2: The Cost of Something Is What You Give Up to Get It</a></li><li><a href="#principle-3-rational-people-think-at-the-margin">Principle 3: Rational People Think at the Margin</a></li><li><a href="#principle-4-people-respond-to-incentives">Principle 4: People Respond to Incentives</a></li></ul></li><li><a href="#how-people-interact">How People Interact</a><ul><li><a href="#principle-5-trade-can-make-everyone-better-off">Principle 5: Trade Can Make Everyone Better Off</a></li><li><a href="#principle-6-markets-are-usually-a-good-way-to-organize-economic-activity">Principle 6: Markets Are Usually a Good Way to Organize Economic Activity</a></li><li><a href="#principle-7-governments-can-sometimes-improve-market-outcomes">Principle 7: Governments Can Sometimes Improve Market Outcomes</a></li></ul></li><li><a href="#how-the-economy-as-a-whole-works">How the Economy as a Whole Works</a><ul><li><a href="#principle-8-a-countrys-standard-of-living-depends-on-its-ability-to-produce-goods-and-services">Principle 8: A Country’s Standard of Living Depends on Its Ability to Produce Goods and Services</a></li><li><a href="#principle-9-prices-rise-when-the-government-prints-too-much-money">Principle 9: Prices Rise When the Government Prints Too Much Money</a></li><li><a href="#principle-10-society-faces-a-short-run-trade-off-between-inflation-and-unemployment">Principle 10: Society Faces a Short-Run Trade-off between Inflation and Unemployment</a></li></ul></li><li><a href="#summary">Summary</a></li></ul><!-- tocstop --><a id="more"></a><h2><span id="how-people-make-decisions">How People Make Decisions</span></h2><p><strong>scarcity</strong>: the limited nature of society’s resources <strong>economics</strong>: the study of how society manages its <em>scarce</em> resources</p><h3><span id="principle-1-people-face-trade-offs">Principle 1: People Face Trade-offs</span></h3><p>To get one thing we like, we usually have to give up another thing that we like. <strong>Making decisions</strong> requires trading-off one goal against another.</p><ul><li>student cannot learn two or more things at the same time</li><li>how to spend family income</li><li>guns (defense) and butter (living conditions)</li></ul><p><strong>Efficiency</strong> and <strong> Equality</strong></p><ul><li><em>Efficiency</em> means the property of society getting the most it can from its scarce resources.</li><li><em>Equality</em> means the property of distributing economic prosperity uniformly among the members of society.</li><li>In other words, <em>efficiency</em> refers to the size of the economic pie, and <em>equality</em> refers to how the pie is divided into individual slices.</li><li>When government tries to cut the economic pie into more equal slices, the pie get smaller.</li></ul><p>Nonetheless, people are likely to make good decisions only if they understand the options they have available. Our study of economics, therefore, starts by acknowledging life’s trade-offs.</p><h3><span id="principle-2-the-cost-of-something-is-what-you-give-up-to-get-it">Principle 2: The Cost of Something Is What You Give Up to Get It</span></h3><p><strong>Opportunity cost</strong>: whatever must be given up to obtain some item. When making any decision, decision makers should be aware of the opportunity costs that accompany each possible action.</p><h3><span id="principle-3-rational-people-think-at-the-margin">Principle 3: Rational People Think at the Margin</span></h3><p><strong>Rational people</strong> systematically and purposefully do the best they can to achieve their objectives, given the available opportunities. <strong> Marginal change</strong>: a small incremental adjustment to a plan of action. e.g. when exam around, study one more hour instead of playing games.</p><p>Rational people often make decisions by comparing <em>marginal benefits</em> and <em>marginal costs</em>.</p><ul><li>airline ticket</li><li>why is water so cheap, while diamonds are so expensive?<ul><li>water is plentiful -&gt; margin benefit is small</li><li>diamonds are so rare -&gt; margin benefit is large A rational decision maker takes an action if and only if the <em>marginal benefit</em> of the action <strong>exceeds</strong> the <em>marginal cost</em>.</li></ul></li></ul><h3><span id="principle-4-people-respond-to-incentives">Principle 4: People Respond to Incentives</span></h3><p>An <strong>incentive</strong> is something that induces a person to act, such as the prospect of a punishment or a reward. <em>People respond to incentives, the rest is commentary.</em></p><p>Auto safety</p><ul><li>1950s, no seat belt, accident is costly -&gt; seat belt law -&gt; accident is not that costly -&gt; people drive faster (cost less time) -&gt; few deaths per accident but more accidents -&gt; little change in driver deaths and an increase in the number of pedestrian deaths.</li></ul><p>When analyzing any policy, we must consider not only the direct effects but also the less obvious indirect effects that work through incentives. If the policy changes incentives, it will cause people to alter their behavior.</p><p><em>Incentive Pay</em> Chicago buses do not take the shortcut when around congestion, because they have no incentive to do so. If they are paid by passengers like taxi rather than by bus company, they will choose the shortcuts to get more passengers like other cars do. It will increase the bus driver’s productivity but also increase the risk of having accidents.</p><h2><span id="how-people-interact">How People Interact</span></h2><h3><span id="principle-5-trade-can-make-everyone-better-off">Principle 5: Trade Can Make Everyone Better Off</span></h3><p><strong>Trade</strong> between two countries is not like a sports contest in which one side wins and the other side loses. In fact, the opposite is true: <em>Trade between two countries can make each country better off</em>.</p><p>Trade allows countries to specialize in what they do best and to enjoy a greater variety of goods and services.</p><h3><span id="principle-6-markets-are-usually-a-good-way-to-organize-economic-activity">Principle 6: Markets Are Usually a Good Way to Organize Economic Activity</span></h3><p><em>Communist</em> countries worked on the premise that government officials were in the best position to allocate the economy’s scarce resources. The theory behind <em>central planning</em> was that only the government could organize economic activity in a way that promoted <em>economic well-being for the country as a whole</em>. <em>Central planners</em> failed because they tried to run the economy with one hand tied behind their backs — the invisible hand of the marketplace.</p><p>In a <strong>market economy</strong>, the decisions of a central planner are replaced by the decisions of millions of firms and households.</p><blockquote><p>Households and firms interacting in markets act as if they are guided by an “invisible hand” that leads them to desirable market outcomes. — Adam Smith</p></blockquote><p>In any market, buyers look at the price when determining how much to demand, and sellers look at the price when deciding how much to supply. As a result of the decisions that buyers and sellers make, <em>market prices</em> reflect both <em>the value of a good to society</em> and <em>the cost to society of making the good</em>. Smith’s great insight was that <strong>prices</strong> adjust to <strong>guide</strong> these individual buyers and sellers to reach outcomes that, in many cases, <em>maximize the well-being of society as a whole</em>.</p><h3><span id="principle-7-governments-can-sometimes-improve-market-outcomes">Principle 7: Governments Can Sometimes Improve Market Outcomes</span></h3><p><strong>property right</strong>: the ability of an individual to own and exercise control over scarce resources. <strong>market failure</strong>: a situation in which a market left on its own fails to allocate resources efficiently. <strong>externality</strong>: the impact of one person’s actions on the well-being of a bystander. <strong>market power</strong>: the ability of a single economic actor (or a small group of actors) to have a substantial influence on market prices.</p><p><em>The invisible hand is powerful, but it is not omnipotent.</em> The economy needs the government to</p><ul><li>enforce the rules and maintain the institutions that are key to a market economy</li><li>enforce <strong>property right</strong><ul><li>We all rely on government-provided police and courts to enforce our rights over the things we produce — and the <em>invisible hand</em> counts on our ability to enforce our rights.</li></ul></li><li>promote <strong>efficiency</strong><ul><li><em>market failure</em> because <strong>externality</strong> (e.g. pollution) and <strong>market power</strong> (e.g. monopoly)</li></ul></li><li>promote <strong>equality</strong></li></ul><h2><span id="how-the-economy-as-a-whole-works">How the Economy as a Whole Works</span></h2><h3><span id="principle-8-a-countrys-standard-of-living-depends-on-its-ability-to-produce-goods-and-services">Principle 8: A Country’s Standard of Living Depends on Its Ability to Produce Goods and Services</span></h3><p>Why the differences in living standards among countries and over time are so large? Almost all variation in living standards is attributable to differences in countries’ <strong>productivity</strong> — that is, the amount of goods and services produced from each unit of labor input. When thinking about how any policy will affect our living standards, the key question is <em>how it will affect our ability to produce goods and services</em>.</p><h3><span id="principle-9-prices-rise-when-the-government-prints-too-much-money">Principle 9: Prices Rise When the Government Prints Too Much Money</span></h3><p><strong>inflation</strong>: an increase in the overall level of prices in the economy</p><p>What cause inflation? In almost all cases of large or persistent inflation, the culprit is <em>growth in the quantity of money</em>.</p><blockquote><p>The broken window fallacy Some teenagers, being the little beasts that they are, toss a brick through a bakery window. A crown gathers and laments, “What a shame”. But before you know it, someone suggests a silver lining to the situation: Now the baker will have to spend money to have the window repaired. This will add to the income of the repairman, who will spend his additional income, which will add to another seller’s income, and so on. The chain of spending will multiply and generate higher income and employment. If the broken window is large enough, it might produce an economic boom! But if the baker hadn’t spent his money on window repair, he would have spent it on the new suit he was saving to buy. Then the tailor would have the new income to spend, and so on. <em>The broken window didn’t create new spending; it just diverted spending from somewhere else.</em></p></blockquote><h3><span id="principle-10-society-faces-a-short-run-trade-off-between-inflation-and-unemployment">Principle 10: Society Faces a Short-Run Trade-off between Inflation and Unemployment</span></h3><p>Short-run effects of monetary injections as follows:</p><ul><li>Increasing the amount of money in the economy stimulates the overall level of spending and thus the demand for goods and services</li><li>Higher demand many over time cause firms to raise their prices, but in the meantime, it also encourage them to hire more workers and produce a larger quantity of goods and services.</li><li>More hiring means lower unemployment.</li></ul><p><strong>business cycle</strong>: fluctuations in economic activity, such as employment and production.</p><p>Case: 2008 deep economic downturn -&gt; Barack Obama: <em>stimulus package of reduced taxes and increased government spending</em> -&gt; Federal Reserve: <em>increased the supply of money</em> -&gt; <strong>reduce unemployment</strong> -&gt; might over time lead to an <strong>excessive level of inflation</strong>.</p><h2><span id="summary">Summary</span></h2><ol type="1"><li>The fundamental lessons about individual decision making are that people face trade-offs among alternative goals, that the cost of any action is measured in terms of forgone opportunities, that rational people make decisions by comparing marginal costs and marginal benefits, and that people change their behavior in response to the incentives they face.</li><li>The fundamental lessons about interactions among people are that trade and interdependence can be mutually beneficial, that markets are usually a good way of coordinating economic activity among people, and that the government can potentially improve market outcomes by remedying a market failure or by promoting greater economic equality.</li><li>The fundamental lessons about the economy as a whole are that productivity is the ultimate source of living standards, that growth in the quantity of money is the ultimate source of inflation, and that society faces a short-run trade-off between inflation and unemployment.</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#how-people-make-decisions&quot;&gt;How People Make Decisions&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#principle-1-people-face-trade-offs&quot;&gt;Principle 1: People Face Trade-offs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-2-the-cost-of-something-is-what-you-give-up-to-get-it&quot;&gt;Principle 2: The Cost of Something Is What You Give Up to Get It&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-3-rational-people-think-at-the-margin&quot;&gt;Principle 3: Rational People Think at the Margin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-4-people-respond-to-incentives&quot;&gt;Principle 4: People Respond to Incentives&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#how-people-interact&quot;&gt;How People Interact&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#principle-5-trade-can-make-everyone-better-off&quot;&gt;Principle 5: Trade Can Make Everyone Better Off&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-6-markets-are-usually-a-good-way-to-organize-economic-activity&quot;&gt;Principle 6: Markets Are Usually a Good Way to Organize Economic Activity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-7-governments-can-sometimes-improve-market-outcomes&quot;&gt;Principle 7: Governments Can Sometimes Improve Market Outcomes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#how-the-economy-as-a-whole-works&quot;&gt;How the Economy as a Whole Works&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#principle-8-a-countrys-standard-of-living-depends-on-its-ability-to-produce-goods-and-services&quot;&gt;Principle 8: A Country’s Standard of Living Depends on Its Ability to Produce Goods and Services&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-9-prices-rise-when-the-government-prints-too-much-money&quot;&gt;Principle 9: Prices Rise When the Government Prints Too Much Money&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#principle-10-society-faces-a-short-run-trade-off-between-inflation-and-unemployment&quot;&gt;Principle 10: Society Faces a Short-Run Trade-off between Inflation and Unemployment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://www.52coding.com.cn/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="微观经济型原理" scheme="http://www.52coding.com.cn/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%9E%8B%E5%8E%9F%E7%90%86/"/>
    
      <category term="inflation" scheme="http://www.52coding.com.cn/tags/inflation/"/>
    
      <category term="marginal benefit" scheme="http://www.52coding.com.cn/tags/marginal-benefit/"/>
    
  </entry>
  
  <entry>
    <title>AlphaGo, AlphaGo Zero and AlphaZero</title>
    <link href="http://www.52coding.com.cn/2018/05/15/AlphaGo%20and%20AlphaGo%20Zero/"/>
    <id>http://www.52coding.com.cn/2018/05/15/AlphaGo and AlphaGo Zero/</id>
    <published>2018-05-15T07:55:19.000Z</published>
    <updated>2018-11-16T07:54:21.150Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="go">Go</span></h2><p>围棋起源于古代中国，是世界上最古老的棋类运动之一。在宋代的《梦溪笔谈》中探讨了围棋的局数变化数目，作者沈括称“大约连书万字四十三个，即是局之大数”，意思是说变化数目要写43个万字。根据围棋规则，没有气的子不能存活，扣除这些状态后的合法状态约有 <span class="math inline">\(2.08×10^{170}\)</span> 种。Robertson 与 Munro 在1978年证得围棋是一种 PSPACE-hard 的问题，其必胜法之记忆计算量在<span class="math inline">\(10^{600}\)</span> 以上，这远远超过可观测宇宙的原子总数 <span class="math inline">\(10^{75}\)</span>，可见围棋对传统的搜索方法是非常有挑战的。 <a id="more"></a></p><p><img src="/images/go1.png"></p><h2><span id="alphago">AlphaGo</span></h2><p><img src="/images/alphago_ori.png"></p><p>AlphaGo是第一个打败人类冠军的电脑程序。</p><p><strong>网络结构</strong></p><p>它由两个卷积神经网络组成，分别是策略网络和价值网络。</p><p><img src="/images/policynet.png"></p><p>策略网络 P 推荐下一步怎么走；它的输入就是棋盘的矩阵：白棋和黑棋的位置。这个网络由许多卷积层组成，逐渐学习围棋知识，最终输出行动（action）的概率分布，来推荐下一步怎么走。</p><p><img src="/images/valuenet.png"></p><p>价值网络也由卷积神经网络组成，它是用来预测这盘棋的胜者。它的输入也是棋盘矩阵，输出是一个属于 <span class="math inline">\([-1, +1]\)</span> 的标量，-1代表AlphaGo一定会输，+1代表一定会赢。</p><p><strong>训练流程</strong></p><p><img src="/images/alphago_train.png"></p><p>首先是监督学习，让策略网络学习人类专家的数据集：每一个棋面都有一个标签，对应人类专家的下法，让AlphaGo首先学习专家的走法。然后使用策略网络进行自我博弈，由于每局都会产生胜者，用这些数据来训练价值网络。</p><p><strong>搜索算法</strong></p><p><img src="/images/rebredth.png"></p><p>使用策略网络减少搜索宽度，只考虑网络推荐的下法。</p><p><img src="/images/red_val.png"></p><p>还可以使用价值网络来降低搜索树的深度，可以把搜索子树替换为一个值来表明这个局面赢的概率。</p><p><img src="/images/mcts_go.png"></p><p>不过实际上还是用的蒙特卡洛搜索树。它分为三步：</p><ol type="1"><li><p>选择</p><p>首先从树根向下遍历，每次选择置信度最高的走法，直到叶节点。置信度是由每个节点中存储的 Q-value 和策略网络给的先验概率 P 组成。</p></li><li><p>扩展和评估</p><p>到了叶节点之后就要扩展这颗树，用策略网络和价值网络分别评估当前局面，把概率最大的节点加入搜索树。</p></li><li><p>回溯</p><p>把新加入节点的价值 v 回溯到路径上的每一个节点的 Q-value 上。</p></li></ol><p>这就是初始版本的AlphaGo，这个版本赢了世界冠军李世石。</p><p><img src="/images/leesd.png"></p><h2><span id="alphago-zero">AlphaGo Zero</span></h2><p>AlphaGo Zero 除了围棋规则本身以外完全移除了人类的围棋知识，它与AlphaGo的主要区别如下：</p><ul><li>无人类数据<ul><li>完全从自我博弈中学习</li></ul></li><li>无手动编码的特征<ul><li>输入只是棋盘本身</li></ul></li><li>单一的神经网络<ul><li>策略网络和价值网络合二为一，并且结构改进为ResNet</li><li>输出部分分为两头，分别输出 policy 和 value</li></ul></li><li>更简单的搜索<ul><li>更简单的MCTS，无随机的快速走子，只用神经网络进行评估</li></ul></li></ul><p><strong>增强学习算法</strong></p><p><img src="/images/rl_zero.png"></p><p>目标：使用高质量（really really high quality）数据来训练神经网络，而最好的数据来源就是AlphaGo自我博弈。</p><p>所以流程就是这样的：</p><ol type="1"><li>输入当前的棋局，使用当前的神经网络来指导进行蒙特卡洛搜索，然后下搜索出的那步棋，接着输入后面的棋局、搜索….直到一盘棋结束。</li></ol><p><img src="/images/train_zero.png"></p><ol start="2" type="1"><li>下一步就是训练神经网络，使用之前自我对局的数据，训练策略的数据的特征就是任一棋局，标签就是蒙特卡洛搜索的结果，即策略更贴近于AlphaGo实际下的策略（MCTS的搜索结果）</li></ol><p><img src="/images/train_zero_val.png"></p><ol start="3" type="1"><li>与此同时，使用每盘对局的胜者训练价值网络部分。</li></ol><p><img src="/images/zero_iterate.png"></p><ol start="4" type="1"><li>最后，经过训练的神经网络又可以继续进行自我博弈，产生更高质量的数据，然后用这个数据继续训练…. 循环往复，循环的关键在于，经过每个循环，我们都会得到更强的棋手（神经网络），所以继续会得到更高质量的数据。最后就产生了非常强的棋手。</li></ol><p><img src="/images/rl_policy_ite.png"></p><p>这个算法可以被看作是增强学习里的策略迭代（Policy Iteration）算法：</p><ul><li>Search-Based Policy Improvement （策略增强）<ul><li>用当前的网络进行MCTS</li><li>MCTS搜索出来的结果 &gt; 神经网络直接选择的结果（因为搜索的结果结合了前瞻）</li></ul></li><li>Search-Based Policy Evaluation （策略评估）<ul><li>使用搜索算法和神经网络进行自我博弈</li><li>评估改进后的策略</li></ul></li></ul><p><strong>学习曲线</strong></p><p><img src="/images/gozero_curve.png"></p><p><strong>实力</strong></p><p><img src="/images/gozero_rating.png"></p><h2><span id="alphazero">AlphaZero</span></h2><p><img src="/images/alphazero.png"></p><p>AlphaZero使用同一种算法学习三种不同的棋类，并都取得了超人的水平。</p><p>棋类AI研究情况总结</p><ul><li>在AI的历史上很早就开始研究棋类，如图灵、香农、冯诺伊曼等</li><li>专一系统曾在国际象棋上成功过<ul><li>深蓝在1997年击败卡氏</li><li>现在的象棋程序人类已无法击败</li></ul></li><li>将棋（日本象棋）比国际象棋更难<ul><li>更大的棋盘和行动空间</li><li>只有最近的程序才达到了龙王的水平</li></ul></li><li>最前沿的引擎都是根据 alpha-beta 搜索<ul><li>人类大师手工优化的评估函数</li><li>搜索域针对不同棋类疯狂优化</li></ul></li></ul><p><img src="/images/gochess.png"></p><p>由上图可见围棋与将棋和象棋还是有很大不同的，但是AlphaZero的主要算法和AlphaGo Zero一样，都是自我博弈的增强学习，只是把一些只针对围棋的细节去掉了（比如通过旋转进行数据增强，因为围棋是对称的）和输入输出维度进行了改变。</p><p>它的学习曲线如下，均达到了顶尖水平：</p><p><img src="/images/zero_curve2.png"></p><h2><span id="alphazero-and-exit">AlphaZero and ExIt</span></h2><p><a href="https://arxiv.org/abs/1705.08439" target="_blank" rel="noopener">Expert Iteration（ExIt）</a>是一种模仿学习（Imitation Learning, IL）算法，普通的 IL 算法中，徒弟模仿专家的策略只能提高自己的策略，专家是不会有任何提高的，而 ExIt 算法就是想让师傅教徒弟的时候自己也有提高。</p><p><strong>ExIt 算法</strong> 师傅根据徒弟的策略进行前向搜索（例如MCTS，alpha-beta，贪心搜索等），得出比徒弟更好的策略，然后徒弟再学习师傅的策略，如此循环，随着徒弟的增强，师傅也会越来越强。</p><p><img src="/images/exit.png"></p><p>可见，AlphaZero也属于 ExIt 算法，师傅为 MCTS，徒弟就是神经网络。</p><h2><span id="summary">Summary</span></h2><p>现在棋类人工智能算法的发展趋势是越来越泛化，趋向于多功能。从 AlphaGo 的学习人类专家的棋谱到 AlphaGo Zero 的从零开始无需人类知识的自我博弈学习再到 AlphaZero 的同一算法适应不同棋类并且都取得超人水平。可见人工智能越来越向通用智能发展，虽然长路漫漫，现在的算法远不够泛化，但是很多东西，比如神经网络结构都是可以用到不同领域的。AlphaGo 系列的作者之一 David Silver 曾说:“每次你专门化一些东西都会伤害你的泛化能力” (Every time you specialize something you hurt your generalization ability.)。事实也的确如此，AlphaGo 系列架构越来越简单，而其性能和泛化能力却越来越强大。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;go&quot;&gt;Go&lt;/h2&gt;
&lt;p&gt;围棋起源于古代中国，是世界上最古老的棋类运动之一。在宋代的《梦溪笔谈》中探讨了围棋的局数变化数目，作者沈括称“大约连书万字四十三个，即是局之大数”，意思是说变化数目要写43个万字。根据围棋规则，没有气的子不能存活，扣除这些状态后的合法状态约有 &lt;span class=&quot;math inline&quot;&gt;\(2.08×10^{170}\)&lt;/span&gt; 种。Robertson 与 Munro 在1978年证得围棋是一种 PSPACE-hard 的问题，其必胜法之记忆计算量在&lt;span class=&quot;math inline&quot;&gt;\(10^{600}\)&lt;/span&gt; 以上，这远远超过可观测宇宙的原子总数 &lt;span class=&quot;math inline&quot;&gt;\(10^{75}\)&lt;/span&gt;，可见围棋对传统的搜索方法是非常有挑战的。
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="AlphaGo" scheme="http://www.52coding.com.cn/tags/AlphaGo/"/>
    
      <category term="AlphaZero" scheme="http://www.52coding.com.cn/tags/AlphaZero/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>论文翻译：在没有人类知识的情况下掌握围棋</title>
    <link href="http://www.52coding.com.cn/2018/03/10/%E5%9C%A8%E6%B2%A1%E6%9C%89%E4%BA%BA%E7%B1%BB%E7%9F%A5%E8%AF%86%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E6%8E%8C%E6%8F%A1%E5%9B%B4%E6%A3%8B/"/>
    <id>http://www.52coding.com.cn/2018/03/10/在没有人类知识的情况下掌握围棋/</id>
    <published>2018-03-10T06:01:09.000Z</published>
    <updated>2018-11-06T03:48:59.304Z</updated>
    
    <content type="html"><![CDATA[<h4><span id="1-前言">1. 前言</span></h4><p>​ 人工智能的一个长期目标是在一些有挑战的领域中从零开始学习出超人熟练程度的算法。最近，AlphaGo成为第一个在围棋比赛中击败世界冠军的程序。 AlphaGo中的树搜索使用深度神经网络评估位置和选定的移动。这些神经网络是通过监督学习来自人类专家的走法以及通过强化自我学习来进行训练的。这里我们只介绍一种基于强化学习的算法，没有超出游戏规则的人类数据，指导或领域知识。AlphaGo成为自己的老师：一个神经网络训练预测AlphaGo的移动选择和游戏的胜者。这个神经网络提高了树搜索的强度，在下一次迭代中拥有更高质量的移动选择和更强的自我学习。我们的新程序AlphaGo Zero从零开始学习，实现了超人的表现，与之前发布的夺冠冠军AlphaGo相比以100-0取胜。</p><a id="more"></a><h4><span id="2-概述">2. 概述</span></h4><p>​ 围棋程序在人工智能方面已经取得了很大的进展，使用经过训练的监督学习系统来复制人类专家的决定。但是，专家数据集通常很昂贵，不可靠或根本无法使用。即使有可靠的数据集，它们也可能会对以这种方式培训的系统的性能施加上限。相比之下，强化学习系统是根据他们自己的经验进行学习的，原则上允许他们超越人类能力，并在缺乏人力专业知识的领域运作。最近，通过强化学习训练的深度神经网络，朝着这个目标快速发展。这些系统在计算机游戏中胜过人类，如Atari游戏和3D虚拟环境。然而，在人类智力方面最具挑战性的领域 - 比如被广泛认为是人工智能的巨大挑战的围棋游戏 - 在广阔的搜索空间中需要精确和复杂的搜索。以前的方法没有在这些领域实现达到人类水平的表现。</p><p>​ AlphaGo 是第一个在围棋中实现超人表现的程序。之前发布的版本，我们称之为AlphaGo Fan，于2015年10月击败了欧洲冠军范辉。AlphaGo Fan 使用了两个深度神经网络：输出移动概率的策略网络和输出位置评估的价值网络。策略网络最初是通过监督学习来准确地预测人类专家的行为，随后通过策略升级强化学习进行了改进。价值网络经过训练，可以预测游戏的胜者。一旦开始训练，这些网络就会与蒙特卡洛树搜索（MCTS）结合使用，从而提供先行搜索，使用策略网络将搜索范围缩小为高概率移动，并使用价值网络来评估树中的位置。我们称之为 AlphaGo Lee 的后续版本使用了类似的方法，并于2016年3月击败了获得18个国际冠军的Lee Sedol。</p><p>​ AlphaGo Zero 与 AlphaGo Fan 和 AlphaGo Lee 在几个重要方面不同。首先，它只是通过自我增强强化学习进行训练，从随机比赛开始，没有任何监督或使用人类数据。其次，它只使用黑白棋位置作为输入。第三，它使用单一的神经网络，而不是单独的策略和价值网络。最后，它使用更简单的搜索树，该搜索依赖于这个单一的神经网络来评估位置和移动，而无需执行任何 Monte Carlo 回溯。为了实现这些结果，我们引入了一种新的强化学习算法，该算法在训练环内部结合了前瞻搜索，从而实现了快速改进和精确而稳定的学习。在方法一栏中中描述了搜索算法，训练过程和网络体系结构中的其他技术差异。</p><h4><span id="3-alphago-zero-中的增强学习">3. AlphaGo Zero 中的增强学习</span></h4><p>​ 我们的新方法使用参数为 <span class="math inline">\(\theta\)</span> 的深度神经网络 <span class="math inline">\(f(\theta)\)</span>。该神经网络将位置及其历史的原始平面表示 s 作为输入，并输出移动概率和价值 <span class="math inline">\((p,v)=f_\theta(s)\)</span>。 移动概率 p 的向量表示选择每个移动 a 的概率，<span class="math inline">\(P_a=Pr(a|s)\)</span> 。价值 v 是一个标量评估，用于估计当前玩家从位置 s 获胜的概率。这个神经网络将策略网络和价值网络结合到一个网络中。神经网络由卷积层，许多残差块组成，批量归一化和非线性整流器（参见方法）组成。</p><p>​ AlphaGo Zero 中的神经网络是通过一种新型的强化学习算法从自我博弈的游戏中训练出来的。在每个位置 <span class="math inline">\(s\)</span>，执行 MCTS 搜索，由神经网络 <span class="math inline">\(f(\theta)\)</span> 指导。MCTS 搜索输出每次移动的概率 <span class="math inline">\(π\)</span>。这些搜索概率通常选择比神经网络的原始移动概率 <span class="math inline">\(p\)</span> 更加强大;因此，MCTS 可被视为策略改进的操作。使用搜索进行自我博弈 - 使用改进的基于 MCTS 的策略来选择每个动作，然后使用游戏获胜者 <span class="math inline">\(z\)</span> 作为价值的样本 - 可以被视为一个强大的策略评估操作。我们的强化学习算法的主要思想是在策略迭代过程中重复使用这些操作：更新神经网络的参数以使移动概率和值 <span class="math inline">\((p,v)=f_\theta(s)\)</span> 更紧密匹配改进的搜索概率和获胜者 <span class="math inline">\((\pi,z)\)</span>；这些新参数将用于下一次自我博弈，以使搜索更加强大。图 1说明了自我博弈训练流程。</p><p><img src="/images/selfplay.png"></p><p>​ <em>图1 AlphaGo Zero中的自我博弈与增强学习训练流程</em></p><p>​ MCTS 使用神经网络 <span class="math inline">\(f(\theta)\)</span> 来指导其模拟（见图 2）。搜索树中的每个边 <span class="math inline">\((s,a)\)</span> 存储先验概率 <span class="math inline">\(P(s,a)\)</span>，访问计数 <span class="math inline">\(N(s,a)\)</span> 和动作价值 <span class="math inline">\(Q(s,a)\)</span> 。每个模拟从根状态开始，并且迭代地选择使置信上限 <span class="math inline">\(Q(s,a)+U(s,a)\)</span> 最大化的移动，其中<span class="math inline">\(U\propto \frac{P(s,a)}{1+N(s,a)}\)</span>，直到遇到叶节点 <span class="math inline">\(s&#39;\)</span>。该叶子位置被网络扩展和评估一次，以产生先验概率和评估，<span class="math inline">\((P(s&#39;, \cdot), v(s&#39;))=f_\theta(s&#39;)\)</span>。在模拟中遍历的每个边 <span class="math inline">\((s,a)\)</span> 被更新以增加其访问计数 <span class="math inline">\(N(s,a)\)</span>，并且将其动作价值更新为在这些模拟上的平均评估 <span class="math inline">\(Q(s,a) = \frac{1}{N(s,a)}\sum_{s&#39;|s,a\rightarrow s&#39;}V(s&#39;)\)</span>， 其中 <span class="math inline">\(s,a\rightarrow s&#39;\)</span> 表示在从位置 <span class="math inline">\(s\)</span> 执行行动 <span class="math inline">\(a\)</span> 后模拟最终达到位置 <span class="math inline">\(s&#39;\)</span>。</p><p><img src="/images/mcts0.png"></p><p>​ <em>图2 AlphaGo Zero中的蒙特卡洛搜索树</em></p><p>​ MCTS 可以被看作是一种自我博弈算法，在给定神经网络参数 <span class="math inline">\(θ\)</span> 和根位置 <span class="math inline">\(s\)</span> 的情况下，计算推荐移动的搜索概率矢量，<span class="math inline">\(\pi = a_\theta(s)\)</span>，与每次移动的访问计数的指数成比例，<span class="math inline">\(\pi_a\propto N(s,a)^{1/\tau}\)</span>，其中 <span class="math inline">\(τ\)</span> 是温度参数。</p><p>​ 神经网络通过使用 MCTS 选择每个动作的自我博弈增强化学习算法进行训练。首先，神经网络被初始化为随机权重 <span class="math inline">\(\theta_0\)</span>。在随后的每次迭代 <span class="math inline">\(i≥1\)</span> 时，产生自我博弈的数据（图 1）。在每个时刻 <span class="math inline">\(t\)</span>，使用先前的神经网络迭代 <span class="math inline">\(f_{\theta_{i-1}}\)</span> 执行 MCTS 搜索，并且通过对搜索概率 <span class="math inline">\(\pi_t\)</span> 进行采样来执行移动。当两个玩家都无路可走时或者当搜索值下降到低于阈值或当游戏超过最大长度时，游戏在时刻 <span class="math inline">\(T\)</span> 终止;然后对游戏进行评分以给出 <span class="math inline">\(r_T\in\{-1,+1\}\)</span> 的最终奖励（详见方法）。每个时刻 <span class="math inline">\(t\)</span> 的数据存储为 <span class="math inline">\((s_t,\pi_t,z_t)\)</span>，其中 <span class="math inline">\(z_t = \pm r_T\)</span> 是时刻 <span class="math inline">\(t\)</span> 从当前玩家角度出发的游戏获胜者。同时（如图 1），新的网络参数 <span class="math inline">\(\theta_i\)</span> 从最后一次自我博弈的所有时间中统一采样的数据 <span class="math inline">\((s,\pi,t)\)</span> 进行训练。调整神经网络 <span class="math inline">\((p,v)=f_{\theta_i}(s)\)</span> 以最小化预测值 <span class="math inline">\(v\)</span> 与实际赢得者 <span class="math inline">\(z\)</span>之间的误差，并使神经网络移动概率 <span class="math inline">\(p\)</span> 与搜索概率 <span class="math inline">\(π\)</span> 的相似性最大化。具体而言，参数 <span class="math inline">\(θ\)</span> 通过梯度下降在损失函数 <span class="math inline">\(l\)</span> 上进行调整，所述损失函数 <span class="math inline">\(l\)</span> 分别对均方误差和交叉熵误差进行求和： <span class="math display">\[(p,v)=f_\theta(s) \mbox{ and }l=(z-v)^2-\pi^T\log p+c||\theta||^2\]</span> 其中 <span class="math inline">\(c\)</span> 是控制 L2 正则化程度的超参数（为了防止过拟合）。</p><h4><span id="4-alphago-zero-的实验分析">4. AlphaGo Zero 的实验分析</span></h4><p>​ 我们使用上述强化学习流程来训练 AlphaGo Zero。训练从完全随机的行为开始，持续约三天且无人为干预。在训练过程中，每个 MCTS 使用 1,600 次模拟，每次移动的思考时间大约为 0.4s，从而产生了 490 万局自我博弈。 参数从 700,000 个包含 2048 个状态的批量中更新。神经网络包含 20 个残余块。</p><p>​ 图 3 显示了 AlphaGo Zero 在自我博弈过程中的表现，横坐标为训练时间，纵坐标为 Elo 量。整个训练过程进展顺利，并且没有遭受先前文献中提出的振荡或灾难性遗忘。令人惊讶的是，AlphaGo Zero 仅仅 36 小时就赢了AlphaGo Lee。相比之下，AlphaGo Lee 训练了几个月。在 72 小时后，我们根据在首尔人机比赛中使用的相同的 2 小时时间控制和匹配条件，对 AlphaGo Zero 与 AlphaGo Lee 的确切版本进行了评估，该版本击败了 Lee Sedol。AlphaGo Zero 使用带有4个张量处理单元（TPU）的单台机器，而 AlphaGo Lee 分布在多台机器上并使用 48 个TPU。AlphaGo Zero 将 AlphaGo Lee 以 100 比 0 击败。</p><p>​ 为了评估自我强化学习的优点，与从人类数据中学习相比，我们训练了第二个神经网络（使用相同的体系结构）来预测 KGS 服务器数据集中的专家动作; 与之前的工作相比，这实现了预测的准确性。 监督式学习的初始表现更好，并且更好地预测人类职业动作（图 3）。 值得注意的是，尽管监督学习获得了更高的移动预测准确度，但自学者的整体表现更好，在训练的前 24 小时内击败了训练有素的选手。这表明 AlphaGo Zero 可能正在学习一种与人类下棋不同的策略。</p><p><img src="/images/ag0em.png"></p><p>​ <em>图3 AlphaGo Zero的实验评估</em></p><p>​ 为了分离架构和算法的贡献，我们将 AlphaGo Zero 中的神经网络架构的性能与 AlphaGo Lee 中使用的以前的神经网络架构进行了比较（见图 4）。 新训练的AlphaGo Zero 有四个版本的神经网络，分别是：使用 AlphaGo Lee 的卷积网络架构；AlphaGo Zero 的剩余网络架构；使用 AlphaGo Zero 的卷积网络架构；使用AlphaGo Lee 的剩余网络架构。每个网络都经过训练，以最小化相同的损失函数，使用由 AlphaGo Zero 在自我训练 72 小时后产生的固定数据集。使用剩余网络更准确，实现了更低的误差，AlphaGo 的性能提高了 600 多 Elo。将策略和价值组合在一起成为一个网络，略微降低了移动预测的准确性，但是将降低了 AlphaGo 的价值误差和提高了博弈性能约 600 个 Elo。部分原因在于提高了计算效率，但更重要的是，双重目标将网络正则化为支持多种用例的表示。</p><p><img src="/images/ag02.png"></p><p>​ <em>图4 AlphaGo Zero和AlphaGo Lee的神经网络结构比较</em></p><h4><span id="5-alphago-zero-学习到的围棋知识">5. AlphaGo Zero 学习到的围棋知识</span></h4><p>​ AlphaGoZero在其自我博弈训练过程中发现了非凡的围棋知识水平。这不仅包括人类围棋知识的基本要素，还包括超出传统围棋知识范围的非标准策略。</p><p>​ 图 5显示了一个时间线，表明何时发现了专业 joseki（角点序列）;最终AlphaGo Zero 更喜欢先前未知的新的 joseki 变体（图5b ）。图5c 显示了几种在不同训练阶段进行的快速自我博弈。在整个训练中定期进行的锦标赛长度比赛在补充信息中显示。 AlphaGo Zero 从完全随机的移动过渡到对围棋概念的复杂理解，包括fuseki（开场），tesuji（战术），生与死，ko（重复棋局），yose（终局），捕捉比赛，sente（倡议），形状，影响力和领土，都是从最初的原则发现的。令人惊讶的是，Shocho - 人类学习的围棋知识的第一要素之一 - 只有在 AlphaGo Zero 的训练中才能被理解。</p><p><img src="/images/ag05.png"></p><p>​ <em>图5 AlphaGo Zero学到的围棋知识</em></p><h4><span id="6-alphago-zero-的最终水平">6. AlphaGo Zero 的最终水平</span></h4><p>​ 随后我们使用更大的神经网络和更长的持续时间将我们的强化学习管道应用于AlphaGo Zero的第二个实例。再次训练从完全随机行为开始并持续大约40天。</p><p>​ 在训练过程中，产生了 2900 万次自我博弈。参数从每个 2,048 个位置的 310 万个小型批量中更新。神经网络包含 40 个残余块。学习曲线如图 6a 所示。在整个训练过程中定期进行的比赛显示在补充信息中。</p><p><img src="/images/ag06.png"></p><p>​ <em>图6 AlphaGo Zero的评估</em></p><p>​ 我们使用 AlphaGo Fan，AlphaGo Lee 和之前的几个围棋程序的内部比赛评估了训练有素的 AlphaGo Zero。我们还与最强大的现有程序 AlphaGo Master 进行了游戏，该程序基于本文提供的算法和体系结构，但使用了人类数据和特征（请参阅方法） - 它在 60-0 在线游戏中击败了最强的人类职业玩家。在我们的评估中，所有程序都允许每个动作有5秒的思考时间; AlphaGo Zero 和 AlphaGo Master 每台在带有 4 个 TPU 的单台机器上博弈; AlphaGo Fan 和 AlphaGo Lee 分别分布有 176 个GPU和 48 个TPU。我们还包括一个完全基于 AlphaGo Zero 原始神经网络的选手; 该选手只是以最大的概率选择移动（不进行 MCTS 搜索）。</p><p>​ 图 6b显示了每个程序在Elo规模上的表现。未使用任何预测的原始神经网络实现了3,055的Elo评级。 AlphaGo Zero 获得了5,185的评分，而 AlphaGo Master 的4,858，AlphaGo Lee 的 3,739和 AlphaGo Fan 的3,144。</p><p>​ 最后，我们评估了 AlphaGo Zero 对阵 AlphaGo Master，在每场2小时的时间限定内进行了100场比赛，AlphaGo Zero 赢得了其中的89场。</p><h4><span id="7-结论">7. 结论</span></h4><p>​ 我们的研究结果全面证明，即使在最具挑战性的领域中，纯粹的强化学习方法也是完全可行的：在不超出基本规则的情况下，没有关于领域的知识，就可以训练到超人的水平，没有人类的例子或指导。此外，与用人类专家数据训练的程序相比，纯粹的强化学习方法只需要几个小时的训练时间就能达到更好的性能。使用这种方法，AlphaGo Zero 大幅度击败了使用人工数据训练的 AlphaGo 最强大的先前版本。</p><p>​ 人类已经积累了几千年来的围棋知识，发展成固定的模式，总结成谚语和书籍。在几天的时间里，AlphaGo Zero 从零学起，就能够重新发现许多的围棋知识，并能为这个古老的游戏提供新见解、新策略。</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;1. 前言&lt;/h4&gt;
&lt;p&gt;​ 人工智能的一个长期目标是在一些有挑战的领域中从零开始学习出超人熟练程度的算法。最近，AlphaGo成为第一个在围棋比赛中击败世界冠军的程序。 AlphaGo中的树搜索使用深度神经网络评估位置和选定的移动。这些神经网络是通过监督学习来自人类专家的走法以及通过强化自我学习来进行训练的。这里我们只介绍一种基于强化学习的算法，没有超出游戏规则的人类数据，指导或领域知识。AlphaGo成为自己的老师：一个神经网络训练预测AlphaGo的移动选择和游戏的胜者。这个神经网络提高了树搜索的强度，在下一次迭代中拥有更高质量的移动选择和更强的自我学习。我们的新程序AlphaGo Zero从零开始学习，实现了超人的表现，与之前发布的夺冠冠军AlphaGo相比以100-0取胜。&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="AlphaGo" scheme="http://www.52coding.com.cn/tags/AlphaGo/"/>
    
      <category term="AlphaZero" scheme="http://www.52coding.com.cn/tags/AlphaZero/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>RL - Integrating Learning and Planning</title>
    <link href="http://www.52coding.com.cn/2018/01/09/RL%20-%20Integrating%20Learning%20and%20Planning/"/>
    <id>http://www.52coding.com.cn/2018/01/09/RL - Integrating Learning and Planning/</id>
    <published>2018-01-09T13:11:09.000Z</published>
    <updated>2018-11-06T03:47:24.502Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="introduction">Introduction</span></h2><p>In last lecture, we learn <strong>policy</strong> directly from experience. In previous lectures, we learn <strong>value function</strong> directly from experience. In this lecture, we will learn <strong>model</strong> directly from experience and use <strong>planning</strong> to construct a value function or policy. Integrate learning and planning into a single architecture.</p><p>Model-Based RL</p><ul><li>Learn a model from experience</li><li><strong>Plan</strong> value function (and/or policy) from model</li></ul><a id="more"></a><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#model-based-reinforcement-learning">Model-Based Reinforcement Learning</a></li><li><a href="#integrated-architectures">Integrated Architectures</a></li><li><a href="#simulation-based-search">Simulation-Based Search</a></li></ul><!-- tocstop --><h2><span id="model-based-reinforcement-learning">Model-Based Reinforcement Learning</span></h2><p><img src="/images/mbrl.png"></p><p>Advantages of Model-Based RL</p><ul><li>Can efficiently learn model by supervised learning methods</li><li>Can reason about model uncertainty</li></ul><p>Disadvantages</p><ul><li>First learn a model, then construct a value function -&gt; two source of approximation error</li></ul><p><strong>What is a Model?</strong></p><p>A model <span class="math inline">\(\mathcal{M}\)</span> is a representation of an MDP <span class="math inline">\(&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}&gt;\)</span> parametrized by <span class="math inline">\(\eta\)</span>.</p><p>We will assume state space <span class="math inline">\(\mathcal{S}\)</span> and action space <span class="math inline">\(\mathcal{A}\)</span> are known. So a model <span class="math inline">\(\mathcal{M}=&lt;\mathcal{P}_, \eta\mathcal{R}_\eta&gt;\)</span> represents state transitions <span class="math inline">\(\mathcal{P}_\eta \approx \mathcal{P}\)</span> and rewards <span class="math inline">\(\mathcal{R}_\eta\approx \mathcal{R}\)</span>. <span class="math display">\[S_{t+1}\sim\mathcal{P}_\eta(S_{t+1}|S_t, A_t)\\R_{t+1}=\mathcal{R}_\eta(R_{t+1}|S_t, A_t)\]</span> Typically assume conditional independence between state transitions and rewards.</p><p>Goal: estimate model <span class="math inline">\(\mathcal{M}_\eta\)</span> from experience <span class="math inline">\(\{S_1, A_1, R_2, …, S_T\}\)</span>.</p><p>This is a supervised learning problem: <span class="math display">\[S_1, A_1 \rightarrow R_2, S_2 \\S_2, A_2 \rightarrow R_3, S_3 \\...\\S_{T-1}, A_{T-1} \rightarrow R_T, S_T \\\]</span> Learning <span class="math inline">\(s, a\rightarrow r\)</span> is a <em>regression</em> problem; learning <span class="math inline">\(s, a\rightarrow s&#39;\)</span> is a <em>density</em> estimation problem. Pick loss function, e.g. mean-squared error, KL divergence, … Find parameters <span class="math inline">\(\eta\)</span> that minimise empirical loss.</p><p>Examples of Models</p><ul><li>Table Lookup Model</li><li>Linear Expectation Model</li><li>Linear Gaussian Model</li><li>Gaussian Process Model</li><li>Deep Belief Network Model</li></ul><p><strong>Table Lookup Model</strong></p><p>Model is an explicit MDP. Count visits <span class="math inline">\(N(s, a)\)</span> to each state action pair: <span class="math display">\[\hat{\mathcal{P}}^a_{s,s&#39;}=\frac{1}{N(s,a)}\sum^T_{t=1}1(S_t,A_t,S_{t+1}=s, a, s&#39;)\\\hat{\mathcal{R}}^a_{s,s&#39;}=\frac{1}{N(s,a)}\sum^T_{t=1}1(S_t,A_t=s, a)R_t\]</span> Alternatively, at each time-step <span class="math inline">\(t\)</span>, record experience tuple <span class="math inline">\(&lt;S_t, A_t, R_{t+1}, S_{t+1}&gt;\)</span>. To sample model, randomly pick tuple matching <span class="math inline">\(&lt;s, a, \cdot, \cdot&gt;\)</span>.</p><p><strong>AB Example</strong></p><p><img src="/images/ab2.png"></p><p>We have contrusted a <strong>table lookup model</strong> from the experience. Next step, we will planning with a model.</p><p><strong>Planning with a model</strong></p><p>Given a model <span class="math inline">\(\mathcal{M}_\eta=&lt;\mathcal{P}_\eta, \mathcal{R}_\eta&gt;\)</span>, solve the MDP <span class="math inline">\(&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}_\eta, \mathcal{R}_\eta&gt;\)</span> using favorite planning algorithms</p><ul><li>Value iteration</li><li>Policy iteration</li><li>Tree search</li><li>….</li></ul><p><strong>Sample-Based Planning</strong></p><p>A simple but powerful approach to planning is to use the model <strong>only</strong> to generate samples.</p><p><strong>Sample</strong> experience from model <span class="math display">\[S_{t+1}\sim\mathcal{P}_\eta(S_{t+1}|S_t,A_t)\\R_{t+1}=\mathcal{R}_\eta(R_{t+1}|S_t,A_t)\]</span> Apply <strong>model-free</strong> RL to samples, e.g.:</p><ul><li>Monte-Carlo control</li><li>Sarsa</li><li>Q-learning</li></ul><p>Sample-based planning methods are often more efficient.</p><p><strong>Back to AB Example</strong></p><p><img src="/images/ab3.png"></p><p>We can use our model to sample more experience and apply model-free RL to them.</p><p><strong>Planning with an Inaccurate Model</strong></p><p>Given an imperfect model <span class="math inline">\(&lt;\mathcal{P}_\eta, \mathcal{R}_\eta&gt; ≠ &lt;\mathcal{P}, \mathcal{R}&gt;\)</span>. Performance of model-based RL is limited to optimal policy for approximate MDP <span class="math inline">\(&lt;\mathcal{S}, \mathcal{A}, \mathcal{P}_\eta, \mathcal{R}_\eta&gt;\)</span> i.e. Model-based RL is only as good as the estimated model.</p><p>When the model is inaccurate, planning process will compute a suboptimal policy.</p><ul><li>Solution1: when model is wrong, use model-free RL</li><li>Solution2: reason explicitly about model uncertainty</li></ul><h2><span id="integrated-architectures">Integrated Architectures</span></h2><p>We consider two sources of experience:</p><ul><li><p>Real experience: Sampled from environment (true MDP) <span class="math display">\[S&#39;\sim \mathcal{P}^a_{s,s&#39;}\\R=\mathcal{R}^a_s\]</span></p></li><li><p>Simulated experience: Sampled from model (approximate MDP) <span class="math display">\[S&#39;\sim \mathcal{P}_\eta(S&#39;|S, A)\\R=\mathcal{R}_\eta(R|S, A)\]</span></p></li></ul><p><strong>Integrating Learning and Planning</strong></p><p>Dyna Architecture</p><ul><li>Learn a model from real experience</li><li>Learn and plan value function (and/or policy) from real and simulated experience</li></ul><p><img src="/images/dyna.png"></p><p>The simplest dyna algorithm is <em>Dyna-Q Algorithm</em>:</p><p><img src="/images/dynaq.png"></p><p><img src="/images/dynaqres.png"></p><p>From the experiments, we can see that using planning is more efficient than direct RL only.</p><p><strong>Dyna-Q with an Inaccurate Model</strong></p><p>The changed envrionment is <strong>harder</strong>:</p><p><img src="/images/dynaqhard.png"></p><p>There is a <strong>easier</strong> change:</p><p><img src="/images/dynaqeasy.png"></p><h2><span id="simulation-based-search">Simulation-Based Search</span></h2><p>Let's back to planning problems. Simulation-based search is another approach to solve MDP.</p><p><strong>Forward Search</strong></p><p>Forward search algorithms select the best action by <strong>lookahead</strong>. They build a <strong>search tree</strong> with the current state <span class="math inline">\(s_t\)</span> at the root using a model of the MDP to look ahead.</p><p><img src="/images/ftree.png"></p><p>We don't need to solve the whole MDP, just sub-MDP starting from <strong>now</strong>.</p><p><strong>Simulation-Based Search</strong></p><p>Simulation-based search is forward search paradigm using sample-based planning. Simulate episodes of experience from <strong>now</strong> with the model. Apply <strong>model-free</strong> RL to simulated episodes.</p><p><img src="/images/sbsearch.png"></p><p>Simulate episodes of experience from <strong>now</strong> with the model: <span class="math display">\[\{s_t^k, A^k_t,R^k_{t+1}, ..., S^k_T\}^K_{k=1}\sim\mathcal{M}_v\]</span> Apply <strong>model-free</strong> RL to simulated episodes</p><ul><li>Monte-Carlo control <span class="math inline">\(\rightarrow\)</span> Monte-Carlo search</li><li>Sarsa <span class="math inline">\(\rightarrow\)</span> TD search</li></ul><p><strong>Simple Monte-Carlo Search</strong></p><p>Given a model <span class="math inline">\(\mathcal{M}_v\)</span> and a simulation policy <span class="math inline">\(\pi\)</span>.</p><p>For each action <span class="math inline">\(a\in\mathcal{A}\)</span></p><ul><li><p>Simulate <span class="math inline">\(K\)</span> episodes from current (real) state <span class="math inline">\(s_t\)</span> <span class="math display">\[\{s_t, a, R^k_{t+1},S^k_{t+1},A^k_{t+1}, ..., S^k_T \}^K_{k=1}\sim \mathcal{M}_v, \pi\]</span></p></li><li><p>Evaluate actions by mean return (<strong>Monte-Carlo evaluation</strong>) <span class="math display">\[Q(s_t, a)=\frac{1}{K}\sum^k_{k=1}G_t\rightarrow q_\pi(s_t, a)\]</span></p></li></ul><p>Select current (real) action with maximum value <span class="math display">\[a_t=\arg\max_{a\in\mathcal{A}}Q(s_t, a)\]</span> <strong>Monte-Carlo Tree Search</strong></p><p>Given a model <span class="math inline">\(\mathcal{M}_v\)</span>. Simulate <span class="math inline">\(K\)</span> episodes from current state <span class="math inline">\(s_t\)</span> using current simulation policy <span class="math inline">\(\pi\)</span>. <span class="math display">\[\{s_t, A_t^k, R^k_{t+1},S^k_{t+1},A^k_{t+1}, ..., S^k_T \}^K_{k=1}\sim \mathcal{M}_v, \pi\]</span> Build a search tree containing visited states and actions. <strong>Evaluate</strong> states <span class="math inline">\(Q(s, a)\)</span> by mean return of episodes from <span class="math inline">\(s, a\)</span>: <span class="math display">\[Q(s, a)=\frac{1}{N(s,a)}\sum^K_{k=1}\sum^T_{u=t}1(S_u, A_u=s,a)G_u \to q_\pi(s,a)\]</span> After search is finished, select current (real) action with maximum value in search tree: <span class="math display">\[a_t=\arg\max_{a\in\mathcal{A}}Q(s_t, a)\]</span> In MCTS, the simulation policy <span class="math inline">\(\pi\)</span> <strong>improves</strong>.</p><p>Each simulation consists of two phases (in-tree, out-of-tree)</p><ul><li><strong>Tree policy</strong> (improves): pick actions to maximise <span class="math inline">\(Q(S,A)\)</span></li><li><strong>Default policy</strong> (fixed): pick actions randomly</li></ul><p>Repeat (each simulation)</p><ul><li><span class="math inline">\(\color{red}{\mbox{Evaluate}}\)</span> states <span class="math inline">\(Q(S,A)\)</span> by Monte-Carlo evaluation</li><li><span class="math inline">\(\color{red}{\mbox{Improve}}\)</span> tree policy, e.g. by <span class="math inline">\(\epsilon\)</span>-greedy(Q)</li></ul><p>MCTS is <strong>Monte-Carlo control</strong> applied to <strong>simulated experience</strong>.</p><p>Converges on the optimal search tree, <span class="math inline">\(Q(S, A) \to q_*(S, A)\)</span>.</p><p><strong>Case Study: the Game of Go</strong></p><p><img src="/images/go_2.png"></p><p><em>Rules of Go</em></p><ul><li>Usually played on 19$<span class="math inline">\(19, also 13\)</span><span class="math inline">\(13 or 9\)</span>$9 board</li><li>Black and white place down stones alternately</li><li>Surrounded stones are captured and removed</li><li>The player with more territory wins the game</li></ul><p><img src="/images/ruleofgo.png"></p><p><em>Position Evaluation in Go</em></p><p>The key problem is how good is a position <span class="math inline">\(s\)</span>?</p><p>So the reward function is if Black wins, the reward of the final position is 1, otherwise 0: <span class="math display">\[R_t = 0 \mbox{ for all non-terminal steps } t&lt;T\\R_T=\begin{cases} 1,  &amp; \mbox{if }\mbox{ Black wins} \\0, &amp; \mbox{if }\mbox{ White wins}\end{cases}\]</span> Policy <span class="math inline">\(\pi=&lt;\pi_B,\pi_W&gt;\)</span> selects moves for both players.</p><p>Value function (how good is position <span class="math inline">\(s\)</span>): <span class="math display">\[v_\pi(s)=\mathbb{E}_\pi[R_T|S=s]=\mathbb{P}[Black wins|S=s]\\v_*(s)=\max_{\pi_B}\min_{\pi_w}v_\pi(s)\]</span> <em>Monte Carlo Evaluation in Go</em></p><p><img src="/images/mcego.png"></p><p><img src="/images/amcts1.png"></p><p><img src="/images/amcts2.png"></p><p><img src="/images/amcts3.png"></p><p><img src="/images/amcts4.png"></p><p><img src="/images/amcts5.png"></p><p>So, MCTS will expand the tree towards the node that is most promising and ignore the useless parts.</p><p><strong>Advantages of MC Tree Search</strong></p><ul><li>Highly selective best-first search</li><li>Evaluates states <em>dynamically</em></li><li>Uses sampling to break curse of dimensionality</li><li>Works for &quot;black-box&quot; models (only requires samples)</li><li>Computationally efficient, anytime, parallelisable</li></ul><p><strong>Temporal-Difference Search</strong></p><ul><li>Simulation-based search</li><li>Using TD instead of MC (bootstrapping)</li><li>MC tree search applies MC control to sub-MDP from now</li><li>TD search applies Sarsa to sub-MDP from now</li></ul><p><strong>MC vs. TD search</strong></p><p>For model-free reinforcement learning, bootstrapping is helpful</p><ul><li>TD learning reduces variance but increase bias</li><li>TD learning is usually more efficient than MC</li><li>TD(<span class="math inline">\(\lambda\)</span>) can be much more efficient than MC</li></ul><p>For simulation-based search, bootstrapping is also helpful</p><ul><li>TD search reduces variance but increase bias</li><li>TD search is usually more efficient than MC search</li><li>TD(<span class="math inline">\(\lambda\)</span>) search can be much more efficient than MC search</li></ul><p><strong>TD Search</strong></p><p>Simulate episodes from the current (real) state <span class="math inline">\(s_t\)</span>. Estimate action-value function <span class="math inline">\(Q(s, a)\)</span>. For each step of simulation, update action-values by Sarsa: <span class="math display">\[\triangle Q(S,A)=\alpha (R+\gamma Q(S&#39;,A&#39;)-Q(S,A))\]</span> Select actions based on action-value <span class="math inline">\(Q(s,a)\)</span>, e.g. <span class="math inline">\(\epsilon\)</span>-greedy. May also use function approximation for <span class="math inline">\(Q\)</span>.</p><p><strong>Dyna-2</strong></p><p>In Dyna-2, the agent stores two sets of feature weights:</p><ul><li><strong>Long-term</strong> memory</li><li><strong>Short-term</strong> (working) memory</li></ul><p>Long-term memory is updated from <strong>real experience</strong> using TD learning</p><ul><li>General domain knowledge that applies to any episode</li></ul><p>Short-term memory is updated from <strong>simulated experience</strong> using TD search</p><ul><li>Specific local knowledge about the current situation</li></ul><p>Over value function is sum of long and short-term memories.</p><p>End.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In last lecture, we learn &lt;strong&gt;policy&lt;/strong&gt; directly from experience. In previous lectures, we learn &lt;strong&gt;value function&lt;/strong&gt; directly from experience. In this lecture, we will learn &lt;strong&gt;model&lt;/strong&gt; directly from experience and use &lt;strong&gt;planning&lt;/strong&gt; to construct a value function or policy. Integrate learning and planning into a single architecture.&lt;/p&gt;
&lt;p&gt;Model-Based RL&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn a model from experience&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt; value function (and/or policy) from model&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="AlphaGo" scheme="http://www.52coding.com.cn/tags/AlphaGo/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="MCTS" scheme="http://www.52coding.com.cn/tags/MCTS/"/>
    
      <category term="TD Search" scheme="http://www.52coding.com.cn/tags/TD-Search/"/>
    
      <category term="Dyna" scheme="http://www.52coding.com.cn/tags/Dyna/"/>
    
  </entry>
  
  <entry>
    <title>RL - Policy Gradient</title>
    <link href="http://www.52coding.com.cn/2018/01/06/RL%20-%20Policy%20Gradient/"/>
    <id>http://www.52coding.com.cn/2018/01/06/RL - Policy Gradient/</id>
    <published>2018-01-06T05:42:09.000Z</published>
    <updated>2018-11-19T04:24:04.939Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="introduction">Introduction</span></h2><p>This lecture talks about methods that optimise policy directly. Instead of working with value function as we consider so far, we seek experience and use the experience to update our policy in the direction that makes it better.</p><p>In the last lecture, we approximated the value or action-value function using parameters <span class="math inline">\(\theta\)</span>, <span class="math display">\[V_\theta(s)\approx V^\pi(s)\\Q_\theta(s, a)\approx Q^\pi(s, a)\]</span> A policy was generated directly from the value function using <span class="math inline">\(\epsilon\)</span>-greedy.</p><p>In this lecture we will directly parametrise the policy <span class="math display">\[\pi_\theta(s, a)=\mathbb{P}[a|s, \theta]\]</span> We will focus again on <span class="math inline">\(\color{red}{\mbox{model-free}}\)</span> reinforcement learning.</p><a id="more"></a><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#finite-difference-policy-gradient">Finite Difference Policy Gradient</a></li><li><a href="#monte-carlo-policy-gradient">Monte-Carlo Policy Gradient</a></li><li><a href="#actor-critic-policy-gradient">Actor-Critic Policy Gradient</a></li><li><a href="#summary-of-policy-gradient-algorithms">Summary of Policy Gradient Algorithms</a></li></ul><!-- tocstop --><p><strong>Value-Based and Policy-Based RL</strong></p><ul><li>Value Based<ul><li>Learnt Value Function</li><li>Implicit policy (e.g. <span class="math inline">\(\epsilon\)</span>-greedy)</li></ul></li><li>Policy Based<ul><li>No Value Function</li><li>Learnt Policy</li></ul></li><li>Actor-Critic<ul><li>Learnt Value Function</li><li>Learnt Policy</li></ul></li></ul><p><img src="/images/vfp.png"></p><p><strong>Advantages of Policy-Based RL</strong></p><p>Advantages:</p><ul><li>Better convergence properties</li><li>Effective in high-dimensional or contimuous action spaces (<em>without computing max</em>)</li><li>Can learn stochastic policies</li></ul><p>Disadvantages:</p><ul><li>Typically converge to a local rather than global optimum</li><li>Evaluating a policy is typically inefficient and high variance</li></ul><p>Deterministic policy or taking max is not also the best. Take the rock-paper-scissors game for example.</p><p><img src="/images/rps.png"></p><p>Consider policies <em>iterated</em> rock-paper-scissors</p><ul><li>A deterministic policy is easily exploited</li><li>A uniform random policy is optimal (according to Nash equilibrium)</li></ul><p><strong>Aliased Gridworld Example</strong></p><p><img src="/images/agw.png"></p><p>The agent cannot differentiate the grey states.</p><p>Consider features of the following form (for all N, E, S, W) <span class="math display">\[\phi(s, a)=1(\mbox{wall to N, a = move E})\]</span> Compare value-based RL, using an approximate value function <span class="math display">\[Q_\theta(s, a)=f(\phi(s, a), \theta)\]</span> To policy-based RL, using a parametrised policy <span class="math display">\[\pi_\theta(s, a)=g(\phi(s, a), \theta)\]</span> Since the agent cannot differentiate the grey states given the feature, if you take a <strong>deterministic</strong> policy, you must pick the same action at two grey states.</p><p><img src="/images/deagw.png"></p><p>Under aliasing, an optimal <span class="math inline">\(\color{red}{\mbox{deterministic}}\)</span> policy will either</p><ul><li>move W in both grey states (as shown by red arrows)</li><li>move E in both grey states</li></ul><p>Either way, it can get stuck and never reach the money.</p><p>Value-based RL learns a near-deterministic policy, so it will traverse the corridor for a long time.</p><p><img src="/images/ranagw.png"></p><p>An optimal <span class="math inline">\(\color{red}{\mbox{stochastic}}\)</span> policy will randomly move E or W in grey states: <span class="math display">\[\pi_\theta(\mbox{wall to N and S, move E}) = 0.5\\\pi_\theta(\mbox{wall to N and S, move W}) = 0.5\\\]</span> It will reach the goal state in a few steps with high probability. Policy-based RL can learn the optimal stochastic policy.</p><p>These examples show that a stochastic policy can be better than the deterministic policy, especially in the case that the MDP is <strong>partialy observed</strong> or cannot fully represent the state.</p><p><strong>Policy Objective Functions</strong></p><p>Goal: given policy <span class="math inline">\(\pi_\theta(s, a)\)</span> with parameters <span class="math inline">\(\theta\)</span>, find best <span class="math inline">\(\theta\)</span>. But how do we measure the quality of a policy <span class="math inline">\(\pi_\theta\)</span>?</p><ul><li><p>In episodic environments we can use the <strong>start value</strong> <span class="math display">\[J_1(\theta)=V^{\pi_\theta}(s_1)=\mathbb{E}_{\pi_\theta}[v_1]\]</span></p></li><li><p>In continuing environments we can use the <strong>average value</strong> <span class="math display">\[J_{av}v(\theta)=\sum_s d^{\pi_\theta}(s)V^{\pi_\theta}(s)\]</span></p></li><li><p>Or the <strong>average reward per time-step</strong></p><p>​ <span class="math display">\[J_{av}R(\theta)=\sum_s d^{\pi_\theta}(s)\sum_a\pi_\theta(s, a)\mathcal{R}^a_s\]</span></p></li><li><p>where <span class="math inline">\(d^{\pi_\theta}(s)\)</span> is <strong>stationary distribution</strong> of Markov chain for <span class="math inline">\(\pi_\theta\)</span>.</p></li></ul><p><strong>Policy Optimisation</strong></p><p>Policy based reinforcement learning is an <strong>optimisation</strong> problem. Find <span class="math inline">\(\theta\)</span> that maximises <span class="math inline">\(J(\theta)\)</span>.</p><p>Some approaches do not use gradient</p><ul><li>Hill climbing</li><li>Simplex / amoeba / Nelder Mead</li><li>Genetic algorithms</li></ul><p>However, greater efficiency often possible using gradient</p><ul><li>Gradient descent</li><li>Conjugate gradient</li><li>Quasi-newton</li></ul><p>We focus on gradient descent, many extensions possible. And on methods that exploit sequential structure.</p><h2><span id="finite-difference-policy-gradient">Finite Difference Policy Gradient</span></h2><p><strong>Policy Gradient</strong></p><p>Let <span class="math inline">\(J(\theta)\)</span> be any policy objective function. Policy gradient algorithms search for a local maximum in <span class="math inline">\(J(\theta)\)</span> by ascending the gradient of the policy, w.r.t. parameters <span class="math inline">\(\theta\)</span> <span class="math display">\[\triangle\theta = \alpha\nabla_\theta J(\theta)\]</span> Where <span class="math inline">\(\bigtriangledown_\theta J(\theta)\)</span> is the <span class="math inline">\(\color{red}{\mbox{policy gradient}}\)</span>, <span class="math display">\[\nabla_\theta J(\theta)=\begin{pmatrix}\frac{\partial J(\theta)}{\partial \theta_1}  \\\vdots\\\frac{\partial J(\theta)}{\partial \theta_n}\end{pmatrix}\]</span> and <span class="math inline">\(\alpha\)</span> is a step-size parameter.</p><p><strong>Computing Gradients By Finite Differences (Numerical)</strong></p><p>To evaluate policy gradient of <span class="math inline">\(\pi_\theta(s, a)\)</span>.</p><ul><li>For each dimension <span class="math inline">\(k\in[1, n]\)</span>:<ul><li><p>Estimate <span class="math inline">\(k\)</span>th partial derivative of objective function w.r.t. <span class="math inline">\(\theta\)</span></p></li><li><p>By perturbing <span class="math inline">\(\theta\)</span> by small amount <span class="math inline">\(\epsilon\)</span> in <span class="math inline">\(k\)</span>th dimension <span class="math display">\[\frac{\partial J(\theta)}{\partial \theta_k}\approx \frac{J(\theta+\epsilon u_k)-J(\theta)}{\epsilon}\]</span> where <span class="math inline">\(u_k\)</span> is unit vector with 1 in <span class="math inline">\(k\)</span>th component, 0 elsewhere</p></li></ul></li><li>Uses <span class="math inline">\(n\)</span> evaluations to compute policy gradient in <span class="math inline">\(n\)</span> dimensions</li></ul><p>This is a simple, noisy, inefficient, but sometimes effective method. It works for <strong>arbitrary</strong> policies, even if policy is <strong>not</strong> differentiable.</p><p>The algorithm is efficient when the dimension of <span class="math inline">\(\theta\)</span> is low.</p><h2><span id="monte-carlo-policy-gradient">Monte-Carlo Policy Gradient</span></h2><p><strong>Score Function</strong></p><p>We now compute the policy gradient <em>analytically</em>.</p><p>Assume policy <span class="math inline">\(\pi_\theta\)</span> is differentiable whenever it is non-zero and we know the gradient <span class="math inline">\(\nabla_\theta\pi_\theta(s, a)\)</span>.</p><p><span class="math inline">\(\color{red}{\mbox{Likelihood ratios}}\)</span> exploit the following identity <span class="math display">\[\begin{align}\nabla_\theta\pi_\theta(s, a) &amp; =\pi_\theta(s, a) \frac{\nabla_\theta\pi_\theta(s, a) }{\pi_\theta(s, a) } \\&amp; = \pi_\theta(s, a) \nabla_\theta\log \pi_\theta(s, a)  \\\end{align}\]</span> The <span class="math inline">\(\color{red}{\mbox{score function}}\)</span> is $<em></em>(s, a) $. Let's take two examples to see what the score function looks like.</p><p><em>Softmax Policy</em></p><p>We will use a softmax policy as a running example. Weight actions using linear combination of features <span class="math inline">\(\phi(s, a)^T\theta\)</span>. Probability of action is proportional to exponentiated weight: <span class="math display">\[\pi_\theta(s, a)\varpropto e^{\phi(s, a)^T\theta}\]</span> The score function is <span class="math display">\[\nabla_\theta\log\pi_\theta(s, a)=\phi(s, a)-\mathbb{E}_{\pi_\theta}[\phi(s, \cdot)]\]</span> (Intuition: log gradient = the feature for the action that we actually took minus the average feature for all actions.)</p><p><em>Gaussian Policy</em></p><p>In continuous action spaces, a Gaussian policy is natural.</p><ul><li>Mean is a linear combination of state features <span class="math inline">\(\mu(s) = \phi(s)^T\theta\)</span>.</li><li>Variance may be fixed <span class="math inline">\(\sigma^2\)</span>, or can also parametrised</li></ul><p>Policy is Gaussian, <span class="math inline">\(a\sim \mathcal{N}(\mu(s), \sigma^2)\)</span>. The score function is <span class="math display">\[\nabla_\theta\log\pi_\theta(s, a)=\frac{(a-\mu(s))\phi(s)}{\sigma^2}\]</span> So far we just have a sense of what does the score function look like. Now we step into policy gradient theorem.</p><p><strong>One-Step MDPs</strong></p><p>Consider a simple class of one-step MDPs:</p><ul><li>Starting in state <span class="math inline">\(s\sim d(s)\)</span></li><li>Terminating after one time-step with reward <span class="math inline">\(r=\mathcal{R}_{s,a}\)</span></li></ul><p>Use likelihood ratios to compute the policy gradient <span class="math display">\[\begin{align}J(\theta) &amp;=\mathbb{E}_{\pi_\theta}[r]\\&amp;=\sum_{s\in\mathcal{S}}d(s)\sum_{a\in\mathcal{A}}\pi_\theta(s, a)\mathcal{R}_{s,a}\end{align}\]</span></p><p><span class="math display">\[\begin{align}\nabla_\theta J(\theta) &amp;=\sum_{s\in\mathcal{S}}d(s)\sum_{a\in\mathcal{A}}\pi_\theta(s, a)\nabla_\theta\log\pi_\theta(s, a)\mathcal{R}_{s,a}\\&amp;=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s, a)r]\end{align}\]</span></p><p>The policy gradient theorem generalises the likelihood ratio approach to multi-step MDPs.</p><ul><li>Replaces instantaneous reward <span class="math inline">\(r\)</span> with long-term value <span class="math inline">\(Q^\pi(s, a)\)</span></li></ul><p>Policy gradient theorem applies to start state objective, average reward, and average value objective.</p><blockquote><p>Theorem</p><p>For any differentiable policy <span class="math inline">\(\pi_\theta(s,a)\)</span>, for any of the policy objective functions mentioned earlier, the policy gradient is <span class="math display">\[\nabla_\theta J(\theta)=\color{red}{\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s, a)Q^{\pi_\theta}(s, a)]}\]</span></p></blockquote><p><strong>Demonstration</strong></p><blockquote><p>Settings: The initial state <span class="math inline">\(s_0\)</span> is sampled from distribution <span class="math inline">\(\rho_0\)</span>. A trajectory <span class="math inline">\(\tau = (s_0, a_0, s_1, a_1, ..., s_{t+1})\)</span> is sampled from policy <span class="math inline">\(\pi_\theta\)</span>.</p><p>The target function would be <span class="math display">\[J(\theta) = E_{\tau\sim\pi}[R(\tau)]\]</span> The probability of trajectory <span class="math inline">\(\tau\)</span> is sampled from <span class="math inline">\(\pi\)</span> is <span class="math display">\[P(\tau|\theta) = \rho_0(s_0)+\prod_{t=0}^TP(s_{t+1}|s_t, a_t)\pi_\theta(a_t|s_t)\]</span> Using the log prob trick: <span class="math display">\[\triangledown_\theta P(\tau|\theta) = P(\tau|\theta)\triangledown_\theta\log P(\tau|\theta)\]</span> Expand the trajectory: <span class="math display">\[\begin{align}\require{cancel}\triangledown_\theta \log P(\tau|\theta) &amp;= \cancel{\triangledown_\theta \log\rho_0(s_0)}+\sum_{t=0}^T\cancel{\triangledown_\theta \log P(s_{t+1}|s_t,a_t)}+ \triangledown_\theta\log\pi_\theta(a_t|s_t)\\&amp;= \sum_{t=0}^T\triangledown_\theta\log\pi_\theta(a_t|s_t)\end{align}\]</span> The gradient of target function <span class="math display">\[\begin{align}\triangledown_\theta J(\theta) &amp;= \triangledown_\theta E_{\tau\sim\pi}[R(\tau)]\\&amp;= \int_\tau \triangledown_\theta P(\tau|\theta)R(\tau)\\&amp;= \int_\tau P(\tau|\theta)\triangledown_\theta \log P(\tau|\theta)R(\tau)\\&amp;= E_{\tau\sim\pi}[\triangledown_\theta \log P(\tau|\theta)R(\tau)]\\&amp;= E_{\tau\sim\pi}[\sum_{t=0}^T\triangledown_\theta\log\pi_\theta(a_t|s_t)R(\tau)]\\&amp;= E_{\tau\sim\pi}[\sum_{t=0}^T \color{red}{\Phi_t}\triangledown_\theta\log\pi_\theta(a_t|s_t)]\end{align}\]</span></p></blockquote><p><strong>Monte-Carlo Policy Gradient (REINFORCE)</strong></p><p>Update parameters by stochastic gradient ascent using policy gradient theorem. And using return <span class="math inline">\(v_t\)</span> as an <strong>unbiased sample</strong> of <span class="math inline">\(Q^{\pi_\theta}(s_t,a_t)\)</span>: <span class="math display">\[\triangle\theta_t=\alpha\nabla_\theta\log\pi_\theta(s_t, a_t)v_t\]</span> <img src="/images/mcpseudo.png"></p><p>(Note: MCPG is slow.)</p><h2><span id="actor-critic-policy-gradient">Actor-Critic Policy Gradient</span></h2><p><strong>Reducing Variance Using a Critic</strong></p><p>Monte-Carlo policy gradient still has high variance, we use a <span class="math inline">\(\color{red}{critic}\)</span> to estimate the action-value function: <span class="math display">\[Q_w(s, a)\approx Q^{\pi_\theta}(s, a)\]</span> Actor-critic algorithms maintain two sets of parameters:</p><ul><li>Critic: Updates action-value function parameters <span class="math inline">\(w\)</span></li><li>Actor: Updates policy parameters <span class="math inline">\(\theta\)</span>, in direction suggested by critic</li></ul><p>Actor-critic algorithms follow an <em>approximate</em> policy gradient: <span class="math display">\[\nabla_\theta J(\theta)\approx \mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s, a)Q_w(s, a)]\\\triangle\theta= \alpha\nabla_\theta\log\pi_\theta(s, a)Q_w(s, a)\]</span> The critic is solving a familiar problem: policy evaluation. This problem was explored in previous lectures:</p><ul><li>Monte-Carlo policy evaluation</li><li>Temporal-Difference learning</li><li>TD(<span class="math inline">\(\lambda\)</span>)</li><li>Least Squares policy evaluation</li></ul><p>Simple actor-critic algorithm based on action-value critic using linear value function approximation. <span class="math inline">\(Q_w(s, a)=\phi(s,a)^Tw\)</span></p><ul><li>Critic: Updates <span class="math inline">\(w\)</span> by linear TD(0)</li><li>Actor: Updates <span class="math inline">\(\theta\)</span> by policy gradient</li></ul><p><img src="/images/qacpseudo.png"></p><p><strong>Bias in Actor-Critic Algorithms</strong></p><p>Approximating the policy gradient introduces bias. A biased policy gradient may not find the right solution. Luckily, if we choose value function approximation carefully, then we can avoid introducing any bias. That is we can still follow the exact policy gradient.</p><blockquote><p><strong>Compatible Function Approximation Theorem</strong></p><p>If the following two conditions are satisdied:</p><ol type="1"><li><p>Value function approximator is <strong>compatible</strong> to the policy <span class="math display">\[\nabla_w Q_w(s, a)=\nabla_\theta \log\pi_\theta(s, a)\]</span></p></li><li><p>Value function parameters <span class="math inline">\(w\)</span> minimise the mean-squared error <span class="math display">\[\epsilon=\mathbb{E}_{\pi_\theta}[(Q^{\pi_\theta}(s, a)-Q_w(s, a))^2]\]</span></p></li></ol><p>Then the policy gradient is exact, <span class="math display">\[\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)Q_w(s,a)]\]</span></p></blockquote><p><strong>Trick: Reducing Variance Using a Baseline</strong></p><p>We substract a baseline function <span class="math inline">\(B(s)\)</span> from the policy gradient. This can <strong>reduce variance, without changing expectation</strong>: <span class="math display">\[\begin{align}\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)B(s)]&amp;=\sum_{s\in\mathcal{S}}d^{\pi_\theta}(s)\sum_a\nabla_\theta\pi_\theta(s,a)B(s)\\&amp;= \sum_{s\in\mathcal{S}}d^{\pi_\theta}B(s)\nabla_\theta\sum_{a\in\mathcal{A}}\pi_\theta(s,a)\\&amp;=  \sum_{s\in\mathcal{S}}d^{\pi_\theta}B(s)\nabla_\theta 1 \\&amp;=0\end{align}\]</span> A good baseline is the state value function <span class="math inline">\(B(s)=V^{\pi_\theta}(s)\)</span>. So we can rewrite the policy gradient using the <span class="math inline">\(\color{red}{\mbox{advantage function}}\ A^{\pi_\theta}(s,a)\)</span>. <span class="math display">\[A^{\pi_\theta}(s,a)=Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s)\\\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)\color{red}{A^{\pi_\theta}(s,a)}]\]</span> where <span class="math inline">\(V^{\pi_\theta}(s)​\)</span> is the state value function of <span class="math inline">\(s​\)</span>.</p><p><strong>Intuition</strong>: The advantage function <span class="math inline">\(A^{\pi_\theta}(s,a)\)</span> tells us how much better than usual is it to take action <span class="math inline">\(a\)</span>.</p><p><strong>Estimating the Advantage Function</strong></p><p>How do we know the state value function <span class="math inline">\(V\)</span>?</p><p>One way to do that is to estimate both <span class="math inline">\(V^{\pi_\theta}(s)\)</span> and <span class="math inline">\(Q^{\pi_\theta}(s,a)\)</span>. Using two function approximators and two parameter vectors, <span class="math display">\[V_v(s)\approx V^{\pi_\theta}(s)\\Q_w(s,a)\approx Q^{\pi_\theta}(s,a)\\A(s,a)=Q_w(s,a)-V_v(s)\]</span> And updating both value functions by e.g. TD learning.</p><p>Another way is to use the TD error to compute the policy gradient. For the true value function <span class="math inline">\(V^{\pi_\theta}(s)\)</span>, the TD error <span class="math inline">\(\delta^{\pi_\theta}\)</span> <span class="math display">\[\delta^{\pi_\theta}=r+\gamma V^{\pi_\theta}(s&#39;)-V^{\pi_\theta}(s)\]</span> is an unbiased estimate of the advantage function: <span class="math display">\[\begin{align}\mathbb{E}_{\pi_\theta}[\delta^{\pi_\theta}|s, a] &amp;= \mathbb{E}_{\pi_\theta}[r+\gamma V^{\pi_\theta}(s&#39;)|s, a]-V^{\pi_\theta}(s)\\&amp;= Q^{\pi_\theta}(s, a) - V^{\pi_\theta}(s)\\&amp;= \color{red}{A^{\pi_\theta}(s,a)}\end{align}\]</span> So we can use the TD error to compute the policy gradient <span class="math display">\[\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)\color{red}{\delta^{\pi_\theta}}]\]</span> In practice we can use an approximate TD error: <span class="math display">\[\delta_v=r+\gamma V_v(s&#39;)-V_v(s)\]</span> This approach only requires one set of critic parameters <span class="math inline">\(v\)</span>.</p><p><strong>Critics and Actors at Different Time-Scales</strong></p><p>Critic can estimate value function <span class="math inline">\(V_\theta(s)\)</span> from many targets at different time-scales</p><ul><li><p>For MC, the target is return <span class="math inline">\(v_t\)</span> <span class="math display">\[\triangle \theta=\alpha(\color{red}{v_t}-V_\theta(s))\phi(s)\]</span></p></li><li><p>For TD(0), the target is the TD target <span class="math inline">\(r+\gamma V(s&#39;)\)</span> <span class="math display">\[\triangle \theta=\alpha(\color{red}{r+\gamma V(s&#39;)}-V_\theta(s))\phi(s)\]</span></p></li><li><p>For forward-view TD(<span class="math inline">\(\lambda\)</span>), the target is the return <span class="math inline">\(_vt^\lambda\)</span> <span class="math display">\[\triangle \theta=\alpha(\color{red}{v_t^\lambda}-V_\theta(s))\phi(s)\]</span></p></li><li><p>For backward-view TD(<span class="math inline">\(\lambda\)</span>), we use eligibility traces <span class="math display">\[\begin{align}\delta_t &amp;= r_{t+1}+\gamma V(s_{t+1})-V(s_t) \\e_t&amp; = \gamma\lambda e_{t-1} +\phi(s_t) \\\triangle\theta&amp;=\alpha\delta_te_t\end{align}\]</span></p></li></ul><p>The policy gradient can also be estimated at many time-scales <span class="math display">\[\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)\color{red}{A^{\pi_\theta}(s,a)}]\]</span></p><ul><li><p>MC policy gradient uses error from complete return <span class="math display">\[\triangle\theta=\alpha(\color{red}{v_t}-V_v(s_t))\nabla_\theta\log\pi_\theta(s_t,a_t)\]</span></p></li><li><p>Actor-critic policy gradient uses the one-step TD error <span class="math display">\[\triangle\theta=\alpha(\color{red}{r+\gamma V_v(s_{t+1})}-V_v(s_t))\nabla_\theta\log\pi_\theta(s_t,a_t)\]</span></p></li><li><p>Just like forward-view TD(<span class="math inline">\(\lambda\)</span>), we can mix over time-scale <span class="math display">\[\triangle \theta=\alpha(\color{red}{v_t^\lambda}-V_v(s_t))\nabla_\theta\log\pi_\theta(s_t,a_t)\]</span> where <span class="math inline">\(v_t^\lambda-V_v(s_t)\)</span> is a biased estimate of advantage function.</p></li><li><p>Like backward-view TD(<span class="math inline">\(\lambda\)</span>), we can also use eligibility traces by substituting <span class="math inline">\(\phi(s)=\nabla_\theta\log\pi_\theta(s,a)\)</span> <span class="math display">\[\begin{align}\delta_t &amp;= r_{t+1}+\gamma V_v(s_{t+1})-V_v(s_t) \\e_{t+1}&amp; = \gamma\lambda e_{t} +\nabla_\theta\log\pi_\theta(s,a) \\\triangle\theta&amp;=\alpha\delta_te_t\end{align}\]</span></p></li></ul><h2><span id="summary-of-policy-gradient-algorithms">Summary of Policy Gradient Algorithms</span></h2><p>The policy gradient has many equivalent forms</p><p><img src="/images/sumpg.png"></p><p>Each leads a stochastic gradient ascent algorithm. Critic uses policy evaluation to estimate <span class="math inline">\(Q^\pi(s, a)\)</span>, <span class="math inline">\(A^\pi(s, a)\)</span> or <span class="math inline">\(V^\pi(s)\)</span>.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This lecture talks about methods that optimise policy directly. Instead of working with value function as we consider so far, we seek experience and use the experience to update our policy in the direction that makes it better.&lt;/p&gt;
&lt;p&gt;In the last lecture, we approximated the value or action-value function using parameters &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;, &lt;span class=&quot;math display&quot;&gt;\[
V_\theta(s)\approx V^\pi(s)\\
Q_\theta(s, a)\approx Q^\pi(s, a)
\]&lt;/span&gt; A policy was generated directly from the value function using &lt;span class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;-greedy.&lt;/p&gt;
&lt;p&gt;In this lecture we will directly parametrise the policy &lt;span class=&quot;math display&quot;&gt;\[
\pi_\theta(s, a)=\mathbb{P}[a|s, \theta]
\]&lt;/span&gt; We will focus again on &lt;span class=&quot;math inline&quot;&gt;\(\color{red}{\mbox{model-free}}\)&lt;/span&gt; reinforcement learning.&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy Gradient" scheme="http://www.52coding.com.cn/tags/Policy-Gradient/"/>
    
      <category term="REINFORCE" scheme="http://www.52coding.com.cn/tags/REINFORCE/"/>
    
      <category term="Actor-Critic" scheme="http://www.52coding.com.cn/tags/Actor-Critic/"/>
    
  </entry>
  
  <entry>
    <title>RL - Value Function Approximation</title>
    <link href="http://www.52coding.com.cn/2018/01/03/RL%20-%20Value%20Function%20Approximation/"/>
    <id>http://www.52coding.com.cn/2018/01/03/RL - Value Function Approximation/</id>
    <published>2018-01-03T10:09:09.000Z</published>
    <updated>2018-11-06T03:47:58.984Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="introduction">Introduction</span></h2><p>This lecture will introduce how to scale up our algorithm to real practical RL problems by value function approximation.</p><p>Reinforcement learning can be used to solve <em>large</em> problems, e.g.</p><ul><li>Backgammon: <span class="math inline">\(10^{20}\)</span> states</li><li>Computer Go: <span class="math inline">\(10^{170}\)</span> states</li><li>Helicopter: continuous state space</li></ul><a id="more"></a><p>How can we scale up the model-free methods for prediction and control from the last two lectures?</p><p>So far we have represented value function by a <strong>lookup</strong> table:</p><ul><li>Every state <span class="math inline">\(s\)</span> has an entry <span class="math inline">\(V(s)\)</span></li><li>Or every state-action pair <span class="math inline">\(s, a\)</span> has an entry <span class="math inline">\(Q(s, a)\)</span></li></ul><p>Problems with large MDPs:</p><ul><li>There are too many states and/or actions to store in memory</li><li>It is too slow to learn the value of each state individually</li></ul><p>Solution for large MDPs: Estimate value function with <em>function approximation</em> <span class="math display">\[\hat{v}(s, \mathbb{w})\approx v_\pi(s)\\\mbox{or }\hat{q}(s, a, \mathbb{w})\approx q_\pi(s, a)\]</span> where <span class="math inline">\(\hat{v}\)</span> or <span class="math inline">\(\hat{q}\)</span> are function approximations of real <span class="math inline">\(v_\pi\)</span> or <span class="math inline">\(q_\pi\)</span>, and <span class="math inline">\(\mathbb{w}\)</span> are the parameters. This apporach has a major advantage:</p><ul><li><strong>Generalise</strong> from seen state to unseen states</li></ul><p>We can fit the <span class="math inline">\(\hat{v}\)</span> or <span class="math inline">\(\hat{q}\)</span> to <span class="math inline">\(v_\pi\)</span> or <span class="math inline">\(q_\pi\)</span> by MC or TD learning.</p><p><strong>Types of Value Function Approximation</strong></p><p><img src="/images/vftypes.png"></p><p>We consider <span class="math inline">\(\color{red}{\mbox{differentiable}}\)</span> function approximators, e.g.</p><ul><li>Linear combinations of features</li><li>Neural network</li></ul><p>Futhermore, we require a training method that is suitable for <span class="math inline">\(\color{red}{\mbox{non-stationary}}\)</span>, <span class="math inline">\(\color{red}{\mbox{non-idd}}\)</span> (idd = independent and identical distributed) data.</p><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#incremental-methods">Incremental Methods</a><ul><li><a href="#value-function-approximation">Value Function Approximation</a></li><li><a href="#action-value-function-approximation">Action-Value Function Approximation</a></li></ul></li><li><a href="#batch-methods">Batch Methods</a><ul><li><a href="#least-square-prediction">Least Square Prediction</a></li><li><a href="#least-squares-control">Least Squares Control</a></li></ul></li></ul><!-- tocstop --><h2><span id="incremental-methods">Incremental Methods</span></h2><h3><span id="value-function-approximation">Value Function Approximation</span></h3><p><strong>Gradient Descent</strong></p><p>Let <span class="math inline">\(J(\mathbb{w})\)</span> be a differentiable function of parameter vector <span class="math inline">\(\mathbb{w}\)</span>.</p><p>Define the gradient of <span class="math inline">\(J(\mathbb{w})\)</span> to be <span class="math display">\[\bigtriangledown_wJ(\mathbb{w})=\begin{pmatrix}\frac{\partial J(\mathbb{w})}{\partial \mathbb{w}_1} \\\vdots\\\frac{\partial J(\mathbb{w})}{\partial \mathbb{w}_n} \end{pmatrix}\]</span> To find a local minimum of <span class="math inline">\(J(\mathbb{w})\)</span>, adjust <span class="math inline">\(\mathbb{w}\)</span> in direction of -ve gradient <span class="math display">\[\triangle \mathbb{w}=-\frac{1}{2}\alpha \bigtriangledown_\mathbb{w}J(\mathbb{w})\]</span> where <span class="math inline">\(\alpha\)</span> is a step-size parameter.</p><p>So let's apply the <em>stochastic gradient descent</em> to <strong>value fucntion approximation</strong>.</p><p>Goal: find parameter vector <span class="math inline">\(\mathbb{w}\)</span> minimising mean-squared error between approximate value function <span class="math inline">\(\hat{v}(s, \mathbb{w})\)</span> and true value function <span class="math inline">\(v_\pi(s)\)</span>. <span class="math display">\[J(\mathbb{w})=\mathbb{E}_\pi[(v_\pi(S)-\hat{v}(S, \mathbb{w}))^2]\]</span> Gradient descent finds a local minimum <span class="math display">\[\begin{align}\triangle\mathbb{w}&amp;=-\frac{1}{2}\alpha \bigtriangledown_\mathbb{w}J(\mathbb{w})\\&amp; = \alpha\mathbb{E}_\pi[(v_\pi(S)-\hat{v}(S, \mathbb{w}))\bigtriangledown_\mathbb{w}\hat{v}(S, \mathbb{w})] \\\end{align}\]</span> Stochastic gradient descent <em>samples</em> the gradient <span class="math display">\[\triangle\mathbb{w}=\alpha(v_\pi(S)-\hat{v}(S, \mathbb{w}))\bigtriangledown_\mathbb{w}\hat{v}(S, \mathbb{w})\]</span> Expected update is equal to full gradient update.</p><p><strong>Feature Vectors</strong></p><p>Let's make this idea more concrete.</p><p>Represent state by a <em>feature vector</em>: <span class="math display">\[x(S) =\begin{pmatrix}x_1(S) \\\vdots\\x_n(S)\end{pmatrix}\]</span> For example:</p><ul><li>Distance of robot from landmarks</li><li>Trend in the stock market</li><li>Piece and pawn configurations in chess</li></ul><p><strong>Linear Value Function Approximation</strong></p><p>Represent value function by a linear combination of features <span class="math display">\[\hat{v}(S, \mathbb{w})=x(S)^T\mathbb{w}=\sum^n_{j=1}x_j(S)\mathbb{w}_j\]</span> Objective function is quadratic in parameters <span class="math inline">\(\mathbb{w}\)</span> <span class="math display">\[J(\mathbb{w})=\mathbb{E}_\pi[(v_\pi(S)-x(S)^T\mathbb{w})^2]\]</span> Stochastic gradient descent converges on global optimum.</p><p>Update rule is particularly simple <span class="math display">\[\bigtriangledown_\mathbb{w}\hat{v}(S, \mathbb{w})=x(S)\\\triangle \mathbb{w}=\alpha(v_\pi(S)-\hat{v}(S, \mathbb{w}))x(S)\]</span> Update = step-size <span class="math inline">\(\times\)</span> prediction error <span class="math inline">\(\times\)</span> feature value.</p><p>So far we have assumed true value function <span class="math inline">\(v_\pi(s)\)</span> given by supervisor. But in RL there is <strong>no supervisor, only rewards</strong>.</p><p>In practice, we substitute a <em>target</em> for <span class="math inline">\(v_\pi(s)\)</span>:</p><ul><li><p>For MC, the target is return <span class="math inline">\(G_t​\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{G_t}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w})\]</span></p></li><li><p>For TD(0), the target is the TD target <span class="math inline">\(R_{t+1}+\gamma\hat{v}(S_{t+1}, \mathbb{w})\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{R_{t+1}+\gamma\hat{v}(S_{t+1}, \mathbb{w})}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w})\]</span></p></li><li><p>For TD(<span class="math inline">\(\lambda\)</span>), the target is the return <span class="math inline">\(G_t^\lambda\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{G_t^\lambda}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w})\]</span></p></li></ul><p><strong>Monte-Carlo with Value Function Approximation</strong></p><p>Return <span class="math inline">\(G_t\)</span> is unbiased, noisy sample of true value <span class="math inline">\(v_\pi(S_t)\)</span>. We can build our &quot;training data&quot; to apply supervised learning: <span class="math display">\[&lt;S_1, G_1&gt;, &lt;S_2, G_2&gt;, ..., &lt;S_T, G_T&gt;\]</span> For example, using <em>linear Monte-Carlo policy evaluation</em> <span class="math display">\[\begin{align}\triangle \mathbb{w}&amp;=\alpha(\color{red}{G_t}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w}) \\&amp; = \alpha(G_t-\hat{v}(S_t, \mathbb{w}))x(S_t)\\\end{align}\]</span> Monte-Carlo evaluation converges to a local optimum even when using non-linear value function approximation.</p><p><strong>TD Learning with Value Function Approximation</strong></p><p>The TD-target <span class="math inline">\(R_{t+1}+\gamma \hat{v}(S_{t+1}, \mathbb{w})\)</span> is a biased sample of true value <span class="math inline">\(v_\pi(S_t)\)</span>. We can still apply supervised learning to &quot;traning data&quot;: <span class="math display">\[&lt;S_1, R_2 +\gamma\hat{v}(S_2, \mathbb{w})&gt;,&lt;S_2, R_3 +\gamma\hat{v}(S_3, \mathbb{w})&gt;,...,&lt;S_{T-1}, R_T&gt;\]</span> For example, using <em>linear TD(0)</em> <span class="math display">\[\begin{align}\triangle \mathbb{w}&amp;=\alpha(\color{red}{R+\gamma\hat{v}(S&#39;, \mathbb{w})}-\hat{v}(S, \mathbb{w}))\bigtriangledown_w \hat{v}(S, \mathbb{w}) \\&amp; = \alpha\delta x(S)\\\end{align}\]</span> Linear TD(0) converges (close) to global optimum.</p><p><strong>TD(<span class="math inline">\(\lambda\)</span>) with Value Function Approximation</strong></p><p>The <span class="math inline">\(\lambda\)</span>-return <span class="math inline">\(G_t^\lambda\)</span> is also a biased sample of true value <span class="math inline">\(v_\pi(s)\)</span>. We can also apply supervised learning to &quot;training data&quot;: <span class="math display">\[&lt;S_1, G_1^\lambda&gt;, &lt;S_2, G_2^\lambda&gt;, ..., &lt;S_{T-1}, G_{T-1}^\lambda&gt;\]</span> Can use either forward view linear TD(<span class="math inline">\(\lambda\)</span>): <span class="math display">\[\begin{align}\triangle \mathbb{w}&amp;=\alpha(\color{red}{G_t^\lambda}-\hat{v}(S_t, \mathbb{w}))\bigtriangledown_w \hat{v}(S_t, \mathbb{w}) \\&amp; = \alpha(G_t-\hat{v}(S_t, \mathbb{w}))x(S_t)\\\end{align}\]</span> or backward view linear TD(<span class="math inline">\(\lambda\)</span>): <span class="math display">\[\begin{align}\delta_t &amp;= R_{t+1}+\gamma \hat{v}(S_{t+1}, \mathbb{w})-\hat{v}(S_t, \mathbb{w}) \\E_t&amp; = \gamma\lambda E_{t-1} +x(S_t) \\\triangle\mathbb{w}&amp;=\alpha\delta_tE_t\end{align}\]</span></p><h3><span id="action-value-function-approximation">Action-Value Function Approximation</span></h3><p><img src="/images/avfa.png"></p><p>Approximate the action-value function: <span class="math display">\[\hat{q}(S, A, \mathbb{w}) \approx q_\pi(S, A)\]</span> Minimise mean-squared error between approximate action-value function <span class="math inline">\(\hat{q}(S, A, \mathbb{w})\)</span> and true action-value function <span class="math inline">\(q_\pi(S, A)\)</span>: <span class="math display">\[J(\mathbb{w})=\mathbb{E}_\pi[(q_\pi(S, A)-\hat{q}(S, A, \mathbb{w}))^2]\]</span> Use stochastic gradient descent to find a local minimum: <span class="math display">\[-\frac{1}{2}\bigtriangledown_w J(\mathbb{w})=(q_\pi(S, A)-\hat{q}(S, A, \mathbb{w}))\bigtriangledown_w\hat{q}(S, A, \mathbb{w})\\\triangle\mathbb{w}=\alpha (q_\pi(S, A)-\hat{q}(S, A, \mathbb{w}))\bigtriangledown_w\hat{q}(S, A, \mathbb{w})\]</span> Represent state and action by a feature vector: <span class="math display">\[\mathbb{x}(S, A)=\begin{pmatrix}x_1(S, A) \\\vdots\\x_n(S, A)\end{pmatrix}\]</span> Represent action-value function by linear combination of features: <span class="math display">\[\hat{q}(S, A, \mathbb{w})=\mathbb{x}(S, A)^T\mathbb{w}=\sum^n_{j=1}x_j (S, A)\mathbb{w}_j\]</span> Stochastic gradient descent update: <span class="math display">\[\bigtriangledown_w\hat{q}(S, A, \mathbb{w})=\mathbb{x}(S, A)\\\triangle \mathbb{w}=\alpha(q_\pi(S, A)-\hat{q}(S,  A, \mathbb{w}))\mathbb{x}(S, A)\]</span> Like prediction, we must subsitute a target for <span class="math inline">\(q_\pi(S, A)\)</span>:</p><ul><li><p>For MC, the target is the return <span class="math inline">\(G_t\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{G_t}-\hat{q}(S_t,  A_t, \mathbb{w}))\bigtriangledown_w\hat{q}(S_t, A_t, \mathbb{w})\]</span></p></li><li><p>For TD(0), the target is the TD target <span class="math inline">\(R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})​\)</span> <span class="math display">\[\triangle \mathbb{w}=\alpha(\color{red}{R_{t+1}+\gamma \hat{q}(S_{t+1},  A_{t+1}, \mathbb{w})}-\hat{q}(S_t,  A_t, \mathbb{w}))\bigtriangledown_w\hat{q}(S_t, A_t, \mathbb{w})\]</span></p></li><li><p>For forward-view TD(<span class="math inline">\(\lambda\)</span>), target is the action-value <span class="math inline">\(\lambda\)</span>-return <span class="math display">\[\triangle\mathbb{w}=\alpha(\color{red}{q_t^\lambda}-\hat{q}(S_t, A_t,\mathbb{w}))\bigtriangledown\hat{q}(S_t, A_t, \mathbb{w})\]</span></p></li><li><p>For backward-view TD(<span class="math inline">\(\lambda\)</span>), equivalent update is <span class="math display">\[\begin{align}\delta_t&amp; =R_{t+1}+\gamma\hat{q}(S_{t+1}, A_{t+1}, \mathbb{w})-\hat{q}(S_t, A_t, \mathbb{w}) \\E_t&amp; = \gamma\lambda E_{t-1}+\bigtriangledown_w\hat{q}(S_t, A_t, \mathbb{w}) \\\triangle\mathbb{w}&amp;= \alpha\delta_t E_t\end{align}\]</span></p></li></ul><p><strong>Linear Sarsa with Coarse Coding in Mountain Car</strong></p><p><img src="/images/linsarsa.png"></p><p>The goal is to control our car to reach the top of the mountain. We represent state by the car's position and velocity. The height of the diagram shows the value of each state. Finally, the value function is like:</p><p><img src="/images/linsarfin.png"></p><p><strong>Study of <span class="math inline">\(\lambda\)</span>: Should We Bootstrap?</strong></p><p><img src="/images/lambdastudy.png"></p><p>The answer is <strong>yes</strong>. We can see from above picture, choose some approprite <span class="math inline">\(\lambda\)</span> can certainly reduce the training steps as well as the cost.</p><p>However, temporal-difference learning in many cases doesn't guarantee to converge. It may also diverge.</p><p><strong>Convergence of Prediction Algorithms</strong></p><p><img src="/images/converge.png"></p><p>TD dose not follow the gradient of <em>any</em> objective function. This is why TD can diverge when off-policy or using non-linear function approximation. <strong>Gradient TD</strong> follows true gradient of projected Bellman error.</p><p><img src="/images/gtd.png"></p><p><strong>Convergence of Control Algorithms</strong></p><p><img src="/images/convca.png"></p><h2><span id="batch-methods">Batch Methods</span></h2><p>Gradient descent is simple and appealing. But it is not <strong>sample efficient</strong>. Batch methods seek to find the best fitting value function given the agent's experience.</p><h3><span id="least-square-prediction">Least Square Prediction</span></h3><p>Give value function approximation <span class="math inline">\(\hat{v}(s, \mathbb{w})\approx v_\pi(s)\)</span> and experience <span class="math inline">\(\mathcal{D}\)</span> consisting of <em>&lt;state, value&gt;</em> pairs: <span class="math display">\[\mathcal{D} = \{&lt;s_1, v_1^\pi&gt;, &lt;s_2, v_2^\pi&gt;, ..., &lt;s_T, v_T^\pi&gt; \}\]</span> Which parameters <span class="math inline">\(\mathbb{w}\)</span> give the best fitting value function <span class="math inline">\(\hat{v}(s, \mathbb{w})\)</span> ?</p><p><span class="math inline">\(\color{red}{\mbox{Least squares}}\)</span> algorithms find parameter vector <span class="math inline">\(\mathbb{w}\)</span> minimising sum-squared error between <span class="math inline">\(\hat{v}(s_t, \mathbb{w})\)</span> and target values <span class="math inline">\(v_t^\pi\)</span>, <span class="math display">\[\begin{align}LS(\mathbb{w}) &amp; = \sum^T_{t=1}(v_t^\pi-\hat{v}(s_t, \mathbb{w}))^2 \\&amp; = \mathbb{E}_\mathcal{D}[(v^\pi-\hat{v}(s, \mathbb{w}))^2] \\\end{align}\]</span> Given experience consisting of <em>&lt;state, value&gt;</em> pairs <span class="math display">\[\mathcal{D}=\{&lt;s_1, v_1^\pi&gt;, &lt;s_2, v_2^\pi&gt;, ..., &lt;s_T, v_T^\pi&gt;\}\]</span> Repeat:</p><ol type="1"><li><p>Sample state, value from experience <span class="math display">\[&lt;s, v^\pi&gt; \sim \mathcal{D}\]</span></p></li><li><p>Apply stochastic gradient descent update <span class="math display">\[\triangle \mathbb{w}=\alpha(v^\pi-\hat{v}(s, \mathbb{w}))\bigtriangledown_w\hat{v}(s, w)\]</span></p></li></ol><p>Converges to least squares solution <span class="math display">\[\mathbb{w}^\pi=\arg\min_w LS(w)\]</span> <strong>Deep Q-Networks (DQN)</strong></p><p>DQN uses <span class="math inline">\(\color{red}{\mbox{experience replay}}\)</span> and <span class="math inline">\(\color{red}{\mbox{fixed Q-targets}}\)</span>:</p><ul><li><p>Take action <span class="math inline">\(a_t\)</span> according to <span class="math inline">\(\epsilon\)</span>-greedy policy</p></li><li><p>Store transition <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span> in replay memory <span class="math inline">\(\mathcal{D}\)</span></p></li><li><p>Sample random mini-batch of transitions <span class="math inline">\((s, a, r, s&#39;)\)</span> from <span class="math inline">\(\mathcal{D}\)</span></p></li><li><p>Compute Q-learning targets w.r.t. old, fixed parameters <span class="math inline">\(w^-\)</span></p></li><li><p>Optimise MSE between Q-network and Q-learning target <span class="math display">\[\mathcal{L}(w_i)=\mathbb{E}_{s,a,r,s&#39;\sim\mathcal{D}_i}[(r+\gamma\max_{a&#39;}Q(s&#39;,a&#39;;w^-_i)-Q(s, a;w_i))^2]\]</span></p></li><li><p>Using variant of stochastic gradient descent</p></li></ul><p>Note: <span class="math inline">\(\color{red}{\mbox{fixed Q-targets}}\)</span> means we use two Q-networks. One of it using fixed old parameters to generate the Q-target to update the fresh Q-network, which can keep the update <strong>stable</strong>. Otherwise, when you update the Q-network, you also update the Q-target, which can cause diverge.</p><p>DQN in Atari</p><ul><li>End-to-end learning of values <span class="math inline">\(Q(s, a)\)</span> from pixels <span class="math inline">\(s\)</span></li><li>Input state <span class="math inline">\(s\)</span> is stack of raw pixels from last 4 frames</li><li>Output is <span class="math inline">\(Q(s, a)\)</span> for 18 joystick/button positions</li><li>Rewards is change in score for that step</li></ul><p><img src="/images/ataridqn.png"></p><p>Results</p><p><img src="/images/dqnres.png"></p><p>How much does DQN help?</p><p><img src="/images/dqnhelp.png"></p><p><strong>Linear Least Squares Prediction - Normal Equation</strong></p><p>Experience replay finds least squares solution but it may take many iterations. Using <em>linear value function approximation</em> <span class="math inline">\(\hat{v}(s, w) = x(s)^Tw\)</span>, we can solve squares soluton directly.</p><p>At minimum of <span class="math inline">\(LS(w)\)</span>, the expected update must be zero:</p><p><img src="/images/llsp.png"></p><p>For <span class="math inline">\(N\)</span> features, direct solution time is <span class="math inline">\(O(N^3)\)</span>. Incremental solution time is <span class="math inline">\(O(N^2)\)</span> using Shermann-Morrison.</p><p>We do not know true values <span class="math inline">\(v_t^\pi\)</span>. In practice, our &quot;training data&quot; must be noisy or biased samples of <span class="math inline">\(v_t^\pi\)</span>:</p><p><img src="/images/llspa.png"></p><p>In each case solve directly for fixed point of MC / TD / TD(<span class="math inline">\(\lambda\)</span>).</p><p><strong>Convergence of Linear Least Squares Prediction Algorithms</strong></p><p><img src="/images/cllspa.png"></p><h3><span id="least-squares-control">Least Squares Control</span></h3><p><strong>Least Squares Policy Iteration</strong></p><p><img src="/images/lspi.png"></p><p><strong>Least Squares Action-Value Function Approximation</strong></p><p>Approximate action-value function <span class="math inline">\(q_\pi(s, a)\)</span> using linear combination of features <span class="math inline">\(\mathbb{x}(s, a)\)</span>: <span class="math display">\[\hat{q}(s, a, \mathbb{w})=\mathbb{x}(s, a)^T\mathbb{w}\approx q_\pi(s, a)\]</span> Minimise least squares error between <span class="math inline">\(\hat{q}(s, a, \mathbb{w})\)</span> and <span class="math inline">\(q_\pi(s, a)\)</span> from experience generated using policy <span class="math inline">\(\pi\)</span> consisting of <em>&lt;(state, action), value&gt;</em> pairs: <span class="math display">\[\mathcal{D}=\{&lt;(s_1,a_1),v_1^\pi&gt;,&lt;(s_2,a_2),v_2^\pi&gt;,...,&lt;(s_T,a_T),v_T^\pi&gt;\}\]</span> <strong>Least Squares Control</strong></p><p>For policy evaluation, we want to efficiently use all experience. For control, we also want to improve the policy. This experience is generated from many policies. So to evaluate <span class="math inline">\(q_\pi(S, A)\)</span> we must learn <span class="math inline">\(\color{red}{\mbox{off-policy}}\)</span>.</p><p>We use the same idea as Q-learning:</p><ul><li>Use experience generated by old policy <span class="math inline">\(S_t, A_t, R_{t+1}, S_{t+1} \sim \pi_{old}\)</span></li><li>Consider alternative successor action <span class="math inline">\(A&#39;=\pi_{new}(S_{t+1})\)</span></li><li>Update <span class="math inline">\(\hat{q}(S_t, A_t,\mathbb{w})\)</span> towards value of alternative action <span class="math inline">\(R_{t+1}+\gamma \hat{q}(S_{t+1}, A&#39;, \mathbb{w})\)</span></li></ul><p>Consider the following linear Q-learning update <span class="math display">\[\delta=R_{t+1}+\gamma \hat{q}(S_{t+1}, \color{red}{\pi(S_{t+1})}, \mathbb{w})-\hat{q}(S_t, A_t, \mathbb{w})\\\triangle \mathbb{w}=\alpha\delta\mathbb{x}(S_t, A_t)\]</span> LSTDQ algorithm: solve for total update = zero:</p><p><img src="/images/lstdq.png"></p><p>The following pseudocode uses LSTDQ for policy evaluation. It repeatedly re-evaluates experience <span class="math inline">\(\mathcal{D}\)</span> with different policies.</p><p><img src="/images/lspipseudo.png"></p><p><strong>Convergence of Control Algorithms</strong></p><p><img src="/images/ccal.png"></p><p>End.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This lecture will introduce how to scale up our algorithm to real practical RL problems by value function approximation.&lt;/p&gt;
&lt;p&gt;Reinforcement learning can be used to solve &lt;em&gt;large&lt;/em&gt; problems, e.g.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Backgammon: &lt;span class=&quot;math inline&quot;&gt;\(10^{20}\)&lt;/span&gt; states&lt;/li&gt;
&lt;li&gt;Computer Go: &lt;span class=&quot;math inline&quot;&gt;\(10^{170}\)&lt;/span&gt; states&lt;/li&gt;
&lt;li&gt;Helicopter: continuous state space&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://www.52coding.com.cn/tags/Deep-Learning/"/>
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="DQN" scheme="http://www.52coding.com.cn/tags/DQN/"/>
    
      <category term="Neural Network" scheme="http://www.52coding.com.cn/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>RL - Model-Free Control</title>
    <link href="http://www.52coding.com.cn/2017/12/21/RL%20-%20Model-Free%20Control/"/>
    <id>http://www.52coding.com.cn/2017/12/21/RL - Model-Free Control/</id>
    <published>2017-12-21T12:00:09.000Z</published>
    <updated>2018-11-06T03:47:38.964Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="introduction">Introduction</span></h2><p>Last lecture:</p><ul><li>Model-free prediction</li><li><em>Estimate</em> the value function of an <em>unknown</em> MDP</li></ul><p>This lecture:</p><ul><li>Model-free control</li><li><strong>Optimise</strong> the value function of an unknown MDP</li></ul><a id="more"></a><p>Why we care about model-free control? So, let's see some example problems that can be modelled as MDPs:</p><ul><li>Helicopter, Robocup Soccer, Quake</li><li>Portfolio management, Game of Go...</li></ul><p>For most of these problems, either:</p><ul><li>MDP model is <strong>unknown</strong>, but experience can be sampled</li><li>MDP model is known, but is <strong>too big to use</strong>, except by samples</li></ul><p><span class="math inline">\(\color{red}{\mbox{Model-free control}}\)</span> can sovlve these problems.</p><p>There are two branches of model-free control:</p><ul><li><span class="math inline">\(\color{red}{\mbox{On-policy}}\)</span> learning<ul><li>&quot;Learn on the job&quot;</li><li>Learn about policy <span class="math inline">\(\pi\)</span> from experience sampled from <span class="math inline">\(\pi\)</span></li></ul></li><li><span class="math inline">\(\color{red}{\mbox{Off-policy}}\)</span> learning<ul><li>&quot;Look over someone's shoulder&quot;</li><li>Learn about policy <span class="math inline">\(\pi\)</span> from experience sampled from <span class="math inline">\(\mu\)</span></li></ul></li></ul><p><strong>Table of Contents</strong></p><!-- toc --><ul><li><a href="#on-policy-monte-carlo-control">On-Policy Monte-Carlo Control</a></li><li><a href="#on-policy-temporal-difference-learning">On-Policy Temporal-Difference Learning</a><ul><li><a href="#sarsalambda">Sarsa(<span class="math inline">\(\lambda\)</span>)</a></li></ul></li><li><a href="#off-policy-learning">Off-Policy Learning</a><ul><li><a href="#q-learning">Q-Learning</a></li></ul></li><li><a href="#summary">Summary</a></li></ul><!-- tocstop --><h2><span id="on-policy-monte-carlo-control">On-Policy Monte-Carlo Control</span></h2><p>In previous lectures, we have seen that using policy iteration to find the best policy. Today, we also use this central idea plugging in MC or TD algorithm.</p><p><img src="/images/pi.png"></p><p><strong>Generalised Policy Iteration With Monte-Carlo Evaluation</strong></p><p>A simple idea is</p><ul><li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(V = v_\pi\)</span>?</li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: Greedy policy improvement?</li></ul><p>Well, this idea has two major problems:</p><ul><li><p>Greedy policy improvement over <span class="math inline">\(V(s)\)</span> requires <strong>model of MDP</strong> <span class="math display">\[\pi^\prime(s) = \arg\max_{a\in\mathcal{A}}\mathcal{R}^a_s+\mathcal{P}^a_{ss&#39;}V(s&#39;)\]</span> since, we do not know the state transition probability matrix <span class="math inline">\(\mathcal{P}\)</span>.</p></li><li><p>Exploration issue: cannot guarantee to explore all states</p></li></ul><p>So, the alternative is to use action-value function <span class="math inline">\(Q\)</span>:</p><ul><li>Greedy policy improvement over <span class="math inline">\(Q(s, a)\)</span> is model-free <span class="math display">\[\pi^\prime=\arg\max_{a\in\mathcal{A}}Q(s,a)\]</span></li></ul><p>Let's replace it in the algorithm:</p><p><img src="/images/avf.png"></p><ul><li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(\color{red}{Q = q_\pi}\)</span></li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: Greedy policy improvement?</li></ul><p>We still have one problems about the algorithm, which is exploration issue. Here is a example of greedy action selection:</p><p><img src="/images/gaseg.png"></p><p>The reward of the two doors are stochastic. However, because of the greedy action selection, we always choose the right door without exploring the value of the left one.</p><p>One simple algorithm to ensure keeping exploration is <strong><span class="math inline">\(\epsilon\)</span>-greedy exploration</strong>.</p><p><strong><span class="math inline">\(\epsilon\)</span>-Greedy Exploration</strong></p><p>All <span class="math inline">\(m\)</span> actions are tried with non-zero probalility,</p><ul><li>With probability <span class="math inline">\(1-\epsilon\)</span> choose the greedy action</li><li>With probability <span class="math inline">\(\epsilon\)</span> choose an action at <strong>random</strong></li></ul><p><span class="math display">\[\pi(a|s)=\begin{cases} \epsilon/m+1-\epsilon,  &amp; \mbox{if } a^* = \arg\max_{a\in\mathcal{A}}Q(s,a) \\\epsilon/m, &amp; \mbox{otherwise }\end{cases}\]</span></p><blockquote><p>Theorem</p><p>For any <span class="math inline">\(\epsilon\)</span>-greedy policy <span class="math inline">\(\pi\)</span>, the <span class="math inline">\(\epsilon\)</span>-greedy policy <span class="math inline">\(\pi^\prime\)</span> with respect to <span class="math inline">\(q_\pi\)</span> is an improvement, <span class="math inline">\(v_{\pi^\prime}≥v_\pi(s)\)</span>.</p></blockquote><p><img src="/images/egpi.png"></p><p>Therefore from policy improvement theorem, <span class="math inline">\(v_{\pi^\prime}(s) ≥ v_\pi(s)\)</span>.</p><p><strong>Monte-Carlo Policy Iteration</strong></p><p><img src="/images/mcpi.png"></p><ul><li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(Q = q_\pi\)</span></li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: <span class="math inline">\(\color{red}{\epsilon}\)</span>-greedy policy improvement</li></ul><p><strong>Monte-Carlo Control</strong></p><p><img src="/images/mcc.png"></p><p><span class="math inline">\(\color{red}{\mbox{Every episode}}\)</span>:</p><ul><li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(\color{red}{Q \approx q_\pi}\)</span></li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: <span class="math inline">\(\epsilon\)</span>-greedy policy improvement</li></ul><p>The method is once evaluate over an episode, immediately improve the policy. The idea is since we already have a better evaluation, why waiting to update the policy after numerous episodes. That is improving the policy right after evaluating one episode.</p><p><strong>GLIE</strong></p><blockquote><p>Definition</p><p><strong>Greedy in the Limit with Infinite Exploration</strong> (GLIE)</p><ul><li><p>All state-action pairs are explored infinitely many times, <span class="math display">\[\lim_{k\rightarrow\infty}N_k(s,a)=\infty\]</span></p></li><li><p>The policy converges on a greedy policy, <span class="math display">\[\lim_{k\rightarrow\infty}\pi_k(a|s)=1(a=\arg\max_{a^\prime \in\mathcal{A}}Q_k(s, a^\prime))\]</span></p></li></ul></blockquote><p>For example, <span class="math inline">\(\epsilon\)</span>-greedy is GLIE if <span class="math inline">\(\epsilon\)</span> reduces to zero at <span class="math inline">\(\epsilon_k=\frac{1}{k}\)</span>.</p><p><strong>GLIE Monte-Carlo Control</strong></p><p>Sample <span class="math inline">\(k\)</span>th episode using <span class="math inline">\(\pi\)</span>: <span class="math inline">\(\{S_1, A_1, R_2, …, S_T\} \sim \pi\)</span></p><ul><li><p><span class="math inline">\(\color{red}{\mbox{Evaluation}}\)</span></p><ul><li>For each state <span class="math inline">\(S_t\)</span> and action <span class="math inline">\(A_t\)</span> in the episode, <span class="math display">\[\begin{array}{lcl}N(S_t, A_t) \leftarrow N(S_t, A_t)+1 \\Q(S_t, A_t) \leftarrow Q(S_t, A_t)+\frac{1}{N(S_t, A_t)}(G_t-Q(S_t, A_t))\end{array}\]</span></li></ul></li><li><p><span class="math inline">\(\color{red}{\mbox{Improvement}}\)</span></p><ul><li>Improve policy based on new action-value function <span class="math display">\[\begin{array}{lcl}\epsilon\leftarrow \frac{1}{k} \\\pi \leftarrow \epsilon\mbox{-greedy}(Q)\end{array}\]</span></li></ul></li></ul><p>GLIE Monte-Carlo control converges to the optimal action-value function, <span class="math inline">\(Q(s,a) \rightarrow q_*(s,a)\)</span>.</p><p><strong>Blackjack Example</strong></p><p><img src="/images/mccb.png"></p><p>Using Monte-Carlo control, we can get the optimal policy above.</p><h2><span id="on-policy-temporal-difference-learning">On-Policy Temporal-Difference Learning</span></h2><p>Temporal-difference (TD) learning has several advantages over Monte-Carlo (MC):</p><ul><li>low variance</li><li>Online</li><li>Incomplete sequences</li></ul><p>A natural idea is using TD instead of MC in our control loop:</p><ul><li>Apply TD to <span class="math inline">\(Q(S, A)\)</span></li><li>Use <span class="math inline">\(\epsilon\)</span>-greedy policy improvement</li><li>Update every <em>time-step</em></li></ul><p><strong>Sarsa Update</strong></p><p><img src="/images/sarsa.png"></p><p>Updating action-value functions with Sarsa: <span class="math display">\[Q(S,A) \leftarrow Q(S, A) + \alpha(R+\gamma Q(S^\prime, A^\prime)-Q(S, A))\]</span> <img src="/images/mcc.png"></p><p>So, the full algorithm is:</p><ul><li>Every <span class="math inline">\(\color{red}{\mbox{time-step}}\)</span>:<ul><li><span class="math inline">\(\color{blue}{\mbox{Policy evaluation}}\)</span> <span class="math inline">\(\color{red}{\mbox{Sarsa}}\)</span>, <span class="math inline">\(Q\approx q_\pi\)</span></li><li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span> <span class="math inline">\(\epsilon\)</span>-greedy policy improvement</li></ul></li></ul><p><img src="/images/pesedotd.png"></p><p><strong>Windy Gridworld Example</strong></p><p><img src="/images/wweg.png"></p><p>The 'S' represents start location and 'G' marks the goal. There is a number at the bottom of each column which represents the wind will blow the agent up how many grids if the agent stays at that column.</p><p>The result of apply Sarsa to the problem is</p><p><img src="/images/wwegres.png"></p><h3><span id="sarsalambda">Sarsa(<span class="math inline">\(\lambda\)</span>)</span></h3><p><strong>n-Step Sarsa</strong></p><p>Consider the following <span class="math inline">\(n\)</span>-step returns for <span class="math inline">\(n=1,2,..\infty\)</span>:</p><p><img src="/images/nsarsa.png"></p><p>Define the <span class="math inline">\(n\)</span>-step <span class="math inline">\(Q\)</span>-return <span class="math display">\[q_t^{(n)}=R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^n Q(S_{t+n})\]</span> <span class="math inline">\(n\)</span>-step Sarsa updates <span class="math inline">\(Q(s, a)\)</span> towards the <span class="math inline">\(n\)</span>-step <span class="math inline">\(Q\)</span>-return <span class="math display">\[Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha(q_t^{(n)}-Q(S_t,A_t))\]</span> <strong>Forward View Sarsa(<span class="math inline">\(\lambda\)</span>)</strong></p><p><img src="/images/sarsalam.png"></p><p>The <span class="math inline">\(q^\lambda\)</span> return combines all <span class="math inline">\(n\)</span>-step Q-returns <span class="math inline">\(q_t^{(n)}\)</span> using weight <span class="math inline">\((1-\lambda)\lambda^{n-1}\)</span>: <span class="math display">\[q_t^\lambda = (1-\lambda)\sum^\infty_{n=1}\lambda^{n-1}q_t^{(n)}\]</span> Forward-view Sarsa(<span class="math inline">\(\lambda\)</span>): <span class="math display">\[Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha(q_t^\lambda-Q(S_t, A_t))\]</span> <strong>Backward View Sarsa(<span class="math inline">\(\lambda\)</span>)</strong></p><p>Just like TD(<span class="math inline">\(\lambda\)</span>), we use <span class="math inline">\(\color{red}{\mbox{eligibility traces}}\)</span> in an online algorithm, but Sarsa(<span class="math inline">\(\lambda\)</span>) has one eligibility trace for each state-action pair: <span class="math display">\[E_0(s, a) = 0\]</span></p><p><span class="math display">\[E_t(s, a) = \gamma\lambda E_{t-1}(s,a)+1(S_t=s, A_t=a)\]</span></p><p><span class="math inline">\(Q(s, a)\)</span> is updated for every state <span class="math inline">\(s\)</span> and action <span class="math inline">\(a\)</span> in proportion to TD-error <span class="math inline">\(\delta_t\)</span> and eligibility trace <span class="math inline">\(E_t(s, a)\)</span>: <span class="math display">\[\delta_t=R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})-Q(S_t, A_t)\]</span></p><p><span class="math display">\[Q(s, a) \leftarrow Q(s, a) +\alpha \delta_t E_t(s, a)\]</span></p><p><img src="/images/sarcode.png"></p><p>The difference between Sarsa and Sarsa(<span class="math inline">\(\lambda\)</span>):</p><p><img src="/images/sarsadiff.png"></p><p>If we initial all <span class="math inline">\(Q(s, a) = 0\)</span>, then we first do a random walk and reach the goal. Using Sarsa, we can only update the Q-value of the previous state before reaching the goal since all other <span class="math inline">\(Q\)</span> are zero. So the reward can only propagate one state. On the contrary, if we using Sarsa(<span class="math inline">\(\lambda\)</span>), the reward can propagate from the last state to the first state with a exponential decay.</p><h2><span id="off-policy-learning">Off-Policy Learning</span></h2><p>Evaluate target policy <span class="math inline">\(\pi(a|s)\)</span> to compute <span class="math inline">\(v_\pi(s)\)</span> or <span class="math inline">\(q_\pi(s, a)\)</span> while following behaviour policy <span class="math inline">\(\mu(a|s)\)</span> <span class="math display">\[\{S_1, A_1, R_2, ..., S_T\}\sim \mu\]</span> So, why is this important? There are several reasons:</p><ul><li>Learn from observing hunman or other agents</li><li>Re-use experience generated from old policies <span class="math inline">\(\pi_1, \pi_2, …, \pi_{t-1}\)</span></li><li>Learn about <strong>optimal</strong> policy while following <span class="math inline">\(\color{red}{\mbox{exploratory policy}}\)</span></li><li>Learn about <strong>multiple</strong> policies while following one policy</li></ul><p><strong>Importance Sampling</strong></p><p>Estimate the expectation of a different distribution <span class="math display">\[\mathbb{E}_{X\sim P}[f(X)] = \sum P(X)f(X)=\sum Q(X)\frac{P(X)}{Q(X)}f(X)=\mathbb{E}_{X\sim Q}[\frac{P(X)}{Q(X)}f(X)]\]</span> <strong>Off-Policy Monte-Carlo</strong></p><p>Use returns generated from <span class="math inline">\(\mu\)</span> to evaluate <span class="math inline">\(\pi\)</span>. Weight return <span class="math inline">\(G_t\)</span> according to <strong>similarity</strong> between policies. Multiply importance sampling corrections along whole episode: <span class="math display">\[G_t^{\pi/\mu}=\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}\frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})}...\frac{\pi(A_T|S_T)}{\mu(A_T|S_T)}G_t\]</span> Update value towards <em>corrected</em> return: <span class="math display">\[V(S_t)\leftarrow V(S_t)+\alpha (\color{red}{G_t^{\pi/\mu}}-V(S_t))\]</span> But it has two major problems:</p><ul><li>Cannot use if <span class="math inline">\(\mu\)</span> is zero when <span class="math inline">\(\pi\)</span> is non-zero</li><li>Importance sampling can dramatically increase variance, so it is useless in practice</li></ul><p><strong>Off-Policy TD</strong></p><p>Use TD targets generated from <span class="math inline">\(\mu\)</span> to evaluate <span class="math inline">\(\pi\)</span>. Weight TD target <span class="math inline">\(R+\gamma V(S&#39;)\)</span> by importance sampling. Only need a single importance sampling correction: <span class="math display">\[V(S_t)\leftarrow V(S_t)+\alpha \left(\color{red}{\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1}+\gamma V(S_{t+1}))}-V(S_t)\right)\]</span> This algorithm has much lower variance than Monte-Carlo importance sampling because policies only need to be similar over a single step.</p><h3><span id="q-learning">Q-Learning</span></h3><p>We now consider off-policy learning of action-values <span class="math inline">\(Q(s, a)\)</span>. The benefit of it is no importance sampling is required.</p><p>The next action is chosen using <strong>behaviour</strong> policy <span class="math inline">\(A_{t+1}\sim\mu(\cdot|S_t)\)</span>. But we consider <strong>alternative</strong> successor action <span class="math inline">\(A&#39;\sim \pi(\cdot|S_t)\)</span>. And update <span class="math inline">\(Q(S_t, A_t)\)</span> towards value of alternative action <span class="math display">\[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1}+\gamma Q(S_{t+1}, \color{red}{A&#39;})-Q(S_t, A_t))\]</span> We now allow both behaviour and target policies to <strong>improve</strong>.</p><p>The <strong>target</strong> policy <span class="math inline">\(\pi\)</span> is <span class="math inline">\(\color{red}{\mbox{greedy}}\)</span> w.r.t <span class="math inline">\(Q(s, a)\)</span>: <span class="math display">\[\pi(S_{t+1})=\arg\max_{a&#39;}Q(S_{t+1}, a&#39;)\]</span> The <strong>behaviour</strong> policy <span class="math inline">\(\mu\)</span> is e.g. <span class="math inline">\(\color{red}{\epsilon \mbox{-greedy}}\)</span> w.r.t. <span class="math inline">\(Q(s,a)\)</span>.</p><p>The <strong>Q-learning</strong> target then simplifies: <span class="math display">\[\begin{align}\mbox{Q-learning Target} &amp;= R_{t+1}+\gamma Q(S_{t+1}, A&#39;) \\&amp; = R_{t+1}+\gamma Q(S_{t+1}, \arg\max_{a&#39;}Q(S_{t+1}, a&#39;)) \\&amp;= R_{t+1}+\max_{a&#39;}\gamma Q(S_{t+1}, a&#39;)\end{align}\]</span> So the Q-learning control algorithm is</p><p><img src="/images/qlalg.png"></p><p>Of course, the Q-learning control still converges to the optimal action-value function, <span class="math inline">\(Q(s, a)\rightarrow q_*(s,a)\)</span>.</p><p><img src="/images/qlcode.png"></p><h2><span id="summary">Summary</span></h2><p><strong>Relationship Between DP and TD</strong></p><p><img src="/images/rbtddp.png"></p><p><img src="/images/rbtddp2.png"></p><p>In a word, TD backup can be seen as the sample of corresponding DP backup. This lecture introduces model-free control which is optimise the value function of an unknown MDP with on-policy and off-policy methods. Next lecture will introduce function approximation which is easy to scale up and can be applied into big MDPs.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Last lecture:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model-free prediction&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Estimate&lt;/em&gt; the value function of an &lt;em&gt;unknown&lt;/em&gt; MDP&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This lecture:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model-free control&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimise&lt;/strong&gt; the value function of an unknown MDP&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://www.52coding.com.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Reinforcement Learning" scheme="http://www.52coding.com.cn/tags/Reinforcement-Learning/"/>
    
      <category term="增强学习" scheme="http://www.52coding.com.cn/tags/%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Q-learning" scheme="http://www.52coding.com.cn/tags/Q-learning/"/>
    
      <category term="Monte-Carlo Control" scheme="http://www.52coding.com.cn/tags/Monte-Carlo-Control/"/>
    
      <category term="Sarsa" scheme="http://www.52coding.com.cn/tags/Sarsa/"/>
    
  </entry>
  
</feed>
