<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  

<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

  <meta charset="utf-8">
  
  <!-- if (config.subtitle) {
    title.push(config.subtitle);
  } -->
  <title>
    RL - Planning by Dynamic Programming | NIUHE
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE的博客">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="RL - Planning by Dynamic Programming">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script>
  <script type="text/javascript" src="/js/search.js"></script>
  <script type="text/javascript" src="/js/jquery.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">日々私たちが过ごしている日常というのは、実は奇迹の连続なのかもしれんな</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">博客</a></li>
      
        <li><a href="/notes">笔记</a></li>
      
        <li><a href="/archives">归档</a></li>
      
        <li><a href="/tags">标签</a></li>
      
        <li><a href="/search">搜索</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      RL - Planning by Dynamic Programming</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/12/07/RL - Planning by Dynamic Programming/" rel="bookmark">
        <time class="entry-date published" datetime="2017-12-07T07:48:19.000Z">
          2017-12-07
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <p><strong>Table of Contents</strong></p>
<!-- toc -->
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#policy-evaluation">Policy Evaluation</a></li>
<li><a href="#policy-iteration">Policy Iteration</a></li>
<li><a href="#value-iteration">Value Iteration</a></li>
<li><a href="#extentions-to-dynamic-programming">Extentions to Dynamic Programming</a></li>
<li><a href="#contraction-mapping">Contraction Mapping</a></li>
</ul>
<!-- tocstop -->
<a id="more"></a>
<h2><span id="introduction">Introduction</span></h2>
<p><strong>What is Dynamic Programming?</strong></p>
<p><strong>Dynamic</strong>: sequential or temporal component to the problem <strong>Programming</strong>: optimising a &quot;program&quot;, i.e. a policy</p>
<ul>
<li>c.f. linear programming</li>
</ul>
<p>So, Dynamic Programming is a method for solving complex problems by breaking them down into <strong>subproblems</strong>.</p>
<ul>
<li>Solve the subproblems</li>
<li>Combine solutions to subproblems</li>
</ul>
<p>Dynamic Programming is a very general solution method for problems which have two properties:</p>
<ul>
<li><strong>Optimal substructure</strong>
<ul>
<li><em>Principle of optimality applies</em></li>
<li>Optimal solution can be decomposed into subproblems</li>
</ul></li>
<li><strong>Overlapping subproblems</strong>
<ul>
<li>Subproblems recur many times</li>
<li>Solution can be cached and reused</li>
</ul></li>
</ul>
<p><strong>Markov decision processes</strong> satisfy both properties:</p>
<ul>
<li><strong>Bellman euqtion</strong> gives recursive decomposition</li>
<li><strong>Value function</strong> stores and reuses solutions</li>
</ul>
<p><strong>Planning by Dynamic Programming</strong></p>
<p><strong>Planning</strong> means dynamic programming assumes full knowledge of the MDP.</p>
<ul>
<li>For prediction (Policy Evaluation):
<ul>
<li>Input: MDP<span class="math inline">\(&lt;S,A,P,R,\gamma&gt;\)</span> and policy <span class="math inline">\(\pi\)</span></li>
<li>Output: value function <span class="math inline">\(v_{\pi}\)</span></li>
</ul></li>
<li>For <strong>control</strong>:
<ul>
<li>Input: MDP<span class="math inline">\(&lt;S,A,P,R,\gamma&gt;\)</span></li>
<li>Output: optimal value function <span class="math inline">\(v_*\)</span> and optimal policy <span class="math inline">\(\pi_*\)</span></li>
</ul></li>
</ul>
<p>We first learn how to evaluate a policy and then put it into a loop to find the optimal policy.</p>
<h2><span id="policy-evaluation">Policy Evaluation</span></h2>
<ul>
<li><p>Problem: evaluate a given policy <span class="math inline">\(\pi\)</span></p></li>
<li><p>Solution: iterative application of <strong>Bellman expectation equation</strong></p></li>
<li><p><span class="math inline">\(v_1\)</span> -&gt; <span class="math inline">\(v_2\)</span> -&gt; <span class="math inline">\(v_3\)</span> -&gt; … -&gt; <span class="math inline">\(v_\pi\)</span></p></li>
<li><p><em>Synchronous</em> backups</p>
<ul>
<li><p>At each iteration <span class="math inline">\(k+1\)</span></p></li>
<li><p>For all states <span class="math inline">\(s \in S\)</span></p></li>
<li><p>Update <span class="math inline">\(v_{k+1}(s)\)</span> from <span class="math inline">\(v_k(s&#39;)\)</span>, where <span class="math inline">\(s&#39;\)</span> is a successor state of <span class="math inline">\(s\)</span></p>
<p><img src="/images/vpi2.png"> <span class="math display">\[
v_{k+1}(s)=\sum_{a\in\mathcal{A}}\pi(a|s)(\mathcal{R}^a_s+\gamma\sum_{s&#39;\in\mathcal{S}}P^a_{ss&#39;}v_k(s&#39;))
\]</span></p>
<p><span class="math display">\[
v^{k+1}=\mathcal{R}^\pi+\gamma \mathcal{P}^\pi v^k
\]</span></p></li>
</ul></li>
</ul>
<p><em>Example</em>: Evaluating a Random Policy in the Small Gridworld</p>
<p><img src="/images/grid.png"></p>
<ul>
<li><p>Actions are move North/East/South/West for one grid.</p></li>
<li><p>Undiscounted episodic MDP (<span class="math inline">\(\gamma = 1\)</span>)</p></li>
<li><p>Nontermial states <span class="math inline">\(1, …, 14\)</span></p></li>
<li><p>One terminal State (shown twice as shaded squares)</p></li>
<li><p>Reward is <span class="math inline">\(-1\)</span> until the terminal state is reahed</p></li>
<li><p>Agent follows uniform random policy <span class="math display">\[
\pi(n|\cdot)=\pi(e|\cdot)=\pi(s|\cdot)=\pi(w|\cdot) = 0.25
\]</span></p></li>
</ul>
<p>Let's use dynamic programming to solve the MDP.</p>
<p><img src="/images/ipe1.png"></p>
<p><img src="/images/ipe2.png"></p>
<p>The grids on the left show the value function of each state, the update rule shown by the illustration. Finally, it converges to the true value function of the policy. It basically tell us <em>if we take the random walk under the policy, how much reward on average we will get when we reach the terminal state</em>.</p>
<p>The right-hand column shows how to find better policy with respect to the value funtions.</p>
<h2><span id="policy-iteration">Policy Iteration</span></h2>
<p>How to improve a Policy</p>
<ul>
<li><p>Given a policy <span class="math inline">\(\pi\)</span></p>
<ul>
<li><p><strong>Evaluate</strong> the policy <span class="math inline">\(\pi\)</span> <span class="math display">\[
v_\pi(s) = E[R_{t+1}+\gamma R_{t+2} + ... | S_t = s]
\]</span></p></li>
<li><p><strong>Improve</strong> the policy by acting <em>greedily</em> with respect to <span class="math inline">\(v_\pi\)</span> <span class="math display">\[
\pi&#39;=greddy(v_\pi)
\]</span></p></li>
</ul></li>
<li><p>In general, need more iterations of improvement / evaluation</p></li>
<li><p>But this process of <em>policy iteration</em> always converges to <span class="math inline">\(\pi_*\)</span></p></li>
</ul>
<p><img src="/images/pi.png"></p>
<p><strong>Demonstration</strong></p>
<p>Consider a deterministic policy <span class="math inline">\(a = \pi(s)\)</span>, we can improve the policy by acting greedily <span class="math display">\[
\pi&#39;(s) = \arg\max_{a\in\mathcal{A}}q_\pi(s, a)
\]</span> (Note: <span class="math inline">\(q_\pi\)</span> is the action value function following policy <span class="math inline">\(\pi\)</span>)</p>
<p>This improves the value from any state <span class="math inline">\(s\)</span> over one step, <span class="math display">\[
q_\pi(s, \pi&#39;(s)) = \max_{a\in\mathcal{A}}q_\pi(s,a)≥q_\pi(s, \pi(s))=v_\pi(s)
\]</span> (Note: <span class="math inline">\(q_\pi(s, \pi&#39;(s))\)</span> means the action value of taking one step following policy <span class="math inline">\(\pi&#39;\)</span> then following policy <span class="math inline">\(\pi\)</span> forever.)</p>
<p>If therefore improves the value function, <span class="math inline">\(v_{\pi&#39;}(s) ≥ v_\pi (s)\)</span> <span class="math display">\[
\begin{align}
v_\pi(s) &amp; ≤ q_\pi(s, \pi&#39;(s))=E_{\pi&#39;}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t = s]  \\
&amp; ≤ E_{\pi&#39;}[R_{t+1} + \gamma q_\pi(S_{t+1}, \pi&#39;(S_{t+1}))|S_t=s] \\
&amp;≤ E_{\pi&#39;}[R_{t+1} + \gamma R_{t+2} + \gamma^2q_\pi(S_{t+2}, \pi&#39;(S_{t+2}))|S_t = s]\\
&amp;≤ E_{\pi&#39;}[R_{t+1} + \gamma R_{t+2} + ..... | S_t = s] = v_{\pi&#39;}(s)
\end{align}
\]</span> (Unroll the equation to the second, third … step by taking the Bellman euqation into it.)</p>
<p>If improvements stop, <span class="math display">\[
q_\pi(s, \pi&#39;(s)) = \max_{a\in\mathcal{A}}q_\pi(s,a) = q_\pi(s, \pi(s)) = v_\pi(s)
\]</span> Then the <strong>Bellman optimality</strong> equation has been satisfied <span class="math display">\[
v_\pi(s) = \max_{a\in\mathcal{A}}q_\pi(s, a)
\]</span> Therefore <span class="math inline">\(v_\pi(s) = v_*(s)\)</span> for all <span class="math inline">\(s \in \mathcal{S}\)</span>, so <span class="math inline">\(\pi\)</span> is an optimal policy.</p>
<p><strong>Early Stopping</strong></p>
<p>Question: Does policy evaluation need to converge to <span class="math inline">\(v_\pi\)</span> ?</p>
<ul>
<li>e.g. in the small gridworld <span class="math inline">\(k = 3\)</span> was sufficient to acheive optimal policy</li>
</ul>
<p>Or shoule we introduce a stopping condition</p>
<ul>
<li>e.g. <span class="math inline">\(\epsilon\)</span>-convergence of value function</li>
</ul>
<p>Or simply stop after <span class="math inline">\(k\)</span> iterations of iterative policy evaluation?</p>
<h2><span id="value-iteration">Value Iteration</span></h2>
<p><strong>Principle of Optimality</strong></p>
<p>Any optimal policy can be subdivided into two components:</p>
<ul>
<li>An optimal first action <span class="math inline">\(A_*\)</span></li>
<li>Followed by an optimal policy from successor state <span class="math inline">\(S&#39;\)</span></li>
</ul>
<blockquote>
<p>Theorem: Principle of Optimality</p>
<p>A policy <span class="math inline">\(\pi(a|s)\)</span> achieves the optimal value from state <span class="math inline">\(s\)</span>, <span class="math inline">\(v_\pi(s) = v_*(s)\)</span> if and only if</p>
<ul>
<li>For any state <span class="math inline">\(s&#39;\)</span> reachable from <span class="math inline">\(s\)</span>, <span class="math inline">\(\pi\)</span> achieves the optimal value from state <span class="math inline">\(s&#39;\)</span></li>
</ul>
</blockquote>
<p>If we know the solution to subproblems <span class="math inline">\(v_\ast(s&#39;)\)</span>, then solution <span class="math inline">\(v_\ast(s)\)</span> can be found by one-step look ahead: <span class="math display">\[
v_\ast(s) \leftarrow \max_{a\in\mathcal{A}}\mathcal{R}^a_s+\gamma \sum_{s&#39;\in \mathcal{S}}P^a_{ss&#39;}v_\ast(s&#39;)
\]</span> The idea of value iteration is to apply these updates iteratively.</p>
<ul>
<li>Intuition: start with final rewards and work backwards</li>
<li>Still works with loopy, stochatis MDPs</li>
</ul>
<p><strong>Example: Shortest Path</strong></p>
<p><img src="/images/grid2.png"></p>
<ul>
<li>The goal state is on the left-up corner</li>
<li>Each step get -1 reward</li>
<li>The number showed in each grid is the value of that state</li>
<li>At each iteration, update all states</li>
</ul>
<p><strong>Value Iteration</strong></p>
<ul>
<li>Problem: find optimal policy <span class="math inline">\(\pi\)</span></li>
<li>Solution: iterative application of Bellman optimality backup</li>
<li><span class="math inline">\(v_1 \rightarrow v_2 \rightarrow … \rightarrow v_*\)</span></li>
<li><em>Synchronous</em> backups
<ul>
<li>At each iteration <span class="math inline">\(k+1\)</span></li>
<li>For all states <span class="math inline">\(s\in \mathcal{S}\)</span></li>
<li>Update <span class="math inline">\(v_{k+1}(s)\)</span> from <span class="math inline">\(v_k(s&#39;)\)</span></li>
</ul></li>
<li>Convergence to <span class="math inline">\(v_*\)</span> will be proven later</li>
<li>Unlike policy iteration, there is no explicit policy</li>
<li>Intermediate value functions may not correspond to any policy</li>
</ul>
<p><strong>Synchronous Dynamic Programming Algorithms</strong></p>
<table>
<colgroup>
<col style="width: 12%">
<col style="width: 51%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>problem</th>
<th>bellman equation</th>
<th>algorithm</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Prediction</td>
<td>Bellman Expectation Equation</td>
<td>Iterative Policy Evaluation</td>
</tr>
<tr class="even">
<td>Control</td>
<td>Bellman Expectation Equation + Greedy Policy Improvement</td>
<td>Policy Iteration</td>
</tr>
<tr class="odd">
<td>Control</td>
<td>Bellman Optimatility Equation</td>
<td>Value Iteration</td>
</tr>
</tbody>
</table>
<p>Algorithms are based on state-value function <span class="math inline">\(v_\pi(s)\)</span> or <span class="math inline">\(v_*(s)\)</span></p>
<ul>
<li><span class="math inline">\(O(mn^2)\)</span> per iteration, for <span class="math inline">\(m\)</span> actions and <span class="math inline">\(n\)</span> states</li>
</ul>
<p>Could also apply to action-value function <span class="math inline">\(q_\pi(s, a)\)</span> or <span class="math inline">\(q_*(s, a)\)</span></p>
<ul>
<li><span class="math inline">\(O(m^2n^2)\)</span> per iteration</li>
</ul>
<h2><span id="extentions-to-dynamic-programming">Extentions to Dynamic Programming</span></h2>
<p><strong>Asynchronous Dynamic Programming</strong></p>
<p><em>Asynchronous DP</em> backs up states individually, in any order. For each selected state, apply the appropriate backup, which can significantly reduce computation. It also guaranteed to converge if all states continue to be selected.</p>
<p>Three simple ideas for asynchronous dynamic programming:</p>
<ul>
<li><p><em>In-place</em> dynamic programming</p>
<p><img src="/images/in-place.png"></p></li>
<li><p><em>Prioritised sweeping</em></p>
<p><img src="/images/ps.png"></p></li>
<li><p><em>Real-time</em> dynamic programming</p>
<p><img src="/images/real-time.png"></p></li>
</ul>
<p><strong>Full-Width Backups</strong></p>
<p>DP uses <em>full-width</em> backups</p>
<ul>
<li><em>full-width</em> means when we look aheah, we consider all branches(actions) that could happen</li>
<li>For each backup (sync or async)
<ul>
<li>Every successor state and action is considered</li>
<li>Using knowledge of the MDP transitions and reward function</li>
</ul></li>
</ul>
<p><img src="/images/fw.png"></p>
<h2><span id="contraction-mapping">Contraction Mapping</span></h2>
<p>Information about <em>contraction mapping theorem</em>, please refer to http://www.math.uconn.edu/~kconrad/blurbs/analysis/contraction.pdf</p>
<p>Consider the vector space <span class="math inline">\(\mathcal{V}\)</span> over value functions. There are <span class="math inline">\(|\mathcal{S}|\)</span> dimensions.</p>
<ul>
<li>Each point in this space fully specifies a value function <span class="math inline">\(v(s)\)</span></li>
</ul>
<p>We will measure distance between state-value functions <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> by the <span class="math inline">\(\infty\)</span>-norm. <span class="math display">\[
||u-v||_\infty = \max_{s\in\mathcal{S}}|u(s)-v(s)|
\]</span> <em>Bellman Expectation Backup is a Contraction</em></p>
<p>Define the <em>Bellman expectation backup operator</em> <span class="math inline">\(T^\pi\)</span>, <span class="math display">\[
T^\pi(v) = \mathcal{R}^\pi + \gamma\mathcal{P}^\pi v
\]</span> This operator is a <span class="math inline">\(\gamma\)</span>-contraction, it makes value functions closer bt at least <span class="math inline">\(\gamma\)</span>, <span class="math display">\[
\begin{align}
||T^\pi(u)-T^\pi(v)||_\infty &amp;= ||(\mathcal{R}^\pi + \gamma\mathcal{P}^\pi v) - (\mathcal{R}^\pi + \gamma\mathcal{P}^\pi u)||_\infty \\
&amp;= ||\gamma P^\pi(u-v)||_\infty\\
&amp;≤||\gamma P^\pi||u-v||_\infty||_\infty\\
&amp;≤\gamma||u-v||_\infty
\end{align}
\]</span></p>
<blockquote>
<p>Theorem: <strong>Contraction Mapping Theorem</strong></p>
<p>For any metric space <span class="math inline">\(\mathcal{V}\)</span> that is complete (closed) under an operator <span class="math inline">\(T(v)\)</span>, where <span class="math inline">\(T\)</span> is a <span class="math inline">\(\gamma\)</span>-contraction,</p>
<ul>
<li><span class="math inline">\(T\)</span> converges to a unique fixed point</li>
<li>At a linear convergence rate of <span class="math inline">\(\gamma\)</span></li>
</ul>
</blockquote>
<p><strong>Convergence of Iterative Policy Evaluation and Policy Iteration</strong></p>
<p>The Bellman expectation operator <span class="math inline">\(T^\pi\)</span> has a unique fixed point <span class="math inline">\(v_\pi\)</span>.</p>
<p>By contraction mapping theorem,</p>
<ul>
<li>Iterative policy evaluation converges on <span class="math inline">\(v_\pi\)</span>;</li>
<li>Policy iteration converges on <span class="math inline">\(v_*\)</span>.</li>
</ul>
<p><em>Bellman Optimality Backup is a Contraction</em></p>
<p>Define the <em>Bellman Optimality backup operator</em> <span class="math inline">\(T^\ast\)</span>, <span class="math display">\[
T^\ast(v) = \max_{a\in\mathcal{A}}\mathcal{R}^a+\gamma \mathcal{P}^av
\]</span> This operator is a <span class="math inline">\(\gamma\)</span>-contraction, it makes value functions closer by at least <span class="math inline">\(\gamma\)</span>, <span class="math display">\[
||T^\ast(u)-T\ast(v)||_\infty≤\gamma ||u-v||_\infty
\]</span> <strong>Convergence of Value Iteration</strong></p>
<p>The Bellman optimality operator <span class="math inline">\(T^∗\)</span> has a unique fixed point <span class="math inline">\(v_*\)</span>.</p>
<p>By contraction mapping theorem, value iteration converges on <span class="math inline">\(v_*\)</span>.</p>
<p>In summary, what does a Bellman backup do to points in value function space is to bring value functions <em>closer</em> to a unique fixed point. And therefore the backups must converge on a unique solution.</p>
<p>This lecture (note) introduces how to use dynamic programming to solve <em>planning</em> problems. Next lecture will introduce model-free prediction, which is a really RL problem.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/笔记/">笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/MDP/">MDP</a><a href="/tags/Dynamic-Programming/">Dynamic Programming</a><a href="/tags/Policy-Iteration/">Policy Iteration</a><a href="/tags/Value-Iteration/">Value Iteration</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>
<script type="text/javascript">
  var $config = {
    image: "icon.png",
  };
  socialShare('.social-share-cs', $config);
</script>



<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80MTI4MC8xNzgyOA==">
	<script type="text/javascript">
		(function (d, s) {
			var j, e = d.getElementsByTagName(s)[0];

			if (typeof LivereTower === 'function') {
				return;
			}

			j = d.createElement(s);
			j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
			j.async = true;

			e.parentNode.insertBefore(j, e);
		})(document, 'script');
	</script>
	<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->


      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2019
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>