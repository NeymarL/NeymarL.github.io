<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>
    RL - Model-Free Control | NIUHE | Be Myself Enjoy My Life
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE&#39;s Blog">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="RL - Model-Free Control">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">Be Myself Enjoy My Life</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      RL - Model-Free Control</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/12/21/RL - Model-Free Control/" rel="bookmark">
        <time class="entry-date published" datetime="2017-12-21T12:00:09.000Z">
          2017-12-21
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <h2><span id="introduction">Introduction</span></h2>
<p>Last lecture:</p>
<ul>
<li>Model-free prediction</li>
<li><em>Estimate</em> the value function of an <em>unknown</em> MDP</li>
</ul>
<p>This lecture:</p>
<ul>
<li>Model-free control</li>
<li><strong>Optimise</strong> the value function of an unknown MDP</li>
</ul>
<a id="more"></a>
<p>Why we care about model-free control? So, let's see some example problems that can be modelled as MDPs:</p>
<ul>
<li>Helicopter, Robocup Soccer, Quake</li>
<li>Portfolio management, Game of Go...</li>
</ul>
<p>For most of these problems, either:</p>
<ul>
<li>MDP model is <strong>unknown</strong>, but experience can be sampled</li>
<li>MDP model is known, but is <strong>too big to use</strong>, except by samples</li>
</ul>
<p><span class="math inline">\(\color{red}{\mbox{Model-free control}}\)</span> can sovlve these problems.</p>
<p>There are two branches of model-free control:</p>
<ul>
<li><span class="math inline">\(\color{red}{\mbox{On-policy}}\)</span> learning
<ul>
<li>&quot;Learn on the job&quot;</li>
<li>Learn about policy <span class="math inline">\(\pi\)</span> from experience sampled from <span class="math inline">\(\pi\)</span></li>
</ul></li>
<li><span class="math inline">\(\color{red}{\mbox{Off-policy}}\)</span> learning
<ul>
<li>&quot;Look over someone's shoulder&quot;</li>
<li>Learn about policy <span class="math inline">\(\pi\)</span> from experience sampled from <span class="math inline">\(\mu\)</span></li>
</ul></li>
</ul>
<p><strong>Table of Contents</strong></p>
<!-- toc -->
<ul>
<li><a href="#on-policy-monte-carlo-control">On-Policy Monte-Carlo Control</a></li>
<li><a href="#on-policy-temporal-difference-learning">On-Policy Temporal-Difference Learning</a>
<ul>
<li><a href="#sarsalambda">Sarsa(<span class="math inline">\(\lambda\)</span>)</a></li>
</ul></li>
<li><a href="#off-policy-learning">Off-Policy Learning</a>
<ul>
<li><a href="#q-learning">Q-Learning</a></li>
</ul></li>
<li><a href="#summary">Summary</a></li>
</ul>
<!-- tocstop -->
<h2><span id="on-policy-monte-carlo-control">On-Policy Monte-Carlo Control</span></h2>
<p>In previous lectures, we have seen that using policy iteration to find the best policy. Today, we also use this central idea plugging in MC or TD algorithm.</p>
<p><img src="/images/pi.png"></p>
<p><strong>Generalised Policy Iteration With Monte-Carlo Evaluation</strong></p>
<p>A simple idea is</p>
<ul>
<li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(V = v_\pi\)</span>?</li>
<li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: Greedy policy improvement?</li>
</ul>
<p>Well, this idea has two major problems:</p>
<ul>
<li><p>Greedy policy improvement over <span class="math inline">\(V(s)\)</span> requires <strong>model of MDP</strong> <span class="math display">\[
\pi^\prime(s) = \arg\max_{a\in\mathcal{A}}\mathcal{R}^a_s+\mathcal{P}^a_{ss&#39;}V(s&#39;)
\]</span> since, we do not know the state transition probability matrix <span class="math inline">\(\mathcal{P}\)</span>.</p></li>
<li><p>Exploration issue: cannot guarantee to explore all states</p></li>
</ul>
<p>So, the alternative is to use action-value function <span class="math inline">\(Q\)</span>:</p>
<ul>
<li>Greedy policy improvement over <span class="math inline">\(Q(s, a)\)</span> is model-free <span class="math display">\[
\pi^\prime=\arg\max_{a\in\mathcal{A}}Q(s,a)
\]</span></li>
</ul>
<p>Let's replace it in the algorithm:</p>
<p><img src="/images/avf.png"></p>
<ul>
<li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(\color{red}{Q = q_\pi}\)</span></li>
<li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: Greedy policy improvement?</li>
</ul>
<p>We still have one problems about the algorithm, which is exploration issue. Here is a example of greedy action selection:</p>
<p><img src="/images/gaseg.png"></p>
<p>The reward of the two doors are stochastic. However, because of the greedy action selection, we always choose the right door without exploring the value of the left one.</p>
<p>One simple algorithm to ensure keeping exploration is <strong><span class="math inline">\(\epsilon\)</span>-greedy exploration</strong>.</p>
<p><strong><span class="math inline">\(\epsilon\)</span>-Greedy Exploration</strong></p>
<p>All <span class="math inline">\(m\)</span> actions are tried with non-zero probalility,</p>
<ul>
<li>With probability <span class="math inline">\(1-\epsilon\)</span> choose the greedy action</li>
<li>With probability <span class="math inline">\(\epsilon\)</span> choose an action at <strong>random</strong></li>
</ul>
<p><span class="math display">\[
\pi(a|s)=\begin{cases} 
\epsilon/m+1-\epsilon,  &amp; \mbox{if } a^* = \arg\max_{a\in\mathcal{A}}Q(s,a) \\
\epsilon/m, &amp; \mbox{otherwise }
\end{cases}
\]</span></p>
<blockquote>
<p>Theorem</p>
<p>For any <span class="math inline">\(\epsilon\)</span>-greedy policy <span class="math inline">\(\pi\)</span>, the <span class="math inline">\(\epsilon\)</span>-greedy policy <span class="math inline">\(\pi^\prime\)</span> with respect to <span class="math inline">\(q_\pi\)</span> is an improvement, <span class="math inline">\(v_{\pi^\prime}≥v_\pi(s)\)</span>.</p>
</blockquote>
<p><img src="/images/egpi.png"></p>
<p>Therefore from policy improvement theorem, <span class="math inline">\(v_{\pi^\prime}(s) ≥ v_\pi(s)\)</span>.</p>
<p><strong>Monte-Carlo Policy Iteration</strong></p>
<p><img src="/images/mcpi.png"></p>
<ul>
<li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(Q = q_\pi\)</span></li>
<li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: <span class="math inline">\(\color{red}{\epsilon}\)</span>-greedy policy improvement</li>
</ul>
<p><strong>Monte-Carlo Control</strong></p>
<p><img src="/images/mcc.png"></p>
<p><span class="math inline">\(\color{red}{\mbox{Every episode}}\)</span>:</p>
<ul>
<li><span class="math inline">\(\color{Blue}{\mbox{Policy evaluation}}\)</span>: Monte-Carlo policy evaluation, <span class="math inline">\(\color{red}{Q \approx q_\pi}\)</span></li>
<li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span>: <span class="math inline">\(\epsilon\)</span>-greedy policy improvement</li>
</ul>
<p>The method is once evaluate over an episode, immediately improve the policy. The idea is since we already have a better evaluation, why waiting to update the policy after numerous episodes. That is improving the policy right after evaluating one episode.</p>
<p><strong>GLIE</strong></p>
<blockquote>
<p>Definition</p>
<p><strong>Greedy in the Limit with Infinite Exploration</strong> (GLIE)</p>
<ul>
<li><p>All state-action pairs are explored infinitely many times, <span class="math display">\[
\lim_{k\rightarrow\infty}N_k(s,a)=\infty
\]</span></p></li>
<li><p>The policy converges on a greedy policy, <span class="math display">\[
\lim_{k\rightarrow\infty}\pi_k(a|s)=1(a=\arg\max_{a^\prime \in\mathcal{A}}Q_k(s, a^\prime))
\]</span></p></li>
</ul>
</blockquote>
<p>For example, <span class="math inline">\(\epsilon\)</span>-greedy is GLIE if <span class="math inline">\(\epsilon\)</span> reduces to zero at <span class="math inline">\(\epsilon_k=\frac{1}{k}\)</span>.</p>
<p><strong>GLIE Monte-Carlo Control</strong></p>
<p>Sample <span class="math inline">\(k\)</span>th episode using <span class="math inline">\(\pi\)</span>: <span class="math inline">\(\{S_1, A_1, R_2, …, S_T\} \sim \pi\)</span></p>
<ul>
<li><p><span class="math inline">\(\color{red}{\mbox{Evaluation}}\)</span></p>
<ul>
<li>For each state <span class="math inline">\(S_t\)</span> and action <span class="math inline">\(A_t\)</span> in the episode, <span class="math display">\[
\begin{array}{lcl}
N(S_t, A_t) \leftarrow N(S_t, A_t)+1 \\
Q(S_t, A_t) \leftarrow Q(S_t, A_t)+\frac{1}{N(S_t, A_t)}(G_t-Q(S_t, A_t))
\end{array}
\]</span></li>
</ul></li>
<li><p><span class="math inline">\(\color{red}{\mbox{Improvement}}\)</span></p>
<ul>
<li>Improve policy based on new action-value function <span class="math display">\[
\begin{array}{lcl}
\epsilon\leftarrow \frac{1}{k} \\
\pi \leftarrow \epsilon\mbox{-greedy}(Q)
\end{array}
\]</span></li>
</ul></li>
</ul>
<p>GLIE Monte-Carlo control converges to the optimal action-value function, <span class="math inline">\(Q(s,a) \rightarrow q_*(s,a)\)</span>.</p>
<p><strong>Blackjack Example</strong></p>
<p><img src="/images/mccb.png"></p>
<p>Using Monte-Carlo control, we can get the optimal policy above.</p>
<h2><span id="on-policy-temporal-difference-learning">On-Policy Temporal-Difference Learning</span></h2>
<p>Temporal-difference (TD) learning has several advantages over Monte-Carlo (MC):</p>
<ul>
<li>low variance</li>
<li>Online</li>
<li>Incomplete sequences</li>
</ul>
<p>A natural idea is using TD instead of MC in our control loop:</p>
<ul>
<li>Apply TD to <span class="math inline">\(Q(S, A)\)</span></li>
<li>Use <span class="math inline">\(\epsilon\)</span>-greedy policy improvement</li>
<li>Update every <em>time-step</em></li>
</ul>
<p><strong>Sarsa Update</strong></p>
<p><img src="/images/sarsa.png"></p>
<p>Updating action-value functions with Sarsa: <span class="math display">\[
Q(S,A) \leftarrow Q(S, A) + \alpha(R+\gamma Q(S^\prime, A^\prime)-Q(S, A))
\]</span> <img src="/images/mcc.png"></p>
<p>So, the full algorithm is:</p>
<ul>
<li>Every <span class="math inline">\(\color{red}{\mbox{time-step}}\)</span>:
<ul>
<li><span class="math inline">\(\color{blue}{\mbox{Policy evaluation}}\)</span> <span class="math inline">\(\color{red}{\mbox{Sarsa}}\)</span>, <span class="math inline">\(Q\approx q_\pi\)</span></li>
<li><span class="math inline">\(\color{blue}{\mbox{Policy improvement}}\)</span> <span class="math inline">\(\epsilon\)</span>-greedy policy improvement</li>
</ul></li>
</ul>
<p><img src="/images/pesedotd.png"></p>
<p><strong>Windy Gridworld Example</strong></p>
<p><img src="/images/wweg.png"></p>
<p>The 'S' represents start location and 'G' marks the goal. There is a number at the bottom of each column which represents the wind will blow the agent up how many grids if the agent stays at that column.</p>
<p>The result of apply Sarsa to the problem is</p>
<p><img src="/images/wwegres.png"></p>
<h3><span id="sarsalambda">Sarsa(<span class="math inline">\(\lambda\)</span>)</span></h3>
<p><strong>n-Step Sarsa</strong></p>
<p>Consider the following <span class="math inline">\(n\)</span>-step returns for <span class="math inline">\(n=1,2,..\infty\)</span>:</p>
<p><img src="/images/nsarsa.png"></p>
<p>Define the <span class="math inline">\(n\)</span>-step <span class="math inline">\(Q\)</span>-return <span class="math display">\[
q_t^{(n)}=R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^n Q(S_{t+n})
\]</span> <span class="math inline">\(n\)</span>-step Sarsa updates <span class="math inline">\(Q(s, a)\)</span> towards the <span class="math inline">\(n\)</span>-step <span class="math inline">\(Q\)</span>-return <span class="math display">\[
Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha(q_t^{(n)}-Q(S_t,A_t))
\]</span> <strong>Forward View Sarsa(<span class="math inline">\(\lambda\)</span>)</strong></p>
<p><img src="/images/sarsalam.png"></p>
<p>The <span class="math inline">\(q^\lambda\)</span> return combines all <span class="math inline">\(n\)</span>-step Q-returns <span class="math inline">\(q_t^{(n)}\)</span> using weight <span class="math inline">\((1-\lambda)\lambda^{n-1}\)</span>: <span class="math display">\[
q_t^\lambda = (1-\lambda)\sum^\infty_{n=1}\lambda^{n-1}q_t^{(n)}
\]</span> Forward-view Sarsa(<span class="math inline">\(\lambda\)</span>): <span class="math display">\[
Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha(q_t^\lambda-Q(S_t, A_t))
\]</span> <strong>Backward View Sarsa(<span class="math inline">\(\lambda\)</span>)</strong></p>
<p>Just like TD(<span class="math inline">\(\lambda\)</span>), we use <span class="math inline">\(\color{red}{\mbox{eligibility traces}}\)</span> in an online algorithm, but Sarsa(<span class="math inline">\(\lambda\)</span>) has one eligibility trace for each state-action pair: <span class="math display">\[
E_0(s, a) = 0
\]</span></p>
<p><span class="math display">\[
E_t(s, a) = \gamma\lambda E_{t-1}(s,a)+1(S_t=s, A_t=a)
\]</span></p>
<p><span class="math inline">\(Q(s, a)\)</span> is updated for every state <span class="math inline">\(s\)</span> and action <span class="math inline">\(a\)</span> in proportion to TD-error <span class="math inline">\(\delta_t\)</span> and eligibility trace <span class="math inline">\(E_t(s, a)\)</span>: <span class="math display">\[
\delta_t=R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})-Q(S_t, A_t)
\]</span></p>
<p><span class="math display">\[
Q(s, a) \leftarrow Q(s, a) +\alpha \delta_t E_t(s, a)
\]</span></p>
<p><img src="/images/sarcode.png"></p>
<p>The difference between Sarsa and Sarsa(<span class="math inline">\(\lambda\)</span>):</p>
<p><img src="/images/sarsadiff.png"></p>
<p>If we initial all <span class="math inline">\(Q(s, a) = 0\)</span>, then we first do a random walk and reach the goal. Using Sarsa, we can only update the Q-value of the previous state before reaching the goal since all other <span class="math inline">\(Q\)</span> are zero. So the reward can only propagate one state. On the contrary, if we using Sarsa(<span class="math inline">\(\lambda\)</span>), the reward can propagate from the last state to the first state with a exponential decay.</p>
<h2><span id="off-policy-learning">Off-Policy Learning</span></h2>
<p>Evaluate target policy <span class="math inline">\(\pi(a|s)\)</span> to compute <span class="math inline">\(v_\pi(s)\)</span> or <span class="math inline">\(q_\pi(s, a)\)</span> while following behaviour policy <span class="math inline">\(\mu(a|s)\)</span> <span class="math display">\[
\{S_1, A_1, R_2, ..., S_T\}\sim \mu
\]</span> So, why is this important? There are several reasons:</p>
<ul>
<li>Learn from observing hunman or other agents</li>
<li>Re-use experience generated from old policies <span class="math inline">\(\pi_1, \pi_2, …, \pi_{t-1}\)</span></li>
<li>Learn about <strong>optimal</strong> policy while following <span class="math inline">\(\color{red}{\mbox{exploratory policy}}\)</span></li>
<li>Learn about <strong>multiple</strong> policies while following one policy</li>
</ul>
<p><strong>Importance Sampling</strong></p>
<p>Estimate the expectation of a different distribution <span class="math display">\[
\mathbb{E}_{X\sim P}[f(X)] = \sum P(X)f(X)=\sum Q(X)\frac{P(X)}{Q(X)}f(X)=\mathbb{E}_{X\sim Q}[\frac{P(X)}{Q(X)}f(X)]
\]</span> <strong>Off-Policy Monte-Carlo</strong></p>
<p>Use returns generated from <span class="math inline">\(\mu\)</span> to evaluate <span class="math inline">\(\pi\)</span>. Weight return <span class="math inline">\(G_t\)</span> according to <strong>similarity</strong> between policies. Multiply importance sampling corrections along whole episode: <span class="math display">\[
G_t^{\pi/\mu}=\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}\frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})}...\frac{\pi(A_T|S_T)}{\mu(A_T|S_T)}G_t
\]</span> Update value towards <em>corrected</em> return: <span class="math display">\[
V(S_t)\leftarrow V(S_t)+\alpha (\color{red}{G_t^{\pi/\mu}}-V(S_t))
\]</span> But it has two major problems:</p>
<ul>
<li>Cannot use if <span class="math inline">\(\mu\)</span> is zero when <span class="math inline">\(\pi\)</span> is non-zero</li>
<li>Importance sampling can dramatically increase variance, so it is useless in practice</li>
</ul>
<p><strong>Off-Policy TD</strong></p>
<p>Use TD targets generated from <span class="math inline">\(\mu\)</span> to evaluate <span class="math inline">\(\pi\)</span>. Weight TD target <span class="math inline">\(R+\gamma V(S&#39;)\)</span> by importance sampling. Only need a single importance sampling correction: <span class="math display">\[
V(S_t)\leftarrow V(S_t)+\alpha \left(\color{red}{\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1}+\gamma V(S_{t+1}))}-V(S_t)\right)
\]</span> This algorithm has much lower variance than Monte-Carlo importance sampling because policies only need to be similar over a single step.</p>
<h3><span id="q-learning">Q-Learning</span></h3>
<p>We now consider off-policy learning of action-values <span class="math inline">\(Q(s, a)\)</span>. The benefit of it is no importance sampling is required.</p>
<p>The next action is chosen using <strong>behaviour</strong> policy <span class="math inline">\(A_{t+1}\sim\mu(\cdot|S_t)\)</span>. But we consider <strong>alternative</strong> successor action <span class="math inline">\(A&#39;\sim \pi(\cdot|S_t)\)</span>. And update <span class="math inline">\(Q(S_t, A_t)\)</span> towards value of alternative action <span class="math display">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1}+\gamma Q(S_{t+1}, \color{red}{A&#39;})-Q(S_t, A_t))
\]</span> We now allow both behaviour and target policies to <strong>improve</strong>.</p>
<p>The <strong>target</strong> policy <span class="math inline">\(\pi\)</span> is <span class="math inline">\(\color{red}{\mbox{greedy}}\)</span> w.r.t <span class="math inline">\(Q(s, a)\)</span>: <span class="math display">\[
\pi(S_{t+1})=\arg\max_{a&#39;}Q(S_{t+1}, a&#39;)
\]</span> The <strong>behaviour</strong> policy <span class="math inline">\(\mu\)</span> is e.g. <span class="math inline">\(\color{red}{\epsilon \mbox{-greedy}}\)</span> w.r.t. <span class="math inline">\(Q(s,a)\)</span>.</p>
<p>The <strong>Q-learning</strong> target then simplifies: <span class="math display">\[
\begin{align}
\mbox{Q-learning Target} &amp;= R_{t+1}+\gamma Q(S_{t+1}, A&#39;) \\
&amp; = R_{t+1}+\gamma Q(S_{t+1}, \arg\max_{a&#39;}Q(S_{t+1}, a&#39;)) \\
&amp;= R_{t+1}+\max_{a&#39;}\gamma Q(S_{t+1}, a&#39;)
\end{align}
\]</span> So the Q-learning control algorithm is</p>
<p><img src="/images/qlalg.png"></p>
<p>Of course, the Q-learning control still converges to the optimal action-value function, <span class="math inline">\(Q(s, a)\rightarrow q_*(s,a)\)</span>.</p>
<p><img src="/images/qlcode.png"></p>
<h2><span id="summary">Summary</span></h2>
<p><strong>Relationship Between DP and TD</strong></p>
<p><img src="/images/rbtddp.png"></p>
<p><img src="/images/rbtddp2.png"></p>
<p>In a word, TD backup can be seen as the sample of corresponding DP backup. This lecture introduces model-free control which is optimise the value function of an unknown MDP with on-policy and off-policy methods. Next lecture will introduce function approximation which is easy to scale up and can be applied into big MDPs.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a href="/tags/增强学习/">增强学习</a><a href="/tags/Monte-Carlo-Control/">Monte-Carlo Control</a><a href="/tags/Sarsa/">Sarsa</a><a href="/tags/Q-learning/">Q-learning</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>


	<section id="comment" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'Niuhe';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>



      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2018
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-71540601-1', 'auto');
    ga('send', 'pageview');

</script>

      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>