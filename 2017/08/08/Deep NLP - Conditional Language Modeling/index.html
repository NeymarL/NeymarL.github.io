<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>
    Deep NLP - Conditional Language Model | NIUHE | Be Myself Enjoy My Life
  </title>

  
  <meta name="author" content="NIUHE">
  

  
  <meta name="description" content="NIUHE&#39;s Blog">
  

  
  <meta name="keywords" content="编程,读书,学习笔记">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  
  <meta property="og:title" content="Deep NLP - Conditional Language Model">
  

  <meta property="og:site_name" content="NIUHE">

  
  <meta property="og:image" content="/favicon.ico">
  

  <link href="/icon.png" type="image/png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="NIUHE" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <script type="text/javascript" src="/js/social-share.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="blog">
    <div class="content">

      <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">NIUHE</a>
    </h1>
    <p class="site-description">Be Myself Enjoy My Life</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">目录</a></li>
      
        <li><a href="/categories">分类</a></li>
      
        <li><a href="/tags">标签</a></li>
      
    </ul>
  </nav>
</header>

      <main class="site-main posts-loop">
        <article>

  
  
  <h3 class="article-title"><span>
      Deep NLP - Conditional Language Model</span></h3>
  
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/08/08/Deep NLP - Conditional Language Modeling/" rel="bookmark">
        <time class="entry-date published" datetime="2017-08-08T13:30:19.000Z">
          2017-08-08
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
      <!-- toc -->
<ul>
<li><a href="#kalchbrenner-and-blunsom-2013">Kalchbrenner and Blunsom 2013</a></li>
<li><a href="#sutskever-et-al-2014">Sutskever et al. (2014)</a></li>
<li><a href="#kiros-et-al2013">Kiros et al.(2013)</a></li>
</ul>
<!-- tocstop -->
<a id="more"></a>
<p>A <strong>conditional language model</strong> assigns probabilities to sequences of words, <span class="math inline">\(w = (w_1, w_2, …, w_l)\)</span>, given some conditioning context, <strong>x</strong>.</p>
<p>As with unconditional models, it is again helpful to use the chain rule to decompose this probability: <span class="math display">\[
p(w|x) = \prod_{t=1}^lp(w_t|x, w_1, w_2, ..., w_{t-1})
\]</span></p>
<table>
<thead>
<tr class="header">
<th>x &quot;input&quot;</th>
<th>w &quot;text output&quot;</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>An author</td>
<td>A document written by that author</td>
</tr>
<tr class="even">
<td>A topic label</td>
<td>An article about that topic</td>
</tr>
<tr class="odd">
<td>{SPAM, NOT_SPAM}</td>
<td>An email</td>
</tr>
<tr class="even">
<td>A sentence in French</td>
<td>Its English translation</td>
</tr>
<tr class="odd">
<td>A sentence in English</td>
<td>Its French translation</td>
</tr>
<tr class="even">
<td>A sentence in English</td>
<td>Its Chinese translation</td>
</tr>
<tr class="odd">
<td>An image</td>
<td>A text description of the image</td>
</tr>
<tr class="even">
<td>A question + a document</td>
<td>Its answer</td>
</tr>
<tr class="odd">
<td>A question + an image</td>
<td>Its answer</td>
</tr>
</tbody>
</table>
<p>To <strong>train</strong> contitional language models, we need paired samples, <span class="math inline">\(\{(x_i, w_i)\}\)</span>.</p>
<p><strong>Algorighmic challenges</strong></p>
<p>We often want to find the most likely <span class="math inline">\(w\)</span> given some <span class="math inline">\(x\)</span>. This is unfortunately generally and <em>intractable problem</em>. <span class="math display">\[
w^* = \arg \max_wp(w|x)
\]</span> We therefore approximate it using a <strong>beam search</strong> or with Monte Carlo methods since <span class="math inline">\(w^{(i)} \approx p(w|x)\)</span> is often computationally easy.</p>
<p><strong>Evaluating conditional LMs</strong></p>
<p>We can use <strong>cross entropy</strong> or <strong>preplexity</strong>, it's okay to implement, but hard to interpret.</p>
<p><strong>Task-specific evaluation</strong>. Compare the model's most likely output to human-generated expected output using a task-specific evaluation metric <span class="math inline">\(L\)</span>. <span class="math display">\[
w^* = \arg \max_wp(w|x)\ \ \ \ \ L(w^*, w_{ref})
\]</span> Examples of <span class="math inline">\(L\)</span>: BLUE, METEOR, WER, ROUGE, easy to implement, okay to interpret.</p>
<p><strong>Encoder-Decoder</strong></p>
<p><img src="/images/NLP/ed1.png"></p>
<p><img src="/images/NLP/ed2.png"></p>
<p>Two questions</p>
<ul>
<li>How do we encode <span class="math inline">\(x\)</span> as a fixed-size vector, <span class="math inline">\(c\)</span> ?</li>
<li>How do we condition on <span class="math inline">\(c\)</span> in the decoing model?</li>
</ul>
<h2><span id="kalchbrenner-and-blunsom-2013">Kalchbrenner and Blunsom 2013</span></h2>
<p>Encoder <span class="math display">\[
c = embed(x)
\]</span></p>
<p><span class="math display">\[
s = Vc
\]</span></p>
<p>Recurrent decoder <span class="math display">\[
h_t = g(W[h_{t-1}; w_{t-1}] + s + b)
\]</span></p>
<p><span class="math display">\[
u_t = Ph_t + b&#39;
\]</span></p>
<p><span class="math display">\[
p(W_t|x, w&lt;t) = softmax(u_t)
\]</span></p>
<p><strong>CSM Encoder</strong></p>
<p>How should we define <span class="math inline">\(c = embed(x)\)</span> ?</p>
<p>Convolutional sentence model(CSM)</p>
<p><img src="/images/NLP/CSM.png"></p>
<p>Good</p>
<ul>
<li>By stacking them, longer range dependencies can be learnt</li>
<li>Convolutions learn interactions among features in a local context</li>
<li>Deep ConvNets have a branching structure similar to trees, but no parser is required</li>
</ul>
<p>Bad</p>
<ul>
<li>Sentences have different lengths, need different depth trees; convnets are not usually so dynamic, but see Kalchbrenner et al. (2014). A convolutional neural network for modelling sentences. In Proc. ACL.</li>
</ul>
<p><img src="/images/NLP/RNNdecoder.png"></p>
<h2><span id="sutskever-et-al-2014">Sutskever et al. (2014)</span></h2>
<p>LSTM encoder</p>
<ul>
<li><span class="math inline">\((c_0, h_0)\)</span> are parameters</li>
<li><span class="math inline">\((c_i, h_i)\)</span> = LSTM(<span class="math inline">\(x_i, c_{i-1}, h_{i-1}\)</span>)</li>
</ul>
<p>The encoding is <span class="math inline">\((c_l, h_l)\)</span> where <span class="math inline">\(l = |x|\)</span></p>
<p>LSTM decoder</p>
<ul>
<li><span class="math inline">\(w_0 = &lt;s&gt;\)</span></li>
<li><span class="math inline">\((c_{t+l}, h_{t+l}) = LSTM(w_{t-1}, c_{t+l-1}, h_{t+l-1})\)</span></li>
<li><span class="math inline">\(u_t = Ph_{t+l} + b\)</span></li>
<li><span class="math inline">\(P(W_t|x, w&lt;t) = softmax(u_t)\)</span></li>
</ul>
<p><img src="/images/NLP/sea.png"></p>
<p>Good</p>
<ul>
<li>RNNs deal naturally with sequences of various lengths</li>
<li>LSTMs in principle can propagate gradients a long</li>
<li>Very simple architecture</li>
</ul>
<p>Bad</p>
<ul>
<li>The hidden state has to remember a lot of information!</li>
</ul>
<p><strong>Tricks</strong></p>
<p>Read the input sequence &quot;backwards&quot; : <strong>+4 BLEU</strong></p>
<p><img src="/images/NLP/backsea.png"></p>
<p>Use an ensemble of J <strong>independently trained</strong> models.</p>
<ul>
<li>Ensemble of 2 models: <strong>+3 BLEU</strong></li>
<li>Ensemble of 5 models; <strong>+4.5 BLEU</strong></li>
</ul>
<p><strong>A word about decoding</strong></p>
<p>In general, we want to find the most probable (MAP) output given the input, i.e. <span class="math display">\[
\begin{align}
w^* = \arg\max_{w}p(w|x) = \arg \max_w\sum_{t=1}^{|w|}\log p(w_t|x, w_{&lt;t})
\end{align}
\]</span> This is, for general RNNs, a hard problem. We therefore approximate it with a <strong>greedy search</strong>： <span class="math display">\[
\begin{array}{lcl}
w_1^* = \arg\max_{w_1}p(w_1|x)\\
w_2^* = \arg\max_{w_2}p(w_2|x, w_1^*)\\
...\\
w^*_t = \arg\max_{w_t}p(w_t|x, w^*_{&lt;t})
\end{array}
\]</span> A slightly better approximation is to use a <strong>beam search</strong> with beam size <span class="math inline">\(b\)</span>. Key idea: <strong>keep track of top b hypothesis</strong>. Use beam search: <strong>+1 BLEU</strong></p>
<p><img src="/images/NLP/beam.png"></p>
<h2><span id="kiros-et-al2013">Kiros et al.(2013)</span></h2>
<p><strong>Image caption generation</strong></p>
<ul>
<li>Neural networks are great for working with multiple modalities - <strong>Everything is a vector!</strong></li>
<li>Image caption generation can therefore use the same techniques as translation modeling</li>
<li>A word about data
<ul>
<li>Relatively few captioned images are avaliable</li>
<li>Pre-train image embedding model using another task, like image identification (e.g., ImageNet)</li>
</ul></li>
</ul>
<p>Look a lot like Kalchbrenner and Blunsom(2013)</p>
<ul>
<li>convolutional network on the input</li>
<li>n-gram language model on the output</li>
</ul>
<p>Innovation: <strong>multiplicative interactions</strong> in the decoder n-gram model</p>
<p>Encoder <strong>x</strong> = enbed(<span class="math inline">\(x\)</span>)</p>
<p>Simple conditional n-gram LM: <span class="math display">\[
\begin{array}{lcl}
h_t = W[w_{t-n+1}; w_{t-n+2};...; w_{t-1}] + Cx\\
u_t = Ph_t+b\\
p(W_t|x, w_{t-n+1}^{t-1}) = softmax(u_t)
\end{array}
\]</span> Multiplicative n-gram LM:</p>
<ul>
<li><span class="math inline">\(w_i = r_{i,j,w}x_j\)</span></li>
<li><span class="math inline">\(w_i = u_{w,i}v_{i,j}\ \ \ \ \ \ \ \ (U\in R^{|V|*d}, V \in R^{d*k})\)</span></li>
<li><span class="math inline">\(r_t = W[w_{t-n+1}; w_{t-n+2};...; w_{t-1}] + Cx\)</span></li>
<li><span class="math inline">\(h_t = (W^{fr}r_t)\odot (W^{fx}x)\)</span></li>
<li><span class="math inline">\(u_t = Ph_t + b\)</span></li>
<li><span class="math inline">\(p(W_t|x, w_{&lt;t}) = softmax(u_t)\)</span></li>
</ul>
<p>Two messages:</p>
<ul>
<li>Feed-forward n-gram models can be used in place of RNNs in conditional models</li>
<li>Modeling interactions between input modalities holds a lot of promise
<ul>
<li>Although MLP-type models can approximate higher order tensors, multiplicative models appear to make learning interactions easier</li>
</ul></li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

      
      

      <span class="post-categories">
        <i class="icon-categories"></i>
        <a href="/categories/学习笔记/">学习笔记</a>
      </span>
      

      
      

      <span class="post-tags">
        <i class="icon-tags"></i>
        <a href="/tags/Deep-Learning/">Deep Learning</a><a href="/tags/NLP/">NLP</a><a href="/tags/NMT/">NMT</a><a href="/tags/Deep-NLP/">Deep NLP</a><a href="/tags/Machine-Translation/">Machine Translation</a>
      </span>
      

    </div>

    
  </div>
</article>

<div class="social-share"></div>


	<section id="comment" class="comment">
	  <div id="disqus_thread">
	  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
	  </div>
	</section>

	<script type="text/javascript">
	var disqus_shortname = 'Niuhe';
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	(function(){
	  var dsq = document.createElement('script');
	  dsq.type = 'text/javascript';
	  dsq.async = true;
	  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
	  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	}());
	</script>



      </main>

      <footer class="site-footer">
  <p class="site-info">
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    <br>
    
    &copy;
    2018
    NIUHE <a href="https://github.com/NeymarL" target="_blank"><i class="fab fa-github"></i></a>
    
  </p>
</footer>
      

<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-71540601-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-71540601-1');
</script>

      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      <script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
    </div>
  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>

</html>